<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Optimization for Nondifferentiable Problem</title>
      <link href="/uncategorized/surveys/nondiff_optimization/"/>
      <url>/uncategorized/surveys/nondiff_optimization/</url>
      
        <content type="html"><![CDATA[<p><a href="/uncategorized/surveys/bilevel_optimization">ä¸Šç¯‡</a>è°ƒç ”æ€»ç»“äº†åŒå±‚ä¼˜åŒ–é—®é¢˜çš„è§£å†³èŒƒå¼ï¼Œå¹¶æ¢³ç†äº†åŒå±‚ä¼˜åŒ–åº”ç”¨åœ¨Sim2realçš„domain randomizationï¼ˆDRï¼‰é—®é¢˜ä¸Šæ—¶çš„éš¾ç‚¹åœ¨äºâ€œ<strong>å¦‚ä½•è§£ä¸å¯å¾®åˆ†çš„å¤–å±‚ä¼˜åŒ–å‡½æ•°</strong>â€ï¼Œæœ¬æ–‡å°†å¯¹è¿™ä¸€è®®é¢˜è¿›è¡Œè°ƒç ”ã€‚</p><p>æ­¤å¤–æœ¬æ–‡ä¹Ÿå°†ç®€è¦ä»‹ç»ä¸€ä¸‹å…³äºè¶…å‚ä¼˜åŒ–ï¼ˆhyperparameter optimizationï¼ŒHOï¼‰çš„å†…å®¹ï¼Œè°ƒç ”HOé—®é¢˜çš„åŸå› æ˜¯ï¼šdomain randomizationä¸HOé—®é¢˜å…·æœ‰ç›¸ä¼¼çš„formulationï¼Œä½†äºŒè€…åˆå­˜åœ¨åŒºåˆ«ï¼ˆæœ€é‡è¦çš„åŒºåˆ«åœ¨äºä¸€èˆ¬çš„HOé—®é¢˜æ˜¯differetiableçš„ï¼Œè€ŒDRé—®é¢˜åˆ™ä¸æ˜¯ï¼‰ï¼Œå› æ­¤é˜…è¯»äº†ä¸€äº›æ–‡çŒ®å¹¶åšäº†ç®€è¦æ€»ç»“ã€‚</p><span id="more"></span><h2 id="1-Formulation-of-Domain-Randomization"><a href="#1-Formulation-of-Domain-Randomization" class="headerlink" title="1. Formulation of Domain Randomization"></a>1. Formulation of Domain Randomization</h2><p><a href="uncategorized/surveys/sim2real">å›é¡¾</a>DRçš„formulationå¦‚ä¸‹</p><p>$$\begin{array}{rl}\min & F(\phi, \theta) = \mathcal{L}(\theta^{*};\mathcal{D}_{real}) \\s.t. & \theta^{*} = \arg\min_{\theta}f(\phi, \theta) = \mathbb{E}_{x \sim P_{\phi}(x)}[\mathcal{L}(\theta;\mathcal{D}_{x})]\\var. & \phi, \theta\end{array}$$</p>å¯ä»¥çœ‹åˆ°ï¼Œå¤–å±‚ä¼˜åŒ–é—®é¢˜å¯å¾®ä½† **å†…å±‚ä¼˜åŒ–å‡½æ•°ä¸å¯å¾®** ï¼ŒåŸå› åœ¨äºï¼š<ol><li>é‡‡æ ·è®­ç»ƒæ•°æ® $\mathcal{D}_{x}$ çš„åˆ†å¸ƒï¼Œå…¶å‚æ•°å—æ§äºä¼˜åŒ–å˜é‡$\phi$ ã€‚</li><li>ä¸€èˆ¬æ¥è®²ï¼Œä»ä»¿çœŸå™¨ä¸­ç”Ÿæˆä»¿çœŸæ•°æ®çš„è¿‡ç¨‹ï¼ˆå¦‚ä»å›¾å½¢æ¸²æŸ“å¼•æ“ä¸­ç”Ÿæˆå›¾åƒï¼‰ä¸å•çº¯æ˜¯ä¸€ä¸ªåˆ†å¸ƒé‡‡æ ·é—®é¢˜ï¼Œä¸ºä¿è¯é—®é¢˜çš„ä¸€èˆ¬æ€§ï¼Œè¿™ä¸ªæ•°æ®ç”Ÿæˆè¿‡ç¨‹è¦è¢«è§†ä¸ºä¸å¯å¾®ã€‚</li></ol><blockquote><p>Ruiz, N., Schulter, S., &amp; Chandraker, M. Learning To Simulate. In International Conference on Learning Representations, 2019.</p></blockquote><h2 id="2-Solving-a-Nondifferential-Problem"><a href="#2-Solving-a-Nondifferential-Problem" class="headerlink" title="2. Solving a Nondifferential Problem"></a>2. Solving a Nondifferential Problem</h2><p>ä¸€èˆ¬æ¥è¯´ï¼Œä¸å¯å¾®é—®é¢˜çš„è§£å†³æ–¹æ³•æœ‰æ¬¡ <strong>æ¢¯åº¦æ³•ï¼ˆsubgradientï¼‰</strong>ã€<strong>è¿‘ç«¯æ¢¯åº¦æ³•ï¼ˆProximal Gradientï¼‰</strong>ã€‚</p><h3 id="2-1-Subgradient"><a href="#2-1-Subgradient" class="headerlink" title="2.1 Subgradient"></a>2.1 Subgradient</h3><p>æœ¬èŠ‚çš„ä¸»è¦å‚è€ƒææ–™ä¸º[1]ï¼ŒåŒæ—¶å‚è€ƒäº†åšå®¢<a href="https://blog.csdn.net/zbwgycm/article/details/104507442">link</a></p><blockquote><p>[1] Polyak, B. T. (1978). Subgradient methods: a survey of Soviet research. <em>Nonsmooth optimization</em>, <em>3</em>, 5-29.</p></blockquote><p><strong>æ¬¡æ¢¯åº¦æ³•é€‚ç”¨äºæ‰€æœ‰ä¸å¯å¾®çš„ç›®æ ‡å‡½æ•°ï¼Œä½†å…¶æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢</strong>ã€‚</p><h4 id="2-1-1-Definition"><a href="#2-1-1-Definition" class="headerlink" title="2.1.1 Definition"></a>2.1.1 Definition</h4><p>å¯¹äºå®šä¹‰åœ¨ $\mathbb{R}^n$ çš„è¿ç»­å‡¸å‡½æ•° $f(x)$ ï¼Œå½“å‘é‡ $\partial f(x) \in \mathbb{R}^n $ æ»¡è¶³ä»¥ä¸‹æ¡ä»¶æ—¶ï¼Œå¯è¢«ç§°ä¸ºå‡½æ•° $f(x)$ åœ¨ $x$ ç‚¹çš„æ¬¡æ¢¯åº¦</p><p>$$f(x+y) \geq f(x)+(\partial f(x), y), \forall y \in R^{n}$$</p>æ³¨ï¼šè¿™é‡Œçš„ $(\cdot,\cdot)$ åº”è¯¥æ˜¯å‘é‡ç§¯çš„æ„æ€ï¼Œ[å‚è€ƒ](https://blog.csdn.net/zbwgycm/article/details/104507442) ä¸­ $f(y) \geq f(x)+g^{T}(y-x)$ã€‚å¯ä»¥çœ‹å‡ºï¼Œç›¸æ¯”ä¸æ¢¯åº¦çš„å®šä¹‰ï¼ˆç­‰å·ï¼‰ï¼Œä¸Šè¿°æ¡ä»¶æ›´ä¸ºæ¾å¼›ã€‚ä¸€äº›æ¬¡æ¢¯åº¦çš„æ€§è´¨å¦‚ä¸‹[link](https://blog.csdn.net/zbwgycm/article/details/104507442)<ul><li>æ¬¡æ¢¯åº¦æ€»æ˜¯å‡ºç°åœ¨å®šä¹‰åŸŸ $dom(f)$ çš„å†…éƒ¨ã€‚</li><li>å¯¹äºæ‰€æœ‰ $x \in \mathbb{R}^n$ ï¼Œæ¬¡æ¢¯åº¦éƒ½å­˜åœ¨ï¼ˆä½†å¯èƒ½ä¸æ˜¯å”¯ä¸€çš„ï¼‰ã€‚å¦‚æœ $f(x)$ æ˜¯å¯å¾®çš„ï¼Œåˆ™æ¬¡æ¢¯åº¦æ˜¯å”¯ä¸€çš„ï¼Œå¹¶ä¸”ä¸æ¢¯åº¦ã€‚å„ç§ç±»å‹å‡½æ•°çš„æ¬¡æ¢¯åº¦è®¡ç®—è§„åˆ™æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„ã€‚</li><li>æ¬¡æ¢¯åº¦çš„å®šä¹‰å¯ä»¥æ¨å¹¿åˆ°éå‡¸å‡½æ•°ä¸­ï¼Œä½† <strong>éå‡¸å‡½æ•°çš„æ¬¡æ¢¯åº¦å¯èƒ½ä¸å­˜åœ¨</strong> ã€‚</li></ul><h4 id="2-1-2-Gradient-Decent"><a href="#2-1-2-Gradient-Decent" class="headerlink" title="2.1.2 Gradient Decent"></a>2.1.2 Gradient Decent</h4><p>ç±»æ¯”äºæ¢¯åº¦ä¸‹é™æ³•ï¼Œæ¬¡æ¢¯åº¦æ³•åªæ˜¯å°†å…¶ä¸­çš„æ¢¯åº¦æ›¿æ¢ä¸ºæ¬¡æ¢¯åº¦ï¼Œå…¶ä»–æ­¥éª¤ä¸å˜ï¼Œå…¶æ›´æ–°å¦‚ä¸‹ï¼š</p><p>$$x_{k+1} = x_{k} - \gamma_{k} \frac{\partial f(x_k)}{\| \partial f(x_k) \|}$$</p>å…¶ä¸­ $\gamma_k$ ä¸º step sizeï¼Œä¸Šå¼çš„ç®€æ´å†™æ³•ä¸º $x_{k+1} = x_k - \gamma_k \cdot g_{k-1}$ã€‚æ¢¯åº¦æ³•å’Œæ¬¡æ¢¯åº¦æ³•ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œä¸€èˆ¬æ¥è¯´ï¼Œæ–¹å‘ $ \partial f(x_k)$ ä¸æ˜¯ $x_k$ å¤„çš„ä¸‹é™æ–¹å‘**ï¼ˆå¹¶éä¸¥æ ¼ä¸‹é™ï¼‰** ï¼Œå³ä¸å¯å¾®å‡½æ•°çš„ $f(x_k)$ çš„å€¼ **ä¸ä¼šå•è°ƒå‡å°ã€‚** <h4 id="2-1-3-Convergence"><a href="#2-1-3-Convergence" class="headerlink" title="2.1.3 Convergence"></a>2.1.3 Convergence</h4><p>convergence rate $O(1/\epsilon^2)$ï¼Œæ…¢äºæ¢¯åº¦ä¸‹é™çš„ $O(1/\epsilon)$ ã€‚</p><h3 id="2-2-Proximal-Gradient"><a href="#2-2-Proximal-Gradient" class="headerlink" title="2.2 Proximal Gradient"></a>2.2 Proximal Gradient</h3><p>æœ¬èŠ‚çš„ä¸»è¦å‚è€ƒææ–™ä¸ºä¸‹æ–‡ï¼Œä»¥åŠåšå®¢ <a href="https://blog.csdn.net/zbwgycm/article/details/83060251">link</a></p><blockquote><p>TODO </p></blockquote><p>è¿‘ç«¯æ¢¯åº¦æ³•é€‚ç”¨äºâ€œæ•´ä½“ä¼˜åŒ–ç›®æ ‡ä¸å¯å¾®åˆ†ï¼Œä½†å¯ä»¥<strong>åˆ†è§£ä¸ºéƒ¨åˆ†å¯å¾®ã€éƒ¨åˆ†ä¸å¯å¾®</strong>â€çš„ç›®æ ‡å‡½æ•°ã€‚</p><h4 id="2-2-1-Definition"><a href="#2-2-1-Definition" class="headerlink" title="2.2.1 Definition"></a>2.2.1 Definition</h4><p>TODO</p><h2 id="3-Hyperparameter-Optimization"><a href="#3-Hyperparameter-Optimization" class="headerlink" title="3. Hyperparameter Optimization"></a>3. Hyperparameter Optimization</h2><p>æœ¬èŠ‚å†…å®¹å‚è€ƒ</p><blockquote><p>Bao, F., Wu, G., Li, C., Zhu, J., &amp; Zhang, B. (2021). Stability and generalization of bilevel programming in hyperparameter optimization. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 4529-4541.</p></blockquote><p>HO é—®é¢˜çš„formulationå¯ä»¥å†™ä¸º</p><p>$$\begin{array}{rl}\hat{\lambda}\left(S^{t r}, S^{v a l}\right) & \approx \underset{\lambda \in \Lambda}{\arg \min } \hat{R}^{v a l}\left(\lambda, \hat{\theta}\left(\lambda, S^{t r}\right), S^{v a l}\right) \\\text{where} \quad \hat{\theta}\left(\lambda, S^{t r}\right) & \approx \underset{\theta \in \Theta}{\arg \min } \hat{R}^{t r}\left(\lambda, \theta, S^{t r}\right)\end{array}$$</p>ä¸Šå¼é‡‡ç”¨çº¦ç­‰äºç¬¦å·ï¼Œæ˜¯å› ä¸ºï¼šåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ˆä¾‹å¦‚ï¼Œç¥ç»ç½‘ç»œå†…å±‚ä¼˜åŒ–å˜é‡ï¼‰ï¼Œä¸Šå¼ä¸­çš„å†…éƒ¨å’Œå¤–éƒ¨é—®é¢˜çš„**å…¨å±€æœ€ä¼˜å€¼æ˜¯éš¾ä»¥å®ç°**çš„ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œä»¥æŸç§æ–¹å¼ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ï¼ˆéšæœºï¼‰æ¢¯åº¦ä¸‹é™ï¼‰å¯¹å…¶è¿›è¡Œè¿‘ä¼¼ã€‚<p>ğŸŒŸæ³¨æ„ï¼šHOé—®é¢˜ä¸å­˜åœ¨DRé—®é¢˜ä¸­æ‰€æåˆ°çš„ä¸¤ä¸ªé˜¶æ®µï¼Œå› æ­¤<strong>å¤§å¤šæ•°å¯ä»¥è®¤ä¸ºæ˜¯å¯å¾®</strong>çš„ã€‚ä¸”å¤§å¤šæ•°æ˜¯å¸¦çº¦æŸçš„ä¼˜åŒ–ï¼ˆå¦‚æ•´æ•°çº¦æŸç­‰ï¼‰ã€‚</p><p><strong>HOçš„è§£å†³æ–¹æ³•ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ç§ï¼š</strong></p><ul><li>Unrolled differentiationï¼šåœ¨å†…å¤–å±‚ä¸Šæ‰§è¡Œæœ‰é™æ­¥æ•°çš„æ¢¯åº¦ä¸‹é™ã€‚æ³¨æ„ï¼š<strong>è¿™é‡Œçš„ $\theta$ ç›¸å¯¹äº $\lambda$ æ˜¯å¯å¾®çš„</strong>ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™ã€‚å…·ä½“çš„åˆ†æï¼šå°†å†…å±‚å˜é‡ï¼ˆç¥ç»ç½‘ç»œå‚æ•°ï¼‰è§†ä¸ºä¸€ä¸ªè¶³å¤Ÿå¤§çš„çŸ©é˜µï¼Œåˆ™å¤–å±‚å˜é‡ï¼ˆè¶…å‚ï¼‰çš„å˜åŒ–å¯¼è‡´çš„å†…å±‚å˜é‡å˜åŒ–ï¼ˆå¦‚ç¥ç»ç½‘ç»œå±‚æ•°å˜åŒ–ï¼‰å¯ä»¥è¢«è§†ä¸ºçŸ©é˜µä¸maskçš„ä¹˜ç§¯ã€‚å› æ­¤äºŒè€…å­˜åœ¨å‡½æ•°å…³ç³» $\theta = g(\lambda)$ï¼Œè¿™ç§å‡½æ•°å…³ç³»æ˜¯å¯å¾®çš„ã€‚</li><li>Cross-validationï¼šCVæ˜¯HOçš„ç»å…¸æ–¹æ³•ã€‚å®ƒé¦–å…ˆé€šè¿‡ ç½‘æ ¼æœç´¢ æˆ– éšæœºæœç´¢ è·å¾—ä¸€ç»„æœ‰é™çš„è¶…å‚æ•°ï¼Œé€šå¸¸æ˜¯ $\Lambda$ çš„å­é›†ã€‚ç„¶åï¼Œå®ƒè®­ç»ƒå†…å±‚é—®é¢˜ï¼Œä»¥è·å¾—ç»™å®šè¶…å‚æ•°çš„ç›¸åº”å‚æ•° $\theta$ ã€‚æœ€åï¼Œæ ¹æ®<strong>éªŒè¯è¯¯å·®</strong>é€‰æ‹©æœ€ä½³ $(\theta^{\star}, \lambda^{\star})$ å¯¹ã€‚</li><li>Implicit gradientï¼šéšå¼æ¢¯åº¦æ–¹æ³•ç›´æ¥ä¼°è®¡å¤–å±‚é—®é¢˜çš„æ¢¯åº¦ï¼Œè¿™é€šå¸¸æ¶‰åŠä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œå¦‚å…±è½­æ¢¯åº¦ã€Neumannè¿‘ä¼¼ï¼Œä»¥ä¼°è®¡HessiançŸ©é˜µçš„é€†ã€‚</li><li>Bayesian optimizationï¼šè´å¶æ–¯ä¼˜åŒ–å°†å¤–å±‚é—®é¢˜è§†ä¸ºä»é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰é‡‡æ ·çš„<strong>é»‘ç®±å‡½æ•°</strong>ï¼Œå¹¶åœ¨è¯„ä¼°æ–°çš„è¶…å‚æ•°æ—¶æ›´æ–°GPçš„åéªŒã€‚</li><li>Hypernetworksï¼šå­¦ä¹ åœ¨ç»™å®šè¶…å‚æ•°çš„æƒ…å†µä¸‹è¾“å‡ºè¿‘ä¼¼æœ€ä¼˜å‡è®¾çš„ä»£ç†å‡½æ•°ã€‚</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> survey </tag>
            
            <tag> optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paradigm of Bi-level Optimization</title>
      <link href="/uncategorized/surveys/bilevel_optimization/"/>
      <url>/uncategorized/surveys/bilevel_optimization/</url>
      
        <content type="html"><![CDATA[<p>æœ¬æ–‡è°ƒç ”äº†åŒå±‚ä¼˜åŒ–é—®é¢˜çš„è§£å†³èŒƒå¼ï¼Œå¯¹ä¸¤ç§popularçš„<strong>åŸºäºæ¢¯åº¦</strong>çš„æ–¹æ³•â€”â€”AIDã€ITDä½œå‡ºä»‹ç»ä¸æ€»ç»“ã€‚æœ€åï¼Œæœ¬æ–‡å¯¹å¦‚ä½•åˆ©ç”¨bilevel optimizationè§£å†³sim2realé—®é¢˜éš¾ç‚¹ä½œå‡ºæ¢³ç†ã€‚</p><p>æœ¬è°ƒç ”çš„motivationæ¥è‡ªäº<a href="//uncategorized/surveys/sim2real/">sim2real</a>é—®é¢˜â€”â€”å€ŸåŠ©å°‘é‡çš„ä»¿çœŸæ•°æ®ï¼Œå¦‚ä½•æ‰¾åˆ°ä¸€ç»„ä»¿çœŸå™¨å‚æ•°$\phi$ï¼Œåœ¨è¯¥ç»„å‚æ•°ä¸‹çš„ä»¿çœŸæ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªå¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼Œå°†ä¼šåœ¨ç°å®æ•°æ®ä¸­æœ‰è¾ƒå¥½çš„æ•ˆæœâ€”â€”è¿™å¯ä»¥ç±»æ¯”äºä¸€ä¸ªè¶…å‚æ•°è°ƒä¼˜é—®é¢˜ã€‚</p><span id="more"></span><h2 id="1-Formulation-of-Bilevel-Optimization"><a href="#1-Formulation-of-Bilevel-Optimization" class="headerlink" title="1. Formulation of Bilevel Optimization"></a>1. Formulation of Bilevel Optimization</h2><p>åŒå±‚ä¼˜åŒ–é—®é¢˜çš„å½¢å¼ä¸€èˆ¬å®šä¹‰ä¸º</p><p>$$\begin{array}{rl}\min & \Phi(\boldsymbol{x}):=f(\boldsymbol{x}, \boldsymbol{y}^{\star}) \\\text { s.t. } & \boldsymbol{y}^{\star}=\underset{\boldsymbol{y}}{\arg \min }  g\left(\boldsymbol{x}, \boldsymbol{y}\right) \\\text { var. } & \boldsymbol{x}, \boldsymbol{y},\end{array}$$</p>åœ¨åŒå±‚ä¼˜åŒ–ä¸­ï¼Œä¸€ä¸ªä¼˜åŒ–é—®é¢˜åµŒå…¥æˆ–åµŒå¥—åœ¨å¦ä¸€ä¸ªé—®é¢˜ä¸­ã€‚å¤–éƒ¨ä¼˜åŒ–é—®é¢˜è¢«ç§°ä¸ºä¸Šå±‚ä¼˜åŒ–é—®é¢˜ï¼Œè€Œå†…éƒ¨ä¼˜åŒ–é—®é¢˜è¢«ç§°ä½œä¸‹å±‚ä¼˜åŒ–é—®é¢˜ï¼Œ$\boldsymbol{x}, \boldsymbol{y}$ åˆ†åˆ«ä¸ºä¸Šä¸‹å±‚çš„ä¼˜åŒ–å˜é‡ã€‚<p>åŒå±‚ä¼˜åŒ–é—®é¢˜çš„è§£å†³èŒƒå¼åŒ…æ‹¬ä»¥ä¸‹ä¸¤ç±»ï¼š</p><ol><li><p>Double Loop / gradient-basedï¼šå†…å¤–å±‚é—®é¢˜åˆ†åˆ«æ ¹æ®å…¶é—®é¢˜ç‰¹æ€§ï¼ˆå¯å¾®ã€å¹³æ»‘ã€å‡¸ç­‰ï¼‰ï¼Œé‡‡ç”¨ä¸åŒçš„ä¼˜åŒ–æ–¹æ³•ï¼Œä¸¤ä¸ªé—®é¢˜çš„è§£é€šè¿‡<strong>è¶…æ¢¯åº¦</strong>ä¸²è”èµ·æ¥ï¼Œä»¥å¯»æ‰¾ä¸€ä¸ªå…¨å±€çš„ï¼ˆæ¸è¿›ï¼‰æœ€ä¼˜è§£ã€‚è¿™ç±»æ–¹æ³•å¯ä»¥æ¦‚æ‹¬ä¸ºâ€œå¯å‘å¼â€â€”â€”æœ‰æ–¹å‘çš„è¿­ä»£ï¼Œä¸€èˆ¬æ¥è¯´ç»™ä¸€ä¸ªåˆå§‹å¯è¡Œè§£ç„¶åæŒ‰ç…§å®é™…é—®é¢˜ç¡®å®šä¸€ä¸ªä¸‹é™æ–¹å‘ï¼Œä¸æ–­æœç´¢ç›´åˆ°gapæ»¡è¶³ç²¾åº¦è¦æ±‚ï¼Œä½†è¿™é‡Œå¿…é¡»æ³¨æ„ï¼Œå¯å‘å¼ç®—æ³•å¾ˆæœ‰å¯èƒ½å¾—åˆ°çš„ä¸æ˜¯æœ€ä¼˜è§£ï¼Œä¸€å®šè¦å¯¹ç»“æœè¿›è¡Œè®ºè¯ã€‚</p><ul><li>æ­¤ç±»èŒƒå¼çš„æ–¹æ³•åŒ…æ‹¬ï¼šapproximate implicit differentiation (AID)ã€iterative differentiation (ITD)ï¼Œè¿™ä¸¤ç±»æ–¹æ³•å‡ä¸ºåŸºäºæ¢¯åº¦çš„æ–¹æ³•ã€‚</li></ul></li><li><p>Single Loop / constrain-basedï¼šåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œä¸‹å±‚ä¼˜åŒ–é—®é¢˜å¯ä»¥è§†ä¸ºæ˜¯ä¸Šå±‚ä¼˜åŒ–é—®é¢˜çš„çº¦æŸï¼Œå› æ­¤åŒå±‚ä¼˜åŒ–ä¹Ÿå¯ä»¥è§†ä¸º<strong>çº¦æŸä¼˜åŒ–</strong>çš„ç‰¹æ®Šæƒ…å†µã€‚è¿™ç±»æ–¹æ³•å¯ä»¥æ¦‚æ‹¬ä¸ºâ€œè§£æå¼â€â€”â€”æ¥ç®—å‡ºè§£æèŠ‚ï¼Œè¿™ç§æ–¹æ³•çš„é€»è¾‘å¤§éƒ½ä½¿ç”¨KKTï¼Œå¯¹å¶ï¼Œç½šå‡½æ•°ç­‰å°†åŒå±‚è§„åˆ’è½¬åŒ–æˆå•å±‚ï¼Œç„¶ååˆ©ç”¨å•å±‚çš„æ–¹æ³•æ±‚è§£ã€‚</p><ul><li>è§£å†³å¸¦çº¦æŸä¼˜åŒ–çš„æ–¹æ³•åŒ…æ‹¬ä½†ä¸é™äºï¼šKarushâ€“Kuhnâ€“Tuckerï¼ˆKKTï¼ŒKKTæ¡ä»¶è¡¨ç°ä¸ºæ‹‰æ ¼æœ—æ—¥å’Œäº’è¡¥çº¦æŸï¼Œå¹¶å°†æ•´ä½“åŒå±‚ä¼˜åŒ–é—®é¢˜ç®€åŒ–ä¸ºå•çº§çº¦æŸä¼˜åŒ–é—®é¢˜ï¼‰ï¼ŒPenalty Function Methodsç­‰[1]ã€‚</li></ul></li></ol><blockquote><p>[1] Sinha, A., Malo, P., &amp; Deb, K. (2017). A review on bilevel optimization: From classical to evolutionary approaches and applications. <em>IEEE Transactions on Evolutionary Computation</em>, <em>22</em>(2), 276-295.</p><p>[2] <a href="https://www.zhihu.com/question/25059001?sort=created">https://www.zhihu.com/question/25059001?sort=created</a></p></blockquote><h2 id="2-Gradient-based-iterative-methods"><a href="#2-Gradient-based-iterative-methods" class="headerlink" title="2. Gradient-based iterative methods"></a>2. Gradient-based iterative methods</h2><p>Single Loop / constrain-based æ–¹æ³•å¾€å¾€æ¶‰åŠå¤§é‡çº¦æŸå› æ­¤éš¾ä»¥åœ¨æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­åº”ç”¨ã€‚åœ¨ä¸‹æ–‡ä¸­ï¼Œå°†å¯¹ä¸¤ç§ Double Loop / gradient-based çš„æ–¹æ³•â€”â€”AID / ITDä½œå‡ºæ€»ç»“ã€‚ä¸»è¦çš„formulationå‚è€ƒä¸‹æ–‡ï¼š</p><blockquote><p>Ji, K., Yang, J., &amp; Liang, Y. (2021, July). Bilevel optimization: Convergence analysis and enhanced design. In <em>International conference on machine learning</em> (pp. 4882-4892). PMLR.</p></blockquote><p>è¿™ä¸¤ç§ç®—æ³•å‡æ¶‰åŠåˆ°ä¸€ä¸ªæ¦‚å¿µï¼š<strong>hyper-gradientï¼ˆè¶…æ¢¯åº¦ï¼‰</strong> <a href="https://github.com/gbaydin/hypergradient-descent">link</a></p><ul><li>æ¢¯åº¦ï¼ˆgradientï¼‰ï¼šåœ¨ä¸€ä¸ªåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–é—®é¢˜ä¸­ï¼Œä¼˜åŒ–ç›®æ ‡å¯¹<strong>æ¨¡å‹å‚æ•°</strong>çš„æ¢¯åº¦ï¼Œè¢«ç§°ä¸º basic gridientï¼Œç®€ç§°ä¸ºæ¢¯åº¦</li><li>è¶…æ¢¯åº¦ï¼ˆhyper-gridientï¼‰ï¼šåœ¨ä¸Šè¿°çš„ä¼˜åŒ–é—®é¢˜ä¸­ï¼Œå°†ä¼˜åŒ–ç›®æ ‡å¯¹<strong>ä¼˜åŒ–è¿‡ç¨‹çš„è¶…å‚æ•°</strong>ï¼ˆå¦‚ learning rate, momentum, regularization parameters, etc.ï¼‰æ±‚æ¢¯åº¦ï¼Œåˆ™ç§°ä¸ºè¶…æ¢¯åº¦ã€‚</li></ul><p>åœ¨åŒå±‚ä¼˜åŒ–ä¸­ï¼Œå¾…ä¼˜åŒ–çš„å‚æ•°å¾€å¾€æ˜¯å†…å±‚ä¼˜åŒ–é—®é¢˜çš„å˜é‡ï¼ˆå¦‚æ¨¡å‹å‚æ•°ï¼‰ï¼Œè€Œå¤–å±‚ä¼˜åŒ–é—®é¢˜çš„å˜é‡ä¸€èˆ¬ä¸ºç®—æ³•è¶…å‚æ•°ï¼ˆå¦‚æ¨¡å‹è¶…å‚ï¼‰ï¼Œå› æ­¤è¿™é‡Œ<strong>ç§°ï¼šå¤–å±‚ä¼˜åŒ–ç›®æ ‡å¯¹å¤–å±‚å˜é‡çš„æ¢¯åº¦â€”â€”è¶…æ¢¯åº¦</strong>ã€‚</p><p>ğŸŒŸ AID å’Œ ITD çš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œä»–ä»¬è®¡ç®—è¶…æ¢¯åº¦çš„æ–¹æ³•ä¸åŒï¼Œå› æ­¤ä½¿ç”¨äº†ä¸åŒçš„è¿‘ä¼¼æ–¹æ³•</p><h3 id="2-1-Approximate-Implicit-Differentiation-AID"><a href="#2-1-Approximate-Implicit-Differentiation-AID" class="headerlink" title="2.1 Approximate Implicit Differentiation (AID)"></a>2.1 Approximate Implicit Differentiation (AID)</h3><p>AIDæ–¹æ³•çš„æ ¸å¿ƒæ˜¯ï¼šç”¨ä¸€ä¸ªè¿‘ä¼¼å€¼æ¥ä»£æ›¿éšå¾®åˆ†ï¼ˆimplicit differentitationï¼‰ï¼Œä»¥æ„é€ è¶…æ¢¯åº¦ã€‚AIDçš„è¶…æ¢¯åº¦è®¡ç®—å…¬å¼ä¸ºï¼š</p><p>$$\nabla \Phi\left(x_k\right)=\nabla_x f\left(x_k, y^{*}\left(x_k\right)\right)-\nabla_x \nabla_y g\left(x_k, y^{*}\left(x_k\right)\right) v_k^{*}$$</p>å…¶ä¸­ $v_k^{*}$ æ˜¯ä¸‹é¢çš„çº¿æ€§ç³»ç»Ÿ $\nabla_y^2 g\left(x_k, y^{*}\left(x_k\right)\right) v=\nabla_y f\left(x_k, y^{*}\left(x_k\right)\right)$ çš„è§£ã€‚ä»¥ä¸Šæ˜¯è¶…æ¢¯åº¦çš„åŸå§‹è®¡ç®—å…¬å¼ã€‚<p><strong>AIDçš„æ±‚è§£æ€è·¯å¦‚ä¸‹ï¼š</strong></p><ol><li>åˆ©ç”¨ $N-step$ çš„ conjugate-gradient (CG) æ–¹æ³•æ¥æ±‚è§£çº¿æ€§ç³»ç»Ÿçš„è§£ $v_k^N$ã€‚</li><li>å°†ä¸Šè¿°è§£ $v_k^N$ å’Œå†…å±‚çš„æœ€è¿‘ä¸€æ¬¡çš„è¿­ä»£è§£ $y_k^T$ å¸¦å…¥ä¸Šè¿°çš„è¶…æ¢¯åº¦å…¬å¼ï¼Œå¾—åˆ°è¶…æ¢¯åº¦çš„è¿‘ä¼¼å€¼ï¼Œå³ï¼š</li></ol><p>$$\nabla \Phi\left(x_k\right)=\nabla_x f\left(x_k, y_k^T\right)-\nabla_x \nabla_y g\left(x_k, y_k^T\right) v_k^N$$</p><p>âš ï¸ Noteï¼šæ³¨æ„â€”â€”ä¸Šå¼ä¸­çš„æœ€åä¸€é¡¹ Jacobian-vector product å¯ä»¥é€šè¿‡è‡ªåŠ¨å¾®åˆ†æ±‚å‡ºï¼Œå› æ­¤ä¸Šå¼å¯ä»¥è¢«è®¡ç®—ã€‚</p><h3 id="2-2-Iterative-Differentiation-ITD"><a href="#2-2-Iterative-Differentiation-ITD" class="headerlink" title="2.2 Iterative Differentiation (ITD)"></a>2.2 Iterative Differentiation (ITD)</h3><p>ITDæ–¹æ³•çš„æ ¸å¿ƒæ˜¯ï¼šè¶…æ¢¯åº¦çš„è¿‘ä¼¼å€¼å…·æœ‰ä¸€ä¸ªè§£æå½¢å¼ï¼Œè¯¥è§£æå½¢å¼å¯ä»¥é€šè¿‡è¿­ä»£çš„æ–¹å¼æ±‚å‡ºã€‚ITDçš„è¶…æ¢¯åº¦è®¡ç®—å…¬å¼ä¸ºï¼š</p><p>$$\nabla \Phi\left(x_k\right)=\frac{\partial f\left(x_k, y^{*}\left(x_k\right)\right)}{\partial x_k}$$</p>å…¶è¿‘ä¼¼ä¼°è®¡ä¸º$\frac{\partial f\left(x_k, y_k^D\left(x_k\right)\right)}{\partial x_k}$ï¼Œè€Œè¯¥æ¢¯åº¦å­˜åœ¨ä¸‹è¿°çš„è§£æå½¢å¼ï¼Œå› æ­¤å¯ä»¥è¢«è®¡ç®—ï¼š<p>$$\begin{aligned}\frac{\partial f\left(x_{k}, y_{k}^{D}\right)}{\partial x_{k}}= & \nabla_{x} f\left(x_{k}, y_{k}^{D}\right)-\alpha \sum_{t=0}^{D-1} \nabla_{x} \nabla_{y} g\left(x_{k}, y_{k}^{t}\right) \\& \times \prod_{j=t+1}^{D-1}\left(I-\alpha \nabla_{y}^{2} g\left(x_{k}, y_{k}^{j}\right)\right) \nabla_{y} f\left(x_{k}, y_{k}^{D}\right)\end{aligned}$$</p><h3 id="2-3-Bilevel-algorithms-via-AID-or-ITD"><a href="#2-3-Bilevel-algorithms-via-AID-or-ITD" class="headerlink" title="2.3 Bilevel algorithms via AID or ITD"></a>2.3 Bilevel algorithms via AID or ITD</h3><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20230216213116561.png" alt="image-20230216213116561" style="zoom:50%;" /><h2 id="3-åˆ©ç”¨åŒå±‚ä¼˜åŒ–è§£sim2realé—®é¢˜çš„éš¾ç‚¹"><a href="#3-åˆ©ç”¨åŒå±‚ä¼˜åŒ–è§£sim2realé—®é¢˜çš„éš¾ç‚¹" class="headerlink" title="3. åˆ©ç”¨åŒå±‚ä¼˜åŒ–è§£sim2realé—®é¢˜çš„éš¾ç‚¹"></a>3. åˆ©ç”¨åŒå±‚ä¼˜åŒ–è§£sim2realé—®é¢˜çš„éš¾ç‚¹</h2><p>ç®€è¦æ¢³ç†ï¼šAID ITDåˆ†åˆ«æ˜¯ä¸¤ç§å€ŸåŠ©è¶…æ¢¯åº¦è¿›è¡Œå†…å¤–å±‚ä¼˜åŒ–ä¸²è”çš„æ–¹æ³•ï¼Œä½†åœ¨å®é™…è®¡ç®—çš„è¿‡ç¨‹ä¸­å¯¹<strong>å†…å±‚ä¼˜åŒ–é—®é¢˜</strong>ä»æœ‰è¯¸å¤šé™åˆ¶ï¼Œå¦‚ï¼šsmoothness, twice differentiability and an invertible Hessianã€‚å¯¹æ¯”domain randomizationå¯ä»¥å‘ç°ï¼ŒDRé—®é¢˜çš„å†…å±‚ä¼˜åŒ–é—®é¢˜ä¾ç„¶æ˜¯ä¸å¯å¾®çš„ã€‚</p><p>å› æ­¤ï¼Œåˆ©ç”¨åŒå±‚ä¼˜åŒ–é—®é¢˜è§£DRé—®é¢˜çš„éš¾ç‚¹åœ¨äºâ€”â€”<strong>å¦‚ä½•è§£ä¸å¯å¾®ä¼˜åŒ–é—®é¢˜</strong>ã€‚è¿™ä¹Ÿæ˜¯<a href="/uncategorized/surveys/nondiff_optimization">ä¸‹ä¸€ç¯‡</a>surveyçš„è°ƒç ”é‡ç‚¹ã€‚</p><blockquote><p>Ruiz, N., Schulter, S., &amp; Chandraker, M. Learning To Simulate. In International Conference on Learning Representations, 2019.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> survey </tag>
            
            <tag> optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NS3æ— çº¿ç½‘ç»œä»¿çœŸå™¨</title>
      <link href="/uncategorized/notes/NS3/"/>
      <url>/uncategorized/notes/NS3/</url>
      
        <content type="html"><![CDATA[<p>æœ¬æ–‡æ•´ç†äº†åœ¨ç”¨NS3åšæ— çº¿ç½‘ç»œä»¿çœŸçš„æ—¶å€™ï¼Œéœ€è¦æŒæ¡çš„ä¸€äº›åŸºç¡€çŸ¥è¯†</p><span id="more"></span><h2 id="1-å…³é”®åè¯"><a href="#1-å…³é”®åè¯" class="headerlink" title="1. å…³é”®åè¯"></a>1. å…³é”®åè¯</h2><ul><li>Nodeï¼šåº”è¯¥å°† <code>Node</code>è§†ä¸ºæ‚¨å°†å‘å…¶æ·»åŠ applicationçš„è®¡ç®—æœºã€‚ä¸€ä¸ªæ˜¯æ·»åŠ è¯¸å¦‚åº”ç”¨ç¨‹åºã€åè®®æ ˆå’Œå¤–å›´å¡åŠå…¶ç›¸å…³é©±åŠ¨ç¨‹åºä¹‹ç±»çš„ä¸œè¥¿ï¼Œä»¥ä½¿è®¡ç®—æœºèƒ½å¤Ÿæ‰§è¡Œæœ‰ç”¨çš„å·¥ä½œã€‚<em>æˆ‘ä»¬åœ¨ns-3</em>ä¸­ä½¿ç”¨ç›¸åŒçš„åŸºæœ¬æ¨¡å‹ã€‚</li><li>Applicationï¼š<em>ns-3</em> <code>application</code>åœ¨ <em>ns-3</em> <code>Nodes</code>ä¸Šè¿è¡Œï¼Œä»¥é©±åŠ¨æ¨¡æ‹Ÿä¸–ç•Œä¸­çš„æ¨¡æ‹Ÿã€‚</li><li>Channelï¼šåœ¨<em>ns-3</em>çš„æ¨¡æ‹Ÿä¸–ç•Œä¸­ï¼Œå°†<code>Node</code>è¿æ¥åˆ°è¡¨ç¤ºé€šä¿¡ä¿¡é“çš„å¯¹è±¡ã€‚åœ¨è¿™é‡Œï¼ŒåŸºæœ¬çš„é€šä¿¡å­ç½‘ç»œæŠ½è±¡è¢«ç§°ä¸º<code>Channel</code>ï¼Œå¹¶åœ¨C++ä¸­ç”±Channelç±»è¡¨ç¤ºã€‚Channelç±»æä¾›äº†ç®¡ç†é€šä¿¡å­ç½‘ç»œå¯¹è±¡å’Œå°†èŠ‚ç‚¹è¿æ¥åˆ°è¿™äº›å¯¹è±¡çš„æ–¹æ³•ã€‚channelçš„å®ä½“å¯ä»¥å»ºæ¨¡åƒå¯¼çº¿è¿™æ ·ç®€å•çš„ä¸œè¥¿ï¼Œè¿˜å¯ä»¥æ¨¡æ‹Ÿåƒå¤§å‹ä»¥å¤ªç½‘äº¤æ¢æœºæˆ–æ— çº¿ç½‘ç»œä¸­å……æ»¡éšœç¢ç‰©çš„ä¸‰ç»´ç©ºé—´è¿™æ ·å¤æ‚çš„äº‹ç‰©ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åä¸º<code>CsmaChannel</code>ã€<code>PointToPointChannel</code>å’Œ<code>WifiChannel</code>çš„Channelçš„å®ä½“ã€‚<ul><li><code>CsmaChannel</code>ä¸ºå®ç°è½½æ³¢ä¾¦å¬å¤šå€é€šä¿¡ä»‹è´¨çš„é€šä¿¡å­ç½‘ç»œçš„ä¸€ä¸ªç‰ˆæœ¬å»ºæ¨¡ã€‚è¿™ä¸ºæˆ‘ä»¬æä¾›äº†ç±»ä¼¼ä»¥å¤ªç½‘çš„åŠŸèƒ½ã€‚</li></ul></li><li>Net Deviceï¼š<em>ns-3</em>ä¸­çš„<code>net device</code>æŠ½è±¡ç±»åŒ…å«äº†è½¯ä»¶é©±åŠ¨ä¸æ¨¡æ‹Ÿçš„ç¡¬ä»¶è®¾å¤‡ï¼Œä¸€ä¸ª<code>net device</code>è¢«â€œå®‰è£…â€åœ¨<code>Node</code>ä¸­ï¼Œç”¨äºä½¿èƒ½<code>Node</code>ä¸å…¶ä»–<code>Node</code>ä¹‹é—´ç»ç”±<code>Channel</code>çš„é€šä¿¡ã€‚æ­£å¦‚ä¸€ä¸ªçœŸå®çš„è®¡ç®—æœºï¼Œä¸€ä¸ª<code>Node</code>å¯ä»¥é€šè¿‡å¤šä¸ª<code>net device</code>é“¾æ¥å¤šä¸ª<code>channel</code>ã€‚</li><li>Topology Helpersï¼šåœ¨<em>ns-3</em>ä¸­ï¼Œè¿æ¥<code>Net Devices</code>åˆ°<code>Node</code>ï¼Œè¿æ¥<code>NetDevice</code>åˆ°<code>channel</code>ï¼Œåˆ†é…IPåœ°å€ç­‰éƒ½æ˜¯å¸¸è§çš„ä»»åŠ¡ï¼Œå› æ­¤æˆ‘ä»¬æä¾›äº†æˆ‘ä»¬ç§°ä¹‹ä¸ºTopology Helpersçš„åŠŸèƒ½ï¼Œä»¥å°½å¯èƒ½ç®€åŒ–è¿™ä¸€è¿‡ç¨‹ã€‚<ul><li>åˆ›å»ºç½‘ç»œè®¾å¤‡ã€æ·»åŠ MACåœ°å€ã€åœ¨èŠ‚ç‚¹ä¸Šå®‰è£…è¯¥ç½‘ç»œè®¾å¤‡ã€é…ç½®èŠ‚ç‚¹çš„åè®®å †æ ˆï¼Œç„¶åå°†ç½‘ç»œè®¾å¤‡è¿æ¥åˆ°ä¿¡é“ï¼Œå¯èƒ½éœ€è¦è®¸å¤šä¸åŒçš„ns-3æ ¸å¿ƒæ“ä½œã€‚ç”šè‡³éœ€è¦æ›´å¤šçš„æ“ä½œæ¥å°†å¤šä¸ªè®¾å¤‡è¿æ¥åˆ°å¤šç‚¹ä¿¡é“ä¸Šï¼Œç„¶åå°†å•ä¸ªç½‘ç»œè¿æ¥åˆ°äº’è”ç½‘ä¸Šã€‚æˆ‘ä»¬æä¾›äº†æ‹“æ‰‘å¸®åŠ©å™¨å¯¹è±¡ï¼Œè¿™äº›å¯¹è±¡å°†è¿™äº›ä¸åŒçš„æ“ä½œç»„åˆæˆä¸€ä¸ªæ˜“äºä½¿ç”¨çš„æ¨¡å‹ã€‚</li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> 5G </tag>
            
            <tag> 4G </tag>
            
            <tag> simulator </tag>
            
            <tag> ns3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sim2Real and Domain Randomization</title>
      <link href="/uncategorized/surveys/sim2real/"/>
      <url>/uncategorized/surveys/sim2real/</url>
      
        <content type="html"><![CDATA[<p>æœ¬æ–‡é¦–å…ˆå¯¹Sim2realé—®é¢˜ä½œå‡ºäº†ç®€è¦ä»‹ç»ï¼Œå¹¶ç®€å•å¯¹å…¶æ–¹æ³•ä½œå‡ºåˆ†ç±»ã€‚æ¥ç€ï¼Œå¯¹äºå…¶ä¸­çš„domain randomizationæ–¹æ³•ç»™å‡ºbilevel optimizationçš„å½¢å¼ï¼Œæœ€åè°ƒç ”äº†åŸºäºä¸Šè¿°ä¼˜åŒ–å½¢å¼ï¼ˆæˆ–ç±»ä¼¼å½¢å¼ï¼‰ä¸‹çš„ä¼˜åŒ–é—®é¢˜æ±‚è§£æ–¹æ¡ˆã€‚</p><span id="more"></span><h2 id="æ¦‚è¿°"><a href="#æ¦‚è¿°" class="headerlink" title="æ¦‚è¿°"></a>æ¦‚è¿°</h2><p>sim2realçš„å…¨ç§°æ˜¯simulation to realityï¼Œæ˜¯å¼ºåŒ–å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼ŒåŒæ—¶ä¹Ÿå±äºtransfer learningçš„ä¸€ç§ã€‚ä¸»è¦è§£å†³çš„é—®é¢˜æ˜¯æœºå™¨äººé¢†åŸŸä¸­ï¼Œç›´æ¥è®©æœºå™¨äººæˆ–è€…æœºæ¢°è‡‚åœ¨ç°å®ç¯å¢ƒä¸­ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ã€é‡‡æ ·æ—¶ï¼Œä¼šå‡ºç°ä»¥ä¸‹ä¸¤ä¸ªæ¯”è¾ƒä¸¥é‡çš„é—®é¢˜ï¼š</p><ul><li>é‡‡æ ·æ•ˆç‡å¤ªä½ï¼ˆåœ¨ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è§£å†³æœºå™¨äººç›¸å…³é—®é¢˜æ—¶ï¼Œæ‰€éœ€è¦çš„æ ·æœ¬é‡ä¸€èˆ¬ä¼šè¾¾åˆ°ä¸Šåƒä¸‡ï¼Œåœ¨ç°å®ç¯å¢ƒä¸­é‡‡é›†å¦‚æ­¤æ•°é‡çº§çš„æ ·æœ¬è¦è€—è´¹å‡ ä¸ªæœˆçš„æ—¶é—´ï¼‰</li><li>å®‰å…¨é—®é¢˜ ï¼ˆç”±äºå¼ºåŒ–å­¦ä¹ éœ€è¦é€šè¿‡æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­è¿›è¡Œå¤§èŒƒå›´çš„éšæœºé‡‡æ ·æ¥è¿›è¡Œè¯•é”™ï¼Œå› è€Œåœ¨æŸäº›æ—¶åˆ»å…¶åšå‡ºçš„è¡Œä¸ºå¯èƒ½ä¼šæŸä¼¤æœºå™¨äººè‡ªèº«ï¼Œä¾‹å¦‚æ‰‹è‡‚è½¬åŠ¨è§’åº¦è¿‡å¤§æˆ–è€…é¿éšœä»»åŠ¡ä¸­ç”±äºç¢°æ’é€ æˆçš„ä¸å¯é€†æŸä¼¤ç­‰ç­‰ï¼›ä¹Ÿå¯èƒ½ä¼šæŸå®³å‘¨å›´çš„ç¯å¢ƒç”šè‡³ç”Ÿç‰©ï¼‰</li></ul><p>ä½†æ˜¯å¦‚æœæˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„è®­ç»ƒï¼Œä»¥ä¸Šä¸¤ä¸ªé—®é¢˜å‡å¯è¿åˆƒè€Œè§£ã€‚ä½†æ˜¯ï¼Œè¿™é‡ŒåŒæ ·ä¼šå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œç”±äºæ¨¡æ‹Ÿå™¨å¯¹äºç‰©ç†ç¯å¢ƒçš„å»ºæ¨¡éƒ½æ˜¯å­˜åœ¨è¯¯å·®çš„ï¼Œå› è€Œåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å­¦ä¹ åˆ°çš„æœ€ä¼˜ç­–ç•¥æ˜¯å¦å¯ä»¥ç›´æ¥åœ¨ç°å®ç¯å¢ƒä¸­åº”ç”¨å‘¢ï¼Ÿç­”æ¡ˆå¾€å¾€æ˜¯å¦å®šçš„ï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªé—®é¢˜ç§°ä¸º â€œreality gapâ€ã€‚è€Œsim2realçš„å·¥ä½œå°±æ˜¯å»å°è¯•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p><p>è¿™é‡Œå€¼å¾—æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œè™½ç„¶è¿™ä¸ªæ–¹å‘å«åšsim2realï¼Œå…¶å®å…¶ä¸­çš„æ‰€æœ‰çš„ç®—æ³•éƒ½å¯ä»¥ç›´æ¥åº”ç”¨åœ¨sim2simï¼Œreal2realç­‰çš„ä»»åŠ¡ä¸­ã€‚</p><p>[å¼•è‡ªï¼š<a href="https://zhuanlan.zhihu.com/p/106216904]">https://zhuanlan.zhihu.com/p/106216904]</a></p><p>æœ¬æ–‡æ‰¾äº†ä¸€ç¯‡surveyï¼Œå¯¹å…¶ä¸­çš„å†…å®¹ä½œå‡ºæ•´ç†ï¼Œæ„å›¾å¯¹æ•´ä¸ªsim2realé¢†åŸŸæœ‰ä¸€ä¸ªå¤§è‡´çš„äº†è§£ã€‚</p><h2 id="1-Sim2real-åˆ†ç±»"><a href="#1-Sim2real-åˆ†ç±»" class="headerlink" title="1. Sim2real åˆ†ç±»"></a>1. Sim2real åˆ†ç±»</h2><p>åœ¨ä¸‹è¿°å·¥ä½œä¸­ï¼ŒSim2realè¢«åˆ†ä¸ºäº†ä»¥ä¸‹4ä¸ªç±»åˆ«</p><blockquote><p>W. Zhao, J. P. Queralta and T. Westerlund, â€œSim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey,â€ 2020 IEEE Symposium Series on Computational Intelligence (SSCI), Canberra, ACT, Australia, 2020, pp. 737-744, doi: 10.1109/SSCI47803.2020.9308468.</p></blockquote><p><strong>tax1.1 Zero-shot Transfer</strong></p><p>å»ºç«‹ä¸€ä¸ªé€¼çœŸçš„æ¨¡æ‹Ÿå™¨ï¼Œæˆ–è€…æœ‰è¶³å¤Ÿçš„æ¨¡æ‹Ÿç»éªŒï¼Œè¿™æ ·æ¨¡å‹å°±å¯ä»¥ç›´æ¥åº”ç”¨åˆ°ç°å®ç¯å¢ƒä¸­ã€‚è¿™ç§ç­–ç•¥é€šå¸¸ç§°ä¸ºzero-shot transferæˆ–direct transferã€‚å»ºç«‹çœŸå®ä¸–ç•Œç²¾ç¡®æ¨¡å‹çš„ç³»ç»Ÿè¯†åˆ«ï¼ˆSystem Identificationï¼‰å’ŒåŸŸéšæœºåŒ–ï¼ˆDomain Randomization Methodsï¼‰æ˜¯å¯ä»¥è¢«è§†ä¸ºä¸€æ¬¡æ€§è¿ç§»çš„æŠ€æœ¯ã€‚æˆ‘ä»¬åœ¨ç¬¬ III-B èŠ‚å’Œç¬¬III-CèŠ‚ä¸­åˆ†åˆ«è®¨è®ºäº†è¿™ä¸¤ä¸ªé—®é¢˜ã€‚</p><p><strong>tax1.2 System Identification</strong></p><p>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¨¡æ‹Ÿå™¨ä¸æ˜¯çœŸå®ä¸–ç•Œçš„å¿ å®ä»£è¡¨ã€‚<strong>ç³»ç»Ÿè¯†åˆ«[51]æ­£æ˜¯ä¸ºäº†å»ºç«‹ç‰©ç†ç³»ç»Ÿçš„ç²¾ç¡®æ•°å­¦æ¨¡å‹ï¼Œå¹¶ä½¿æ¨¡æ‹Ÿå™¨æ›´çœŸå®ï¼Œéœ€è¦ä»”ç»†æ ¡å‡†</strong>ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè·å¾—è¶³å¤Ÿé€¼çœŸçš„æ¨¡æ‹Ÿå™¨çš„æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ã€‚ä¾‹å¦‚ï¼Œå¾ˆéš¾æ„å»ºé«˜è´¨é‡çš„æ¸²æŸ“å›¾åƒæ¥æ¨¡æ‹ŸçœŸå®çš„è§†è§‰ã€‚æ­¤å¤–ï¼ŒåŒä¸€æœºå™¨äººçš„è®¸å¤šç‰©ç†å‚æ•°å¯èƒ½ä¼šå› æ¸©åº¦ã€æ¹¿åº¦ã€å®šä½æˆ–å…¶ç£¨æŸè€Œå‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œè¿™ç»™ç³»ç»Ÿè¯†åˆ«å¸¦æ¥äº†æ›´å¤§çš„å›°éš¾ã€‚</p><p><strong>tax1.3 Domain Randomization Methods</strong></p><p>é¢†åŸŸéšæœºåŒ–æ˜¯è¿™æ ·ä¸€ç§æƒ³æ³•[52]ï¼Œæˆ‘ä»¬å¯ä»¥é«˜åº¦éšæœºåŒ–æ¨¡æ‹Ÿï¼Œè€Œä¸æ˜¯ä»”ç»†å»ºæ¨¡çœŸå®ä¸–ç•Œçš„æ‰€æœ‰å‚æ•°ï¼Œä»¥è¦†ç›–çœŸå®ä¸–ç•Œæ•°æ®çš„çœŸå®åˆ†å¸ƒï¼Œå°½ç®¡æ¨¡å‹å’ŒçœŸå®ä¸–ç•Œä¹‹é—´å­˜åœ¨åå·®ã€‚å›¾3aæ˜¾ç¤ºäº†åŸŸéšæœºåŒ–çš„èŒƒä¾‹ã€‚</p><p><strong>tax1.4 Domain Adaptation Methods</strong></p><p>åŸŸè‡ªé€‚åº”æ–¹æ³•ä½¿ç”¨æ¥è‡ªæºåŸŸçš„æ•°æ®æ¥æé«˜å­¦ä¹ æ¨¡å‹åœ¨æ•°æ®æ€»æ˜¯è¾ƒå°‘å¯ç”¨çš„ä¸åŒç›®æ ‡åŸŸä¸Šçš„æ€§èƒ½ã€‚ç”±äºæºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´é€šå¸¸å­˜åœ¨ä¸åŒçš„ç‰¹å¾ç©ºé—´ï¼Œä¸ºäº†æ›´å¥½åœ°è¿ç§»æºæ•°æ®ä¸­çš„çŸ¥è¯†ï¼Œæˆ‘ä»¬åº”è¯¥å°è¯•ä½¿è¿™ä¸¤ä¸ªç‰¹å¾ç©ºé—´ç»Ÿä¸€ã€‚è¿™æ˜¯åŸŸè‡ªé€‚åº”çš„ä¸»è¦ç²¾ç¥ï¼Œå¯ä»¥ç”¨å›¾3bä¸­çš„å›¾æ¥æè¿°ã€‚</p><p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20230206202202709.png" alt="image-20230206202202709"></p><p>åœ¨æ­¤åˆ†ç±»ä¸‹ï¼Œæ ¹æ®ä»»åŠ¡ç›®çš„ï¼Œè¿˜æœ‰ä¸‹è¿°ä¸¤ç§åˆ†ç±»ï¼Œå‚è€ƒä¸‹æ–‡ï¼ˆä½†è¿™ç¯‡æ–‡ç« ä¸æ€ä¹ˆæ ·ï¼‰ï¼Œè¿™é‡Œå‚è€ƒäº†ç»å…¸Robot policy learningçš„åœºæ™¯ã€‚</p><blockquote><p>Dimitropoulos, K., Hatzilygeroudis, I., &amp; Chatzilygeroudis, K. (2022). A brief survey of Sim2Real methods for robot learning. <em>Advances in Service and Industrial Robotics: RAAD 2022</em>, 133-140.</p></blockquote><ol><li><p><strong>Randomizing for Perception</strong>ï¼šè¿™ç±»æ–¹æ³•ä¸»è¦æè¿°äº†â€œç»™å®šä¸€ä¸ªç®€å•ç¼–ç¨‹çš„æ§åˆ¶å™¨ï¼Œåˆ›å»ºå¯ä»¥æ¨å¹¿åˆ°ç‰©ç†ä¸–ç•Œçš„æ„ŸçŸ¥æ¨¡å‹â€ã€‚è¿™ç±»æ–¹æ³•çš„éšæœºåŒ–å¯¹è±¡ä¸»è¦æ˜¯ç¯å¢ƒå› ç´ ã€‚</p></li><li><p><strong>Randomizing for Control</strong>ï¼šè¿™ç±»æ–¹æ³•ä¸»è¦ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªçµå·§çš„æ§åˆ¶ä¸»ä½“ï¼Œå…¶éšæœºåŒ–åŒ…æ‹¬ä½†ä¸é™äºï¼šå…³èŠ‚ä½ç½®ã€ç›¸æœºä½ç½®ã€éšæœºå¹²æ‰°ç‰©ã€è´¨é‡ã€æ»šåŠ¨å’Œæ—‹è½¬æ‘©æ“¦ç³»æ•°ã€å…‰çº¿ã€ç‰©ä½“æ‘†æ”¾ä½ç½®ç­‰ã€‚</p></li></ol><p><strong>tax1.5 Learning with Disturbances (å¹²æ‰°)</strong></p><p>Domain randomization å’Œ dynamics randomizationä¾§é‡äºåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å¼•å…¥æ‰°åŠ¨ï¼Œç›®çš„æ˜¯ä½¿ä»£ç†ä¸æ˜“å—æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´ä¸åŒ¹é…çš„å½±å“[30]ã€[38]ã€[40]ã€‚åœ¨å…¶ä»–ä½œå“ä¸­ä¹Ÿæ‰©å±•äº†ç›¸åŒçš„æ¦‚å¿µï¼Œå…¶ä¸­å¼•å…¥äº†æ‰°åŠ¨ä»¥è·å¾—æ›´é²æ£’çš„ä»£ç†ã€‚ä¾‹å¦‚ï¼Œåœ¨[72]ä¸­ï¼Œä½œè€…è€ƒè™‘äº†å˜ˆæ‚çš„å¥–åŠ±ã€‚è™½ç„¶ä¸æ¨¡æ‹Ÿåˆ°çœŸå®çš„è¿ç§»æ²¡æœ‰ç›´æ¥å…³ç³»ï¼Œä½†å˜ˆæ‚çš„å¥–åŠ±å¯ä»¥æ›´å¥½åœ°æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„ä»£ç†è®­ç»ƒã€‚æ­¤å¤–ï¼Œåœ¨æˆ‘ä»¬æœ€è¿‘çš„ä¸€äº›ä½œå“ä¸­[8]ï¼Œ[73]ï¼Œæˆ‘ä»¬è€ƒè™‘äº†å½±å“å¹¶è¡Œå­¦ä¹ çš„ä¸åŒä»£ç†çš„ç¯å¢ƒæ‰°åŠ¨ã€‚å½“è¦ä½¿ç”¨é€šç”¨ç­–ç•¥éƒ¨ç½²æˆ–è®­ç»ƒå¤šä¸ªçœŸå®ä»£ç†æ—¶ï¼Œè¿™æ˜¯éœ€è¦è€ƒè™‘çš„ä¸€ä¸ªæ–¹é¢ã€‚</p><p><strong>tax1.6 Simulation Environments</strong></p><p>sim2realçš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯<strong>æ¨¡æ‹Ÿå™¨</strong>çš„é€‰æ‹©ã€‚ç‹¬ç«‹äºç”¨äºæœ‰æ•ˆåœ°å°†çŸ¥è¯†è½¬ç§»åˆ°çœŸå®æœºå™¨äººçš„æŠ€æœ¯ï¼Œæ¨¡æ‹Ÿå™¨è¶Šé€¼çœŸï¼Œé¢„æœŸçš„ç»“æœå°±è¶Šå¥½ã€‚æ–‡çŒ®ä¸­ä½¿ç”¨æœ€å¹¿æ³›çš„æ¨¡æ‹Ÿå™¨æ˜¯ Gazebo [74]ã€Unity3D å’Œ PyBullet [75]æˆ– MuJoCo [17]ã€‚Gazebo å…·æœ‰ä¸æœºå™¨äººæ“ä½œç³»ç»Ÿ (ROS) ä¸­é—´ä»¶å¹¿æ³›é›†æˆçš„ä¼˜åŠ¿ï¼Œå› æ­¤å¯ä»¥ä¸çœŸå®æœºå™¨äººä¸­å­˜åœ¨çš„éƒ¨åˆ†æœºå™¨äººå †æ ˆä¸€èµ·ä½¿ç”¨ã€‚å¦ä¸€æ–¹é¢ï¼ŒPyBullet å’Œ MuJoCo ä¸ DL å’Œ RL åº“ä»¥åŠä½“è‚²é¦†ç¯å¢ƒè¿›è¡Œäº†æ›´å¹¿æ³›çš„é›†æˆã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒGazebo é€‚åˆæ›´å¤æ‚çš„åœºæ™¯ï¼Œè€Œ PyBullet å’Œ MuJoCo æä¾›æ›´å¿«çš„è®­ç»ƒã€‚åœ¨ä»¥ä¸€æ¬¡æ€§ä¼ è¾“çš„ç³»ç»Ÿè¯†åˆ«ä¸ºç›®æ ‡çš„æƒ…å†µä¸‹ï¼Œç ”ç©¶äººå‘˜é€šå¸¸ä¼šæ„å»ºæˆ–å®šåˆ¶æ»¡è¶³ç‰¹å®šé—®é¢˜è¦æ±‚å’Œçº¦æŸçš„ç‰¹å®šæ¨¡æ‹Ÿ[32]ã€[36]ã€[41]ã€‚</p><h2 id="2-Domain-randomizationä¼˜åŒ–é—®é¢˜è¡¨è¿°"><a href="#2-Domain-randomizationä¼˜åŒ–é—®é¢˜è¡¨è¿°" class="headerlink" title="2. Domain randomizationä¼˜åŒ–é—®é¢˜è¡¨è¿°"></a>2. Domain randomizationä¼˜åŒ–é—®é¢˜è¡¨è¿°</h2><p>Domain randomizationé—®é¢˜å¯ä»¥è¡¨è¿°ä¸ºä»¥ä¸‹çš„åŒå±‚ä¼˜åŒ–é—®é¢˜ï¼Œ</p><p>$$\begin{array}{rl}\min & F(\phi, \theta) = \mathcal{L}(\theta^{*};\mathcal{D}_{real}) \\s.t. & \theta^{*} = \arg\min_{\theta}f(\phi, \theta) = \mathbb{E}_{x \sim P_{\phi}(x)}[\mathcal{L}(\theta;\mathcal{D}_{x})]\\var. & \phi, \theta\end{array}$$</p><ul><li><p>$\phi$ï¼šä¸Šå±‚ä¼˜åŒ–é—®é¢˜çš„å˜é‡ï¼Œæ˜¯ä¸€ä¸ª<strong>æ§åˆ¶ç”ŸæˆéšæœºåŒ–æ ·æœ¬çš„åˆ†å¸ƒ</strong>çš„å‚æ•°ã€‚</p></li><li><p>$\theta$ ï¼šä¸‹å±‚ä¼˜åŒ–é—®é¢˜çš„å˜é‡ï¼Œæ˜¯è¦å­¦ä¹ çš„æ§åˆ¶å™¨ã€ç¥ç»ç½‘ç»œç­‰å…·ä½“æ¨¡å‹çš„å‚æ•°ã€‚</p></li><li><p>$\mathcal{D}_{real}$ ï¼šçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ã€‚</p></li><li><p>$\mathcal{D}<em>{x}$ ï¼šåˆæˆæ•°æ®é›†ï¼Œç”Ÿæˆè¯¥æ•°æ®é›†çš„åˆ†å¸ƒ $P</em>{\phi}$ å—æ§äº $\phi$ ã€‚ </p></li></ul><blockquote><p>ğŸŒŸ Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., &amp; Pontil, M. (2018, July). Bilevel programming for hyperparameter optimization and meta-learning. In <em>International Conference on Machine Learning</em> (pp. 1568-1577). PMLR.</p><p>Marez, D., Borden, S., &amp; Nans, L. (2020, May). UAV detection with a dataset augmented by domain randomization. In <em>Geospatial Informatics X</em> (Vol. 11398, pp. 39-50). SPIE.</p></blockquote><h2 id="3-è°ƒç ”ï¼šåŸºäºbi-level-optimizationçš„æ ·æœ¬ç”Ÿæˆ"><a href="#3-è°ƒç ”ï¼šåŸºäºbi-level-optimizationçš„æ ·æœ¬ç”Ÿæˆ" class="headerlink" title="3. è°ƒç ”ï¼šåŸºäºbi-level optimizationçš„æ ·æœ¬ç”Ÿæˆ"></a>3. è°ƒç ”ï¼šåŸºäºbi-level optimizationçš„æ ·æœ¬ç”Ÿæˆ</h2><ol><li>åŸºäº<strong>policy gradients</strong> (RL, 4è¯´çš„) è§£ä¸Šè¿°åŒå±‚ä¼˜åŒ–é—®é¢˜ï¼ˆæ— ä»£ç ï¼‰</li></ol><blockquote><p>Ruiz, N., Schulter, S., &amp; Chandraker, M. Learning To Simulate. In <em>2019 International Conference on Learning Representations</em>.</p></blockquote><p>æœ¬æ–‡å¦‚ç¬¬äºŒèŠ‚ï¼Œå°†domain randomizationé—®é¢˜è¡¨è¾¾ä¸ºåŒå±‚ä¼˜åŒ–é—®é¢˜åï¼ŒæŒ‡å‡ºâ€œåŸºäºæ¢¯åº¦çš„æ–¹æ³•æ˜¯æ— æ³•è§£è¯¥é—®é¢˜çš„ï¼ˆç”±äºå¯¹å†…å±‚é—®é¢˜çš„è‹›åˆ»æ€§è´¨é™åˆ¶ã€æ•°æ®ç”Ÿæˆå‡½æ•°å—ä¼˜åŒ–ç›®æ ‡çš„å½±å“ã€æ•°æ®ç”Ÿæˆè¿‡ç¨‹æœ¬èº«æ˜¯ä¸å¯å¾®çš„ï¼‰â€ï¼Œæå‡ºä½¿ç”¨<strong>policy gradients</strong>è§£ä¸Šè¿°åŒå±‚ä¼˜åŒ–é—®é¢˜ï¼ˆè§Sec. 2.2ï¼‰ã€‚æ­¤å¤–æ–‡ç« è¿˜è®¨è®ºäº†simulatoråœ¨å®é™…åœºæ™¯ä¸‹åº”è¯¥å»ºæ¨¡ä¸ºä¸€ä¸ªBayesian networkæˆ–æ›´å¤æ‚çš„ç½‘ç»œï¼Œå› ä¸ºâ€œactual data description (e.g., what objects are rendered in an image) is sampled from a distribution $S$ parametrized by the provided simulation parameters $\rho$ and specific rendering settings $\phi$ (e.g., lighting conditions) are sampled from a distribution $P$ also parameterized by $\psi$ , i.e. $G(x,y|\psi)=\mathcal{R}(S(\rho|\psi),P(\phi|\psi))$â€</p><ol start="2"><li>ç”¨<strong>Bayes Optimization</strong>æ¥è§£å†³å¤–å±‚ä¼˜åŒ–é—®é¢˜ï¼Œè§‚å¯Ÿä¸Šè¿°çš„formulationå¯ä»¥çœ‹å‡ºï¼Œå¤–å±‚ä¼˜åŒ–é—®é¢˜æ˜¯ä¸€ä¸ªåˆ†å¸ƒçš„ä¼˜åŒ–é—®é¢˜ï¼Œæœ¬å·¥ä½œç”¨Gaussian Processæ¥å»ºæ¨¡real worldä¸­<strong>ä»£ä»·å‡½æ•°</strong>ï¼Œç”¨è´å¶æ–¯æ³•è°ƒæ•´GPä¸­çš„å‚æ•°ï¼Œä»¥é€¼è¿‘çœŸå®ä¸–ç•Œçš„ä»£ä»·å‡½æ•°ã€‚**(Third party <a href="https://github.com/wibox/MLDLRL/tree/dee539e60e6c7e5e930776e12083875f107cf45b">code</a> available)**</li></ol><blockquote><p>Muratore, F., Eilers, C., Gienger, M., &amp; Peters, J. (2021). Data-efficient domain randomization with bayesian optimization. <em>IEEE Robotics and Automation Letters</em>, <em>6</em>(2), 911-918.</p></blockquote><p>$$\begin{aligned}\phi^{\star} & =\arg \max _{\phi \in \Phi} J^{\text {real }}\left(\theta^{\star}(\phi)\right) \quad \text { with } \\\theta^{\star}(\phi) & =\arg \max _{\theta \in \Theta} \mathbb{E}_{\xi \sim \nu(\xi ; \phi)}[J(\theta, \xi)]\end{aligned}$$</p><p>å…¶ä¸­å¤–å±‚ä¼˜åŒ–é—®é¢˜ç”¨BOæ¥è§£å†³ï¼Œ$\hat{J}^{real}(\theta^{\star}(\phi))$ è¢«å»ºæ¨¡ä¸ºGPï¼ŒThe GPsâ€™s mean and covariance is updated using all recorded inputs $\phi$ and the corresponding observations $\hat{J}^{real}(\theta^{\star}(\phi))$. </p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20230213152156628.png" alt="image-20230213152156628" style="zoom: 33%;" /><ol start="3"><li>æœ¬æ–‡æå‡ºï¼Œå€ŸåŠ©å°‘é‡çš„çœŸå®ä¸–ç•Œæ•°æ®ï¼Œè®©RLç®—æ³•ä»çœŸå®æœ‰æ•ˆçš„åˆ†å¸ƒå¼€å§‹ï¼Œ<strong>æ¸è¿›ã€è¿­ä»£å¼çš„</strong>å­¦ä¹ æ›´å¹¿ã€æ›´å®½çš„åˆ†å¸ƒä¸Šçš„ç­–ç•¥ã€‚</li></ol><blockquote><p>Y. Chebotar et al., â€œClosing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience,â€ 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 8973-8979, doi: 10.1109/ICRA.2019.8793789.</p></blockquote><p>æœ¬æ–‡çš„å¤–å±‚ä¼˜åŒ–ç›®æ ‡åœ¨ä¸ŠèŠ‚çš„åŸºç¡€ä¸Šç»†åŒ–ä¸ºä¸‹å¼ï¼š</p><p>$$\begin{array}{rl}\min _{\phi_{i+1}} & \mathbb{E}_{P_{\xi_{i+1} \sim p_{\phi_{i+1}}}}\left[\mathbb{E}_{\pi_{\theta, p_{\phi_i}}}\left[D\left(\tau_{\xi_{i+1}}^{o b}, \tau_{\text {real }}^{o b}\right)\right]\right] \\\text { s.t. } & D_{K L}\left(p_{\phi_{i+1}} \| p_{\phi_i}\right) \leq \epsilon\end{array}$$</p>è¿™é‡Œé™åˆ¶äº†simulatoræ¯æ¬¡çš„å˜åŒ–ç¨‹åº¦ã€‚æœ¬æ–‡æŒ‡å‡ºï¼Œ$p_{\phi_0}$ åº”å½“ä»ä¸€ä¸ªçœŸå®æœ‰æ•ˆçš„åˆ†å¸ƒå¼€å§‹å­¦èµ·ã€‚å¯¹äºä¸Šè¿°å¤–å±‚ä¼˜åŒ–ç›®æ ‡ï¼Œæœ¬æ–‡é‡‡ç”¨**relative entropy policy search**æ¥è§£å†³ï¼Œè¿™æ˜¯ä¸€ç§sampling-basedã€gradient-freeçš„æ–¹æ³•ã€‚<ol start="4"><li>åŸºäº<strong>Cross entropy method (CEM)</strong> å’Œproximal policy optimization (PPO)åˆ†åˆ«è§£å†³ä¸Šè¿°çš„å¤–å±‚å’Œå†…å±‚ä¼˜åŒ–é—®é¢˜<strong>ï¼ˆdockerized code aviliableï¼‰</strong></li></ol><blockquote><p>Vuong, Q., Vikram, S., Su, H., Gao, S., &amp; Christensen, H. I. (2019). How to pick the domain randomization parameters for sim-to-real transfer of reinforcement learning policies?. <em>arXiv preprint arXiv:1903.11774</em>.</p></blockquote><p>æœ¬æ–‡çš„å†…å±‚ä¼˜åŒ–é—®é¢˜æ˜¯ä¸€ä¸ªpolicy learningé—®é¢˜ï¼Œæœ¬æ–‡çš„ä¸­å¿ƒæ€æƒ³æ˜¯å€ŸåŠ©å°‘é‡ä¼˜å…ˆçš„çœŸå®ä¸–ç•Œæ ·æœ¬æ¥çŸ«æ­£simulatorã€‚æœ¬æ–‡æå‡ºåŸºäºCross entropy method (CEM) å’Œproximal policy optimization (PPO)åˆ†åˆ«è§£å†³ä¸Šè¿°çš„å¤–å±‚å’Œå†…å±‚ä¼˜åŒ–é—®é¢˜ï¼Œå…¶ä¸­CEMä½œä¸ºè§£domain randomizationé—®é¢˜çš„å…³é”®ï¼Œæ˜¯ä¸€ç§iterative gradient-free stochastic optimization methodã€‚</p><ol start="5"><li>å¼•å…¥<strong>Goldilocks Principle</strong>ï¼šåœ¨curriculum learningçš„é¢†åŸŸï¼Œä»¥ä¸€ç§meaningfulçš„orderè®­ç»ƒæ¨¡å‹å¯¹æ¨¡å‹å­¦ä¹ ç­–ç•¥æ˜¯ååˆ†æœ‰æ•ˆçš„ï¼Œæ¢è¨€ä¹‹ï¼Œåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æœ‰ä¸€ä¸ªå­¦ä¹ ä»»åŠ¡çš„sweet pointï¼Œåœ¨è¯¥ç‚¹ä¸ŠæŒç»­å­¦ä¹ ï¼Œå¯ä»¥æœ€å¤§ç¨‹åº¦çš„æå‡æ¨¡å‹æ€§èƒ½ï¼ˆå¤§æ¦‚æ˜¯è¿™ä¸ªæ„æ€ï¼Œå¯èƒ½ä¸å‡†ï¼‰ã€‚æ”¹å·¥ä½œçš„formulationä¸åŒäºä¸Šä¸€èŠ‚ã€‚</li></ol><blockquote><p>Suzuki, T., Hanaoka, S., &amp; Sato, I. (2022). Goldilocks-curriculum Domain Randomization and Fractal Perlin Noise with Application to Sim2Real Pneumonia Lesion Detection. <em>arXiv preprint arXiv:2204.13849</em>.</p></blockquote><p>è¯¥å·¥ä½œçš„å†…å¤–å±‚ä¼˜åŒ–ç›®æ ‡åˆ†åˆ«ä¸ºå‡½æ•°ä¸º </p><p>$$\begin{array}{c}\phi^{t+1}=\arg \max_{\phi \in \Phi}-\left|V(\theta^t, S_\phi)-k\right| \\\theta^t=\arg \min_{\theta} \sum_{i=1}^t L(\theta, S_{\phi^i})\end{array}$$</p>å†…å±‚ï¼šæ±‚è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œè¯¥åˆ†ç±»å™¨åœ¨æ‰€æœ‰æ¨¡æ‹Ÿå™¨å‚æ•°ä¸Šçš„å¹³å‡æ€§èƒ½è¾¾åˆ°æœ€ä¼˜ã€‚å¤–å±‚ï¼šåŒºåˆ«äºå…¶ä»–å·¥ä½œï¼Œæ˜¯ä¸€ç§curriculum-based methodï¼Œå®ƒé€šè¿‡è°ƒæ•´ $k$ æ¥è·å¾—å¯¹å¯¹ç»™å®šçš„æ¨¡å‹æ¥è¯´å…·æœ‰ä¸åŒéš¾åº¦çš„ä»»åŠ¡ï¼Œè¯¥æ–¹æ³•è¯•å›¾æ‰¾åˆ°ä¸€ä¸ªsweet pointï¼Œåœ¨è¯¥ç‚¹ä¸Šï¼Œæ¨¡å‹å¯ä»¥æœ€æœ‰æ•ˆçš„è¿›è¡Œå­¦ä¹ ï¼ˆåŸæ–‡ï¼šGoldilocks principle suggests that there might be a **sweet spot** of task difficulty that **is effective to successfully progress the training** of the current modelï¼‰ã€‚<ol start="6"><li><strong>Active domain randomization (RL-based)<strong>ï¼šä¼ ç»ŸDRéœ€è¦æ˜¾å¼çš„ç»™å‡ºä¸€ç»„éšæœºå‚æ•°ã€ä»¥åŠå¯¹åº”çš„å–å€¼ç©ºé—´ï¼Œå¸¸ç”¨çš„ç»å…¸åšæ³•æ˜¯â€”â€”åœ¨å–å€¼ç©ºé—´ä¸­åšå‡åŒ€é‡‡æ ·ï¼Œä»¥ç”Ÿæˆä¸åŒç±»å‹çš„ç¯å¢ƒã€‚ä½†æœ‰å·¥ä½œè¯æ˜ï¼Œè¿™æ ·çš„å‡åŒ€é‡‡æ ·å¯¼è‡´æ¨¡å‹å­¦ä¹ äº†è¿‡å¤šä¸å¸¸å‡ºç°çš„ç¯å¢ƒç­–ç•¥ï¼Œä»è€Œå¯¼è‡´ä½æ•ˆå­¦ä¹ ã€‚æœ¬æ–‡æå‡ºï¼Œ</strong>å°†simulatorå˜é‡å–å€¼ç©ºé—´ä¸­çš„å˜é‡ç”Ÿæˆé—®é¢˜ä¹Ÿçœ‹ä½œç§°ä¸€ä¸ªRLé—®é¢˜ï¼Œå¹¶ä½¿ç”¨å­¦ä¹ ç®—æ³•æ¥å­¦ä¹ å¦‚ä½•åœ¨ç©ºé—´ä¸­é‡‡æ ·</strong>ã€‚è¯¥æ–¹æ³•çš„å¤–å±‚ä¼˜åŒ–ç›®æ ‡ä¹Ÿæ˜¯ä¸€ä¸ªRLé—®é¢˜ã€‚</li></ol><blockquote><p>Raparthy, S. C., Mehta, B., Golemo, F., &amp; Paull, L. (2020). Generating automatic curricula via self-supervised active domain randomization. <em>arXiv preprint arXiv:2002.07911</em>.</p></blockquote><h2 id="4-æåŠbilevel-formulationä½†æœªç»™å‡ºè§£å†³æ–¹æ³•çš„å·¥ä½œ"><a href="#4-æåŠbilevel-formulationä½†æœªç»™å‡ºè§£å†³æ–¹æ³•çš„å·¥ä½œ" class="headerlink" title="4. æåŠbilevel formulationä½†æœªç»™å‡ºè§£å†³æ–¹æ³•çš„å·¥ä½œ"></a>4. æåŠbilevel formulationä½†æœªç»™å‡ºè§£å†³æ–¹æ³•çš„å·¥ä½œ</h2><p>è¿˜æœ‰ä¸€äº›æ–‡çŒ®æ²¡æœ‰æå‡ºå…·ä½“çš„bilevelä¼˜åŒ–è§£å†³æ–¹æ³•ï¼Œä½†å¥—ç”¨äº†è¯¥æ¦‚å¿µåšäº†è®ºæ–‡é™ˆè¿°ï¼Œå¦‚:</p><p>[1] ä»…è¯´æ˜äº† $S \subseteq R $ å³ï¼šthe feature space of synthetic dataset must encompass features from the real world dataã€‚</p><p>[1] Shamsuddin, A. F., Ragunathan, K., Abhijith, P., PM, D. R. S., &amp; Sankaran, P. (2022). From synthetic to naturalâ€”single natural image dehazing deep networks using synthetic dataset domain randomization. <em>Journal of Visual Communication and Image Representation</em>, <em>89</em>, 103636.</p>]]></content>
      
      
      
        <tags>
            
            <tag> survey </tag>
            
            <tag> optimization </tag>
            
            <tag> sim2real </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3GPPä½“ç³»</title>
      <link href="/uncategorized/notes/3GPP%E4%BD%93%E7%B3%BB/"/>
      <url>/uncategorized/notes/3GPP%E4%BD%93%E7%B3%BB/</url>
      
        <content type="html"><![CDATA[<p>æœ¬æ–‡ä¸»è¦æ¢³ç†3GPPåœ¨æ— çº¿é€šä¿¡é¢†åŸŸçš„åè®®ä½“ç³»ï¼Œä»¥ä¸“å®¶çŸ¥è¯†èµ‹èƒ½ã€ŒåŸºäº3GPPåè®®è¯­æ–™çš„foundation modelã€ã€‚åˆæ­¥çš„,æœ¬æ–‡å°†æ¢³ç†ä»¥ä¸‹å‡ ç‚¹ï¼ˆæŒç»­æ›´æ–°ï¼‰</p><ul><li>ä»RANå‡ºå‘ï¼Œå¯¹æ¢³ç†ç›¸å…³ä¸“ä¸šåè¯</li><li>3GPPç›¸å…³çš„åè®®æ¶æ„ <ul><li>å‚è€ƒï¼š<a href="https://zhuanlan.zhihu.com/p/102176081">link</a>ï¼Œå¯¹3GPPåè®®çš„æ¶æ„ã€å‘½åã€ä¸‹è½½æ–¹å¼ç­‰åšå‡ºäº†æ€»ç»“ â€¼ï¸</li></ul></li></ul><span id="more"></span><h2 id="1-ä¸åŒæ—¶ä»£ä¸‹çš„åŸºç«™-åŠå…³é”®ç»„ä»¶åç§°"><a href="#1-ä¸åŒæ—¶ä»£ä¸‹çš„åŸºç«™-åŠå…³é”®ç»„ä»¶åç§°" class="headerlink" title="1. ä¸åŒæ—¶ä»£ä¸‹çš„åŸºç«™ åŠå…³é”®ç»„ä»¶åç§°"></a>1. ä¸åŒæ—¶ä»£ä¸‹çš„åŸºç«™ åŠå…³é”®ç»„ä»¶åç§°</h2><p>æ„å»ºfoundation modelæ—¶ï¼ŒåŒºåˆ†å®ä½“åœ¨RANä¸­çš„ä½ç½®ä¸èº«ä»½æ˜¯è‡³å…³é‡è¦çš„ã€‚ä»¥ä¸‹å†…å®¹è½¬è‡ª <a href="https://commsbrief.com/radio-access-network-ran-geran-utran-e-utran-and-ng-ran/">link</a>ï¼Œå¹¶å¯¹å…¶ä¸­çš„ç¼©å†™åšå‡ºæ³¨é‡Šã€‚</p><h3 id="1-1-RAN-Radio-Access-Network"><a href="#1-1-RAN-Radio-Access-Network" class="headerlink" title="1.1 RAN (Radio Access Network)"></a>1.1 RAN (Radio Access Network)</h3><p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/IMG_0054.jpg" alt="IMG_0054"></p><p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/IMG_0055.jpg" alt="IMG_0055"></p><p>ã€Œ1Gã€2Gã€3Gã€4G å’Œ 5G è¿™äº›æœ¯è¯­çš„çœŸæ­£å«ä¹‰æ˜¯ä»€ä¹ˆã€: <a href="https://commsbrief.com/what-do-the-terms-1g-2g-3g-4g-and-5g-really-mean/">link</a></p><p>ã€ŒGSMã€UMTS å’Œ LTE ä¹‹é—´æœ‰ä»€ä¹ˆåŒºåˆ«ã€: <a href="https://commsbrief.com/difference-between-gsm-umts-lte/">link</a></p><p>ã€Œé•¿æœŸæ¼”è¿›ï¼šä»€ä¹ˆæ˜¯ 4G LTE åŠå…¶å·¥ä½œåŸç†ï¼Ÿã€: <a href="https://commsbrief.com/long-term-evolution-what-is-4g-lte-and-how-does-it-work/">link</a></p><p>ã€Œä»€ä¹ˆæ˜¯ç§»åŠ¨æ ¸å¿ƒç½‘ã€: <a href="https://commsbrief.com/what-is-a-mobile-core-network/">link</a></p><p>ã€ŒGSM ä¸­çš„åŸºç«™å­ç³»ç»Ÿä¸ç½‘ç»œäº¤æ¢å­ç³»ç»Ÿã€: <a href="https://commsbrief.com/base-station-subsystem-vs-network-switching-subsystem-in-gsm/">link</a></p><p>ã€ŒGGSN å’Œ SGSN æœ‰ä»€ä¹ˆåŒºåˆ«ã€: <a href="https://commsbrief.com/what-is-the-difference-between-ggsn-and-sgsn/">link</a></p><p>ã€Œæ— çº¿æ¥å…¥ç½‘ (RAN)ï¼šGERANã€UTRANã€E-UTRAN å’Œ NG-RANã€: <a href="https://commsbrief.com/radio-access-network-ran-geran-utran-e-utran-and-ng-ran/">link</a></p><p>ã€ŒNode Bã€ENodeB å’Œ GNB æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Œ2G/3G/4Gä½¿ç”¨çš„åŸºç«™æœ‰ä»€ä¹ˆåŒºåˆ«ã€: <a href="https://commsbrief.com/what-is-the-difference-between-node-b-enodeb-ng-enb-and-gnb/">link</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> 5G </tag>
            
            <tag> 4G </tag>
            
            <tag> 3GPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformers in Hugging Face</title>
      <link href="/uncategorized/notes/hugging_face/"/>
      <url>/uncategorized/notes/hugging_face/</url>
      
        <content type="html"><![CDATA[<p>Hugging Face çš„å…¥é—¨æ•™ç¨‹ï¼Œç›®æ ‡æ˜¯ä»0å¼€å§‹è®­ç»ƒè‡ªå·±çš„å¤§æ¨¡å‹ã€‚</p><span id="more"></span><h2 id="é‡ç‚¹æ•™ç¨‹"><a href="#é‡ç‚¹æ•™ç¨‹" class="headerlink" title="é‡ç‚¹æ•™ç¨‹"></a>é‡ç‚¹æ•™ç¨‹</h2><ul><li>ç»„è£…æ‰€æœ‰çš„ç»„ä»¶ï¼š<a href="https://huggingface.co/course/chapter2/6?fw=pt">https://huggingface.co/course/chapter2/6?fw=pt</a></li><li>processing data: <a href="https://huggingface.co/course/chapter3/2?fw=pt">https://huggingface.co/course/chapter3/2?fw=pt</a></li><li>Fine-tune: <a href="https://huggingface.co/course/chapter3/3?fw=pt">https://huggingface.co/course/chapter3/3?fw=pt</a></li><li><strong>Full training</strong>: <a href="https://huggingface.co/course/chapter3/4?fw=pt">https://huggingface.co/course/chapter3/4?fw=pt</a></li><li><strong>Train a new tokenizer from a old one</strong>: <a href="https://huggingface.co/course/chapter6/2?fw=pt">https://huggingface.co/course/chapter6/2?fw=pt</a></li><li><strong>Use open source dataset</strong>: <a href="https://huggingface.co/course/chapter5/1?fw=pt">https://huggingface.co/course/chapter5/1?fw=pt</a></li><li>Check tokenizers is fast or not: <a href="https://huggingface.co/course/chapter6/3?fw=pt">https://huggingface.co/course/chapter6/3?fw=pt</a></li><li>Normalization and pre-tokenization (maybe we wonâ€™t use): <a href="https://huggingface.co/course/chapter6/4?fw=pt">https://huggingface.co/course/chapter6/4?fw=pt</a></li></ul><h2 id="1-Hugging-Face"><a href="#1-Hugging-Face" class="headerlink" title="1. Hugging Face"></a>1. Hugging Face</h2><p><code>pipeline</code>: ä¸€ä¸ªç«¯åˆ°ç«¯çš„transformerå®ç°ï¼Œå¯ä»¥ç›´æ¥ç”¨äºæ¥æ”¶æ–‡æœ¬ä¿¡æ¯ï¼Œå¾—åˆ°æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å‘é‡è¡¨ç¤ºï¼Œå¹¶æœ€ç»ˆå¤„ç†ä¸ºäººç±»å¯ç†è§£çš„å½¢å¼ã€‚</p><p><code>pipeline</code> = <code>tokenizer</code> + <code>model</code> +  <code>post processing</code></p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20221123193602726.png" alt="image-20221123193602726" style="zoom: 50%;" /><h3 id="1-1-Tokenizer"><a href="#1-1-Tokenizer" class="headerlink" title="1.1 Tokenizer"></a>1.1 Tokenizer</h3><p>tokenizer: </p><ol><li>[åˆ†è¯] Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens<ul><li>split on spaces</li><li>Character-based</li><li>sub-word tokenization</li></ul></li><li>[æŸ¥è¡¨] Mapping each token to an integer</li><li>[add attention mask, etc] Adding additional inputs that may be useful to the model</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># load a pretrained tokenizer</span><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizercheckpoint <span class="token operator">=</span> <span class="token string">"distilbert-base-uncased-finetuned-sst-2-english"</span>tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span><span class="token comment"># get result </span>raw_inputs <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">"I've been waiting for a HuggingFace course my whole life."</span><span class="token punctuation">,</span>    <span class="token string">"I hate this so much!"</span><span class="token punctuation">,</span><span class="token punctuation">]</span>inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>raw_inputs<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token triple-quoted-string string">'''&#123;    'input_ids': tensor([        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]    ]), åˆ†è¯ä¹‹åï¼Œæ¯ä¸ªè¯åœ¨è¯è¡¨ä¸­çš„idï¼Œæ³¨æ„è¿™é‡Œç”¨äº†word and subwordåˆ†è¯æ–¹æ³•ï¼Œå³åˆ†å‰²è¯è¯­åˆ°ä¸å¯åˆ†å‰²çš„å¸¸è§è¯è¯­ä¸ºæ­¢ï¼Œå…¶ä¸­åŒ…å«äº†ç”¨äºå°†åºåˆ—å¡«å……ä¸ºç­‰é•¿åºåˆ—çš„å ä½ç¬¦    'attention_mask': tensor([        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]    ])ï¼Œhave the same shape as input ids, &#125;'''</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>API:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># load</span><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizertokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bert-base-cased"</span><span class="token punctuation">)</span><span class="token comment"># use</span>tokenizer<span class="token punctuation">(</span><span class="token string">"Using a Transformer network is simple"</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''&#123;'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]&#125;'''</span><span class="token comment"># split (tokenize)</span>sequence <span class="token operator">=</span> <span class="token string">"Using a Transformer network is simple"</span>tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>sequence<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span><span class="token triple-quoted-string string">'''output: ['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']'''</span><span class="token comment"># From tokens to input IDs</span>ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>ids<span class="token punctuation">)</span><span class="token triple-quoted-string string">'''output: [7993, 170, 11303, 1200, 2443, 1110, 3014]'''</span><span class="token comment"># decoding</span>decoded_string <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">7993</span><span class="token punctuation">,</span> <span class="token number">170</span><span class="token punctuation">,</span> <span class="token number">11303</span><span class="token punctuation">,</span> <span class="token number">1200</span><span class="token punctuation">,</span> <span class="token number">2443</span><span class="token punctuation">,</span> <span class="token number">1110</span><span class="token punctuation">,</span> <span class="token number">3014</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>decoded_string<span class="token punctuation">)</span><span class="token triple-quoted-string string">'''output: 'Using a Transformer network is simple'''</span>'<span class="token comment"># save</span>tokenizer<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span><span class="token string">"directory_on_my_computer"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-2-Model"><a href="#1-2-Model" class="headerlink" title="1.2 Model"></a>1.2 Model</h3><p><code>Model</code> = <code>transformer</code> + <code>model heads</code></p><p><code>transformer</code>: input: tokenized raw data; output: high-dimensional output shape like <code>[b, t, d]</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># load pretrained transformer</span><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelcheckpoint <span class="token operator">=</span> <span class="token string">"distilbert-base-uncased-finetuned-sst-2-english"</span>model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span> <span class="token comment"># tokenized input</span><span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>last_hidden_state<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment"># output: torch.Size([2, 16, 768]), [b, t, d]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>model heads</code>: input: output of transformer; output: the result of downstream task, maybe the output of a sigmoid network.</p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20221123194149899.png" alt="image-20221123194149899" style="zoom: 33%;" /><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># transformer with subsequent network</span><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassificationcheckpoint <span class="token operator">=</span> <span class="token string">"distilbert-base-uncased-finetuned-sst-2-english"</span>model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>logits<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment"># output: torch.Size([2, 2]), we have just two sentences and two labels, the result we get from our model is of shape 2 x 2.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>The Choice of <code>Model</code></strong></p><ul><li><code>*Model</code> <strong>(retrieve the hidden states)</strong></li><li><code>*ForCausalLM</code></li><li><code>*ForMaskedLM</code></li><li><code>*ForMultipleChoice</code></li><li><code>*ForQuestionAnswering</code></li><li><code>*ForSequenceClassification</code></li><li><code>*ForTokenClassification</code></li><li>and others (non-exhaustive list)</li></ul><h3 id="1-3-Post-processing"><a href="#1-3-Post-processing" class="headerlink" title="1.3 Post-processing"></a>1.3 Post-processing</h3><p>Map tensor value output by model head (mentioned above) to text (according to id2text, etc.).</p>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> hugging face </tag>
            
            <tag> transformers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in ACL 2022</title>
      <link href="/uncategorized/paperlistfile/ACL2022/"/>
      <url>/uncategorized/paperlistfile/ACL2022/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.2022.aclweb.org/papers">Link</a></p><span id="more"></span><h2 id="Anomaly-detection"><a href="#Anomaly-detection" class="headerlink" title="Anomaly detection"></a>Anomaly detection</h2><ul><li><p>Self-Attentive, Multi-Context One-Class Classification for Unsupervised Anomaly Detection on Text</p><p>Lukas RuffYury ZemlyanskiyRobert VandermeulenThomas SchnakeMarius Kloft</p></li></ul><h2 id="Big-model-foundation-model"><a href="#Big-model-foundation-model" class="headerlink" title="Big model / foundation model"></a>Big model / foundation model</h2><ul><li><p>BMInf: An Efficient Toolkit for Big Model Inference and Tuning</p><p>Xu HanGuoyang ZengWeilin ZhaoZhiyuan LiuZhengyan ZhangJie ZhouJun ZhangJia ChaoMaosong Sun</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Foundation models</title>
      <link href="/uncategorized/surveys/foundation_models/"/>
      <url>/uncategorized/surveys/foundation_models/</url>
      
        <content type="html"><![CDATA[<p>è¿™é‡Œåˆ—å‡ºä¸€äº›è¿‘å¹´æ¥å…³äºå¤§æ¨¡å‹çš„æ€»ç»“ã€è°ƒç ”ï¼Œè¿˜æœ‰ç›¸å…³é¡¶ä¼šè®ºæ–‡ã€‚æ€»ç»“é¡¶ä¼šè®ºæ–‡çš„åŸå› åœ¨äºï¼Œåœ¨æˆ‘çœ‹æ¥å¤§æ¨¡å‹ï¼ˆæˆ–åŸºç¡€æ¨¡å‹ï¼‰å¤§å¤šéƒ½æ˜¯åœ¨å·¥ç¨‹é¢†åŸŸçš„åˆ›æ–°ï¼Œå¦‚ä½•åˆ©ç”¨å·¥ç¨‹åˆ›æ–°ï¼ŒåŠ©åŠ›æ˜¯ç§‘å­¦åˆ›æ–°ã€‚ä¸­é—´çš„æ¡¥æ¢åº”è¯¥è¢«æ‰¾åˆ°ã€‚</p><span id="more"></span><p>æ³¨ï¼šæœ‰äº›è°ƒç ”ç›´æ¥æˆªå›¾äº†å¹³æ—¥çš„å·¥ä½œæ±‡æŠ¥ï¼Œæ³¨æ„ä¸æœ€æ–°çš„å·¥ä½œè¿›å±•åŠæ—¶åŒæ­¥ã€‚</p><h2 id="1-Big-Model-after-GPT-3"><a href="#1-Big-Model-after-GPT-3" class="headerlink" title="1. Big Model, after GPT-3"></a>1. Big Model, after GPT-3</h2><p>æ¦‚æ‹¬ï¼š<strong>è¶…å¤§å‚æ•°è§„æ¨¡</strong>çš„æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨<strong>è¶…å¤§è§„æ¨¡æ•°æ®</strong>ï¼Œå¤§å¤šä»¥self-supervisedæ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œæ¥å­¦ä¹ æ•°æ®çš„é€šç”¨è¡¨å¾ã€‚åç»­é€šè¿‡promptã€fine-tuneç­‰è¿ç§»å­¦ä¹ æ–¹æ³•é€‚åº”ä¸åŒä¸‹æ¸¸ä»»åŠ¡çš„é€šç”¨æ¨¡å‹èŒƒå¼ã€‚</p><p><strong>ç›®å‰foundation modelçš„åº”ç”¨é¢†åŸŸä»¥åŠä¸‹æ¸¸ä»»åŠ¡åŒ…æ‹¬</strong></p><ul><li><p>NLPï¼ˆæˆç†Ÿï¼‰</p><ul><li><p>ä¸‹æ¸¸ä»»åŠ¡ï¼šç¿»è¯‘ã€é—®ç­”ã€è¯­ä¹‰æ€»ç»“ï¼Œç­‰</p></li><li><p>ä»£è¡¨æ¨¡å‹ï¼šGPT-3ï¼ŒLaMDAã€PaLMã€BLOOMï¼Œç­‰</p></li></ul></li><li><p>CV</p><ul><li><p>ä¸‹æ¸¸ä»»åŠ¡ï¼šæ–‡ç”Ÿå›¾ã€æ–‡ç”Ÿè§†é¢‘ã€å›¾ç‰‡æè¿°ã€é£æ ¼è¿ç§»ï¼Œç­‰</p></li><li><p>ä»£è¡¨æ¨¡å‹ï¼šDALL-E 2ã€Imagenã€Partiï¼Œç­‰</p></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20221121142447307.png" alt="image-20221121142447307"></p><p><strong>Foundation modelçš„ç‰¹ç‚¹ï¼šemergence, homogenization</strong></p><ul><li><p>Emergenceï¼šé™¤â€œéšç”Ÿæ€§â€ï¼Œå³æ¨¡å‹å­¦ä¹ åˆ°çš„è¡¨å¾æ˜¯éšæ€§çš„ï¼Œè€Œéäººç±»æŒ‡å®šçš„ã€‚è¿˜æœ‰ä¸€ç§è§£é‡Šä¸ºâ€œ<strong>æ¶Œç°æ€§</strong>â€ï¼Œå³ï¼šæ¨¡å‹å‚æ•°è§„æ¨¡ä¸Šï¼Œç”±é‡å˜å¼•èµ·è´¨å˜çš„è¿‡ç¨‹ï¼Œä¸€äº›æ¨¡å‹çš„ç‰¹æ€§åœ¨å°æ¨¡å‹ä¸Šä¸å…·å¤‡ï¼Œè€Œå½“å‚æ•°è§„æ¨¡æ‰©å¤§åæ‰ä¼šæ˜¾éœ²çš„ç‰¹æ€§ã€‚</p></li><li><p>Homogenizationï¼šfoundation modelçš„åŸºç¡€æ¨¡å‹å‘ˆç°åŒè´¨åŒ–è¶‹åŠ¿ï¼Œç›®å‰NLPå¤§æ¨¡å‹å‡ ä¹éƒ½ç”±transformerç»“æ„ä¸­æ”¹å˜è€Œæ¥ã€‚</p></li></ul><p><strong>Foundation modelå¯¹ä¸‹æ¸¸ä»»åŠ¡çš„é€‚é…</strong></p><ul><li><p>Fine-tuneï¼šé’ˆå¯¹ç‰¹å®šçš„ä»»åŠ¡ï¼Œåˆ©ç”¨ç‰¹å®šçš„æ ‡ç­¾æ•°æ®å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œfine-tuneï¼Œå¾—åˆ°çš„æ¨¡å‹å°†åªåœ¨<strong>ç‰¹å®šä»»åŠ¡</strong>ä¸Šæœ‰è¾ƒå¥½æ€§èƒ½ï¼Œæ— æ³•ç”¨äºå…¶ä»–ä»»åŠ¡</p></li><li><p>Promptï¼šå¯¹è¾“å…¥çš„æ–‡æœ¬æŒ‰ç…§ç‰¹å®šæ¨¡æ¿è¿›è¡Œå¤„ç†ï¼Œé€šè¿‡æ°å½“çš„æ–¹å¼<strong>é‡æ–°å®šä¹‰ä¸‹æ¸¸ä»»åŠ¡</strong>ï¼Œä½¿ä¹‹æ›´é€‚é…é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å½¢å¼ï¼Œä½¿ä¹‹å›å¿†èµ·é¢„è®­ç»ƒæ—¶çš„çŸ¥è¯†</p><ul><li><p>Few-shot learning setting</p></li><li><p>Zero-shot learning setting</p></li></ul></li></ul><h3 id="1-1-å¤§æ¨¡å‹è°ƒç ”"><a href="#1-1-å¤§æ¨¡å‹è°ƒç ”" class="headerlink" title="1.1 å¤§æ¨¡å‹è°ƒç ”"></a>1.1 å¤§æ¨¡å‹è°ƒç ”</h3><p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20221121143241729.png" alt="image-20221121143241729"></p><p>ä¸Šè¿°æ¨¡å‹çš„ä½“é‡æ€»ç»“å¦‚ä¸‹è¡¨</p><table><thead><tr><th align="center">æ¨¡å‹</th><th align="center">è®­ç»ƒæ—¶é—´</th><th align="center">è®­ç»ƒç©ºé—´</th><th align="center">æ¨¡å‹å¤§å°</th><th align="center">ä¼˜åŒ–å™¨+æ¨¡å‹å¤§å°</th><th align="center">å‚æ•°é‡</th><th align="center">æ•°æ®é‡</th><th align="center">æ¨¡å‹ç»“æ„</th></tr></thead><tbody><tr><td align="center">GPT-3 (OpenAI)</td><td align="center">3.14e11 TFLOPS</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">175B</td><td align="center">45TB   (raw data)     570GB</td><td align="center">Sparse Transformer</td></tr><tr><td align="center">PanGu (Huawei, CN)</td><td align="center"></td><td align="center">2048   Ascend 910 AI processors</td><td align="center"></td><td align="center">750GB</td><td align="center">200B</td><td align="center">1.1T</td><td align="center">Transformer</td></tr><tr><td align="center">GPT-J (EleutherAI)</td><td align="center">1.5e10 TFLOPs</td><td align="center"></td><td align="center">9GB</td><td align="center">61GB</td><td align="center">6B</td><td align="center">825G (raw data)</td><td align="center">Sparse Transformer      (like GPT-3)</td></tr><tr><td align="center">Ernie 3.0 Titan (Baidu)</td><td align="center">3.14e11 TFLOPS</td><td align="center">Nvidia   V100 GPU and Ascend 910 NPU clusters      (åˆ†å¸ƒå¼)</td><td align="center"></td><td align="center">2.1TB</td><td align="center">260B</td><td align="center"></td><td align="center">Transformer-XL</td></tr><tr><td align="center">GPT-NeoX (EleutherAI)</td><td align="center"></td><td align="center"></td><td align="center">39GB</td><td align="center">268GB</td><td align="center">20B</td><td align="center">825G (raw data)</td><td align="center">Sparse Transformer      (like GPT-3)</td></tr><tr><td align="center">OPT (Meta)</td><td align="center">4.48e10   TFLOPs*</td><td align="center">992   80GB A100 GPUs</td><td align="center"></td><td align="center"></td><td align="center">175B</td><td align="center">800GB   (raw data)</td><td align="center">Transformer</td></tr><tr><td align="center">BLOOM (BigScience)</td><td align="center">3.5   month</td><td align="center">384   A100 80GB GPUs (48 nodes)</td><td align="center">0.33TB</td><td align="center">2.3TB</td><td align="center">176B</td><td align="center"></td><td align="center">Transformer     (like GPT-2)</td></tr><tr><td align="center">GLM-130B (Tsinghua)</td><td align="center">2 month</td><td align="center">96   NVIDIA DGX-A100 (8*40G)     GPU   nodes</td><td align="center"></td><td align="center"></td><td align="center">130B</td><td align="center">2.3T (raw data)</td><td align="center">Transformer     (like GLM)</td></tr></tbody></table><p><strong>å¤§æ¨¡å‹åŸºç¡€æ¶æ„</strong></p><p>ç›®å‰åœ¨NLPé¢†åŸŸè¢«æˆåŠŸè®­ç»ƒå¹¶å¤§è§„æ¨¡åº”ç”¨çš„æ¨¡å‹ï¼Œéƒ½æ˜¯åŸºäºTransformerçš„self-attentionæ¶æ„çš„ï¼š</p><ol><li><p>Autoregressiveï¼ˆä»…åŒ…å«decoderï¼‰ï¼šè‡ªå›å½’æ¨¡å‹çš„ä»£è¡¨æ˜¯GPTã€‚æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä»å·¦åˆ°å³çš„è¯­è¨€æ¨¡å‹ï¼Œè®­ç»ƒç›®æ ‡æ˜¯ä»å·¦åˆ°å³çš„æ–‡æœ¬ç”Ÿæˆã€‚</p><ul><li>å¸¸ç”¨äº<strong>æ— æ¡ä»¶é•¿æ–‡æœ¬ç”Ÿæˆ</strong>ï¼ˆå¯¹è¯ç”Ÿæˆã€æ•…äº‹ç”Ÿæˆç­‰ï¼‰ï¼Œä½†ç¼ºç‚¹æ˜¯å•å‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸åˆ©äºNLUï¼ˆè‡ªç„¶è¯­è¨€ç†è§£ï¼‰ä»»åŠ¡ã€‚</li></ul></li><li><p>Autoencodingï¼ˆä»…åŒ…å«encoderï¼‰ï¼šä»£è¡¨æ¨¡å‹æ˜¯BERTã€ALBERTã€DeBERTa ã€‚è‡ªç¼–ç æ¨¡å‹æ˜¯é€šè¿‡å»å™ªä»»åŠ¡ï¼ˆå¦‚åˆ©ç”¨æ©ç è¯­è¨€æ¨¡å‹ï¼‰å­¦ä¹ åŒå‘çš„ä¸Šä¸‹æ–‡ç¼–ç å™¨ï¼Œè®­ç»ƒç›®æ ‡æ˜¯å¯¹æ–‡æœ¬è¿›è¡Œéšæœºæ©ç ï¼Œç„¶åé¢„æµ‹è¢«æ©ç çš„è¯ã€‚</p><ul><li>å¸¸ç”¨äº<strong>è‡ªç„¶è¯­è¨€ç†è§£</strong>ï¼ˆäº‹å®æ¨æ–­ã€è¯­æ³•åˆ†æã€åˆ†ç±»ç­‰ï¼‰ï¼Œç¼ºç‚¹æ˜¯ä¸èƒ½ç›´æ¥ç”¨äºæ–‡æœ¬ç”Ÿæˆã€‚</li></ul></li><li><p>Encoder-decoderï¼ˆå®Œæ•´çš„Transformerç»“æ„ï¼‰ï¼šä»£è¡¨æ¨¡å‹æ˜¯T5ã€BARTã€‚åŒ…å«ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ï¼Œæ¥å—ä¸€æ®µæ–‡æœ¬ï¼Œä»å·¦åˆ°å³çš„ç”Ÿæˆå¦ä¸€æ®µæ–‡æœ¬ã€‚</p><ul><li>å¸¸ç”¨äº<strong>æœ‰æ¡ä»¶çš„ç”Ÿæˆä»»åŠ¡</strong>ï¼ˆæ‘˜è¦ç”Ÿæˆã€å¯¹è¯ç­‰ï¼‰ã€‚ç¼ºç‚¹æ˜¯æ¯”BERT-basedæ¨¡å‹åœ¨åŒæ€§èƒ½ä¸‹éœ€è¦æ›´å¤šå‚æ•°ã€‚</li></ul></li><li><p>Hybird-modelï¼šGLM</p></li></ol><p>è¿˜æœ‰ä¸€äº›æ¨¡å‹ç»“åˆäº†transformer-basedæ¨¡å‹ï¼Œä»¥åŠå…¶ä»–æ¨¡å‹ï¼Œç”¨äºæ”¹å–„transformerç¼ºä¹é•¿æœŸè®°å¿†çš„ç¼ºç‚¹ã€‚</p><ul><li><p>ä¸GNNç»“åˆï¼š CogQA [1]</p></li><li><p>ä¸knowledge graphç»“åˆï¼š OAG-BERT [2] </p></li></ul><blockquote><p>[1] Ding, M., Zhou, C., Chen, Q., Yang, H., &amp; Tang, J. (2019, July). Cognitive Graph for Multi-Hop Reading Comprehension at Scale. In <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em> (pp. 2694-2703).</p><p>[2] Liu, X., Yin, D., Zhang, X., Su, K., Wu, K., Yang, H., &amp; Tang, J. (2021). Oag-bert: Pre-train heterogeneous entity-augmented academic language models. <em>arXiv</em> <em>preprint arXiv:2103.02410</em>.</p></blockquote><h3 id="1-2-å¤§æ¨¡å‹å¤æ‚åº¦åˆ†æ"><a href="#1-2-å¤§æ¨¡å‹å¤æ‚åº¦åˆ†æ" class="headerlink" title="1.2 å¤§æ¨¡å‹å¤æ‚åº¦åˆ†æ"></a>1.2 å¤§æ¨¡å‹å¤æ‚åº¦åˆ†æ</h3><p>æ·±åº¦å­¦ä¹ çš„ä¿å­˜æ¨¡å‹é‡ŒåŒ…å«æ‰€æœ‰trainable variablesçš„ç²¾ç¡®å€¼ã€‚ä¸‹æ–‡ä»¥sparse transformerä¸ºä¾‹ï¼Œåˆ†æè¯¥æ¨¡å‹çš„ç©ºé—´å¤æ‚åº¦ã€è®¡ç®—å¤æ‚åº¦ã€‚</p><ul><li><p>Self-attentionçš„éšç©ºé—´ç»´åº¦ä¸º$d_{model}$ï¼Œheadæ•°ç›®ä¸º$n_{head}$ï¼Œåˆ™æ¯ä¸ªheadçš„ç»´åº¦ä¸º$d_{head}=d_{model}/n_{head} $ã€‚Feed-forwardçš„éšç©ºé—´ç»´åº¦ä¸º$d_{ff}$</p></li><li><p>è®°è¾“å…¥åˆ°ä¸‹è¿°çš„ä¸€å±‚sparse transformerçš„æ•°æ®ä¸º$\mathbf{X} \in \mathbb{R}^{N Ã— d_{model}}$ï¼Œ$N$ä¸ºè¾“å…¥çš„å¥å­é•¿åº¦ã€‚</p></li><li><p>è®°self-attentionçš„å±‚æ•°ä¸º$n_{layer}$</p></li></ul><p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20221121144017190.png" alt="image-20221121144017190"></p><h2 id="2-How-to-train-a-big-model-from-zero-to-one"><a href="#2-How-to-train-a-big-model-from-zero-to-one" class="headerlink" title="2. How to train a big model, from zero to one"></a>2. How to train a big model, from zero to one</h2><p>åœ¨specific-domainæ„å»ºä¸€ä¸ªå¤§æ¨¡å‹ï¼Œé¦–å…ˆéœ€è¦</p><ol><li><p>ç¡®å®šä¸‹æ¸¸ä»»åŠ¡</p><ul><li><p>ä¸‹æ¸¸ä»»åŠ¡å†³å®šäº†æ¨¡å‹çš„é¢„è®­ç»ƒä»»åŠ¡ï¼šé¢„è®­ç»ƒä»»åŠ¡åº”å½“challengeï¼Œä¸”è´´è¿‘ä¸‹æ¸¸ä»»åŠ¡ã€‚åˆ†æä¸‹æ¸¸ä»»åŠ¡ä¸»è¦åˆ†ætoken-wiseè¿˜æ˜¯sentence-wise relationshipï¼Œå¯ä»¥æŒ‰éœ€é€‰æ‹©é¢„è®­ç»ƒä»»åŠ¡ã€‚</p></li><li><p>ä¸‹æ¸¸ä»»åŠ¡å†³å®šäº†æ¨¡å‹éª¨æ¶ï¼šæ¨¡å‹æ³¨é‡NLUè¿˜æ˜¯NLGï¼Ÿ</p></li></ul></li><li><p>ç¡®å®šæ¨¡å‹éª¨æ¶ï¼šä»Autoencoding / Autoregressive / Encoder-decoderä¸­é€‰æ‹©åˆé€‚çš„æ¡†æ¶</p></li><li><p>ç¡®å®šæ¨¡å‹é¢„è®­ç»ƒä»»åŠ¡</p></li><li><p>ä»ä¸‹æ¸¸ä»»åŠ¡å’Œé¢„è®­ç»ƒä»»åŠ¡å‡ºå‘ï¼Œå¤„ç†å¹¶å‡†å¤‡è¯­æ–™</p></li></ol><p>åœ¨ç¡®å®šäº†ä»¥ä¸Šè¦ç´ åï¼Œåœ¨specific-domain foundation modelä¸­å­˜åœ¨â€œè¯è¡¨ä¸é€šç”¨é¢†åŸŸä¸åŒâ€çš„é—®é¢˜ï¼Œå³å¯èƒ½æŸäº›è¯è¯­åœ¨é€šç”¨è¯­æ–™åº“ä¸­ä¸å­˜åœ¨ï¼Œæˆ–å…·æœ‰æ­§ä¹‰ï¼Œå› æ­¤æ¨¡å‹çš„word embeddingå±‚éœ€è¦æ›¿æ¢ä¸ºspecific domainçš„è¯è¡¨ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºé‡‡ç”¨skip-gramå­¦ä¹ embeddingã€‚</p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20221123161546624.png" alt="image-20221123161546624" style="zoom: 33%;" /><p><strong>ä¸»è¦å‚è€ƒæ–‡çŒ®</strong><br>Kalyan, K. S., Rajasekharan, A., &amp; Sangeetha, S. (2021). Ammus: A survey of transformer-based pretrained models in natural language processing. <em>arXiv preprint arXiv:2108.05542</em>.</p><h2 id="3-Related-work-after-2020"><a href="#3-Related-work-after-2020" class="headerlink" title="3. Related work, after 2020"></a>3. Related work, after 2020</h2><p>è¿™é‡Œåˆ—å‡ºäº†å¤§æ¨¡å‹çš„è®­ç»ƒç®—æ³•ã€åº”å¯¹å¤§è§„æ¨¡æ¨¡å‹å‚æ•°çš„è§£æ³•ï¼Œåº”å¯¹åˆ†å¸ƒå¼æ•°æ®ã€ç¯å¢ƒçš„è®­ç»ƒæ–¹æ³•ã€‚æœ‰æ ‡æ³¨ä¼šè®®åç§°çš„è®ºæ–‡å‡ä¸ºé¡¶ä¼š/é¢†åŸŸé¡¶ä¼šè®ºæ–‡ã€‚</p><ul><li><p>[ACL2022] BMInf: An Efficient Toolkit for Big Model Inference and Tuning</p><p>Xu Han; Guoyang Zeng; Weilin Zhao; Zhiyuan Liu; Zhengyan Zhang; Jie Zhou; Jun Zhang; Jia Chao; Maosong Sun</p></li><li><p>[KDD2022] Beyond Traditional Characterizations in the Age of Data: Big Models, Scalable Algorithms, and Meaningful Solutions</p><p>Shang-Hua Teng</p></li><li><p>[NIPS2022] Contrastive Adapters for Foundation Model Group Robustness</p><p>Michael Zhang; Christopher Re</p></li><li><p>[NIPS2022] Decentralized Training of Foundation Models in Heterogeneous Environments</p><p>Binhang Yuan; Yongjun He; Jared Quincy Davis; Tianyi Zhang; Tri Dao; Beidi Chen; Percy Liang; Christopher Re; Ce Zhang</p></li><li><p>[IMCL2021] PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models</p><p>  Chaoyang He; Shen Li; Mahdi Soltanolkotabi; Salman Avestimehr</p></li><li><p>[IJCAI2022] Heterogeneous Ensemble Knowledge Transfer for Training Large Models in Federated Learning</p></li></ul><p>â€‹        Yae Jee Cho; Andre Manoel; Gauri Joshi; Robert Sim; Dimitrios Dimitriadis</p><ul><li>[MLSYS2021] Pipelined Backpropagation at Scale: Training Large Models without Batches</li></ul><p>â€‹        Atli Kosson; Vitaliy Chiley; Abhinav Venigalla; Joel Hestness; Urs Koster</p><h3 id="3-1-å¤§æ¨¡å‹åœ¨å‚ç›´é¢†åŸŸï¼ˆå®šä¹‰åœ¨æ–‡æœ¬é¢†åŸŸï¼‰çš„æ„å»º"><a href="#3-1-å¤§æ¨¡å‹åœ¨å‚ç›´é¢†åŸŸï¼ˆå®šä¹‰åœ¨æ–‡æœ¬é¢†åŸŸï¼‰çš„æ„å»º" class="headerlink" title="3.1 å¤§æ¨¡å‹åœ¨å‚ç›´é¢†åŸŸï¼ˆå®šä¹‰åœ¨æ–‡æœ¬é¢†åŸŸï¼‰çš„æ„å»º"></a>3.1 å¤§æ¨¡å‹åœ¨å‚ç›´é¢†åŸŸï¼ˆå®šä¹‰åœ¨æ–‡æœ¬é¢†åŸŸï¼‰çš„æ„å»º</h3><ul><li><p>[RECSYS2021] Large-Scale Modeling of Mobile User Click Behaviors Using Deep Learning</p><p>Xin ZhouYang Li</p></li><li><p>Lewis, P., Ott, M., Du, J., &amp; Stoyanov, V. (2020, November). Pretrained language models for biomedical and clinical tasks: Understanding and extending the state-of-the-art. In <em>Proceedings of the 3rd Clinical Natural Language Processing Workshop</em> (pp. 146-157).</p></li><li><p>Xiao, C., Hu, X., Liu, Z., Tu, C., &amp; Sun, M. (2021). Lawformer: A pre-trained language model for chinese legal long documents. <em>AI Open</em>, <em>2</em>, 79-84.ã€ä¸­å›½æ³•å¾‹é•¿æ–‡æ¡£ï¼Œåšæ³•å¾‹åˆ¤å†³é¢„æµ‹ã€ç›¸ä¼¼æ¡ˆä¾‹æ£€ç´¢ã€æ³•å¾‹é˜…è¯»ç†è§£å’Œæ³•å¾‹é—®ç­”ã€‘</p></li><li><p>Beltagy, I., Lo, K., &amp; Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. <em>arXiv preprint arXiv:1903.10676</em>.ã€ç§‘å­¦æ–‡æœ¬çš„è¯­è¨€æ¨¡å‹ã€‘</p></li><li><p>Kierszbaum, S., Klein, T., &amp; Lapasset, L. (2022). ASRS-CMFS vs. RoBERTa: Comparing Two Pre-Trained Language Models to Predict Anomalies in Aviation Occurrence Reports with a Low Volume of In-Domain Data Available. <em>Aerospace</em>, <em>9</em>(10), 591.ã€èˆªå¤©äº‹æ•…æ–‡æ¡£ï¼Œä¸‹æ¸¸ä»»åŠ¡æ˜¯å…³äºæ•…éšœç§ç±»çš„å¤šåˆ†ç±»é—®é¢˜ã€‘</p></li><li><p>Shen, J. T., Yamashita, M., Prihar, E., Heffernan, N., Wu, X., Graff, B., &amp; Lee, D. (2021). Mathbert: A pre-trained language model for general nlp tasks in mathematics education. <em>arXiv preprint arXiv:2106.07340</em>.ã€æ•°å­¦æ–‡æœ¬ä¸­çš„è¯­è¨€æ¨¡å‹ã€‘</p></li></ul><h3 id="3-2-å¤§æ¨¡å‹åœ¨å‚ç›´é¢†åŸŸï¼ˆå®šä¹‰åœ¨ç‰©ç†ä¸–ç•Œï¼‰çš„æ„å»º"><a href="#3-2-å¤§æ¨¡å‹åœ¨å‚ç›´é¢†åŸŸï¼ˆå®šä¹‰åœ¨ç‰©ç†ä¸–ç•Œï¼‰çš„æ„å»º" class="headerlink" title="3.2 å¤§æ¨¡å‹åœ¨å‚ç›´é¢†åŸŸï¼ˆå®šä¹‰åœ¨ç‰©ç†ä¸–ç•Œï¼‰çš„æ„å»º"></a>3.2 å¤§æ¨¡å‹åœ¨å‚ç›´é¢†åŸŸï¼ˆå®šä¹‰åœ¨ç‰©ç†ä¸–ç•Œï¼‰çš„æ„å»º</h3><ul><li>Zheng, Z., Lu, X. Z., Chen, K. Y., Zhou, Y. C., &amp; Lin, J. R. (2022). Pretrained domain-specific language model for natural language processing tasks in the AEC domain. <em>Computers in Industry</em>, <em>142</em>, 103733. ã€å»ºç­‘æ–½å·¥æ ‡å‡†é¢†åŸŸçš„è¯­è¨€æ¨¡å‹ã€‘</li><li>Zhou, Y. C., Zheng, Z., Lin, J. R., &amp; Lu, X. Z. (2022). Integrating NLP and context-free grammar for complex rule interpretation towards automated compliance checking. <em>Computers in Industry</em>, <em>142</em>, 103746.ã€ä¸Šä¸€ç¯‡çš„å»¶ç»­ï¼Œä»å¤æ‚åˆè§„æ ‡å‡†ä¸­æå–è§„åˆ™ä»¥åšåˆè§„æ£€éªŒã€‘</li><li>Webersinke, N., Kraus, M., Bingler, J. A., &amp; Leippold, M. (2021). Climatebert: A pretrained language model for climate-related text. <em>arXiv preprint arXiv:2110.12010</em>.ã€æ°”å€™æ•°æ®ä¸Šçš„è¯­è¨€æ¨¡å‹ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªå¾ˆæœ‰è¶£çš„ä¾‹å­æ˜¯ï¼š<em>Fact-Checking</em>ï¼Œå³é’ˆå¯¹æŸä¸ªè¯æ®ï¼Œç”±æ¨¡å‹ç»™å‡ºâ€œè¯¥è¯æ®æ”¯æŒä»€ä¹ˆå£°æ˜â€çš„åˆ¤æ–­ã€‚ã€‘</li><li>Berquand, A., Darm, P., &amp; Riccardi, A. (2021). SpaceTransformers: language modeling for space systems. <em>IEEE Access</em>, <em>9</em>, 133111-133122.ã€ç©ºé—´ç³»ç»Ÿä¸­çš„è¯­è¨€æ¨¡å‹ï¼Œæ ¹æ®ç©ºé—´æ ‡å‡†åˆ¶å®šï¼Œä»¥concept recognizationä¸ºæœ€åçš„è¯„ä¼°ä»»åŠ¡ï¼Œè¿™ä¸ªä»»åŠ¡åº”å½“è¢«è§†ä¸ºè§„èŒƒ/æ ‡å‡†ç±»çš„åŸºç¡€ä»»åŠ¡ã€‘</li></ul><p>æ­¤å¤–è¿˜æœ‰ä¸€äº›å¤§æ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°æ®ã€é’ˆå¯¹å¤§æ¨¡å‹çš„security issueï¼ˆlike backdoor attack, etc.ï¼‰ç­‰è®®é¢˜ï¼›åœ¨æ­¤ä¸åˆ—å‡ºã€‚</p>]]></content>
      
      
      
        <tags>
            
            <tag> survey </tag>
            
            <tag> foundation models </tag>
            
            <tag> big models </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markov switching Model &amp; Markov Chain Monte Carlo</title>
      <link href="/uncategorized/notes/mcmc/"/>
      <url>/uncategorized/notes/mcmc/</url>
      
        <content type="html"><![CDATA[<p>æ’°å†™æœ¬ç¯‡ç¬”è®°çš„åŠ¨æœºæ¥æºäºä¸€ä¸ªçœŸå®åœºæ™¯ä¸­çš„é¡¹ç›®éœ€æ±‚â€”â€”<strong>ç”¨æˆ·è¡Œä¸ºæ¨¡å¼ç ”ç©¶</strong>ã€‚æˆ‘ä»¬æ€»å¸Œæœ›èƒ½å¤Ÿä½¿ç”¨æ•°å­¦çš„è¯­è¨€æ¥æè¿°ç”¨æˆ·è¡Œä¸ºï¼Œä»¥ä¾¿èƒ½å¤Ÿæ¨æ–­ç”¨æˆ·çš„æœªæ¥åŠ¨æ€ï¼Œç”¨äºä¼˜åŒ–ç³»ç»Ÿã€‚å› æ­¤æœ¬æ–‡é¦–å…ˆå°†ç»™å‡ºä¸Šè¿°çš„motivation scenarioï¼Œç„¶åä»‹ç»ç”¨äºå»ºæ¨¡åœºæ™¯ä¸­ç”¨æˆ·è¡Œä¸ºçš„æ•°å­¦æ¨¡å‹â€”â€”Markov switching Modelï¼Œä»¥åŠåŸå› ã€‚æœ€åä»‹ç»è¯¥å»ºæ¨¡æ–¹æ³•èƒŒåçš„æ•°å­¦å·¥å…·â€”â€”Markov Chain Monte Carloã€‚</p><span id="more"></span><h2 id="ç›®å½•"><a href="#ç›®å½•" class="headerlink" title="ç›®å½•"></a>ç›®å½•</h2><ul><li>Motivation</li><li>Regime Switching Model &amp; Markov Switching Model</li><li>Markov Chain Monte Carlo (MCMC)</li><li>æ¡ˆä¾‹åˆ†æä¸å®ç°<ul><li>å¼€æºä»£ç è°ƒç ”</li><li>æ¡ˆä¾‹åˆ†æ</li></ul></li></ul><h2 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1. Motivation"></a>1. Motivation</h2><p>æœ¬æ–‡çš„ç¤ºä¾‹åœºæ™¯æ¥æºäºâ€œå¤æ‚ç§»åŠ¨æ¥å…¥ç½‘ä¸­ï¼Œé¢å‘ç”¨æˆ·çš„ä½“éªŒä¼˜åŒ–ä¸ç½‘ç»œé—®é¢˜å‘ç°â€ã€‚</p><h3 id="1-1-ç¤ºä¾‹åœºæ™¯"><a href="#1-1-ç¤ºä¾‹åœºæ™¯" class="headerlink" title="1.1 ç¤ºä¾‹åœºæ™¯"></a>1.1 ç¤ºä¾‹åœºæ™¯</h3><p>è®¾æœ‰ç”¨æˆ·Aï¼Œè¯¥ç”¨æˆ·åœ¨ç§»åŠ¨æ¥å…¥ç½‘ä¸­çš„è¡Œä¸ºæ¨¡å¼å¯ä»¥æ¦‚æ‹¬ä¸ºä»¥ä¸‹å‡ ç‚¹ï¼š</p><ol><li>ç”¨æˆ·Aå¯èƒ½ä¼šéšèŠ‚å‡æ—¥/å·¥ä½œæ—¥çš„è§„åˆ’ï¼Œæ¥å…¥åˆ°ä¸åŒåŒºåŸŸçš„ç½‘ç»œä¸­ï¼Œè€Œè¿™äº›åŒºåŸŸçš„åŸºç«™ç”±äºæ‰€å¤„ä½ç½®ä¸åŒï¼Œæ‰€èƒ½æä¾›çš„ç½‘ç»œæ€§èƒ½ä¹Ÿæœ‰å·®å¼‚ã€‚ä¾‹å¦‚ï¼šå¤§å‹å±…æ°‘åŒº/åŸå¸‚çƒ­ç‚¹åŒºåŸŸçš„åŸºç«™ä¸€èˆ¬å…·æœ‰è¾ƒå¤§çš„å®¹é‡ï¼ˆé‡æœåŠ¡ï¼‰ï¼Œè€ŒéƒŠåŒº/æˆ–éåŸå¸‚ä¸»å¹²åŒºåŸŸåˆ™ç›¸åï¼ˆé‡è¦†ç›–ï¼‰ã€‚å› æ­¤ç”¨æˆ·åœ¨ä¸Šè¿°åŒºåŸŸå†…ä½“éªŒåˆ°çš„ç½‘ç»œæœåŠ¡è´¨é‡ä¹Ÿæœ‰æ‰€å·®å¼‚ã€‚</li><li>ç”¨æˆ·Aåœ¨èŠ‚å‡æ—¥æˆ–å·¥ä½œæ—¥å†…ï¼Œå°†éšæœºæ¥å…¥ç”¨æˆ·æ‰€åœ¨åœ°é™„è¿‘çš„å°åŒºï¼Œè¿™äº›å°åŒºå°†ç»¼åˆè‡ªèº«çš„æœåŠ¡èƒ½åŠ›ã€ç”¨æˆ·çš„ä¸šåŠ¡è´¨é‡è¦æ±‚ã€ç”¨æˆ·æ•°æ®é‡ï¼Œæä¾›å·®å¼‚åŒ–çš„ç½‘ç»œæœåŠ¡ã€‚ç”¨æˆ·åœ¨æ­¤è¿‡ç¨‹ä¸­å¯æ„ŸçŸ¥åˆ°çš„ç½‘ç»œæœåŠ¡æ°´å¹³æŒ‡æ ‡é€šå¸¸ç”±â€œé€Ÿç‡ï¼ˆDLUserThrpAvgwithoutLastTTI(Mbps)ï¼‰â€åˆ»ç”»ã€‚</li></ol><h3 id="1-2-ç›®æ ‡"><a href="#1-2-ç›®æ ‡" class="headerlink" title="1.2 ç›®æ ‡"></a>1.2 ç›®æ ‡</h3><p>åˆ†æç”¨æˆ·çš„è¡Œä¸ºç‰¹å¾ï¼Œç›®æ ‡åœ¨äºå€ŸåŠ©å•ä¸ªç”¨æˆ·çš„ä½“éªŒåé¦ˆï¼Œæ‰¾åˆ°ï¼ˆé¢‘ç¹ï¼‰å½±å“ç”¨æˆ·ç½‘ç»œä½“éªŒçš„<strong>ç½‘ç»œé—®é¢˜</strong>ã€‚è¿™é‡Œï¼Œç½‘ç»œé—®é¢˜ä¸»è¦æŒ‡â€œæ¥å…¥ç½‘â€ä¸­å¯èƒ½å­˜åœ¨çš„æ•…éšœã€å‘Šè­¦ã€ä¸æ°å½“çš„ç»„ç½‘ç­–ç•¥ç­‰ã€‚</p><p>ç½‘ç»œé—®é¢˜å¸¸å¸¸è¡¨å¾åœ¨<strong>ç”¨æˆ·çº§KPIçš„å˜åŒ–</strong>ä¸Šï¼Œå› æ­¤ï¼Œåˆ†æç”¨æˆ·çº§KPIä¸Šçš„<strong>å¼‚å¸¸é“¾è·¯</strong>ï¼Œæ˜¯å¸®åŠ©æˆ‘ä»¬å‘ç°ç½‘ç»œé—®é¢˜çš„å¿…ç»é€”å¾„ã€‚è¿™é‡Œï¼Œå¼‚å¸¸é“¾è·¯æŒ‡ä¸€ç³»åˆ—æ—¶é—´ã€ç©ºé—´ã€å› æœä¸Šå…·æœ‰ç›¸å…³å…³ç³»çš„æŒ‡æ ‡å¼‚å¸¸ã€‚ä¾‹å¦‚ï¼šç”¨æˆ·TAå¼‚å¸¸â€”â€”ç”¨æˆ·RSRPå¼‚å¸¸â€”â€”MACå±‚æŒ‡æ ‡å¼‚å¸¸â€”â€”ç”¨æˆ·ä½“éªŒå¼‚å¸¸ã€‚åœ¨åˆ†æè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸ä»…<strong>å…³æ³¨ç”¨æˆ·çš„ä½“éªŒå¼‚å¸¸</strong>ï¼Œä¹Ÿå…³æ³¨<strong>æ•°ä¼ æŒ‡æ ‡çš„å¼‚å¸¸</strong>ï¼Œå› ä¸ºè¿™ä¸¤è€…éƒ½ä»£è¡¨äº†æ½œåœ¨çš„ç½‘ç»œé—®é¢˜ã€‚</p><p>ä¸‹å›¾å±•ç¤ºäº†åœ¨åŒä¸€å°åŒºå†…ï¼Œç”¨æˆ·çº§æŒ‡æ ‡ä¹‹é—´çš„æœºç†ï¼Œå…¶ä¸­å·¦æ¡†ä¸­ä¸ºèµ„æºå‹æŒ‡æ ‡ï¼Œå³æ¡†ä¸­ä¸ºæ•°ä¼ æŒ‡æ ‡ã€‚å®çº¿è¡¨ç¤ºæŒ‡æ ‡ä¹‹é—´çš„ç›¸äº’å½±å“å…³ç³»ï¼Œè™šçº¿åˆ™è¡¨ç¤ºâ€œæ•°ä¼ æŒ‡æ ‡å·®æ—¶ï¼Œå°åŒºå°†è°ƒç”¨æ›´å¤šçš„èµ„æºæ¥å®Œæˆç”¨æˆ·çš„è¯·æ±‚â€ã€‚å¾…å‘ç°çš„å¼‚å¸¸é“¾è·¯ï¼Œå³ä¸ºä¸‹å›¾ä¸­çš„KPIèŠ‚ç‚¹æ‰€è¿æ¥èµ·æ¥çš„ä¸€æ¡è·¯å¾„ã€‚</p><div align="center">  <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20220527195755138.png" alt="image-20220527195628244" width="70%" /></div><h3 id="1-3-å»ºæ¨¡æ€è·¯"><a href="#1-3-å»ºæ¨¡æ€è·¯" class="headerlink" title="1.3 å»ºæ¨¡æ€è·¯"></a>1.3 å»ºæ¨¡æ€è·¯</h3><p>å¦‚<a href="#%E7%A4%BA%E4%BE%8B%E5%9C%BA%E6%99%AF">2.1èŠ‚</a>æ‰€è¿°ï¼Œç”¨æˆ·çš„è¡Œä¸ºæ¨¡å¼ä¾ç…§æ—¶é—´çš„ç²’åº¦å¯ä»¥åˆ†ä¸ºä¸¤ç§æ¨¡å¼ï¼š</p><ul><li>ç²—ç²’åº¦æ¨¡å¼ï¼šç”¨æˆ·çš„è¡Œä¸ºæ¨¡å¼å¯èƒ½éšæ—¶åœ¨â€èŠ‚å‡æ—¥æ¨¡å¼â€œä¸â€å·¥ä½œæ¨¡å¼â€œä¸­åˆ‡æ¢ã€‚åœ¨æ¯ç§æ¨¡å¼åœ¨ï¼Œç”¨æˆ·çš„ä¸šåŠ¡éœ€æ±‚ã€ä¸šåŠ¡ç±»å‹ã€æ•°æ®ç±»å‹å°†æœ‰è¾ƒå¤§çš„å·®å¼‚ã€‚</li><li>ç»†ç²’åº¦æ¨¡å¼ï¼šç”¨æˆ·åœ¨æ¯ç§ç²—ç²’åº¦æ¨¡å¼ä¸‹äº«å—åˆ°çš„ç½‘ç»œæœåŠ¡ï¼Œä¹Ÿå°†å—å°åŒºçŠ¶æ€ã€å°åŒºèƒ½åŠ›ã€æ˜¼å¤œçš„å½±å“ï¼Œæœ‰è¾ƒå¤§çš„å·®å¼‚ã€‚</li></ul><p>å› æ­¤ï¼Œæœ¬ç ”ç©¶ä½¿ç”¨é©¬å°”å¯å¤«æœºåˆ¶è½¬æ¢æ¨¡å‹ï¼ˆMarkov regime switching modelï¼‰ä¸é©¬å°”å¯å¤«é“¾æ¨¡å‹ï¼ˆMarkov Chainï¼‰æ¥åˆ†åˆ«å»ºæ¨¡ç”¨æˆ·çš„ç²—ç²’åº¦æ¨¡å¼ä¸ç»†ç²’åº¦æ¨¡å¼ã€‚å…¶ä¸­ï¼Œç”¨æˆ·çš„æ¨¡å¼åˆ‡æ¢ï¼Œä»¥åŠæ‰€äº«å—åˆ°çš„ç½‘ç»œæœåŠ¡ï¼Œéƒ½è¢«è§†ä¸ºéšæœºå˜é‡ï¼Œç²—ç²’åº¦æ¨¡å¼è¢«è§†ä¸ºä¸€ç§regimeï¼Œå…¶è½¬æ¢å—Markov Chainæ‰€é©±åŠ¨ã€‚ç”¨æˆ·ä½“éªŒåˆ°çš„ç½‘ç»œæœåŠ¡åˆ™è¢«è§†ä¸ºç”±å‚æ•°ä¸åŒçš„å¦ä¸€ä¸ªMarkov Chainé©±åŠ¨çš„éšæœºå˜é‡ã€‚</p><h2 id="2-Regime-Switching-Model-amp-Markov-Switching-Model"><a href="#2-Regime-Switching-Model-amp-Markov-Switching-Model" class="headerlink" title="2. Regime Switching Model &amp; Markov Switching Model"></a>2. Regime Switching Model &amp; Markov Switching Model</h2><h3 id="2-1-Why-regime-switching-model"><a href="#2-1-Why-regime-switching-model" class="headerlink" title="2.1 Why regime switching model"></a>2.1 Why regime switching model</h3><p>æœºåˆ¶è½¬æ¢æ¨¡å‹ï¼ˆRegime switching modelï¼‰æ˜¯å¯¹éå¹³ç¨³ã€éçº¿æ€§æ—¶é—´åºåˆ—å»ºæ¨¡çš„ä¸€ç±»å¸¸è§æ¨¡å‹ã€‚æœºåˆ¶éš¾ä»¥è§‚æµ‹ã€ç”±éšæœºè¿‡ç¨‹æ‰€é©±åŠ¨ï¼Œè€Œæ¯ä¸€ç§æœºåˆ¶ä¸‹çš„æ—¶é—´åºåˆ—å¯ä»¥è§†ä¸ºç‹¬ç«‹çš„éšæœºè¿‡ç¨‹ï¼Œå…·æœ‰ä¸åŒçš„å‚æ•°<strong>ã€‚</strong>æœºåˆ¶è½¬æ¢æ¨¡å‹å¸¸è¢«ç”¨äºå»ºæ¨¡ç»æµä¸­çš„å‘¨æœŸæ€§å˜åŒ–å½±å“ã€‚</p><p>ä½¿ç”¨å•ä¸€çš„éšæœºè¿‡ç¨‹ï¼ˆä»¥é©¬å°”å¯å¤«è¿‡ç¨‹ä¸ºä¾‹ï¼‰å¯¹æ•°æ®åœ¨ä¸€æ®µæ—¶é—´å†…çš„å˜åŒ–è¿›è¡Œæè¿°æ—¶ï¼Œå…¶æ¦‚ç‡è½¬ç§»çŸ©é˜µé€šå¸¸æ˜¯å›ºå®šä¸å˜çš„ã€‚ä½†æ˜¯è¿™ä¸ªå‡è®¾å¯¹äºæˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œæ•°æ®ä¸­é‡åˆ°çš„è®¸å¤šé—®é¢˜å¹¶ä¸æ€»æ˜¯æœ‰æ•ˆçš„ï¼ŒåŸå› å¦‚ä¸‹ï¼š</p><ul><li>å®é™…æ—¶é—´åºåˆ—æ•°æ®åœ¨ä¸åŒæ—¶é—´æ®µï¼Œå¯èƒ½å…·æœ‰ä¸åŒç‰¹å¾ï¼Œä¾‹å¦‚å‡å€¼å’Œæ–¹å·®ï¼›</li><li>å¯¹äºå…·æœ‰ä¸åŒç‰¹å¾çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œå…¶åœ¨ä¸åŒç‰¹å¾æ—¶æœŸçš„æ¨¡å‹å‚æ•°ï¼ˆçŠ¶æ€ç©ºé—´ã€æ¦‚ç‡è½¬ç§»çŸ©é˜µç­‰ï¼‰å¯èƒ½ä¸ä¸€æ ·ï¼Œå¦‚è‚¡å¸‚åœ¨å¹³ç¨³æœŸã€éœ‡è¡æœŸï¼ˆå®½å¹…éœ‡è¡ã€çª„å¹…éœ‡è¡ï¼‰ï¼›</li></ul><p>å› æ­¤ï¼Œ<strong>æœºåˆ¶è½¬æ¢æ¨¡å‹</strong>å¯ä»¥è®¤ä¸ºæ˜¯<strong>æœ€æ¥è¿‘å®é™…é—®é¢˜çš„ç†è®ºæ¨¡å‹</strong>ï¼Œæ˜¯å¯¹çœŸå®ç³»ç»Ÿçš„ä¸€ç§è¿‘ä¼¼ï¼Œå¹¶ä¸”è¯¥ç†è®ºæ¨¡å‹çš„å¯ä»¥æŒ‡å¯¼æˆ‘ä»¬è®¾è®¡è§£å†³å®é™…é—®é¢˜çš„æ–¹æ¡ˆã€‚</p><h3 id="2-2-Application-of-regime-switching-model-in-wireless-access-network"><a href="#2-2-Application-of-regime-switching-model-in-wireless-access-network" class="headerlink" title="2.2 Application of regime switching model in wireless access network"></a>2.2 Application of regime switching model in wireless access network</h3><p>åœ¨æ— çº¿ç½‘ç»œä¸­ï¼Œç”±äºå„å°åŒºç¯å¢ƒåŠåŸºç«™é…ç½®ä¸åŒï¼Œç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡çš„å˜åŒ–æ¨¡å¼ä¹Ÿä¸åŒï¼Œå…·æœ‰<strong>éå¹³ç¨³æ€§</strong>ï¼Œä¸”ç”¨æˆ·çš„<strong>ç§»åŠ¨</strong>ä¼šå¯¼è‡´å°åŒºåˆ‡æ¢ã€åŒæ—¶å°åŒºä¹Ÿå­˜åœ¨<strong>çŠ¶æ€é—´åˆ‡æ¢</strong>ã€‚ç›¸æ¯”ä¼ ç»Ÿæ¨¡å‹ï¼Œ<strong>æœºåˆ¶è½¬æ¢æ¨¡å‹</strong>é€šè¿‡ä»¥ä¸‹è®¾å®šï¼Œå¯ä»¥æ›´å¥½åœ°åˆ»ç”»éå¹³ç¨³çš„æ—¶é—´åºåˆ—ï¼Œå¹¶æ•æ‰ç°å®ä¸–ç•Œæ•°æ®çš„çœŸå®è¡Œä¸ºã€‚</p><ul><li>å°†æ•°æ®æè¿°ä¸ºå±äºä¸åŒçš„ã€é‡å¤å‡ºç°çš„çŠ¶æ€ï¼ˆregimeï¼‰</li><li>å…è®¸æ—¶é—´åºåˆ—æ•°æ®çš„ç‰¹å¾ï¼ˆä¾‹å¦‚å‡å€¼ã€æ–¹å·®å’Œæ¨¡å‹å‚æ•°ç­‰ï¼‰åœ¨ä¸åŒçŠ¶æ€ä¸‹å‘ç”Ÿæ”¹å˜</li><li>å‡è®¾åœ¨ä»»ä½•ç»™å®šçš„æ—¶é—´æ®µå†…ï¼Œåºåˆ—æ•°æ®éƒ½å¯èƒ½å¤„äºä»»ä½•ä¸€ç§çŠ¶æ€å¹¶å¯èƒ½è¿‡æ¸¡åˆ°ä¸åŒçš„çŠ¶æ€</li></ul><h3 id="2-3-Markov-Switching-Model"><a href="#2-3-Markov-Switching-Model" class="headerlink" title="2.3 Markov Switching Model"></a>2.3 Markov Switching Model</h3><p>ç”±ä¸ŠèŠ‚å¯ä»¥çŸ¥é“ï¼Œregime switching modelä¹Ÿæ˜¯å—éšæœºè¿‡ç¨‹é©±åŠ¨çš„ï¼Œå¦‚æœè¿™ä¸ªè¿‡ç¨‹æ˜¯ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œé‚£ä¹ˆæˆ‘ä»¬ç§°ä¹‹ä¸º Markov switching modelã€‚</p><p>ç»¼ä¸Šï¼Œåœ¨æœ¬æ¡ˆä¾‹ï¼ˆå³ç”¨æˆ·è¡Œä¸ºå»ºæ¨¡ï¼‰ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸¤ä¸ªå±‚é¢çš„é©¬å°”å¯å¤«é“¾æ¥ä»ç”¨æˆ·çº§CHRæ•°æ®å»ºæ¨¡ç”¨æˆ·è¡Œä¸ºï¼š</p><ul><li>Markov Chain 1ï¼šé©±åŠ¨regimeçš„éšæœºå˜æ¢ã€‚</li><li>Markov Chain 2ï¼šé©±åŠ¨æŸä¸ªregimeå†…çš„ç”¨æˆ·ä½“éªŒå˜åŒ–ä¸æ•°ä¼ æŒ‡æ ‡å˜åŒ–ã€‚</li></ul><h2 id="3-Markov-Chain-Monte-Carlo-MCMC"><a href="#3-Markov-Chain-Monte-Carlo-MCMC" class="headerlink" title="3. Markov Chain Monte Carlo (MCMC)"></a>3. Markov Chain Monte Carlo (MCMC)</h2><p>MCMCç”±ä¸¤ä¸ªMCç»„æˆï¼Œå³è’™ç‰¹å¡ç½—æ–¹æ³•ï¼ˆMonte Carlo Simulationï¼Œç®€ç§°MCï¼‰å’Œé©¬å°”ç§‘å¤«é“¾ï¼ˆMarkov Chain ï¼Œä¹Ÿç®€ç§°MCï¼‰ã€‚MCMCç®—æ³•çš„ç›®çš„åœ¨äºâ€œ<strong>åœ¨æ¦‚ç‡ç©ºé—´ï¼Œç”¨éšæœºé‡‡æ ·çš„æ€è·¯ï¼Œä¼°è®¡æ¦‚ç‡åéªŒåˆ†å¸ƒ</strong>â€ã€‚</p><p>è¿™é‡Œéœ€è¦é˜æ˜å®é™…é—®é¢˜ä¸­çš„å‡ ä¸ªæ¦‚å¿µï¼šâ€œå…ˆéªŒåˆ†å¸ƒï¼ˆprior distributionï¼‰â€ã€â€œå¯èƒ½æ€§åˆ†å¸ƒï¼ˆlikelihood distributionï¼‰â€ã€â€œåéªŒåˆ†å¸ƒï¼ˆPosterior distributionï¼‰â€</p><ul><li>Priorï¼šå…ˆéªŒåˆ†å¸ƒï¼Œä»£è¡¨äº†åœ¨æœªçŸ¥çœŸå®æ•°æ®çš„æƒ…å†µä¸‹ï¼Œäººä»¬å¯¹äºè¯¥æ•°æ®åˆ†å¸ƒåšå‡ºçš„å…ˆè¡Œå‡è®¾ã€‚å…ˆéªŒåˆ†å¸ƒä¹Ÿè¢«ç§°ä¸ºä¿¡å¿µï¼ˆbriefï¼‰åˆ†å¸ƒï¼Œå› ä¸ºå®ƒæŒ‡æ˜äº†äººä»¬åœ¨æœªçŸ¥çœŸå®æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¯¹æ•°æ®åˆ†å¸ƒçš„ä¿¡å¿µã€‚</li><li>Likelihoodï¼šä¼¼ç„¶åˆ†å¸ƒï¼Œè¡¨ç¤ºåœ¨å·²çŸ¥çœŸå®æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ªè§‚æµ‹æ•°æ®å‡ºç°çš„å¯èƒ½æ€§ï¼Œå®ƒæ€»ç»“äº†å·²è§‚æµ‹æ•°æ®çš„ç»Ÿè®¡æ„ä¹‰ã€‚ä¸ä¹‹ç›¸å…³çš„maximum likelihood estimationï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰å›ç­”äº†ä¸€ä¸ªé—®é¢˜ï¼šæ€æ ·çš„æ ·æœ¬å€¼èŒƒå›´æ‰æœ€æœ‰å¯èƒ½è®©æˆ‘ä»¬é‡‡æ ·åˆ°å·²ç»è§‚å¯Ÿè¿‡çš„æ•°æ®ï¼Ÿè¯¥é—®é¢˜åœ¨æ²¡æœ‰å…ˆéªŒåˆ†å¸ƒçš„æƒ…å†µä¸‹æ—¶æ²¡æœ‰æ„ä¹‰çš„ã€‚</li><li>Posteriorï¼šåéªŒåˆ†å¸ƒï¼Œè¿™æ˜¯è´å¶æ–¯åˆ†æçš„æœ€ç»ˆç›®æ ‡ã€‚æ—¨åœ¨ç»¼åˆprior distributionï¼ˆå…ˆéªŒçš„ä¿¡å¿µï¼‰ä¸likelihood distributionï¼ˆå®é™…çš„è§‚å¯Ÿï¼‰ï¼Œæ¥æ¨æ–­æ•°æ®çš„çœŸå®åˆ†å¸ƒã€‚</li></ul><p>åœ¨<a href="#ref1">[1]</a>ä¸­ç»™å‡ºäº†ä¸€ä¸ªç®€æ˜çš„æ¡ˆä¾‹ï¼Œç”¨äºè¯´æ˜ä¸‰è€…ä¹‹é—´çš„å…³ç³»ã€‚</p><blockquote><div align="center">  <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20220527204614539.png" alt="image-20220527204614539" width="60%" /></div><p>ä¸Šå›¾ä¸­ï¼Œçº¢è‰²çš„æ›²çº¿ä»£è¡¨åéªŒåˆ†å¸ƒã€‚ä½ å¯ä»¥æŠŠå®ƒçœ‹ä½œå…ˆéªŒåˆ†å¸ƒå’Œå¯èƒ½æ€§åˆ†å¸ƒçš„å¹³å‡å€¼ã€‚ç”±äºå…ˆéªŒåˆ†å¸ƒè¾ƒçŸ­ä¸”åˆ†æ•£ï¼Œæ‰€ä»¥å®ƒåæ˜ äº†äººä»¬å¹¶ä¸å¤ªç¡®å®šäººç±»çš„å¹³å‡èº«é«˜æ˜¯å¤šå°‘ã€‚åŒæ—¶ï¼Œå¯èƒ½æ€§åœ¨ç›¸å¯¹è¾ƒçª„çš„èŒƒå›´å†…æ±‡æ€»äº†æ•°æ®ï¼Œå› æ­¤å®ƒå¯¹çœŸæ˜¯å‚æ•°å€¼æ›´åŠ ç¡®å®šã€‚</p><p>å½“å…ˆéªŒåˆ†å¸ƒå’Œå¯èƒ½æ€§åˆ†å¸ƒè¢«åˆå¹¶æ—¶ï¼Œæ•°æ®ï¼ˆç”±å¯èƒ½æ€§è¡¨ç¤ºï¼‰æ”¯é…äº†ä¹‹å‰é‚£ä¸ªåœ¨å·¨äººå †é‡Œé•¿å¤§çš„äººçš„å¼±å…ˆéªŒä¿¡å¿µã€‚å°½ç®¡é‚£ä¸€ä¸ªä½“ä»ç„¶è®¤ä¸ºäººç±»å¹³å‡èº«é«˜æ¯”æ•°æ®å‘Šè¯‰ä»–çš„ç¨é«˜äº›ï¼Œä½†ä»–æœ€ç›¸ä¿¡çš„æ˜¯æ•°æ®ã€‚</p></blockquote><h3 id="3-1-ç›®æ ‡"><a href="#3-1-ç›®æ ‡" class="headerlink" title="3.1 ç›®æ ‡"></a>3.1 ç›®æ ‡</h3><p>åœ¨å›é¡¾äº†ä¸Šè¿°æ¦‚å¿µåï¼Œæˆ‘ä»¬å¯ä»¥è½»æ˜“çš„è¯´æ˜ï¼ŒMCMCç®—æ³•å¦‚ä½•æœåŠ¡äºå»ºæ¨¡ç”¨æˆ·è¡Œä¸ºæ¨¡å¼ã€‚</p><p>MCMCç®—æ³•è§£å†³åéªŒæ¦‚ç‡åˆ†å¸ƒä¼°è®¡é—®é¢˜çš„æ€è·¯ä¸ºï¼š</p><ol><li>å¯¹äºå¤æ‚çš„é—®é¢˜ï¼Œæˆ‘ä»¬éƒ½é‡‡ç”¨<strong>è’™ç‰¹å¡æ´›æ³•</strong>ï¼Œå³é‡‡ç”¨éšæœºé‡‡æ ·çš„ç­–ç•¥ï¼Œä¼°è®¡æ ·æœ¬çš„åéªŒåˆ†å¸ƒ$p(x)$ <a href="#ref_liu_montecarlo">[2]</a>ã€‚</li><li>å¯¹äºå¤æ‚çš„æ¦‚ç‡åˆ†å¸ƒ$p(x)$ï¼Œæˆ‘ä»¬å¾€å¾€æ— æ³•ç›´æ¥å¯¹å…¶è¿›è¡Œé‡‡æ ·ï¼Œæ­¤æ—¶æˆ‘ä»¬å°†å¼•å…¥â€œ<strong>æ¥å—-æ‹’ç»é‡‡æ ·</strong>â€ï¼Œå³è®¾å®šä¸€ä¸ªå®¹æ˜“é‡‡æ ·çš„åˆ†å¸ƒ$q(x)$ï¼ˆé€šå¸¸ä¸ºé«˜æ–¯åˆ†å¸ƒï¼‰ï¼Œç„¶åæŒ‰ç…§ä¸€å®šçš„æ–¹æ³•æ‹’ç»æŸäº›æ ·æœ¬ï¼Œä»¥è¾¾åˆ°æ¥è¿‘$p(x)$åˆ†å¸ƒçš„ç›®çš„ <a href="#ref_liu_montecarlo">[2]</a>ã€‚</li><li>å¯¹äºé«˜ç»´åˆ†å¸ƒï¼Œæˆ‘ä»¬ä¸€èˆ¬åªèƒ½å¾—åˆ°æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œéè”åˆæ¦‚ç‡åˆ†å¸ƒï¼Œè¿™æ—¶æˆ‘ä»¬æ— æ³•ä½¿ç”¨â€œæ¥å—-æ‹’ç»é‡‡æ ·â€ï¼Œå¦å¤–ï¼Œå¯¹äºé«˜ç»´åˆ†å¸ƒï¼Œå¯»æ‰¾åˆé€‚çš„$q(x)$ä¹Ÿå¾ˆå›°éš¾ <a href="#ref_liu_montecarlo">[2]</a>ã€‚</li><li>ä½†å¦‚æœæˆ‘ä»¬è¦å»ºæ¨¡çš„åéªŒåˆ†å¸ƒæ˜¯ä¸€ä¸ªéšæœºè¿‡ç¨‹ï¼Œä¸”è¯¥éšæœºè¿‡ç¨‹æ˜¯<strong>é©¬å°”å¯å¤«é“¾</strong>ï¼Œé©¬å°”ç§‘å¤«é“¾æ¨¡å‹ä¸­çŠ¶æ€è½¬ç§»çŸ©é˜µ$\mathbf{P}$çš„æ€§è´¨å¯ä»¥è®©æˆ‘ä»¬<strong>ç”±ä»»æ„ä¸€ä¸ªåˆå§‹åˆ†å¸ƒå¼€å§‹ï¼Œæ¨æ–­å‡º$n$æ¬¡çŠ¶æ€è½¬ç§»åæ”¶æ•›äº†çš„å¹³ç¨³åéªŒåˆ†å¸ƒ</strong>  <a href="#ref_liu_markov">[3]</a>ã€‚</li><li>ç„¶è€Œï¼Œç»™å®šä¸€ä¸ªå¹³ç¨³çš„åˆ†å¸ƒï¼Œé©¬å°”å¯å¤«é“¾ä¸­çš„è½¬ç§»çŸ©é˜µ$\mathbf{P}$è¿˜æ˜¯æ— æ³•å¾—åˆ°ï¼Œå› æ­¤å¯ä»¥å¼•å…¥<strong>MCMCé‡‡æ ·</strong>ï¼Œè¯¥é‡‡æ ·æ–¹æ³•ä½¿å¾—ç›®æ ‡çŸ©é˜µ$\mathbf{P}(i,j)$å¯ä»¥ç”±é€šè¿‡ä»»æ„ä¸€ä¸ªé©¬å°”ç§‘å¤«é“¾çŠ¶æ€è½¬ç§»çŸ©é˜µ$\mathbf{Q}$ä¹˜ä»¥$\alpha(i,j)$å¾—åˆ°ï¼Œå³$\mathbf{P}(i,j)=\mathbf{Q}(i,j)\alpha(i,j)$ã€‚$\alpha(i,j)$ä¸€èˆ¬ç§°ä¹‹ä¸ºæ¥å—ç‡ã€‚å–å€¼åœ¨$[0,1]$ä¹‹é—´ï¼Œå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ¦‚ç‡å€¼ <a href="#ref_liu_mcmc">[4]</a>ã€‚</li><li>åç»­çš„ï¼Œ<strong>Metropolis-Hastingsé‡‡æ ·</strong>ä¸<strong>Gibbsé‡‡æ ·</strong>éƒ½æ˜¯åœ¨MCMCé‡‡æ ·çš„åŸºç¡€ä¸Šï¼Œé’ˆå¯¹è¿ç®—å¤æ‚åº¦ã€é«˜ç»´åˆ†å¸ƒæ‹“å±•æ€§æ‰€åšå‡ºçš„ç®—æ³•æ”¹è¿›ã€‚éœ€è¦æŒ‡å‡ºçš„æ˜¯ï¼šGibbsé‡‡æ ·åœ¨é«˜ç»´ç‰¹å¾æ—¶æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œå› æ­¤é€šå¸¸æ„ä¹‰ä¸Šçš„MCMCé‡‡æ ·éƒ½æ˜¯ç”¨çš„Gibbsé‡‡æ ·ã€‚å½“ç„¶Gibbsé‡‡æ ·æ˜¯ä»M-Hé‡‡æ ·çš„åŸºç¡€ä¸Šçš„è¿›åŒ–è€Œæ¥çš„ï¼ŒåŒæ—¶Gibbsé‡‡æ ·è¦æ±‚æ•°æ®è‡³å°‘æœ‰ä¸¤ä¸ªç»´åº¦ï¼Œä¸€ç»´æ¦‚ç‡åˆ†å¸ƒçš„é‡‡æ ·æ˜¯æ²¡æ³•ç”¨Gibbsé‡‡æ ·çš„,è¿™æ—¶M-Hé‡‡æ ·ä»ç„¶æˆç«‹  <a href="#ref_liu_mcmc">[4]</a><a href="#ref_liu_gibbis">[5]</a>ã€‚</li></ol><p>å»ºæ¨¡ç”¨æˆ·è¡Œä¸ºæ¨¡å¼çš„ç›®æ ‡åœ¨äºé¢„æµ‹ç”¨æˆ·åœ¨ä»»æ„æ—¶åˆ»å¤„äºæŸç§çŠ¶æ€çš„å¯èƒ½æ€§ï¼Œè‹¥ç”¨æˆ·çš„æ¨¡å¼å¯ä»¥ç”¨é©¬å°”å¯å¤«é“¾æ¥è¡¨ç¤ºï¼Œåˆ™ç”¨æˆ·åœ¨æ—¶åˆ»$t$çš„çŠ¶æ€$s(t)$å°†ä»…ä¸çŠ¶æ€è½¬ç§»çŸ©é˜µ$\mathbf{P}$å’Œå‰ä¸€ä¸ªæ—¶åˆ»çš„çŠ¶æ€$s(t-1)$æœ‰å…³ã€‚å› æ­¤ï¼Œæ¨æ–­çŠ¶æ€è½¬ç§»çŸ©é˜µ$\mathbf{P}$ï¼Œå°±æ˜¯å»ºæ¨¡ç”¨æˆ·è¡Œä¸ºçš„é‡è¦åŸºç¡€ã€‚å›é¡¾ä¸Šè¿°çš„MCMCç®—æ³•è„‰ç»œï¼Œå¯ä»¥çœ‹å‡ºMCMCé‡‡æ ·è§£å†³äº†ä¼°è®¡çŠ¶æ€è½¬ç§»çŸ©é˜µ$\mathbf{P}$çš„é—®é¢˜ã€‚å› æ­¤ï¼ŒMCMCç®—æ³•å¯ä»¥è¢«æ‹†è§£ï¼Œç”¨äºæˆ‘ä»¬å®é™…çš„ç”¨æˆ·è¡Œä¸ºå»ºæ¨¡é—®é¢˜ä¸Šã€‚</p><p>ã€ä¸´æ—¶ç¬”è®°ã€‘</p><p>æˆ‘ä»¬å¯ä»¥ä»é‚£äº›æ–¹é¢æ¥æ„é€ regime</p><ol><li>ä»ç”¨æˆ·ä½“éªŒå±‚é¢ï¼šé€šè¿‡è¯†åˆ«ä»£è¡¨ç”¨æˆ·ä½“éªŒçš„KPIæ‰€å±çš„regimeï¼Œæˆ‘ä»¬å¯ä»¥åˆ†æç”¨æˆ·åœ¨å“ªäº›æ—¶é—´æœ‰è¾ƒå·®çš„ç½‘ç»œä½“éªŒï¼Œåœ¨åˆ†ææä¾›äº†å·®ä½“éªŒçš„å°åŒºåï¼Œæˆ‘ä»¬ä¹Ÿè®¸å¯ä»¥åœ¨è¿™äº›å·®å°åŒºå†…æ„é€ å¤šå°åŒºä¹‹é—´çš„å¼‚å¸¸é“¾è·¯ã€‚</li><li>ä»å°åŒºäº¤ä»˜èƒ½åŠ›å±‚é¢ï¼šé€šè¿‡å°åŒºçš„å†å²æ€§èƒ½è®°å½•ï¼Œæˆ‘ä»¬å°†æ€§èƒ½ç›¸ä¼¼çš„å°åŒºå½’ä¸ºä¸€ä¸ªregime<ol><li>é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç›¸åŒregimeçš„å°åŒºä¸­åšå¼‚å¸¸æ£€æµ‹ï¼Œé¿å…ä¸€äº›å°å®¹é‡å°åŒºä¸Šçš„æ­£å¸¸KPIè¢«æ£€æµ‹ä¸ºå¼‚å¸¸ï¼Œä¹Ÿé¿å…å¤§å®¹é‡å°åŒºä¸Šçš„å¼‚å¸¸è¢«æ¼æ£€ã€‚</li><li>è¿™ç§æ–¹æ³•å°†ä¼šè¿‡æ»¤æ‰å› ä¸ºå°åŒºè®¾è®¡å®¹é‡å¯¼è‡´çš„ä½“éªŒå·®ï¼Œè€Œæ›´ä¸“æ³¨äºç½‘ç»œé—®é¢˜å‘ç°ä¸Šï¼Œå³ï¼šåœ¨åŒä¸€ä¸ªregimeæ£€æµ‹å‡ºçš„å¼‚å¸¸å°†æ›´ä»£è¡¨ç½‘ç»œé—®é¢˜ã€‚</li></ol></li></ol><h3 id="3-2-Monte-Carloæ³•"><a href="#3-2-Monte-Carloæ³•" class="headerlink" title="3.2 Monte Carloæ³•"></a>3.2 Monte Carloæ³•</h3><p>[To be continued]</p><h3 id="3-3-Markov-Chain-amp-çŠ¶æ€è½¬ç§»çŸ©é˜µçš„æ€§è´¨"><a href="#3-3-Markov-Chain-amp-çŠ¶æ€è½¬ç§»çŸ©é˜µçš„æ€§è´¨" class="headerlink" title="3.3 Markov Chain &amp; çŠ¶æ€è½¬ç§»çŸ©é˜µçš„æ€§è´¨"></a>3.3 Markov Chain &amp; çŠ¶æ€è½¬ç§»çŸ©é˜µçš„æ€§è´¨</h3><p>[To be continued]</p><h3 id="3-4-MCMCé‡‡æ ·"><a href="#3-4-MCMCé‡‡æ ·" class="headerlink" title="3.4 MCMCé‡‡æ ·"></a>3.4 MCMCé‡‡æ ·</h3><p>[To be continued]</p><h2 id="4-æ¡ˆä¾‹åˆ†æä¸å®ç°"><a href="#4-æ¡ˆä¾‹åˆ†æä¸å®ç°" class="headerlink" title="4. æ¡ˆä¾‹åˆ†æä¸å®ç°"></a>4. æ¡ˆä¾‹åˆ†æä¸å®ç°</h2><h3 id="4-1-å¼€æºä»£ç è°ƒç ”"><a href="#4-1-å¼€æºä»£ç è°ƒç ”" class="headerlink" title="4.1 å¼€æºä»£ç è°ƒç ”"></a>4.1 å¼€æºä»£ç è°ƒç ”</h3><ol><li><strong>statsmodels.tsa</strong>ï¼ˆæ—¶é—´åºåˆ—åˆ†æåŒ…ï¼Œgithub 7k starsï¼‰<a href="#ref_tsa">[6]</a></li></ol><div align="left">  <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20220530095613227.png" alt="image-20220530095613227" width="80%" /></div><p>å…³é”®å‚æ•°è¯´æ˜ï¼š</p><ul><li><code>k_regimes</code>: è®¾å®šçš„regimeä¸ªæ•°ï¼Œä½œä¸ºè¶…å‚æ•°ï¼›</li></ul><ol start="2"><li><strong>Pythonç‰ˆ regime-switching model</strong>ï¼ˆgithub 9 starsï¼‰<a href="#ref_py_regime">[7]</a></li></ol><div align="left">  <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20220530095819553.png" alt="image-20220530095819553" width="40%" /></div><p>å…³é”®å‚æ•°è¯´æ˜ï¼š</p><ul><li><code>n_components</code>: è®¾å®šçš„regimeä¸ªæ•°ï¼Œä½œä¸ºè¶…å‚æ•°ï¼›</li></ul><ol start="3"><li><strong>Matlabç‰ˆ regime-switching modelï¼ˆ</strong>github 36 starsï¼‰</li></ol><div align="left">  <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20220530100045914.png" alt="image-20220530100045914" width="80%" /></div><p>ç‰¹æ€§è¯´æ˜ï¼š</p><ul><li>æ”¯æŒå•ç»´å’Œé«˜ç»´æ—¶é—´åºåˆ—çš„å»ºæ¨¡ï¼›</li><li>å¯é€‰æ‹©æ¨¡å‹ä¸­çš„å“ªäº›å‚æ•°éšæ—¶é—´åˆ‡æ¢çŠ¶æ€ï¼›</li><li>æ”¯æŒä»»æ„æ•°é‡çš„regimeè®¾ç½®ä¸å¯è§£é‡Šå˜é‡ï¼›</li></ul><p>æœ¬èŠ‚é¸£è°¢Tongji DNA Labçš„Chengbo QiuåŒå­¦ã€‚</p><h3 id="4-2-æ¡ˆä¾‹åˆ†æ"><a href="#4-2-æ¡ˆä¾‹åˆ†æ" class="headerlink" title="4.2 æ¡ˆä¾‹åˆ†æ"></a>4.2 æ¡ˆä¾‹åˆ†æ</h3><p>[To be continued]</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a id="ref1">[1]</a> <a href="https://zhuanlan.zhihu.com/p/32982140">https://zhuanlan.zhihu.com/p/32982140</a></p><p><a id="ref_liu_montecarlo">[2]</a> <a href="https://www.cnblogs.com/pinard/p/6625739.html">https://www.cnblogs.com/pinard/p/6625739.html</a></p><p><a id="ref_liu_markov">[3]</a> <a href="https://www.cnblogs.com/pinard/p/6632399.html">https://www.cnblogs.com/pinard/p/6632399.html</a></p><p><a id="ref_liu_mcmc">[4]</a> <a href="https://www.cnblogs.com/pinard/p/6638955.html">https://www.cnblogs.com/pinard/p/6638955.html</a></p><p><a id="ref_liu_gibbis">[5]</a> <a href="https://www.cnblogs.com/pinard/p/6645766.html">https://www.cnblogs.com/pinard/p/6645766.html</a></p><p><a id="ref_tsa">[6]</a> Kim, Chang-Jin, and Charles R. Nelson. 1999. â€œState-Space Models with Regime Switching: Classical and Gibbs-Sampling Approaches with Applicationsâ€. MIT Press Books. The MIT Press.</p><p><a id="ref_py_regime">[7]</a> Ma, Ying, Leonard MacLean, Kuan Xu, and Yonggan Zhao. â€œA portfolio optimization model with regime-switching risk factors for sector exchange traded funds.â€ Pac J Optim 7, no. 2 (2011): 281-296.</p>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> Bayesian </tag>
            
            <tag> Monte Carlo </tag>
            
            <tag> Markov </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Probabilistic Graphical Model</title>
      <link href="/uncategorized/notes/graph_model_probability/"/>
      <url>/uncategorized/notes/graph_model_probability/</url>
      
        <content type="html"><![CDATA[<p>æœ¬æ–‡ä¸»è¦æ¢³ç†å»ºç«‹åœ¨æ¦‚ç‡å›¾æ¨¡å‹ä¸Šçš„å¤šç§æ¦‚å¿µï¼ŒåŠå…¶ç›¸å…³çš„ç®—æ³•ã€‚</p><span id="more"></span><h2 id="ç›®å½•"><a href="#ç›®å½•" class="headerlink" title="ç›®å½•"></a>ç›®å½•</h2><p>ç›®å‰åŒ…å«çš„æ¦‚å¿µ/ç®—æ³•å¦‚ä¸‹ï¼š</p><p><strong>æ¦‚å¿µ</strong>ï¼š</p><ul><li>æ¦‚ç‡å›¾æ¨¡å‹<ul><li>è´å¶æ–¯ç½‘ç»œ (Bayes Model) </li><li>é©¬å°”å¯å¤«éšæœºåœº (Markov Random Field) </li><li>å› å­å›¾ (Factor Graph) </li></ul></li></ul><p><strong>ç®—æ³•</strong>ï¼š</p><ul><li>æ¶ˆæ¯ä¼ é€’ç®—æ³• (message passing algorithm) <ul><li>ç½®ä¿¡ä¼ æ’­ (Belief propagation) </li></ul></li></ul><h2 id="1-æ¦‚ç‡å›¾æ¨¡å‹"><a href="#1-æ¦‚ç‡å›¾æ¨¡å‹" class="headerlink" title="1. æ¦‚ç‡å›¾æ¨¡å‹"></a>1. æ¦‚ç‡å›¾æ¨¡å‹</h2><p>æ¦‚ç‡å›¾æ¨¡å‹ (Probabilistic Graphical Model) æ˜¯ä¸€ç§è¡¨ç¤ºéšæœºå˜é‡ä¹‹é—´æ¡ä»¶ä¾èµ–çš„ç»“æ„ã€‚ä½¿ç”¨æ¦‚ç‡å›¾æ¨¡å‹çš„åŠ¨æœºåœ¨äºâ€”â€”åˆ©ç”¨åŸºäºå›¾çš„è¡¨ç¤ºå¯¹å¤šç»´ç©ºé—´ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œç¼–ç ã€‚å¯¹äºç‰¹å®šåˆ†å¸ƒä¸­çš„ä¸€ç»„ç‹¬ç«‹æ€§ (independency) ï¼Œå›¾ç»“æ„æ˜¯è¿™ç§ç‹¬ç«‹æ€§çš„<strong>ç´§å‡‘</strong>è¡¨ç¤ºã€æˆ–<strong>åˆ†è§£</strong>è¡¨ç¤º<a href="#graphical_model">[1]</a>ã€‚</p><p>å³ï¼š</p><ul><li>å¯¹äºä¸€ä¸ªå¤šå…ƒçš„åˆ†å¸ƒï¼Œå¯ä»¥é€šè¿‡ä¸åŒçš„è¯±å¯¼æ–¹å¼å°†å…¶è¡¨å¾ä¸ºå›¾æ¨¡å‹ã€‚</li><li>ä¸€ä¸ªå¤šå…ƒåˆ†å¸ƒå¯ä»¥æ ¹æ®å·²æœ‰çš„æ¦‚ç‡å›¾æ¨¡å‹ä¸Šçš„è¿æ¥ï¼Œåšå…³ç³»åˆ†è§£ã€‚</li></ul><p>æ¦‚ç‡å›¾æ¨¡å‹çš„<strong>åˆ†ç±»</strong>å¦‚ä¸‹ (å°šä¸å®Œæ•´) ï¼š</p><ul><li>æœ‰å‘å›¾æ¨¡å‹<ul><li>æœ‰å‘æ— ç¯å›¾ (è´å¶æ–¯ç½‘ç»œï¼ŒBayesian Network) </li><li>å¾ªç¯æœ‰å‘å›¾æ¨¡å‹</li></ul></li><li>æ— å‘å›¾æ¨¡å‹ (éšæœºåœºæ¨¡å‹) <ul><li>é©¬å°”å¯å¤«éšæœºåœº (Markov Random Field) </li><li>æ¡ä»¶éšæœºåœº (Conditional Random fields) </li></ul></li><li>å› å­å›¾ (Factor Graph) </li></ul><h3 id="1-1-è´å¶æ–¯ç½‘ç»œ-Bayesian-Model"><a href="#1-1-è´å¶æ–¯ç½‘ç»œ-Bayesian-Model" class="headerlink" title="1.1 è´å¶æ–¯ç½‘ç»œ (Bayesian Model)"></a>1.1 è´å¶æ–¯ç½‘ç»œ (Bayesian Model)</h3><p><strong>åˆå</strong>ï¼šBayes network, Bayes net, belief network, or decision network</p><p><strong>è´å¶æ–¯ç½‘ç»œ</strong>æ˜¯ä¸€ç§æ¦‚ç‡å›¾å½¢æ¨¡å‹ï¼Œå®ƒé€šè¿‡<strong>æœ‰å‘æ— ç¯å›¾</strong> (Directed Acyclic Graph) è¡¨ç¤ºä¸€ç»„å˜é‡åŠå…¶æ¡ä»¶ä¾èµ–å…³ç³»ã€‚</p><p>è´å¶æ–¯ç½‘ç»œéå¸¸é€‚åˆç”¨äºè·å–å·²å‘ç”Ÿçš„äº‹ä»¶å¹¶é¢„æµ‹å‡ ç§å¯èƒ½çš„å·²çŸ¥åŸå› ä¸­çš„ä»»ä½•ä¸€ç§æ˜¯ä¿ƒæˆå› ç´ çš„å¯èƒ½æ€§ã€‚ä¾‹å¦‚ï¼Œè´å¶æ–¯ç½‘ç»œå¯ä»¥è¡¨ç¤ºç–¾ç—…å’Œç—‡çŠ¶ä¹‹é—´çš„æ¦‚ç‡å…³ç³»ã€‚ç»™å®šç—‡çŠ¶ï¼Œè¯¥ç½‘ç»œå¯ç”¨äºè®¡ç®—å„ç§ç–¾ç—…å­˜åœ¨çš„æ¦‚ç‡<a href="#bayesian_model">[2]</a>ã€‚</p><p>åœ¨å½¢å¼ä¸Šï¼Œè´å¶æ–¯ç½‘ç»œæ˜¯æœ‰å‘æ— ç¯å›¾ï¼Œå…¶èŠ‚ç‚¹ä¸è¾¹çš„å®šä¹‰å¦‚ä¸‹ï¼š</p><p><strong>èŠ‚ç‚¹</strong>ï¼šä»£è¡¨æ¦‚ç‡æ„ä¹‰ä¸Šçš„å˜é‡ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½ä¸ä¸€ä¸ªæ¦‚ç‡å‡½æ•°ç›¸å…³è” (ä»¥çˆ¶èŠ‚ç‚¹å˜é‡ä¸ºæ¡ä»¶/è¾“å…¥ï¼Œè¯¥èŠ‚ç‚¹ä¸ºè¾“å‡º) ã€‚å˜é‡ç±»å‹åŒ…æ‹¬</p><ul><li>å¯è§‚å¯Ÿçš„å˜é‡</li><li>æ½œåœ¨å˜é‡</li><li>æœªçŸ¥çš„å‚æ•°æˆ–è€…å‡è®¾</li></ul><p><strong>è¾¹</strong>ï¼šä»£è¡¨æ¡ä»¶ä¾èµ–ï¼Œæœªè¢«è¿æ¥çš„èŠ‚ç‚¹è¡¨ç¤º<strong>æ¡ä»¶ç‹¬ç«‹</strong>çš„å˜é‡ (ä¸¤èŠ‚ç‚¹ä¹‹é—´ä¸å­˜åœ¨ä»»æ„è·¯å¾„) </p><h3 id="1-2-é©¬å°”å¯å¤«éšæœºåœº-Markov-Random-Field"><a href="#1-2-é©¬å°”å¯å¤«éšæœºåœº-Markov-Random-Field" class="headerlink" title="1.2 é©¬å°”å¯å¤«éšæœºåœº (Markov Random Field)"></a>1.2 é©¬å°”å¯å¤«éšæœºåœº (Markov Random Field)</h3><p><strong>åˆå</strong>ï¼šMarkov Network</p><p><strong>é©¬å°”å¯å¤«éšæœºåœº</strong>æ˜¯ä¸€ç»„å…·æœ‰ç”±æ— å‘å›¾æè¿°çš„ï¼Œå…·æœ‰<strong>é©¬å°”å¯å¤«æ€§è´¨</strong>çš„éšæœºå˜é‡ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœéšæœºåœºæ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨ï¼Œåˆ™ç§°å…¶ä¸ºé©¬å°”å¯å¤«éšæœºåœºã€‚</p><p>é©¬å°”å¯å¤«ç½‘ç»œæˆ– MRF åœ¨å…¶ä¾èµ–å…³ç³»çš„è¡¨ç¤ºæ–¹é¢ç±»ä¼¼äºè´å¶æ–¯ç½‘ç»œï¼›åŒºåˆ«åœ¨äºè´å¶æ–¯ç½‘ç»œæ˜¯æœ‰å‘æ— ç¯çš„ï¼Œè€Œé©¬å°”å¯å¤«ç½‘ç»œæ˜¯<strong>æ— å‘</strong>çš„å¹¶ä¸”<strong>å¯èƒ½æ˜¯å¾ªç¯çš„</strong>ã€‚å› æ­¤ï¼Œé©¬å°”å¯å¤«ç½‘ç»œå¯ä»¥è¡¨ç¤ºè´å¶æ–¯ç½‘ç»œä¸èƒ½è¡¨ç¤ºçš„æŸäº›ä¾èµ–å…³ç³» (ä¾‹å¦‚å¾ªç¯ä¾èµ–å…³ç³»ï¼Œcyclic dependencies) ï¼›å¦ä¸€æ–¹é¢ï¼Œå®ƒä¸èƒ½è¡¨ç¤ºè´å¶æ–¯ç½‘ç»œå¯ä»¥è¡¨ç¤ºçš„æŸäº›ä¾èµ–å…³ç³» (ä¾‹å¦‚è¯±å¯¼ä¾èµ–å…³ç³»ï¼Œinduced dependencies) ã€‚é©¬å°”å¯å¤«éšæœºåœºçš„åŸºç¡€å›¾å¯èƒ½æ˜¯<strong>æœ‰é™çš„</strong>æˆ–<strong>æ— é™çš„</strong>ã€‚</p><h4 id="1-2-1-å®šä¹‰"><a href="#1-2-1-å®šä¹‰" class="headerlink" title="1.2.1 å®šä¹‰"></a>1.2.1 å®šä¹‰</h4><ol><li><p>é©¬å°”å¯å¤«æ€§è´¨</p><p> ç»™å®šä¸€ä¸ªæ— å‘å›¾$G=(V,E)$ï¼Œä¸€ç³»åˆ—éšæœºå˜é‡$X = (X_v)_{v \in V}$ï¼Œå½“éšæœºå˜é‡æ»¡è¶³ä»¥ä¸‹æ€§è´¨æ—¶ï¼Œæˆ‘ä»¬ç§°ä»–ä»¬ç›¸å¯¹äº$G$å½¢æˆäº†ä¸€ä¸ªé©¬å°”å¯å¤«éšæœºåœºã€‚</p><ol><li><strong>æˆå¯¹é©¬å°”å¯å¤«æ€§è´¨</strong> (Pairwise Markov property) ï¼šç»™å®šæ‰€æœ‰çš„å…¶ä»–å˜é‡ï¼Œä»»ä½•ä¸¤ä¸ªåœ¨å›¾ä¸Šä¸ç›¸é‚»çš„å˜é‡éƒ½æ˜¯æ¡ä»¶ç‹¬ç«‹çš„ã€‚å³ï¼š$X_{u} \perp X_{v} \mid X_{V \backslash{u, v}}$</li><li><strong>å±€éƒ¨é©¬å°”å¯å¤«æ€§è´¨</strong> (Local Markov property) ï¼šä¸€ä¸ªå˜é‡åœ¨ç»™å®šå…¶é‚»å±…çš„æƒ…å†µä¸‹ï¼Œæ¡ä»¶ç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–å˜é‡ã€‚å³ï¼š$X_{v} \perp X_{V \backslash \mathrm{N}[v]} \mid X_{\mathrm{N}(v)}$ï¼Œ$\mathrm{N}[v]$æ˜¯$v$çš„é‚»å±…é›†åˆï¼Œ$\mathrm{N}[v]=v \cup \mathrm{N}(v)$æ˜¯èŠ‚ç‚¹$v$çš„closed neighborhoodã€‚</li><li><strong>å…¨å±€é©¬å°”å¯å¤«æ€§è´¨</strong> (Global Markov property) ï¼šç»™å®šä¸€ä¸ªåˆ†ç¦»å­é›†ï¼Œä»»ä½•ä¸¤ä¸ªå˜é‡å­é›†éƒ½æ˜¯æ¡ä»¶ç‹¬ç«‹çš„ã€‚å³ï¼š$X_{A} \perp X_{B} \mid X_{S}$ï¼Œå…¶ä»»æ„ä»$A$ä¸­çš„èŠ‚ç‚¹åˆ°$B$ä¸­çš„èŠ‚ç‚¹çš„è·¯å¾„ï¼Œéƒ½ç»è¿‡å­é›†$S$ã€‚</li></ol><p> <strong>å±æ€§å¼ºå¼±</strong>ï¼šGlobal Markov property &gt; Local Markov property &gt; Pairwise Markov propertyã€‚ä¸Šè¿°ä¸‰ä¸ªé©¬å°”å¯å¤«æ€§è´¨å¯¹äºæ­£åˆ†å¸ƒ (é‚£äº›åªä¸ºç›¸å…³å˜é‡åˆ†é…éé›¶æ¦‚ç‡çš„åˆ†å¸ƒ) æ˜¯ç­‰ä»·çš„ã€‚</p></li><li><p>éšæœºåœºï¼šå½“ç»™å›¾ä¸­çš„æ¯ä¸€ä¸ªå˜é‡ï¼Œéƒ½æŒ‰ç…§æŸç§åˆ†å¸ƒéšæœºèµ‹äºˆç›¸ç©ºé—´ (phase space) çš„ä¸€ä¸ªå€¼ä¹‹åï¼Œå…¶æ„æˆçš„å…¨ä½“å°±å«åšéšæœºåœºã€‚</p></li></ol><h3 id="1-3-å› å­å›¾-Factor-Graph"><a href="#1-3-å› å­å›¾-Factor-Graph" class="headerlink" title="1.3 å› å­å›¾ (Factor Graph)"></a>1.3 å› å­å›¾ (Factor Graph)</h3><p>å› å­å›¾ (Factor Graph) æ˜¯è¡¨ç¤ºå‡½æ•°åˆ†è§£çš„äºŒåˆ†å›¾ã€‚åœ¨æ¦‚ç‡è®ºåŠå…¶åº”ç”¨ä¸­ï¼Œå› å­å›¾ç”¨äºè¡¨ç¤º<strong>æ¦‚ç‡åˆ†å¸ƒå‡½æ•°çš„å› å¼åˆ†è§£</strong>ï¼Œä»è€Œå®ç°é«˜æ•ˆè®¡ç®—ï¼Œä¾‹å¦‚é€šè¿‡sum-product ç®—æ³•è®¡ç®—è¾¹ç¼˜åˆ†å¸ƒã€‚</p><h4 id="1-3-1-å®šä¹‰"><a href="#1-3-1-å®šä¹‰" class="headerlink" title="1.3.1 å®šä¹‰"></a>1.3.1 å®šä¹‰</h4><p>å› å­å›¾æ˜¯ä¸€ç§äºŒéƒ¨å›¾ï¼Œç”¨äºè¡¨ç¤ºè”åˆæ¦‚ç‡åˆ†å¸ƒçš„å‡½æ•°åˆ†è§£ï¼Œä»¥å¦‚ä¸‹çš„è”åˆæ¦‚ç‡ä¸ºä¾‹ï¼š</p><p>$$g\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\prod_{j=1}^{m} f_{j}\left(S_{j}\right)$$</p>å…¶ä¸­$g\left(X_{1}, X_{2}, \ldots, X_{n}\right)$æ˜¯è”åˆæ¦‚ç‡åˆ†å¸ƒï¼Œé»˜è®¤ä¸ºå®å€¼å‡½æ•°ã€‚$S_j\subseteq\left\{X_{1}, X_{2}, \ldots, X_{n}\right\}$ï¼Œå³$S_j$æ˜¯å˜é‡èŠ‚ç‚¹çš„ä¸€ä¸ªå­é›†ã€‚<p>ä¸Šå¼å¯¹åº”çš„å› å­å›¾è¡¨ç¤ºä¸º$G = (X, F, E)$ï¼Œå…¶ä¸­$X={X_1, X_2, \cdots, X_n}$ï¼Œè¡¨ç¤ºå˜é‡èŠ‚ç‚¹æ„æˆçš„é›†åˆï¼›$F={f_1, f_2, \cdots f_m}$è¡¨ç¤ºå› å­èŠ‚ç‚¹æ„æˆçš„é›†åˆã€‚å› å­å›¾ä¸Šçš„è¾¹$E$å®šä¹‰åœ¨å› å­èŠ‚ç‚¹ä¸å˜é‡èŠ‚ç‚¹ä¹‹é—´ï¼Œå³å› å­èŠ‚ç‚¹$f_j$ä¸å˜é‡èŠ‚ç‚¹$X_k, X_k \in S_j$ä¹‹é—´å­˜åœ¨ä¸€æ¡æ— å‘çš„è¾¹ã€‚</p><p><strong>ä¸¾ä¾‹</strong>ï¼š</p><div width="100%" align="center"> <svg width="35%" viewBox="0 0 396 388" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">    <g id="é¡µé¢-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">        <g id="A4" transform="translate(-104.000000, -240.000000)">            <line x1="166.5" y1="596.5" x2="265.5" y2="596.5" id="ç›´çº¿" stroke="#979797" stroke-width="2" stroke-linecap="square"></line>            <line x1="333.5" y1="595.5" x2="435.5" y2="595.5" id="ç›´çº¿" stroke="#979797" stroke-width="2" stroke-linecap="square"></line>            <line x1="466.5" y1="473.5" x2="466.5" y2="565.5" id="ç›´çº¿" stroke="#979797" stroke-width="2" stroke-linecap="square"></line>            <line x1="466.5" y1="303.5" x2="466.5" y2="407.5" id="ç›´çº¿" stroke="#979797" stroke-width="2" stroke-linecap="square"></line>            <line x1="333.5" y1="272.5" x2="435.5" y2="272.5" id="ç›´çº¿" stroke="#979797" stroke-width="2" stroke-linecap="square"></line>            <line x1="320.5" y1="416.5" x2="435.5" y2="272.5" id="ç›´çº¿" stroke="#979797" stroke-width="2" stroke-linecap="square"></line>            <line x1="320.5" y1="465.5" x2="435.5" y2="595.5" id="ç›´çº¿" stroke="#979797" stroke-width="2" stroke-linecap="square"></line>            <g id="f1" opacity="0.597795759" transform="translate(265.000000, 240.000000)" stroke="#000000">                <ellipse id="æ¤­åœ†å½¢" stroke-width="2" cx="34" cy="32.5" rx="33" ry="31.5"></ellipse>                <g id="ç¼–ç»„" transform="translate(34.000000, 33.000000) scale(-1, 1) rotate(-180.000000) translate(-34.000000, -33.000000) translate(21.000000, 17.000000)" fill="#000000" fill-rule="nonzero">                    <path d="M2.22285853,1.51208791 C2.26990316,1.51208791 2.34047009,1.48864469 2.43455934,1.44175824 C2.52864859,1.39487179 2.65802131,1.35970696 2.8226775,1.33626374 C2.98733369,1.31282051 3.12846756,1.3010989 3.24607912,1.3010989 C3.55186919,1.3010989 3.83413694,1.45347985 4.09288237,1.75824176 C4.35162781,2.06300366 4.53980631,2.4029304 4.65741787,2.77802198 C4.892641,3.41098901 5.29252031,5.2043956 5.85705581,8.15824176 C6.42159131,11.1120879 6.9626045,13.9252747 7.48009537,16.5978022 C7.99758625,19.2703297 8.25633169,20.618315 8.25633169,20.6417582 L8.25633169,20.7472527 L6.59800866,20.7472527 C5.49245997,20.7472527 4.90440216,20.770696 4.83383522,20.8175824 C4.73974597,20.8879121 4.69270134,21.0051282 4.69270134,21.1692308 L4.93968562,22.1538462 C4.98673025,22.2710623 5.0808195,22.3296703 5.22195337,22.3296703 C5.36308725,22.3296703 5.91586159,22.3413919 6.88027641,22.3648352 C8.00934741,22.3648352 8.57388291,22.3765568 8.57388291,22.4 C8.57388291,22.4468864 8.67973331,23.032967 8.89143412,24.1582418 C9.10313494,25.2835165 9.24426881,25.96337 9.31483575,26.1978022 C10.1851613,30.0659341 11.9022901,32 14.4662222,32 C15.3365477,31.9531136 16.0539783,31.6952381 16.6185138,31.2263736 C17.1830493,30.7575092 17.465317,30.1362637 17.465317,29.3626374 C17.465317,28.5186813 17.2183327,27.9091575 16.7243642,27.5340659 C16.2303956,27.1589744 15.7364271,26.959707 15.2424585,26.9362637 C14.2074767,26.9362637 13.6899859,27.4285714 13.6899859,28.4131868 C13.6899859,28.8586081 13.8193586,29.2454212 14.078104,29.5736264 C14.3368495,29.9018315 14.6191172,30.1479853 14.9249073,30.3120879 L15.2424585,30.4879121 C14.8661015,30.6285714 14.5132668,30.6989011 14.1839544,30.6989011 C13.9016867,30.6989011 13.6311801,30.581685 13.3724347,30.3472527 C13.1136892,30.1128205 12.9372719,29.7846154 12.8431826,29.3626374 C12.6785264,28.6827839 12.4903479,27.7684982 12.2786471,26.6197802 C12.0669463,25.4710623 11.8787678,24.4981685 11.7141116,23.7010989 C11.5494554,22.9040293 11.4671273,22.4820513 11.4671273,22.4351648 C11.4671273,22.3882784 12.1139909,22.3648352 13.4077181,22.3648352 C14.4662222,22.3648352 15.0778023,22.3531136 15.2424585,22.3296703 C15.4071147,22.3062271 15.5364874,22.2358974 15.6305767,22.1186813 C15.654099,22.0249084 15.6305767,21.825641 15.5600097,21.5208791 C15.4894428,21.2161172 15.430637,21.0285714 15.3835924,20.9582418 C15.3365477,20.8410256 15.2306973,20.7824176 15.0660412,20.7824176 C14.901385,20.7824176 14.2662825,20.770696 13.1607338,20.7472527 L11.1848596,20.7472527 L10.3733398,16.4571429 C9.24426881,10.6432234 8.45627134,7.00952381 8.00934741,5.55604396 C7.37424497,3.56336996 6.55096403,2.08644689 5.53950459,1.12527473 C4.66917903,0.375091575 3.81061462,0 2.96381137,0 C2.21109737,0 1.52895031,0.222710623 0.917370187,0.668131868 C0.305790062,1.11355311 -4.54133108e-15,1.74652015 -4.54133108e-15,2.56703297 C-4.54133108e-15,3.43443223 0.246984281,4.06739927 0.740952844,4.46593407 C1.23492141,4.86446886 1.72888997,5.06373626 2.22285853,5.06373626 C3.25784028,5.06373626 3.77533116,4.57142857 3.77533116,3.58681319 C3.77533116,3.14139194 3.64595844,2.75457875 3.387213,2.42637363 C3.12846756,2.0981685 2.84619981,1.85201465 2.54040975,1.68791209 L2.22285853,1.51208791 Z" id="è·¯å¾„"></path>                    <g transform="translate(17.418778, 1.934066)" id="è·¯å¾„" stroke-width="0.707">                        <path d="M3.24290361,14.3699692 L2.91861325,14.2456615 C2.6857894,14.1627897 2.3531839,14.0799179 1.92079676,13.9970462 C1.48840961,13.9141744 1.00613163,13.8561641 0.473962836,13.8230154 L1.59951051e-14,13.8230154 L1.59951051e-14,14.9666462 L0.473962836,14.9666462 C1.25558576,14.9997949 1.97900272,15.1241026 2.64421371,15.3395692 C3.30942471,15.5550359 3.77507241,15.7539282 4.04115681,15.9362462 C4.30724121,16.1185641 4.54006506,16.3008821 4.73962836,16.4832 C4.77288891,16.5329231 4.87267056,16.5577846 5.03897331,16.5577846 C5.18864578,16.5577846 5.33000312,16.5080615 5.46304532,16.4086154 L5.46304532,8.97501538 L5.48799073,1.51655385 C5.60440265,1.40053333 5.7041843,1.32594872 5.78733568,1.2928 C5.87048705,1.25965128 6.07005035,1.22650256 6.38602558,1.19335385 C6.7020008,1.16020513 7.21753932,1.14363077 7.93264114,1.14363077 L8.58122187,1.14363077 L8.58122187,0 L8.30682233,0 C7.95758656,0.0497230769 6.6354797,0.0745846154 4.34050176,0.0745846154 C2.07878437,0.0745846154 0.773307784,0.0497230769 0.424072011,0 L0.124727062,0 L0.124727062,1.14363077 L0.773307784,1.14363077 C1.13917383,1.14363077 1.45514906,1.14363077 1.72123346,1.14363077 C1.98731785,1.14363077 2.19519629,1.15191795 2.34486877,1.16849231 C2.49454124,1.18506667 2.62758344,1.20992821 2.74399536,1.24307692 C2.86040729,1.27622564 2.92692839,1.2928 2.94355866,1.2928 C2.96018894,1.2928 3.01007976,1.3342359 3.09323114,1.41710769 C3.17638251,1.49997949 3.22627334,1.53312821 3.24290361,1.51655385 L3.24290361,14.3699692 Z"></path>                    </g>                </g>            </g>            <g id="f2" opacity="0.597795759" transform="translate(265.000000, 408.000000)" stroke="#000000">                <ellipse id="æ¤­åœ†å½¢" stroke-width="2" cx="34" cy="32.5" rx="33" ry="31.5"></ellipse>                <g id="ç¼–ç»„" transform="translate(34.500000, 32.000000) scale(-1, 1) rotate(-180.000000) translate(-34.500000, -32.000000) translate(21.000000, 16.000000)" fill="#000000" fill-rule="nonzero">                    <path d="M2.26063635,1.51208791 C2.30848051,1.51208791 2.38024674,1.48864469 2.47593505,1.44175824 C2.57162337,1.39487179 2.70319479,1.35970696 2.87064934,1.33626374 C3.03810388,1.31282051 3.18163635,1.3010989 3.30124674,1.3010989 C3.61223375,1.3010989 3.89929868,1.45347985 4.16244154,1.75824176 C4.4255844,2.06300366 4.61696102,2.4029304 4.73657141,2.77802198 C4.97579219,3.41098901 5.38246751,5.2043956 5.95659738,8.15824176 C6.53072724,11.1120879 7.08093503,13.9252747 7.60722075,16.5978022 C8.13350646,19.2703297 8.39664931,20.618315 8.39664931,20.6417582 L8.39664931,20.7472527 L6.71014283,20.7472527 C5.58580517,20.7472527 4.98775323,20.770696 4.91598699,20.8175824 C4.82029868,20.8879121 4.77245452,21.0051282 4.77245452,21.1692308 L5.02363634,22.1538462 C5.0714805,22.2710623 5.16716881,22.3296703 5.31070128,22.3296703 C5.45423374,22.3296703 6.01640257,22.3413919 6.99720776,22.3648352 C8.1454675,22.3648352 8.71959736,22.3765568 8.71959736,22.4 C8.71959736,22.4468864 8.82724672,23.032967 9.04254542,24.1582418 C9.25784412,25.2835165 9.40137658,25.96337 9.47314282,26.1978022 C10.3582597,30.0659341 12.1045714,32 14.7120779,32 C15.5971947,31.9531136 16.3268181,31.6952381 16.900948,31.2263736 C17.4750778,30.7575092 17.7621428,30.1362637 17.7621428,29.3626374 C17.7621428,28.5186813 17.510961,27.9091575 17.0085973,27.5340659 C16.5062337,27.1589744 16.0038701,26.959707 15.5015064,26.9362637 C14.448935,26.9362637 13.9226493,27.4285714 13.9226493,28.4131868 C13.9226493,28.8586081 14.0542207,29.2454212 14.3173636,29.5736264 C14.5805064,29.9018315 14.8675714,30.1479853 15.1785584,30.3120879 L15.5015064,30.4879121 C15.1187532,30.6285714 14.759922,30.6989011 14.4250129,30.6989011 C14.137948,30.6989011 13.8628441,30.581685 13.5997012,30.3472527 C13.3365584,30.1128205 13.1571428,29.7846154 13.0614545,29.3626374 C12.8939999,28.6827839 12.7026233,27.7684982 12.4873246,26.6197802 C12.2720259,25.4710623 12.0806493,24.4981685 11.9131948,23.7010989 C11.7457402,22.9040293 11.6620129,22.4820513 11.6620129,22.4351648 C11.6620129,22.3882784 12.3198701,22.3648352 13.6355844,22.3648352 C14.7120779,22.3648352 15.3340519,22.3531136 15.5015064,22.3296703 C15.668961,22.3062271 15.8005324,22.2358974 15.8962207,22.1186813 C15.9201428,22.0249084 15.8962207,21.825641 15.8244545,21.5208791 C15.7526882,21.2161172 15.692883,21.0285714 15.6450389,20.9582418 C15.5971947,20.8410256 15.4895454,20.7824176 15.3220908,20.7824176 C15.1546363,20.7824176 14.5087402,20.770696 13.3844025,20.7472527 L11.374948,20.7472527 L10.5496363,16.4571429 C9.40137658,10.6432234 8.59998698,7.00952381 8.1454675,5.55604396 C7.4995714,3.56336996 6.66229867,2.08644689 5.63364933,1.12527473 C4.74853245,0.375091575 3.87537661,0 3.01418181,0 C2.24867531,0 1.55493506,0.222710623 0.932961035,0.668131868 C0.310987012,1.11355311 -4.37171275e-15,1.74652015 -4.37171275e-15,2.56703297 C-4.37171275e-15,3.43443223 0.251181817,4.06739927 0.753545451,4.46593407 C1.25590909,4.86446886 1.75827272,5.06373626 2.26063635,5.06373626 C3.31320778,5.06373626 3.83949349,4.57142857 3.83949349,3.58681319 C3.83949349,3.14139194 3.70792206,2.75457875 3.44477921,2.42637363 C3.18163635,2.0981685 2.89457142,1.85201465 2.5835844,1.68791209 L2.26063635,1.51208791 Z" id="è·¯å¾„"></path>                    <g transform="translate(16.877624, 1.934066)" id="è·¯å¾„" stroke-width="0.707">                        <path d="M1.49679245,10.6656 C1.0401439,10.6656 0.676516361,10.8147692 0.405909816,11.1131077 C0.135303272,11.4114462 2.54964879e-16,11.7760821 2.54964879e-16,12.2070154 C2.54964879e-16,13.3837949 0.448192089,14.4031179 1.34457627,15.2649846 C2.24096044,16.1268513 3.35721244,16.5577846 4.69333225,16.5577846 C6.23240697,16.5577846 7.51778806,16.0937026 8.54947551,15.1655385 C9.58116296,14.2373744 10.1054631,13.0357333 10.122376,11.5606154 C10.122376,10.8479179 9.95324696,10.1683692 9.61498878,9.52196923 C9.2767306,8.87556923 8.87082078,8.31204103 8.39725933,7.83138462 C7.92369787,7.35072821 7.24718151,6.73747692 6.36771025,5.99163077 C5.75884552,5.47782564 4.91320007,4.71540513 3.83077389,3.70436923 L2.33398144,2.31212308 L4.26205307,2.28726154 C6.91737979,2.28726154 8.32960769,2.32869744 8.49873678,2.41156923 C8.61712714,2.44471795 8.82008205,3.18227692 9.10760151,4.62424615 L9.10760151,4.69883077 L10.122376,4.69883077 L10.122376,4.62424615 C10.1054631,4.57452308 9.99552923,3.82038974 9.79257432,2.36184615 C9.58961941,0.903302564 9.4627726,0.140882051 9.41203387,0.0745846154 L9.41203387,0 L2.54964879e-16,0 L2.54964879e-16,0.472369231 L2.54964879e-16,0.770707692 C2.54964879e-16,0.886728205 0.0507387271,1.0110359 0.152216181,1.14363077 C0.253693635,1.27622564 0.507387271,1.56627692 0.913297087,2.01378462 C1.40377145,2.5441641 1.82659417,3.00824615 2.18176526,3.40603077 C2.33398144,3.57177436 2.6215009,3.8784 3.04432362,4.32590769 C3.46714635,4.77341538 3.7546658,5.08004103 3.90688198,5.24578462 C4.05909816,5.41152821 4.30433535,5.68500513 4.64259353,6.06621538 C4.98085171,6.44742564 5.21763243,6.72918974 5.3529357,6.91150769 C5.48823898,7.09382564 5.68273743,7.34244103 5.93643107,7.65735385 C6.1901247,7.97226667 6.36771025,8.23745641 6.4691877,8.45292308 C6.57066515,8.66838974 6.69751197,8.90871795 6.84972815,9.17390769 C7.00194433,9.43909744 7.11187824,9.70428718 7.17952988,9.96947692 C7.24718151,10.2346667 7.3063767,10.4832821 7.35711542,10.7153231 C7.40785415,10.9473641 7.43322351,11.220841 7.43322351,11.5357538 C7.43322351,12.5799385 7.14570406,13.483241 6.57066515,14.2456615 C5.99562625,15.0080821 5.17535016,15.3892923 4.10983689,15.3892923 C3.55171089,15.3892923 3.06123653,15.2484103 2.63841381,14.9666462 C2.21559108,14.6848821 1.91961517,14.4114051 1.75048608,14.1462154 C1.58135699,13.8810256 1.49679245,13.7235692 1.49679245,13.6738462 C1.49679245,13.6572718 1.53907472,13.6489846 1.62363927,13.6489846 C1.92807163,13.6489846 2.24096044,13.5329641 2.56230572,13.3009231 C2.88365099,13.0688821 3.04432362,12.6876718 3.04432362,12.1572923 C3.04432362,11.7429333 2.90902035,11.3948718 2.63841381,11.1131077 C2.36780726,10.8313436 1.98726681,10.6821744 1.49679245,10.6656 Z"></path>                    </g>                </g>            </g>            <g id="f3" opacity="0.597795759" transform="translate(432.000000, 408.000000)" stroke="#000000">                <ellipse id="æ¤­åœ†å½¢" stroke-width="2" cx="34" cy="32.5" rx="33" ry="31.5"></ellipse>                <g id="ç¼–ç»„" transform="translate(34.500000, 32.000000) scale(-1, 1) rotate(-180.000000) translate(-34.500000, -32.000000) translate(21.000000, 16.000000)" fill="#000000" fill-rule="nonzero">                    <path d="M2.24377027,1.51208791 C2.29125747,1.51208791 2.36248828,1.48864469 2.45746268,1.44175824 C2.55243708,1.39487179 2.68302689,1.35970696 2.84923209,1.33626374 C3.0154373,1.31282051 3.1578989,1.3010989 3.27661691,1.3010989 C3.58528372,1.3010989 3.87020693,1.45347985 4.13138653,1.75824176 C4.39256614,2.06300366 4.58251495,2.4029304 4.70123295,2.77802198 C4.93866896,3.41098901 5.34231017,5.2043956 5.91215659,8.15824176 C6.48200301,11.1120879 7.02810583,13.9252747 7.55046504,16.5978022 C8.07282426,19.2703297 8.33400387,20.618315 8.33400387,20.6417582 L8.33400387,20.7472527 L6.66008002,20.7472527 C5.54413078,20.7472527 4.95054076,20.770696 4.87930996,20.8175824 C4.78433556,20.8879121 4.73684835,21.0051282 4.73684835,21.1692308 L4.98615616,22.1538462 C5.03364336,22.2710623 5.12861777,22.3296703 5.27107937,22.3296703 C5.41354098,22.3296703 5.97151559,22.3413919 6.94500323,22.3648352 C8.08469606,22.3648352 8.65454248,22.3765568 8.65454248,22.4 C8.65454248,22.4468864 8.76138868,23.032967 8.97508109,24.1582418 C9.1887735,25.2835165 9.3312351,25.96337 9.4024659,26.1978022 C10.2809791,30.0659341 12.014262,32 14.6023145,32 C15.4808277,31.9531136 16.2050075,31.6952381 16.7748539,31.2263736 C17.3447004,30.7575092 17.6296236,30.1362637 17.6296236,29.3626374 C17.6296236,28.5186813 17.3803158,27.9091575 16.8817001,27.5340659 C16.3830845,27.1589744 15.8844689,26.959707 15.3858533,26.9362637 C14.3411349,26.9362637 13.8187756,27.4285714 13.8187756,28.4131868 C13.8187756,28.8586081 13.9493655,29.2454212 14.2105451,29.5736264 C14.4717247,29.9018315 14.7566479,30.1479853 15.0653147,30.3120879 L15.3858533,30.4879121 C15.0059557,30.6285714 14.6498017,30.6989011 14.3173913,30.6989011 C14.0324681,30.6989011 13.7594166,30.581685 13.498237,30.3472527 C13.2370574,30.1128205 13.0589804,29.7846154 12.964006,29.3626374 C12.7978008,28.6827839 12.607852,27.7684982 12.3941596,26.6197802 C12.1804672,25.4710623 11.9905184,24.4981685 11.8243132,23.7010989 C11.658108,22.9040293 11.5750054,22.4820513 11.5750054,22.4351648 C11.5750054,22.3882784 12.2279544,22.3648352 13.5338524,22.3648352 C14.6023145,22.3648352 15.2196481,22.3531136 15.3858533,22.3296703 C15.5520585,22.3062271 15.6826483,22.2358974 15.7776227,22.1186813 C15.8013663,22.0249084 15.7776227,21.825641 15.7063919,21.5208791 C15.6351611,21.2161172 15.5758021,21.0285714 15.5283149,20.9582418 C15.4808277,20.8410256 15.3739815,20.7824176 15.2077763,20.7824176 C15.0415711,20.7824176 14.4004939,20.770696 13.2845446,20.7472527 L11.2900822,20.7472527 L10.4709279,16.4571429 C9.3312351,10.6432234 8.53582448,7.00952381 8.08469606,5.55604396 C7.44361884,3.56336996 6.61259281,2.08644689 5.59161798,1.12527473 C4.71310475,0.375091575 3.84646332,0 2.9916937,0 C2.23189847,0 1.54333405,0.222710623 0.92600043,0.668131868 C0.30866681,1.11355311 -1.45051597e-17,1.74652015 -1.45051597e-17,2.56703297 C-1.45051597e-17,3.43443223 0.249307808,4.06739927 0.747923424,4.46593407 C1.24653904,4.86446886 1.74515466,5.06373626 2.24377027,5.06373626 C3.28848871,5.06373626 3.81084792,4.57142857 3.81084792,3.58681319 C3.81084792,3.14139194 3.68025812,2.75457875 3.41907851,2.42637363 C3.1578989,2.0981685 2.87297569,1.85201465 2.56430888,1.68791209 L2.24377027,1.51208791 Z" id="è·¯å¾„"></path>                    <g transform="translate(16.550263, 1.387112)" id="è·¯å¾„" stroke-width="0.707">                        <path d="M2.14030753,12.0578462 C1.68706594,12.0578462 1.3345447,12.1987282 1.08274381,12.4804923 C0.830942924,12.7622564 0.696649118,13.1268923 0.679862393,13.5744 C0.679862393,14.4859897 1.08274381,15.2981333 1.88850665,16.0108308 C2.69426948,16.7235282 3.66789958,17.0798769 4.80939693,17.0798769 C5.39693233,17.0798769 5.76624029,17.0715897 5.91732082,17.0550154 C7.15953853,16.8561231 8.10798853,16.4169026 8.76267084,15.7373538 C9.41735314,15.0578051 9.75308766,14.3202462 9.76987438,13.5246769 C9.76987438,12.6628103 9.48450005,11.8258051 8.91375137,11.0136615 C8.34300269,10.2015179 7.55402658,9.61312821 6.54682304,9.24849231 L6.47128277,9.19876923 C6.47128277,9.18219487 6.54682304,9.15733333 6.69790357,9.12418462 C6.8489841,9.0910359 7.09239163,9.01645128 7.42812614,8.90043077 C7.76386066,8.78441026 8.08280845,8.61037949 8.38496951,8.37833846 C9.76148102,7.49989744 10.4497368,6.33969231 10.4497368,4.89772308 C10.4497368,3.58834872 9.92934828,2.44471795 8.88857128,1.46683077 C7.84779429,0.48894359 6.53003631,1.24930591e-16 4.93529737,1.24930591e-16 C3.59235931,1.24930591e-16 2.43407523,0.356348718 1.46044514,1.06904615 C0.486815047,1.78174359 3.79593969e-16,2.68504615 3.79593969e-16,3.77895385 C3.79593969e-16,4.2430359 0.151080532,4.61595897 0.453241595,4.89772308 C0.755402658,5.17948718 1.13310399,5.32865641 1.58634558,5.34523077 C2.0563739,5.34523077 2.4424686,5.19606154 2.74462966,4.89772308 C3.04679072,4.59938462 3.19787125,4.22646154 3.19787125,3.77895385 C3.19787125,3.5966359 3.17269117,3.43089231 3.12233099,3.28172308 C3.07197081,3.13255385 3.01321727,2.99995897 2.94607037,2.88393846 C2.87892347,2.76791795 2.78659647,2.66847179 2.66908939,2.5856 C2.55158231,2.50272821 2.45086196,2.43643077 2.36692833,2.38670769 C2.2829947,2.33698462 2.19906107,2.3038359 2.11512744,2.28726154 C2.03119382,2.27068718 1.96404691,2.24582564 1.91368673,2.21267692 L1.81296638,2.18781538 C2.66908939,1.44196923 3.70986639,1.06904615 4.93529737,1.06904615 C5.85856728,1.06904615 6.5552164,1.50826667 7.02524472,2.38670769 C7.31061906,2.93366154 7.45330623,3.77066667 7.45330623,4.89772308 L7.45330623,5.39495385 C7.45330623,6.96951795 6.91613101,8.02198974 5.84178056,8.55236923 C5.58997967,8.65181538 5.07798454,8.70982564 4.30579515,8.7264 L3.24823143,8.75126154 L3.17269117,8.80098462 C3.13911771,8.85070769 3.12233099,8.98330256 3.12233099,9.19876923 C3.12233099,9.49710769 3.18947789,9.64627692 3.3237717,9.64627692 C3.79380002,9.64627692 4.28061506,9.68771282 4.78421684,9.77058462 C5.35496551,9.85345641 5.87535401,10.2015179 6.34538233,10.8147692 C6.81541065,11.4280205 7.05042481,12.3561846 7.05042481,13.5992615 L7.05042481,13.7981538 C7.05042481,14.7428923 6.75665711,15.3975795 6.16912171,15.7622154 C5.79981374,15.9942564 5.40532569,16.1102769 4.98565755,16.1102769 C4.44848232,16.1102769 3.95327391,16.0191179 3.50003232,15.8368 C3.04679072,15.6544821 2.72784293,15.4638769 2.54318895,15.2649846 C2.35853497,15.0660923 2.26620798,14.9666462 2.26620798,14.9666462 L2.34174824,14.9666462 C2.39210842,14.9500718 2.45925532,14.9334974 2.54318895,14.9169231 C2.62712258,14.9003487 2.71105621,14.8589128 2.79498984,14.7926154 C2.87892347,14.7263179 2.97964382,14.6683077 3.0971509,14.6185846 C3.21465798,14.5688615 3.29859161,14.4777026 3.34895179,14.3451077 C3.39931196,14.2125128 3.46645887,14.0882051 3.55039249,13.9721846 C3.63432612,13.8561641 3.65950621,13.6904205 3.62593276,13.4749538 C3.62593276,13.1103179 3.50842568,12.7871179 3.27341152,12.5053538 C3.03839736,12.2235897 2.66069603,12.0744205 2.14030753,12.0578462 Z"></path>                    </g>                </g>            </g>            <g id="f4" opacity="0.597795759" transform="translate(265.000000, 563.000000)" stroke="#000000">                <ellipse id="æ¤­åœ†å½¢" stroke-width="2" cx="34" cy="32.5" rx="33" ry="31.5"></ellipse>                <g id="ç¼–ç»„" transform="translate(33.500000, 33.000000) scale(-1, 1) rotate(-180.000000) translate(-33.500000, -33.000000) translate(20.000000, 17.000000)" fill="#000000" fill-rule="nonzero">                    <path d="M2.2148524,1.51208791 C2.26172758,1.51208791 2.33204036,1.48864469 2.42579073,1.44175824 C2.51954109,1.39487179 2.64844785,1.35970696 2.81251099,1.33626374 C2.97657413,1.31282051 3.11719968,1.3010989 3.23438763,1.3010989 C3.53907632,1.3010989 3.82032742,1.45347985 4.07814093,1.75824176 C4.33595444,2.06300366 4.52345517,2.4029304 4.64064313,2.77802198 C4.87501904,3.41098901 5.2734581,5.2043956 5.8359603,8.15824176 C6.39846249,11.1120879 6.9375271,13.9252747 7.45315411,16.5978022 C7.96878113,19.2703297 8.22659464,20.618315 8.22659464,20.6417582 L8.22659464,20.7472527 L6.57424443,20.7472527 C5.47267763,20.7472527 4.88673784,20.770696 4.81642506,20.8175824 C4.7226747,20.8879121 4.67579951,21.0051282 4.67579951,21.1692308 L4.92189423,22.1538462 C4.96876941,22.2710623 5.06251978,22.3296703 5.20314532,22.3296703 C5.34377087,22.3296703 5.89455428,22.3413919 6.85549553,22.3648352 C7.98049992,22.3648352 8.54300212,22.3765568 8.54300212,22.4 C8.54300212,22.4468864 8.64847128,23.032967 8.85940961,24.1582418 C9.07034793,25.2835165 9.21097348,25.96337 9.28128626,26.1978022 C10.1484771,30.0659341 11.8594213,32 14.4141188,32 C15.2813097,31.9531136 15.9961562,31.6952381 16.5586584,31.2263736 C17.1211606,30.7575092 17.4024117,30.1362637 17.4024117,29.3626374 C17.4024117,28.5186813 17.156317,27.9091575 16.6641276,27.5340659 C16.1719382,27.1589744 15.6797487,26.959707 15.1875593,26.9362637 C14.1563053,26.9362637 13.6406783,27.4285714 13.6406783,28.4131868 C13.6406783,28.8586081 13.769585,29.2454212 14.0273985,29.5736264 C14.2852121,29.9018315 14.5664632,30.1479853 14.8711518,30.3120879 L15.1875593,30.4879121 C14.8125579,30.6285714 14.460994,30.6989011 14.1328677,30.6989011 C13.8516166,30.6989011 13.5820843,30.581685 13.3242708,30.3472527 C13.0664573,30.1128205 12.8906754,29.7846154 12.796925,29.3626374 C12.6328618,28.6827839 12.4453611,27.7684982 12.2344228,26.6197802 C12.0234845,25.4710623 11.8359837,24.4981685 11.6719206,23.7010989 C11.5078575,22.9040293 11.4258259,22.4820513 11.4258259,22.4351648 C11.4258259,22.3882784 12.0703596,22.3648352 13.3594272,22.3648352 C14.4141188,22.3648352 15.0234962,22.3531136 15.1875593,22.3296703 C15.3516225,22.3062271 15.4805292,22.2358974 15.5742796,22.1186813 C15.5977172,22.0249084 15.5742796,21.825641 15.5039668,21.5208791 C15.433654,21.2161172 15.3750601,21.0285714 15.3281849,20.9582418 C15.2813097,20.8410256 15.1758405,20.7824176 15.0117774,20.7824176 C14.8477142,20.7824176 14.2148993,20.770696 13.1133325,20.7472527 L11.1445748,20.7472527 L10.3359779,16.4571429 C9.21097348,10.6432234 8.42581416,7.00952381 7.98049992,5.55604396 C7.34768495,3.56336996 6.52736925,2.08644689 5.51955281,1.12527473 C4.65236192,0.375091575 3.79688983,0 2.95313654,0 C2.20313361,0 1.52344345,0.222710623 0.914066071,0.668131868 C0.30468869,1.11355311 -7.41304549e-15,1.74652015 -7.41304549e-15,2.56703297 C-7.41304549e-15,3.43443223 0.246094711,4.06739927 0.738284134,4.46593407 C1.23047356,4.86446886 1.72266298,5.06373626 2.2148524,5.06373626 C3.24610643,5.06373626 3.76173344,4.57142857 3.76173344,3.58681319 C3.76173344,3.14139194 3.63282669,2.75457875 3.37501318,2.42637363 C3.11719968,2.0981685 2.83594858,1.85201465 2.53125989,1.68791209 L2.2148524,1.51208791 Z" id="è·¯å¾„"></path>                    <g transform="translate(15.988984, 1.934066)" id="å½¢çŠ¶" stroke-width="0.707">                        <path d="M10.7873156,0 C10.4890488,0.0497230769 9.42025945,0.0745846154 7.58094758,0.0745846154 C5.65878382,0.0745846154 4.54856855,0.0497230769 4.25030176,0 L4.02660167,0 L4.02660167,1.14363077 L4.79712421,1.14363077 C5.12853175,1.14363077 5.35223184,1.14363077 5.46822449,1.14363077 C5.58421713,1.14363077 5.72506533,1.16020513 5.8907691,1.19335385 C6.05647288,1.22650256 6.17246552,1.26793846 6.23874703,1.31766154 C6.30502854,1.36738462 6.36302486,1.43368205 6.41273599,1.51655385 C6.42930636,1.54970256 6.43759155,1.99721026 6.43759155,2.85907692 L6.43759155,4.10215385 L-7.17664205e-15,4.10215385 L-7.17664205e-15,5.24578462 L3.75319044,10.9888 C6.28845816,14.8340513 7.57266239,16.7649641 7.60580315,16.7815385 C7.6389439,16.8146872 7.81293286,16.8312615 8.12777003,16.8312615 L8.57517022,16.8312615 L8.72430361,16.6820923 L8.72430361,5.24578462 L11.0110157,5.24578462 L11.0110157,4.10215385 L8.72430361,4.10215385 L8.72430361,2.83421538 C8.72430361,2.15466667 8.72430361,1.75688205 8.72430361,1.64086154 C8.72430361,1.52484103 8.77401474,1.42539487 8.87343701,1.34252308 C9.00600002,1.22650256 9.47825577,1.16020513 10.2902043,1.14363077 L11.0110157,1.14363077 L11.0110157,0 L10.7873156,0 Z M6.58672495,5.24578462 L6.58672495,13.5495385 L1.14335603,5.27064615 L3.85261271,5.24578462 L6.58672495,5.24578462 Z"></path>                    </g>                </g>            </g>            <g id="x1" opacity="0.597795759" transform="translate(435.000000, 241.000000)" stroke="#000000">                <rect id="çŸ©å½¢" stroke-width="2" x="1" y="1" width="60" height="60"></rect>                <g id="ç¼–ç»„" transform="translate(30.500000, 30.000000) scale(-1, 1) rotate(-180.000000) translate(-30.500000, -30.000000) translate(9.000000, 14.000000)" fill="#000000" fill-rule="nonzero">                    <path d="M0.62325107,5.76230492 L0.545344686,5.76230492 C0.181781562,5.76230492 0,5.90316126 0,6.18487395 C0,6.28731493 0.0389531919,6.49219688 0.116859576,6.79951981 C0.220734754,7.15806323 0.311625535,7.36294518 0.389531919,7.41416567 C0.467438302,7.46538615 0.714141851,7.50380152 1.12964256,7.52941176 C3.3629589,7.6062425 5.11585253,8.27210884 6.38832346,9.5270108 C6.64801141,9.78311325 8.15420149,11.3965586 10.9068937,14.3673469 C13.6595859,17.3381353 15.0229477,18.8491397 14.9969789,18.9003601 C12.1663803,26.020008 10.686159,29.6438575 10.556315,29.7719088 C10.3225958,30.0536214 9.50457881,30.2072829 8.10226391,30.2328932 L7.01157453,30.2328932 C6.85576177,30.3865546 6.77785538,30.4889956 6.77785538,30.5402161 C6.77785538,30.5914366 6.80382418,30.8347339 6.85576177,31.270108 C6.95963694,31.6030412 7.08948092,31.8463385 7.24529368,32 L7.79063837,32 C8.67357739,31.9487795 10.2446895,31.9231693 12.5039746,31.9231693 C13.3609448,31.9231693 14.152993,31.9231693 14.8801193,31.9231693 C15.6072455,31.9231693 16.1915434,31.9359744 16.6330129,31.9615846 C17.0744824,31.9871949 17.3341704,31.9871949 17.4120768,31.9615846 C17.8535463,31.9615846 18.074281,31.8207283 18.074281,31.5390156 C18.074281,31.5134054 18.0483122,31.3469388 17.9963746,31.0396158 C17.8924995,30.6554622 17.8016087,30.42497 17.7237023,30.3481393 C17.6457959,30.2713085 17.4380456,30.2328932 17.1004512,30.2328932 C16.2954186,30.1560624 15.5942611,29.9383754 14.9969789,29.5798319 L17.9963746,22.1272509 L20.0608938,24.3937575 C22.7876172,27.2877151 24.1509789,28.8883553 24.1509789,29.1956783 C24.1509789,29.6566627 23.8393534,29.9767907 23.2161023,30.1560624 C23.0862584,30.1560624 22.9174612,30.1816727 22.7097108,30.2328932 C22.3201789,30.2328932 22.125413,30.3737495 22.125413,30.6554622 C22.125413,30.7066827 22.1513818,30.9115646 22.2033194,31.270108 C22.3071945,31.6030412 22.4370385,31.8463385 22.5928513,32 L23.0602896,32 C23.0862584,32 23.4108683,32 24.0341194,32 C24.6573704,32 25.3585279,31.9743898 26.1375917,31.9231693 C26.9166556,31.8719488 27.4490159,31.8591437 27.7346726,31.8847539 C30.0718641,31.8847539 31.3573194,31.9231693 31.5910386,32 L31.9026641,32 C32.0844457,31.8207283 32.1753365,31.6798719 32.1753365,31.577431 C32.1233989,30.9371749 31.9675861,30.4889956 31.7078982,30.2328932 L31.0846471,30.2328932 C30.2017081,30.2072829 29.4096598,30.0920368 28.7085024,29.8871549 C28.0073449,29.6822729 27.4749847,29.4645858 27.1114215,29.2340936 C26.7478584,29.0036014 26.4362329,28.7731092 26.1765449,28.542617 L25.7480598,28.1968788 C25.7480598,28.222489 24.5145421,26.8907563 22.0475066,24.2016807 L18.6585789,20.5138055 C18.6585789,20.4881953 19.0610952,19.4637855 19.8661278,17.4405762 C20.6711605,15.4173669 21.5151463,13.3429372 22.3980853,11.2172869 C23.2810243,9.09163665 23.761447,7.99039616 23.8393534,7.91356543 C24.0990414,7.68307323 24.8910896,7.55502201 26.2154981,7.52941176 C27.1244059,7.52941176 27.5788598,7.41416567 27.5788598,7.18367347 C27.5788598,7.13245298 27.552891,6.95318127 27.5009534,6.64585834 C27.3970783,6.23609444 27.3061875,5.99279712 27.2282811,5.91596639 C27.1503747,5.83913565 26.9685932,5.80072029 26.6829364,5.80072029 C26.60503,5.80072029 26.1765449,5.80072029 25.3974811,5.80072029 C24.6184173,5.80072029 23.4628059,5.81352541 21.930647,5.83913565 C20.3465505,5.83913565 19.1390016,5.83913565 18.3080002,5.83913565 C17.4769987,5.83913565 17.0225448,5.82633053 16.9446385,5.80072029 C16.5031689,5.80072029 16.2824342,5.91596639 16.2824342,6.14645858 C16.2824342,6.17206883 16.308403,6.36414566 16.3603406,6.72268908 C16.4122782,6.97879152 16.4642158,7.15806323 16.5161533,7.2605042 C16.5680909,7.36294518 16.6330129,7.42697079 16.7109193,7.45258103 C16.7888257,7.47819128 16.9056853,7.50380152 17.061498,7.52941176 C17.2173108,7.55502201 17.4250612,7.56782713 17.6847491,7.56782713 C17.944437,7.56782713 18.2820314,7.65746299 18.6975321,7.83673469 C19.1390016,8.04161665 19.3597364,8.16966787 19.3597364,8.22088836 C19.3337676,8.22088836 18.7235009,9.71908764 17.5289363,12.7154862 L15.6981363,17.2484994 C10.8160029,12.0240096 8.31001426,9.27090836 8.18017029,8.98919568 C8.07629511,8.78431373 8.02435752,8.63065226 8.02435752,8.52821128 C8.02435752,8.04161665 8.40090504,7.7214886 9.15400009,7.56782713 C9.17996888,7.56782713 9.25787526,7.56782713 9.38771924,7.56782713 C9.51756321,7.56782713 9.59546959,7.55502201 9.62143839,7.52941176 C9.69934477,7.52941176 9.75128236,7.52941176 9.77725115,7.52941176 C9.80321995,7.52941176 9.84217314,7.51660664 9.89411073,7.4909964 C9.94604832,7.46538615 9.98500151,7.42697079 10.0109703,7.3757503 C10.0369391,7.32452981 10.0499235,7.23489396 10.0499235,7.10684274 C10.0499235,6.87635054 10.0239547,6.67146859 9.97201711,6.49219688 C9.89411073,6.15926371 9.81620435,5.96718687 9.73829796,5.91596639 C9.66039158,5.8647459 9.46562562,5.82633053 9.15400009,5.80072029 C9.0760937,5.80072029 8.84237455,5.80072029 8.45284263,5.80072029 C8.06331071,5.80072029 7.51796603,5.81352541 6.81680857,5.83913565 C6.11565112,5.8647459 5.34957168,5.8647459 4.51857025,5.83913565 C2.49300428,5.83913565 1.19456455,5.81352541 0.62325107,5.76230492 Z" id="è·¯å¾„"></path>                    <g transform="translate(33.526272, 0.000000)" id="è·¯å¾„" stroke-width="0.707">                        <path d="M3.58018786,15.6982857 L3.22216908,15.5624874 C2.96512995,15.4719552 2.59793119,15.381423 2.12057281,15.2908908 C1.64321443,15.2003585 1.11077623,15.136986 0.523258226,15.1007731 L-1.1071163e-15,15.1007731 L-1.1071163e-15,16.3501176 L0.523258226,16.3501176 C1.3861753,16.3863305 2.18483259,16.5221289 2.9192301,16.7575126 C3.65362761,16.9928964 4.16770587,17.2101737 4.46146488,17.4093445 C4.75522388,17.6085154 5.01226301,17.8076863 5.23258226,18.0068571 C5.26930214,18.0611765 5.37946176,18.0883361 5.56306114,18.0883361 C5.72830058,18.0883361 5.88436005,18.0340168 6.03123955,17.9253782 L6.03123955,9.80463866 L6.05877946,1.6567395 C6.18729902,1.5299944 6.29745865,1.44851541 6.38925834,1.41230252 C6.48105803,1.37608964 6.70137728,1.33987675 7.0502161,1.30366387 C7.39905492,1.26745098 7.96821299,1.24934454 8.75769031,1.24934454 L9.47372788,1.24934454 L9.47372788,0 L9.17078891,0 C8.78523022,0.0543193277 7.32561517,0.0814789916 4.79194376,0.0814789916 C2.29499222,0.0814789916 0.853737106,0.0543193277 0.468178413,0 L0.137699533,0 L0.137699533,1.24934454 L0.853737106,1.24934454 C1.25765574,1.24934454 1.60649455,1.24934454 1.90025356,1.24934454 C2.19401256,1.24934454 2.42351178,1.25839776 2.58875122,1.2765042 C2.75399066,1.29461064 2.90087017,1.32177031 3.02938973,1.35798319 C3.15790929,1.39419608 3.23134905,1.41230252 3.24970898,1.41230252 C3.26806892,1.41230252 3.32314873,1.45756863 3.41494842,1.54810084 C3.50674811,1.63863305 3.56182793,1.67484594 3.58018786,1.6567395 L3.58018786,15.6982857 Z"></path>                    </g>                </g>            </g>            <g id="x2" opacity="0.597795759" transform="translate(435.000000, 565.000000)" stroke="#000000">                <rect id="çŸ©å½¢" stroke-width="2" x="1" y="1" width="60" height="60"></rect>                <g id="ç¼–ç»„" transform="translate(31.500000, 32.000000) scale(-1, 1) rotate(-180.000000) translate(-31.500000, -32.000000) translate(10.000000, 16.000000)" fill="#000000" fill-rule="nonzero">                    <path d="M0.614591364,5.76230492 L0.537767443,5.76230492 C0.179255814,5.76230492 0,5.90316126 0,6.18487395 C0,6.28731493 0.0384119602,6.49219688 0.115235881,6.79951981 C0.217667775,7.15806323 0.307295682,7.36294518 0.384119602,7.41416567 C0.460943523,7.46538615 0.704219271,7.50380152 1.11394685,7.52941176 C3.31623257,7.6062425 5.04477078,8.27210884 6.29956148,9.5270108 C6.55564121,9.78311325 8.04090368,11.3965586 10.7553489,14.3673469 C13.4697941,17.3381353 14.8142127,18.8491397 14.7886047,18.9003601 C11.9973356,26.020008 10.5376811,29.6438575 10.4096412,29.7719088 C10.1791695,30.0536214 9.3725183,30.2072829 7.98968773,30.2328932 L6.91415284,30.2328932 C6.760505,30.3865546 6.68368108,30.4889956 6.68368108,30.5402161 C6.68368108,30.5914366 6.70928905,30.8347339 6.760505,31.270108 C6.8629369,31.6030412 6.99097676,31.8463385 7.1446246,32 L7.68239205,32 C8.55306315,31.9487795 10.1023455,31.9231693 12.3302392,31.9231693 C13.1753024,31.9231693 13.9563456,31.9231693 14.6733688,31.9231693 C15.3903921,31.9231693 15.9665715,31.9359744 16.401907,31.9615846 C16.8372426,31.9871949 17.0933223,31.9871949 17.1701462,31.9615846 C17.6054818,31.9615846 17.8231495,31.8207283 17.8231495,31.5390156 C17.8231495,31.5134054 17.7975416,31.3469388 17.7463256,31.0396158 C17.6438937,30.6554622 17.5542658,30.42497 17.4774419,30.3481393 C17.400618,30.2713085 17.1957542,30.2328932 16.8628505,30.2328932 C16.0690034,30.1560624 15.3775881,29.9383754 14.7886047,29.5798319 L17.7463256,22.1272509 L19.7821595,24.3937575 C22.4709967,27.2877151 23.8154153,28.8883553 23.8154153,29.1956783 C23.8154153,29.6566627 23.5081197,29.9767907 22.8935283,30.1560624 C22.7654884,30.1560624 22.5990366,30.1816727 22.3941728,30.2328932 C22.0100532,30.2328932 21.8179934,30.3737495 21.8179934,30.6554622 C21.8179934,30.7066827 21.8436014,30.9115646 21.8948173,31.270108 C21.9972492,31.6030412 22.1252891,31.8463385 22.2789369,32 L22.7398805,32 C22.7654884,32 23.0855881,32 23.7001795,32 C24.3147708,32 25.0061861,31.9743898 25.7744253,31.9231693 C26.5426645,31.8719488 27.067628,31.8591437 27.3493157,31.8847539 C29.6540333,31.8847539 30.921628,31.9231693 31.1520997,32 L31.4593954,32 C31.6386512,31.8207283 31.7282792,31.6798719 31.7282792,31.577431 C31.6770632,30.9371749 31.5234154,30.4889956 31.2673356,30.2328932 L30.6527443,30.2328932 C29.7820732,30.2072829 29.00103,30.0920368 28.3096147,29.8871549 C27.6181994,29.6822729 27.093236,29.4645858 26.7347243,29.2340936 C26.3762127,29.0036014 26.068917,28.7731092 25.8128373,28.542617 L25.3903057,28.1968788 C25.3903057,28.222489 24.173927,26.8907563 21.7411695,24.2016807 L18.399329,20.5138055 C18.399329,20.4881953 18.7962525,19.4637855 19.5900997,17.4405762 C20.3839469,15.4173669 21.216206,13.3429372 22.0868771,11.2172869 C22.9575482,9.09163665 23.4312957,7.99039616 23.5081197,7.91356543 C23.7641994,7.68307323 24.5452426,7.55502201 25.8512492,7.52941176 C26.7475283,7.52941176 27.1956678,7.41416567 27.1956678,7.18367347 C27.1956678,7.13245298 27.1700599,6.95318127 27.1188439,6.64585834 C27.016412,6.23609444 26.9267841,5.99279712 26.8499602,5.91596639 C26.7731363,5.83913565 26.5938805,5.80072029 26.3121928,5.80072029 C26.2353688,5.80072029 25.8128373,5.80072029 25.0445981,5.80072029 C24.2763589,5.80072029 23.136804,5.81352541 21.6259336,5.83913565 C20.0638472,5.83913565 18.8730765,5.83913565 18.0536213,5.83913565 C17.2341662,5.83913565 16.7860266,5.82633053 16.7092027,5.80072029 C16.2738672,5.80072029 16.0561994,5.91596639 16.0561994,6.14645858 C16.0561994,6.17206883 16.0818074,6.36414566 16.1330233,6.72268908 C16.1842392,6.97879152 16.2354552,7.15806323 16.2866711,7.2605042 C16.3378871,7.36294518 16.401907,7.42697079 16.4787309,7.45258103 C16.5555549,7.47819128 16.6707907,7.50380152 16.8244386,7.52941176 C16.9780864,7.55502201 17.1829502,7.56782713 17.4390299,7.56782713 C17.6951097,7.56782713 18.0280133,7.65746299 18.4377409,7.83673469 C18.8730765,8.04161665 19.0907442,8.16966787 19.0907442,8.22088836 C19.0651363,8.22088836 18.4633489,9.71908764 17.2853821,12.7154862 L15.48002,17.2484994 C10.665721,12.0240096 8.19455152,9.27090836 8.06651165,8.98919568 C7.96407976,8.78431373 7.91286381,8.63065226 7.91286381,8.52821128 C7.91286381,8.04161665 8.28417942,7.7214886 9.02681065,7.56782713 C9.05241863,7.56782713 9.12924255,7.56782713 9.25728242,7.56782713 C9.38532228,7.56782713 9.4621462,7.55502201 9.48775418,7.52941176 C9.5645781,7.52941176 9.61579405,7.52941176 9.64140202,7.52941176 C9.66700999,7.52941176 9.70542195,7.51660664 9.7566379,7.4909964 C9.80785385,7.46538615 9.84626581,7.42697079 9.87187378,7.3757503 C9.89748175,7.32452981 9.91028574,7.23489396 9.91028574,7.10684274 C9.91028574,6.87635054 9.88467777,6.67146859 9.83346182,6.49219688 C9.7566379,6.15926371 9.67981398,5.96718687 9.60299006,5.91596639 C9.52616614,5.8647459 9.33410634,5.82633053 9.02681065,5.80072029 C8.94998673,5.80072029 8.71951497,5.80072029 8.33539537,5.80072029 C7.95127577,5.80072029 7.41350833,5.81352541 6.72209304,5.83913565 C6.03067776,5.8647459 5.27524254,5.8647459 4.45578739,5.83913565 C2.45836545,5.83913565 1.17796678,5.81352541 0.614591364,5.76230492 Z" id="è·¯å¾„"></path>                    <g transform="translate(32.164255, 0.000000)" id="è·¯å¾„" stroke-width="0.707">                        <path d="M1.6022781,11.6514958 C1.11344749,11.6514958 0.72419349,11.8144538 0.434516094,12.1403697 C0.144838698,12.4662857 2.72933393e-16,12.8646275 2.72933393e-16,13.335395 C2.72933393e-16,14.6209524 0.479778187,15.7344986 1.43933456,16.6760336 C2.39889094,17.6175686 3.5938102,18.0883361 5.02409234,18.0883361 C6.67163253,18.0883361 8.04760016,17.5813557 9.15199523,16.567395 C10.2563903,15.5534342 10.8176403,14.2407171 10.8357451,12.6292437 C10.8357451,11.8506667 10.6546967,11.1083025 10.2926,10.4021513 C9.93050324,9.696 9.49598714,9.08038095 8.9890517,8.55529412 C8.48211625,8.03020728 7.75792276,7.36026891 6.81647123,6.54547899 C6.16469709,5.98417927 5.25945522,5.15128291 4.10074564,4.04678992 L2.49846754,2.52584874 L4.56241899,2.49868908 C7.40487844,2.49868908 8.91663235,2.54395518 9.09768072,2.63448739 C9.22441458,2.67070028 9.44167263,3.47643697 9.74945486,5.05169748 L9.74945486,5.13317647 L10.8357451,5.13317647 L10.8357451,5.05169748 C10.8176403,4.99737815 10.6999588,4.17353501 10.4827008,2.58016807 C10.2654427,0.98680112 10.1296564,0.153904762 10.0753419,0.0814789916 L10.0753419,0 L2.72933393e-16,0 L2.72933393e-16,0.516033613 L2.72933393e-16,0.84194958 C2.72933393e-16,0.968694678 0.0543145118,1.104493 0.162943535,1.24934454 C0.271572559,1.39419608 0.543145118,1.71105882 0.977661212,2.19993277 C1.50270149,2.77933894 1.95532242,3.28631933 2.33552401,3.72087395 C2.49846754,3.90193838 2.80624977,4.23690756 3.25887071,4.72578151 C3.71149164,5.21465546 4.01927387,5.54962465 4.18221741,5.73068908 C4.34516094,5.9117535 4.60768108,6.2105098 4.96977783,6.62695798 C5.33187457,7.04340616 5.58534229,7.35121569 5.73018099,7.55038655 C5.87501969,7.74955742 6.08322532,8.02115406 6.35479788,8.36517647 C6.62637044,8.70919888 6.81647123,8.99890196 6.92510025,9.23428571 C7.03372927,9.46966947 7.16951555,9.73221289 7.33245909,10.021916 C7.49540262,10.311619 7.61308407,10.6013221 7.68550342,10.8910252 C7.75792276,11.1807283 7.82128969,11.4523249 7.87560421,11.7058151 C7.92991872,11.9593053 7.95707597,12.2580616 7.95707597,12.602084 C7.95707597,13.7427899 7.64929374,14.729591 7.03372927,15.5624874 C6.41816481,16.3953838 5.5400802,16.8118319 4.39947545,16.8118319 C3.80201582,16.8118319 3.27697554,16.6579272 2.82435461,16.3501176 C2.37173368,16.0423081 2.05489903,15.7435518 1.87385066,15.4538487 C1.69280228,15.1641457 1.6022781,14.9921345 1.6022781,14.9378151 C1.6022781,14.9197087 1.64754019,14.9106555 1.73806438,14.9106555 C2.06395145,14.9106555 2.39889094,14.7839104 2.74288284,14.5304202 C3.08687475,14.27693 3.25887071,13.8604818 3.25887071,13.2810756 C3.25887071,12.8284146 3.11403201,12.4481793 2.82435461,12.1403697 C2.53467722,11.8325602 2.12731838,11.6696022 1.6022781,11.6514958 Z"></path>                    </g>                </g>            </g>            <g id="x3" opacity="0.597795759" transform="translate(104.000000, 565.000000)" stroke="#000000">                <rect id="çŸ©å½¢" stroke-width="2" x="1" y="1" width="60" height="60"></rect>                <g id="ç¼–ç»„" transform="translate(30.500000, 31.000000) scale(-1, 1) rotate(-180.000000) translate(-30.500000, -31.000000) translate(9.000000, 15.000000)" fill="#000000" fill-rule="nonzero">                    <path d="M0.611501743,6.24324203 L0.535064025,6.24324203 C0.178354675,6.24324203 0,6.38151648 0,6.65806537 C0,6.7586286 0.038218859,6.95975507 0.114656577,7.26144476 C0.216573534,7.61341608 0.305750872,7.81454254 0.38218859,7.86482416 C0.458626308,7.91510578 0.700679081,7.95281699 1.10834691,7.9779578 C3.29956149,8.05338022 5.01941014,8.70704123 6.26789287,9.93894083 C6.52268526,10.1903489 8.00048114,11.7742198 10.7012805,14.6905536 C13.4020799,17.6068873 14.7397399,19.090195 14.7142607,19.1404766 C11.9370236,26.1296213 10.484707,29.6870456 10.3573108,29.8127497 C10.1279976,30.0892986 9.32540159,30.2401434 7.94952266,30.2652842 L6.87939461,30.2652842 C6.72651918,30.4161291 6.65008146,30.5166923 6.65008146,30.5669739 C6.65008146,30.6172556 6.6755607,30.8560932 6.72651918,31.283487 C6.82843613,31.6103175 6.95583233,31.8491552 7.10870777,32 L7.64377179,32 C8.51006593,31.9497184 10.0515599,31.9245776 12.2682537,31.9245776 C13.1090686,31.9245776 13.8861854,31.9245776 14.5996041,31.9245776 C15.3130228,31.9245776 15.8863057,31.937148 16.3194528,31.9622888 C16.7525998,31.9874296 17.0073922,31.9874296 17.08383,31.9622888 C17.516977,31.9622888 17.7335506,31.8240143 17.7335506,31.5474655 C17.7335506,31.5223246 17.7080713,31.3589094 17.6571128,31.0572197 C17.5551959,30.6801076 17.4660185,30.4538403 17.3895808,30.3784179 C17.3131431,30.3029954 17.1093092,30.2652842 16.7780791,30.2652842 C15.9882227,30.1898618 15.3002832,29.9761649 14.7142607,29.6241936 L17.6571128,22.3082185 L19.6827124,24.53318 C22.3580325,27.3740913 23.6956926,28.9453918 23.6956926,29.2470815 C23.6956926,29.6996161 23.3899417,30.0138762 22.7784399,30.1898618 C22.6510437,30.1898618 22.4854287,30.2150026 22.2815948,30.2652842 C21.8994062,30.2652842 21.7083119,30.4035587 21.7083119,30.6801076 C21.7083119,30.7303892 21.7337911,30.9315157 21.7847496,31.283487 C21.8866666,31.6103175 22.0140628,31.8491552 22.1669382,32 L22.6255645,32 C22.6510437,32 22.9695342,32 23.581036,32 C24.1925377,32 24.8804772,31.9748592 25.6448544,31.9245776 C26.4092315,31.874296 26.9315559,31.8617256 27.2118276,31.8868664 C29.5049591,31.8868664 30.7661815,31.9245776 30.9954946,32 L31.3012455,32 C31.4796002,31.8240143 31.5687775,31.6857399 31.5687775,31.5851767 C31.517819,30.9566565 31.3649436,30.5166923 31.1101512,30.2652842 L30.4986495,30.2652842 C29.6323553,30.2401434 28.8552385,30.1270098 28.1672991,29.9258833 C27.4793596,29.7247569 26.9570352,29.51106 26.6003258,29.2847927 C26.2436165,29.0585254 25.9378656,28.8322582 25.6830732,28.6059909 L25.2626658,28.26659 C25.2626658,28.2917308 24.0524019,26.9844088 21.6318742,24.3446239 L18.3068334,20.7243475 C18.3068334,20.6992067 18.7017617,19.6935744 19.4916181,17.7074506 C20.2814745,15.7213267 21.1095498,13.6849213 21.9758439,11.5982342 C22.842138,9.51154709 23.313504,8.43049234 23.3899417,8.35506992 C23.6447341,8.12880265 24.4218509,8.0030986 25.7212921,7.9779578 C26.6130655,7.9779578 27.0589521,7.86482416 27.0589521,7.63855689 C27.0589521,7.58827527 27.0334729,7.41228961 26.9825144,7.11059991 C26.8805975,6.70834698 26.7914201,6.46950931 26.7149824,6.39408688 C26.6385447,6.31866446 26.46019,6.28095325 26.1799184,6.28095325 C26.1034807,6.28095325 25.6830732,6.28095325 24.918696,6.28095325 C24.1543189,6.28095325 23.0204927,6.29352365 21.5172176,6.31866446 C19.962984,6.31866446 18.7781994,6.31866446 17.9628637,6.31866446 C17.1475281,6.31866446 16.7016414,6.30609405 16.6252036,6.28095325 C16.1920566,6.28095325 15.975483,6.39408688 15.975483,6.62035416 C15.975483,6.64549496 16.0009623,6.83405102 16.0519208,7.18602234 C16.1028792,7.43743042 16.1538377,7.61341608 16.2047962,7.71397931 C16.2557547,7.81454254 16.3194528,7.87739456 16.3958905,7.90253537 C16.4723282,7.92767618 16.5869848,7.95281699 16.7398602,7.9779578 C16.8927357,8.0030986 17.0965696,8.01566901 17.351362,8.01566901 C17.6061544,8.01566901 17.9373845,8.10366184 18.3450523,8.27964749 C18.7781994,8.48077396 18.9947729,8.606478 18.9947729,8.65675962 C18.9692937,8.65675962 18.3705315,10.1274969 17.1984865,13.0689715 L15.4022002,17.5188945 C10.6121032,12.3901696 8.15335658,9.68753275 8.02596038,9.41098386 C7.92404342,9.2098574 7.87308495,9.05901255 7.87308495,8.95844931 C7.87308495,8.48077396 8.24253392,8.16651386 8.98143186,8.01566901 C9.0069111,8.01566901 9.08334881,8.01566901 9.21074501,8.01566901 C9.33814121,8.01566901 9.41457892,8.0030986 9.44005816,7.9779578 C9.51649588,7.9779578 9.56745436,7.9779578 9.5929336,7.9779578 C9.61841284,7.9779578 9.6566317,7.96538739 9.70759018,7.94024658 C9.75854866,7.91510578 9.79676751,7.87739456 9.82224675,7.82711295 C9.84772599,7.77683133 9.86046561,7.6888385 9.86046561,7.56313446 C9.86046561,7.33686719 9.83498637,7.13574072 9.78402789,6.95975507 C9.70759018,6.63292456 9.63115246,6.4443685 9.55471474,6.39408688 C9.47827702,6.34380527 9.28718273,6.30609405 8.98143186,6.28095325 C8.90499414,6.28095325 8.67568098,6.28095325 8.29349239,6.28095325 C7.91130381,6.28095325 7.37623978,6.29352365 6.68830032,6.31866446 C6.00036086,6.34380527 5.2487233,6.34380527 4.43338764,6.31866446 C2.44600697,6.31866446 1.17204501,6.29352365 0.611501743,6.24324203 Z" id="è·¯å¾„"></path>                    <g transform="translate(31.786396, 0.000000)" id="è·¯å¾„" stroke-width="0.707">                        <path d="M2.29676233,12.9309861 C1.81038913,12.9309861 1.43209886,13.0820698 1.16189153,13.3842372 C0.891684198,13.6864046 0.747573621,14.0774447 0.729559799,14.5573576 C0.729559799,15.5349579 1.16189153,16.4059109 2.026555,17.1702166 C2.89121846,17.9345223 3.93602015,18.3166752 5.16096006,18.3166752 C5.79144383,18.3166752 6.18774792,18.3077879 6.34987232,18.2900134 C7.68289516,18.0767187 8.70067612,17.6056931 9.40321518,16.8769365 C10.1057542,16.1481799 10.4660307,15.3572124 10.4840445,14.5040339 C10.4840445,13.5797572 10.1778095,12.6821424 9.56533958,11.8111894 C8.95286963,10.9402364 8.10621999,10.3092398 7.02539065,9.91819967 L6.94432845,9.86487601 C6.94432845,9.84710146 7.02539065,9.82043963 7.18751505,9.78489053 C7.34963945,9.74934143 7.61083988,9.66935595 7.97111632,9.54493409 C8.33139276,9.42051223 8.67365538,9.23387944 8.99790418,8.98503572 C10.4750376,8.0429845 11.2136043,6.7987659 11.2136043,5.25237993 C11.2136043,3.84819037 10.6551758,2.62174633 9.53831885,1.5730478 C8.42146187,0.524349266 7.00737683,1.3397714e-16 5.29606372,1.3397714e-16 C3.85495795,1.3397714e-16 2.61200422,0.382152855 1.56720253,1.14645856 C0.522400844,1.91076427 4.07341989e-16,2.87947732 4.07341989e-16,4.05259771 C4.07341989e-16,4.55028515 0.1621244,4.95021256 0.486373199,5.25237993 C0.810621999,5.5545473 1.215933,5.71451827 1.7023062,5.73229282 C2.20669322,5.73229282 2.62101113,5.57232186 2.94525993,5.25237993 C3.26950873,4.93243801 3.43163313,4.5325106 3.43163313,4.05259771 C3.43163313,3.85707765 3.40461239,3.67933213 3.35057093,3.51936117 C3.29652946,3.35939021 3.23348108,3.2171938 3.16142579,3.09277194 C3.08937051,2.96835008 2.99029448,2.86170277 2.86419773,2.77283001 C2.73810097,2.68395726 2.63001804,2.61285905 2.53994893,2.5595354 C2.44987982,2.50621174 2.35981071,2.47066264 2.2697416,2.45288809 C2.17967249,2.43511354 2.1076172,2.40845171 2.05357573,2.37290261 L1.9454928,2.34624078 C2.86419773,1.54638597 3.9810547,1.14645856 5.29606372,1.14645856 C6.28682394,1.14645856 7.03439757,1.61748418 7.53878459,2.5595354 C7.84501956,3.14609559 7.99813705,4.04371044 7.99813705,5.25237993 L7.99813705,5.78561647 C7.99813705,7.47419885 7.42169474,8.60288287 6.26881012,9.17166851 C5.99860279,9.27831582 5.44918121,9.34052675 4.62054539,9.3583013 L3.48567459,9.38496313 L3.40461239,9.43828678 C3.36858475,9.49161043 3.35057093,9.63380684 3.35057093,9.86487601 C3.35057093,10.1848179 3.42262622,10.3447889 3.56673679,10.3447889 C4.07112382,10.3447889 4.59352466,10.3892253 5.13393932,10.478098 C5.74640928,10.5669708 6.30483777,10.9402364 6.80922479,11.5978948 C7.31361181,12.2555532 7.56580532,13.250928 7.56580532,14.5840194 L7.56580532,14.797314 C7.56580532,15.8104634 7.25056343,16.5125582 6.62007966,16.9035984 C6.22377557,17.1524421 5.80045075,17.2768639 5.35010519,17.2768639 C4.77366288,17.2768639 4.24225513,17.1791039 3.75588193,16.9835838 C3.26950873,16.7880638 2.92724611,16.5836564 2.72909406,16.3703618 C2.53094202,16.1570672 2.431866,16.0504199 2.431866,16.0504199 L2.5129282,16.0504199 C2.56696966,16.0326453 2.63902495,16.0148708 2.72909406,15.9970962 C2.81916317,15.9793217 2.90923228,15.9348853 2.99930139,15.8637871 C3.08937051,15.7926889 3.19745344,15.730478 3.32355019,15.6771543 C3.44964695,15.6238307 3.53971606,15.5260706 3.59375753,15.3838742 C3.64779899,15.2416778 3.71985428,15.1083687 3.80992339,14.9839468 C3.8999925,14.8595249 3.92701324,14.6817794 3.89098559,14.4507103 C3.89098559,14.0596701 3.76488884,13.7130664 3.51269533,13.410899 C3.26050182,13.1087316 2.85519082,12.9487607 2.29676233,12.9309861 Z"></path>                    </g>                </g>            </g>        </g>    </g></svg></div><p>ä¸Šå›¾æè¿°äº†å¦‚ä¸‹æ¦‚ç‡åˆ†å¸ƒçš„åˆ†è§£ (è¯¥å›¾æ˜¯ä¸€ä¸ªæœ‰ç¯å›¾çš„ç‰¹ä¾‹) ï¼š</p><p>$$g\left(X_{1}, X_{2}, X_{3}\right)=f_{1}\left(X_{1}\right) f_{2}\left(X_{1}, X_{2}\right) f_{3}\left(X_{1}, X_{2}\right) f_{4}\left(X_{2}, X_{3}\right)$$</p><h4 id="1-3-2-è”ç³»"><a href="#1-3-2-è”ç³»" class="headerlink" title="1.3.2 è”ç³»"></a>1.3.2 è”ç³»</h4><p>æ— è®ºæ˜¯<strong>æœ‰å‘æ¦‚ç‡å›¾æ¨¡å‹</strong>è¿˜æ˜¯<strong>æ— å‘æ¦‚ç‡å›¾æ¨¡å‹</strong>ï¼Œä»–ä»¬éƒ½èƒ½ä½¿ç”¨å› å­å›¾æ¥æè¿°ã€‚å› ä¸ºå› å­èŠ‚ç‚¹ (factor) ç†è®ºä¸Šå¯ä»¥æè¿°å˜é‡ä¹‹é—´çš„ä»»ä½•å®šä¹‰åœ¨å®æ•°åŸŸä¸Šçš„å‡½æ•°å…³ç³»ï¼Œè¿˜æ‘†è„±äº†probability functionçš„å–å€¼å¿…é¡»ä¸º$[0,1]$çš„é™åˆ¶ï¼Œå› æ­¤ä½¿ç”¨å› å­å›¾è¡¨ç¤ºçš„æ¦‚ç‡å›¾æ¨¡å‹å¯ä»¥å®ç°é«˜æ•ˆã€ç»Ÿä¸€çš„è®¡ç®—ã€‚</p><h2 id="2-ç®—æ³•"><a href="#2-ç®—æ³•" class="headerlink" title="2. ç®—æ³•"></a>2. ç®—æ³•</h2><h3 id="2-1-æ¶ˆæ¯ä¼ é€’ç®—æ³•-Massage-Passing-Algorithm"><a href="#2-1-æ¶ˆæ¯ä¼ é€’ç®—æ³•-Massage-Passing-Algorithm" class="headerlink" title="2.1 æ¶ˆæ¯ä¼ é€’ç®—æ³• (Massage Passing Algorithm)"></a>2.1 æ¶ˆæ¯ä¼ é€’ç®—æ³• (Massage Passing Algorithm)</h3><p><strong>æ¨æ–­</strong> (inference) æ˜¯æ¦‚ç‡å›¾æ¨¡å‹ä¸Šçš„ä¸€ä¸ªé‡è¦ä»»åŠ¡ï¼Œç®€å•æ¥è®²å°±æ˜¯å·²çŸ¥ä¸€ç³»åˆ—æ¡ä»¶æ¦‚ç‡å‡½æ•°ï¼Œæ±‚æ¯ä¸ªéšæœºå˜é‡çš„è¾¹ç¼˜æ¦‚ç‡ã€‚</p><p>ç›´æ¥åœ°ï¼Œè§£ä¸Šè¿°é—®é¢˜å¯ä»¥æœ‰ä¸¤ç§æ€è·¯ï¼š</p><ol><li>é‡‡ç”¨è§£æçš„æ–¹å¼ï¼Œåˆ©ç”¨è´å¶æ–¯æ³•åˆ™æ±‚è¾¹ç¼˜æ¦‚ç‡ï¼Œä½†è¿™ç§æ–¹æ³•çš„å¤æ‚åº¦éšå˜é‡è§„æ¨¡å‘ˆæŒ‡æ•°å¢åŠ <a href="#belief_propagation">[3]</a>ã€‚</li><li>é‡‡ç”¨æ¨¡æ‹Ÿçš„æ–¹æ³•ï¼Œè®©æ¶ˆæ¯æŒ‰ç…§æ¦‚ç‡å›¾æ¨¡å‹è¿›è¡Œä¼ é€’ï¼Œåœ¨è¿­ä»£$N$æ¬¡ååˆ†ææ¶ˆæ¯çš„ä¼ é€’æ•ˆæœã€‚</li></ol><p>åè€…ç”±äºç®€å•çš„æ€è·¯ä¸è®¡ç®—å¤æ‚åº¦ï¼Œå…·æœ‰è¾ƒé«˜çš„å®ç”¨æ€§ï¼Œå› æ­¤è¢«å¹¿æ³›ä½¿ç”¨ï¼Œè¿™ç±»æ–¹æ³•è¢«ç§°ä¸ºæ¶ˆæ¯ä¼ é€’ç®—æ³•ã€‚</p><p>å…³äºæ¶ˆæ¯ä¼ é€’ï¼Œ<a href="https://www.cnblogs.com/ironstark/p/5146818.html">è¿™ç¯‡blog</a>åšäº†è¯¦ç»†çš„é˜è¿°ï¼Œå¯ä»¥å‚è€ƒã€‚</p><h4 id="2-1-1-ç½®ä¿¡ä¼ æ’­ç®—æ³•-Belief-propagation"><a href="#2-1-1-ç½®ä¿¡ä¼ æ’­ç®—æ³•-Belief-propagation" class="headerlink" title="2.1.1 ç½®ä¿¡ä¼ æ’­ç®—æ³• (Belief propagation)"></a>2.1.1 ç½®ä¿¡ä¼ æ’­ç®—æ³• (Belief propagation)</h4><p><strong>åˆå</strong>ï¼šsumâ€“product message passing</p><p><strong>ç½®ä¿¡ä¼ æ’­ç®—æ³•</strong> (Belief propagation) ï¼Œæ˜¯ä¸€ç§æ¶ˆæ¯ä¼ é€’ç®—æ³• (message passing algorithm) ï¼Œç”¨äºå¯¹å›¾æ¨¡å‹ (ä¾‹å¦‚è´å¶æ–¯ç½‘ç»œå’Œé©¬å°”å¯å¤«éšæœºåœº) è¿›è¡Œæ¨ç†ã€‚å®ƒä»¥ä»»ä½•è§‚å¯Ÿåˆ°çš„èŠ‚ç‚¹ (æˆ–å˜é‡) ä¸ºæ¡ä»¶ï¼Œè®¡ç®—æ¯ä¸ªæœªè§‚å¯Ÿåˆ°çš„èŠ‚ç‚¹ (æˆ–å˜é‡) çš„<strong>è¾¹ç¼˜åˆ†å¸ƒ</strong>ã€‚</p><p>BPç®—æ³•åœ¨è´å¶æ–¯ç½‘ç»œä¸é©¬å°”å¯å¤«éšæœºåœºä¸Šéƒ½æœ‰ç›¸å…³çš„å˜ä½“ï¼Œä½†ç”±äºä¸Šè¿°ä¸¤å¼ å›¾æ¨¡å‹éƒ½å¯ä»¥ä½¿ç”¨å› å­å›¾æ¥æè¿°ï¼Œå› æ­¤åæ–‡ç›´æ¥å™è¿°BPç®—æ³•åœ¨å› å­å›¾ä¸Šçš„è®¡ç®—æ–¹æ³•ã€‚</p><h5 id="2-1-1-1-ç®—æ³•æè¿°"><a href="#2-1-1-1-ç®—æ³•æè¿°" class="headerlink" title="2.1.1.1 ç®—æ³•æè¿°"></a>2.1.1.1 ç®—æ³•æè¿°</h5><p>ç»™å®šå› å­å›¾$G=(V, F, E)$ï¼Œæœ‰è”åˆè´¨é‡å‡½æ•°$p(\mathbf{x})=\prod_{a \in F} f_{a}\left(\mathbf{x}<em>{a}\right)$ï¼Œå…¶ä¸­$\mathbf{x}</em>{a}$æ˜¯å› å­èŠ‚ç‚¹$a$çš„æ‰€æœ‰é‚»å±…å˜é‡èŠ‚ç‚¹æ‰€æ„æˆçš„å‘é‡ã€‚æ³¨ï¼šè”åˆè´¨é‡å‡½æ•° (Joint mass function) æ˜¯å˜é‡åœ¨æ¯ä¸ªå¯èƒ½çš„å€¼ä¸Šå–å¾—è¯¥å€¼çš„æ¦‚ç‡ï¼Œä¸åŒäºæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œæ¦‚ç‡è´¨é‡å‡½æ•°æ˜¯åœ¨ç¦»æ•£éšæœºå˜é‡ä¸Šå®šä¹‰çš„ï¼Œè€Œæ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯åœ¨è¿ç»­éšæœºå˜é‡ä¸Šå®šä¹‰çš„ã€‚</p><p>BPç®—æ³•çš„åŸç†åœ¨äºï¼Œè®©è¢«ç§°ä¸ºæ¶ˆæ¯ (Message) çš„å®å€¼å‡½æ•°æ²¿ç€æ¦‚ç‡å›¾ä¸­éšè—èŠ‚ç‚¹ä¹‹é—´çš„è¾¹è¿›è¡Œä¼ é€’ã€‚å…·ä½“è€Œè¨€åŒ…å«ä¸¤ç§æƒ…å†µï¼Œå…¶ä¸­$\operatorname{Dom}(v)$è¡¨ç¤ºå˜é‡$v$çš„å®šä¹‰åŸŸã€‚</p><ul><li>ä»å˜é‡èŠ‚ç‚¹$v$åˆ°å› å­èŠ‚ç‚¹$a$çš„ä¼ é€’ï¼Œè®°ä¼ é€’çš„æ¶ˆæ¯ä¸º$\mu_{v \to a}$ï¼Œ$\mu_{v \rightarrow a}: \operatorname{Dom}(v) \rightarrow \mathbb{R}$ã€‚</li><li>ä»å› å­èŠ‚ç‚¹$a$åˆ°å˜é‡èŠ‚ç‚¹$v$çš„ä¼ é€’ï¼Œè®°ä¸º$\mu_{a \to v}$ï¼Œ$\mu_{a \rightarrow v}: \operatorname{Dom}(v) \rightarrow \mathbb{R}$ã€‚</li></ul><p>BPç®—æ³•ä¸­å¯¹äºä¸Šè¿°ä¸¤ç§æ¶ˆæ¯çš„ä¼ é€’ï¼Œå®šä¹‰ä¸åŒçš„è®¡ç®—æ–¹å¼</p><ul><li>å¯¹äºä»å˜é‡èŠ‚ç‚¹åˆ°å› å­èŠ‚ç‚¹çš„æ¶ˆæ¯ä¼ é€’$\mu_{v \to a}$  <p>$$  \mu_{v \rightarrow a}\left(x_{v}\right)=\prod_{a^{*} \in N(v) \backslash\{a\}} \mu_{a^{*} \rightarrow v}\left(x_{v}\right)  $$</p>å…¶ä¸­$x_{v} \in \operatorname{Dom}(v)$ï¼Œ$N(v)$æ˜¯èŠ‚ç‚¹$v$çš„é‚»å±…èŠ‚ç‚¹ï¼Œå¦‚æœ$N(v) \backslash\{a\} = \emptyset$ï¼Œåˆ™ä¸Šå¼è¢«ç½®ä¸ºå‡åŒ€åˆ†å¸ƒã€‚</li><li>å¯¹äºä»å› å­èŠ‚ç‚¹åˆ°å˜é‡èŠ‚ç‚¹çš„æ¶ˆæ¯ä¼ é€’$\mu_{a \to v}$è¢«å®šä¹‰ä¸º<strong>å› å­ä¸æ¥è‡ªæ‰€æœ‰å…¶ä»–èŠ‚ç‚¹çš„æ¶ˆæ¯çš„ä¹˜ç§¯</strong>ï¼Œé™¤ä¸$v$ç›¸å…³çš„å˜é‡å¤–ï¼Œæ‰€æœ‰å˜é‡éƒ½è¢«è¾¹ç¼˜åŒ– (marginalized)   <p>$$  \mu_{a \rightarrow v}\left(x_{v}\right)=\sum_{\mathbf{x}_{a}^{\prime}: x_{v}^{\prime}=x_{v}} f_{a}\left(\mathbf{x}_{a}^{\prime}\right) \prod_{v^{*} \in N(a) \backslash\{v\}} \mu_{v^{*} \rightarrow a}\left(x_{v^{*}}^{\prime}\right)  $$</p>      å…¶ä¸­$x_{v} \in \operatorname{Dom}(v)$ï¼Œ$N(a)$æ˜¯èŠ‚ç‚¹$a$çš„é‚»å±…èŠ‚ç‚¹ï¼Œå¦‚æœ$N(a) \backslash{v} = \emptyset$ï¼Œåˆ™$\mu_{a \rightarrow v}\left(x_{v}\right)=f_{a}\left(x_{v}\right)$</li></ul><p>ç”±ä¸Šå¼å¯ä»¥çœ‹å‡ºï¼ŒBPç®—æ³•å°†å®Œå…¨è¾¹ç¼˜åŒ–è®¡ç®— (complete marginalization) ç®€åŒ–ä¸ºäº†æ›´ç®€å•çš„é¡¹çš„ä¹˜ç§¯å’Œï¼Œè¿™ä¹Ÿæ˜¯BPç®—æ³•è¢«ç§°ä¸ºsum-product message passingæˆ–sum-product algorithmçš„åŸå› ã€‚</p><p>åœ¨å…¸å‹çš„è¿è¡Œæƒ…å†µä¸­ï¼Œæ¯æ¡æ¶ˆæ¯éƒ½å°†ä»ç›¸é‚»æ¶ˆæ¯çš„å…ˆå‰å€¼ä¸Šè¿­ä»£æ›´æ–°ã€‚å¯ä»¥ä½¿ç”¨ä¸åŒçš„è°ƒåº¦ç­–ç•¥æ¥æ›´æ–°æ¶ˆæ¯ã€‚åœ¨å›¾æ¨¡å‹æ˜¯æ ‘çš„æƒ…å†µä¸‹ï¼Œæœ€ä¼˜è°ƒåº¦å…è®¸åœ¨åªè®¡ç®—æ¯æ¡æ¶ˆæ¯ä¸€æ¬¡åè¾¾åˆ°æ”¶æ•›ã€‚å½“å› å­å›¾æœ‰å¾ªç¯æ—¶ï¼Œè¿™æ ·çš„æœ€ä¼˜è°ƒåº¦æ˜¯ä¸å­˜åœ¨çš„ï¼Œå…¸å‹çš„é€‰æ‹©æ˜¯åœ¨æ¯æ¬¡è¿­ä»£æ—¶åŒæ—¶æ›´æ–°æ‰€æœ‰æ¶ˆæ¯ã€‚</p><h5 id="2-1-1-2-æ”¶æ•›æ€§"><a href="#2-1-1-2-æ”¶æ•›æ€§" class="headerlink" title="2.1.1.2 æ”¶æ•›æ€§"></a>2.1.1.2 æ”¶æ•›æ€§</h5><p>å½“BPç®—æ³•æ”¶æ•›æ—¶ï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„<strong>ä¼°è®¡è¾¹é™…åˆ†å¸ƒ</strong>ä¸æ¥è‡ªç›¸é‚»å› å­èŠ‚ç‚¹çš„æ‰€æœ‰æ¶ˆæ¯çš„ä¹˜ç§¯æˆæ­£æ¯” (ä¸‹å¼ç¼ºå°‘å½’ä¸€åŒ–å¸¸æ•°) </p><p>$$p_{X_{v}}\left(x_{v}\right) \propto \prod_{a \in N(v)} \mu_{a \rightarrow v}\left(x_{v}\right)$$</p><p>å±äºä¸€ä¸ªå› å­èŠ‚ç‚¹çš„ä¸€ç»„å˜é‡èŠ‚ç‚¹çš„<strong>ä¼°è®¡è”åˆè¾¹é™…åˆ†å¸ƒ</strong>ä¸å› å­å’Œå˜é‡æ¶ˆæ¯çš„ä¹˜ç§¯æˆæ­£æ¯”ï¼Œå³ï¼š</p><p>$$p_{X_{a}}\left(\mathbf{x}_{a}\right) \propto f_{a}\left(\mathbf{x}_{a}\right) \prod_{v \in N(a)} \mu_{v \rightarrow a}\left(x_{v}\right)$$</p>åœ¨å› å­å›¾æ˜¯éå¾ªç¯çš„ (å³æ ‘æˆ–æ£®æ—) çš„æƒ…å†µä¸‹ï¼Œè¿™äº›ä¼°è®¡çš„è¾¹é™…å®é™…ä¸Šä¼šåœ¨æœ‰é™æ¬¡æ•°çš„è¿­ä»£ä¸­æ”¶æ•›åˆ°çœŸå®çš„è¾¹é™…ã€‚è¿™å¯ä»¥é€šè¿‡[æ•°å­¦å½’çº³æ³•](https://en.wikipedia.org/wiki/Mathematical_induction)æ¥è¯æ˜ã€‚<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a id="graphical_model">[1]</a> <a href="https://en.wikipedia.org/wiki/Graphical_model">https://en.wikipedia.org/wiki/Graphical_model</a></p><p><a id="bayesian_model">[2]</a> <a href="https://en.wikipedia.org/wiki/Bayesian_network">https://en.wikipedia.org/wiki/Bayesian_network</a></p><p><a id="belief_propagation">[3]</a> <a href="https://en.wikipedia.org/wiki/Belief_propagation#Motivation">https://en.wikipedia.org/wiki/Belief_propagation#Motivation</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Bayesian </tag>
            
            <tag> Belief Propagation </tag>
            
            <tag> Markov Random Field </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Root Cause Analysis and Diagnosis in Cellular Network</title>
      <link href="/uncategorized/surveys/rca_in_cellular/"/>
      <url>/uncategorized/surveys/rca_in_cellular/</url>
      
        <content type="html"><![CDATA[<p>æœ¬æ–‡ä¸»è¦è°ƒç ”äº†ï¼ŒLTEåœºæ™¯ä¸‹çš„å¼‚å¸¸æ ¹å› åˆ†æï¼ˆRCAï¼‰æ–¹æ³•ï¼Œç”¨äºæ”¯æŒåœ¨4G LTE/5Gåœºæ™¯ä¸‹çš„ç”¨æˆ·æŠ•è¯‰åˆ†æã€‚</p><p>2022-04-18: ä»¥â€œroot cause analysis, cellular networkâ€ä¸ºå…³é”®è¯ï¼Œè°ƒç ”äº†google scholarä¸­top 2 pagesçš„ç›¸å…³å·¥ä½œï¼ŒTODO: å®Œå–„æŸäº›è®ºæ–‡çš„å¯å€Ÿé‰´ç‚¹æ€»ç»“ã€‚</p><span id="more"></span><h2 id="ä»»åŠ¡ç›®æ ‡"><a href="#ä»»åŠ¡ç›®æ ‡" class="headerlink" title="ä»»åŠ¡ç›®æ ‡"></a>ä»»åŠ¡ç›®æ ‡</h2><ul><li><p><strong>æ€»ä½“ç›®æ ‡</strong>ï¼šå‘ç°ç”¨æˆ·çš„æŠ•è¯‰æ ¹å› ï¼Œ<strong>è§£é‡Š</strong>ï¼šç½‘ç»œå‘ç”Ÿäº†ä»€ä¹ˆé—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¦‚ä½•ä¸€æ­¥ä¸€æ­¥å½±å“ç”¨æˆ·ï¼ˆåœ¨ç½‘ç»œæœºç†æ–¹é¢æ„æˆå®Œæ•´é“¾æ¡ï¼‰ï¼Œæœ€ç»ˆå¯¼è‡´ç”¨æˆ·æŠ•è¯‰ã€‚</p></li><li><p><strong>ä¸»è¦å‘ç°å‡ ç±»é—®é¢˜</strong>ï¼š</p><ul><li><p>ç½‘ç»œä¾§é—®é¢˜ï¼ˆé‡ç‚¹ï¼‰ï¼šå°†ç”¨æˆ·æŠ•è¯‰è§†ä¸ºç½‘ç»œå‡ºç°é—®é¢˜çš„æŒ‡ç¤ºå‰‚ï¼Œå‘ç°ç½‘ç»œä¸­å‡ºç°äº†ä»€ä¹ˆé—®é¢˜</p></li><li><p>ç”¨æˆ·ä¾§é—®é¢˜ï¼šä¸Šä¸€ä¸ªé—®é¢˜å‡è®¾ç”¨æˆ·çš„æŠ•è¯‰éƒ½æ˜¯å…¬å¹³çš„ï¼Œè¿˜æœ‰ä¸€ç§å¯èƒ½ï¼Œç”¨æˆ·æ‰€åœ¨çš„ä½ç½®æœ‰ä¸å¯é¿å…çš„ç¯å¢ƒå› ç´ ï¼Œå¯¼è‡´æœåŠ¡ä½“éªŒå·®</p></li></ul></li><li><p><strong>ä¸‹ä¸€æ­¥å·¥ä½œæ–¹å‘</strong>ï¼šå°†å‘æ˜åˆ°çš„<u>å¼‚å¸¸æ ¹å› </u>ä»â€œ<u>å•ç»´KPIå¼‚å¸¸</u>â€æ‹“å±•åˆ°â€œ<u>å¸¦ç½‘ç»œæœºç†é“¾æ¡çš„å¤šç»´KPIå¼‚å¸¸pattern</u>â€ä¸Š</p></li></ul><p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/FM%26PM%20logic%20related%20graph.png" alt="ç½‘ç»œæœºç†å›¾-KPIæŒ‡æ ‡å…³è”å…³ç³»"></p><h2 id="ç›¸å…³å·¥ä½œè°ƒç ”"><a href="#ç›¸å…³å·¥ä½œè°ƒç ”" class="headerlink" title="ç›¸å…³å·¥ä½œè°ƒç ”"></a>ç›¸å…³å·¥ä½œè°ƒç ”</h2><ol><li><p><strong>Root Cause Analysis in 5G/6G Networks</strong></p><p> Dinis Canastro; Ricardo Rocha; MÃ¡rio Antunes; Diogo Gomes; Rui L. Aguiar</p><p> 2021 8th International Conference on Future Internet of Things and Cloud (FiCloud)</p><p> æ–‡ç« æ¯”è¾ƒæ–°ï¼Œä¼šè®®ä¸çŸ¥åï¼Œä½†å¯ä»¥å‚è€ƒrelated workéƒ¨åˆ†ï¼Œä»¥åŠå…¶ä¸­æåˆ°çš„å…¶ä»–å·¥ä½œã€‚</p></li><li><p>ğŸŒŸ<strong>Automatic root cause analysis based on traces for LTE self-organizing networks</strong></p><p> Ana Gomez-Andrades; Raquel Barco; Immaculada Serrano; Patricia Delgado; Patricia Caro-Oliver; Pablo Munoz</p><p> IEEE Wireless Communications (Volume: 23, Issue: 3, June 2016)</p><p> æœ¬æ–‡çš„ç ”ç©¶é‡ç‚¹åœ¨äºï¼ŒåŸºäºç”¨æˆ·çº§åˆ«çš„æ•°æ®ï¼Œåˆ†æç”¨æˆ·è¿æ¥è¢«é‡Šæ”¾çš„åŸå› ï¼Œæœ¬æ–‡åŒ…å«å¤§é‡æœ‰å…³èœ‚çªç½‘ç»œçš„<u>èƒŒæ™¯çŸ¥è¯†</u>ï¼Œå¾ˆå€¼å¾—å€Ÿé‰´</p><ul><li>ç”¨æˆ·è¿æ¥è¢«é‡Šæ”¾çš„åŸå› å¯åˆ†ä¸ºï¼šNormal Release, Access Failure, Dropped Connection</li><li>RFå±‚é¢ä¸Šå®ç°è‡ªåŠ¨è¯Šæ–­æ‰€éœ€è¦å…³æ³¨çš„æŒ‡æ ‡ä¿¡æ¯ï¼ˆIndicators and Measurementsï¼‰</li><li>RFå±‚é¢ä¸Šçš„å¤§ä½“æ ¹å› åŒ…æ‹¬ï¼šCoverage Hole (CH), Lack of Dominant Cells (LD), Cell Edge (CE), Mobility Problems (MP), Interference (I)<ul><li>æ³¨ï¼šå…¶ä»–å±‚é¢çš„å¼‚å¸¸åŸå› è¿˜åŒ…æ‹¬ï¼šexcessive antenna downtilt (EAD), too small antenna downtilt (TSAD), coverage hole (CH), too late handover (TLH), inter-system interference (ISI), excessive reduction of transmit power (ERTP), and normal cell (Normal) <a href="https://link.springer.com/article/10.1007/s11036-020-01589-1">link</a></li></ul></li><li>æœ¬æ–‡è¿˜åŒ…å«ä¸€äº›åœ¨èœ‚çªç½‘ç»œä¸­åšRCAçš„å¿…è¦èƒŒæ™¯çŸ¥è¯†ï¼ˆLTE Traces and Eventsï¼‰ï¼Œä»¥åŠä¸€ä¸ªuser case</li></ul><p> æœ¬æ–‡æ‰€æå‡ºçš„æ¡†æ¶å¤§ä½“ä¸ºï¼šâ€œé¦–å…ˆï¼Œå®ƒæ ¹æ®é‡Šæ”¾çš„ç±»å‹å¯¹è¿æ¥è¿›è¡Œåˆ†ç±»ï¼Œéšåï¼Œæ ¹æ®å¼‚å¸¸é‡Šæ”¾çš„è¿æ¥çš„äº‹ä»¶ä¿¡æ¯ï¼Œç¡®å®šå…·ä½“çš„æ•…éšœåŸå› ã€‚â€</p></li><li><p>ğŸŒŸ<strong>Root Cause Analysis Based on Temporal Analysis ofMetrics Toward Self-Organizing 5G Networks</strong></p><p> Pablo MuÃ±oz, Isabel de la Bandera, Emil J. Khatib, Ana GÃ³mez-Andrades, Inmaculada Serrano, and Raquel Barco</p><p> IEEE Transactions on Vehicular Technology ( Volume: 66, Issue: 3, March 2017)</p><p> æœ¬æ–‡æå‡ºçš„è‡ªåŠ¨RCAæ–¹æ³•æ˜¯åŸºäºå°åŒºçº§åˆ«çš„KPIçš„ã€‚ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•è€ƒè™‘äº†ç½‘ç»œåº¦é‡çš„<u>æ—¶é—´ä¾èµ–æ€§</u>å’Œ<u>æ•…éšœå¯¹ç›¸é‚»å°åŒºçš„å½±å“</u>ï¼Œä»¥å®ç°æ›´å¥½çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚</p><ul><li>ç®—æ³•çš„è¾“å…¥åŒ…æ‹¬<u>å‘Šè­¦å°åŒºä»¥åŠå‘¨å›´å°åŒºçš„å¤§é‡KPI</u>ã€‚</li><li>å…³è”æ€§åº¦é‡æ–¹æ³•è¢«ç”¨äºè®¡ç®—è¿™äº›æŒ‡æ ‡ä¹‹é—´çš„ç›¸å…³æ€§ä»¥è¡¨å¾ç½‘ç»œçš„çŠ¶æ€ã€‚</li><li>é€šè¿‡è®¡ç®—åŠ æƒç›¸å…³æ€§å°†è¯¥çŠ¶æ€ä¸å­˜å‚¨çš„æ•…éšœæ¨¡å¼è¿›è¡Œæ¯”è¾ƒä»¥æä¾›è¯Šæ–­ã€‚</li><li>æ­¤ç›¸å…³æ€§ç”±æ ¹æ®å…ˆå‰è®¡ç®—çš„åº¦é‡ç›¸å…³æ€§å’Œä¸“å®¶çŸ¥è¯†æ„å»ºçš„æœ‰æ•ˆæƒé‡è¿›è¡Œè°ƒåˆ¶ã€‚</li></ul></li><li><p><strong>Root Cause Analysis of Reduced Accessibility in 4G Networks</strong></p><p> Diogo Ferreira, Carlos Senna, Paulo Salvador, LuÃ­s CortesÃ£o, Cristina Pires, Rui Pedro &amp; Susana Sargento </p><p> International Conference on Machine Learning for Networking, MLN 2019: Machine Learning for Networking pp 117â€“133</p><p> æœ¬æ–‡ç®—æ˜¯ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œåˆ†æäº† 4G ç½‘ç»œå¯è®¿é—®æ€§é™ä½çš„å¯èƒ½æ ¹æœ¬åŸå› ï¼Œå¯å‚è€ƒæ€§åœ¨äº<u>åˆ†æç»“è®º</u>ï¼Œä»¥åŠL<u>TEæ•°æ®ä¸­é€šç”¨çš„æ•°æ®ï¼ˆé¢„ï¼‰å¤„ç†æ€è·¯</u>ã€‚</p><ul><li><p>æœ¬æ–‡åˆ†æäº† 4G ç½‘ç»œå¯è®¿é—®æ€§é™ä½çš„å¯èƒ½æ ¹æœ¬åŸå› ï¼ŒåŒæ—¶è€ƒè™‘äº†é‡è¦å…³é”®ç»©æ•ˆæŒ‡æ ‡ (KPI) çš„ä¿¡æ¯ï¼Œå¹¶è€ƒè™‘äº†å®ƒä»¬åœ¨ä»¥å‰æ—¶é—´æ¡†æ¶ä¸­çš„æ¼”å˜ã€‚</p></li><li><p>ç»“æœè¡¨æ˜ï¼Œç½‘ç»œå¯è®¿é—®æ€§é™ä½çš„ä¸»è¦åŸå› ä¸æ•…éšœåˆ‡æ¢æ¬¡æ•°ã€ç½‘ç»œä¸­çš„ç”µè¯å’ŒçŸ­ä¿¡æ•°é‡ã€æ•´ä½“ä¸‹è½½é‡å’Œå°åŒºå¯ç”¨æ€§æœ‰å…³ã€‚ç„¶è€Œï¼Œæ¯ä¸ªå°åŒºçš„å¯è®¿é—®æ€§é™ä½çš„ä¸»è¦åŸå› æ›´å¤šåœ°ä¸æ¯ä¸ªå°åŒºçš„ç”¨æˆ·æ•°é‡åŠå…¶äº§ç”Ÿçš„ä¸‹è½½é‡æœ‰å…³ã€‚</p></li></ul></li><li><p>ğŸŒŸ<strong>Automatic Root Cause Analysis for LTE Networks Based on Unsupervised Techniques</strong></p><p> Ana GÃ³mez-Andrades; Pablo MuÃ±oz; Inmaculada Serrano; Raquel Barco</p><p> IEEE Transactions on Vehicular Technology ( Volume: 65, Issue: 4, April 2016)</p><p> ã€å¾…è¡¥å……ç»†èŠ‚ã€‘</p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> survey </tag>
            
            <tag> root cause analysis </tag>
            
            <tag> diagnosis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>æœ¬ç«™ç»´æŠ¤æ‰‹å†Œ</title>
      <link href="/uncategorized/notes/hexo_notes/"/>
      <url>/uncategorized/notes/hexo_notes/</url>
      
        <content type="html"><![CDATA[<p>æœ¬æ–‡æ¡£è®°å½•äº†å»ºç«‹åŸºäºhexoçš„ç½‘é¡µç³»ç»Ÿçš„æ­¥éª¤ï¼Œå¹¶å¯¹ç»´æŠ¤æ‰€éœ€è¦çš„æ“ä½œä¸€ä¸€åˆ—å‡ºï¼Œæ—¥åçš„æœ¬ç«™æ›´æ–°å°†ä¾æ®è¯¥æ–‡æ¡£è¿›è¡Œã€‚</p><p>Note: éšæ—¶ä¼šæ›´æ–°ç»´æŠ¤ç­–ç•¥</p><span id="more"></span><h2 id="ğŸŒŸ-ç›®å‰æ­£åœ¨ç»´æŠ¤çš„è®ºæ–‡ä¸»é¢˜åˆ—è¡¨"><a href="#ğŸŒŸ-ç›®å‰æ­£åœ¨ç»´æŠ¤çš„è®ºæ–‡ä¸»é¢˜åˆ—è¡¨" class="headerlink" title="ğŸŒŸ ç›®å‰æ­£åœ¨ç»´æŠ¤çš„è®ºæ–‡ä¸»é¢˜åˆ—è¡¨"></a>ğŸŒŸ ç›®å‰æ­£åœ¨ç»´æŠ¤çš„è®ºæ–‡ä¸»é¢˜åˆ—è¡¨</h2><h3 id="æŒ‰ä»»åŠ¡åˆ†ç±»"><a href="#æŒ‰ä»»åŠ¡åˆ†ç±»" class="headerlink" title="æŒ‰ä»»åŠ¡åˆ†ç±»"></a>æŒ‰ä»»åŠ¡åˆ†ç±»</h3><ul><li>Anomaly detection / Outlier / Out-of-distribution</li><li>Interpretable / Explainable</li><li>Causal discovery</li><li>Data augmentation </li></ul><h3 id="æŒ‰æ•°æ®åˆ†ç±»"><a href="#æŒ‰æ•°æ®åˆ†ç±»" class="headerlink" title="æŒ‰æ•°æ®åˆ†ç±»"></a>æŒ‰æ•°æ®åˆ†ç±»</h3><ul><li>Time series</li><li>Missing value / Irregular sampled / Imputation</li><li>Sequence</li><li>Heterogeneous</li></ul><h3 id="æŒ‰æ·±åº¦å­¦ä¹ æ¶æ„åˆ†ç±»"><a href="#æŒ‰æ·±åº¦å­¦ä¹ æ¶æ„åˆ†ç±»" class="headerlink" title="æŒ‰æ·±åº¦å­¦ä¹ æ¶æ„åˆ†ç±»"></a>æŒ‰æ·±åº¦å­¦ä¹ æ¶æ„åˆ†ç±»</h3><ul><li>Recurrent neural network / RNN / LSTM / GRU </li><li>Autoencoder</li></ul><h3 id="æŒ‰åº”ç”¨åˆ†ç±»"><a href="#æŒ‰åº”ç”¨åˆ†ç±»" class="headerlink" title="æŒ‰åº”ç”¨åˆ†ç±»"></a>æŒ‰åº”ç”¨åˆ†ç±»</h3><ul><li>Cloud native</li><li>Micro-service</li></ul><h2 id="ğŸŒŸ-ç›®å‰æ­£åœ¨ç»´æŠ¤çš„é¡¶ä¼šåˆ—è¡¨"><a href="#ğŸŒŸ-ç›®å‰æ­£åœ¨ç»´æŠ¤çš„é¡¶ä¼šåˆ—è¡¨" class="headerlink" title="ğŸŒŸ ç›®å‰æ­£åœ¨ç»´æŠ¤çš„é¡¶ä¼šåˆ—è¡¨"></a>ğŸŒŸ ç›®å‰æ­£åœ¨ç»´æŠ¤çš„é¡¶ä¼šåˆ—è¡¨</h2><ul><li>NeurIPS</li><li>SIGKDD</li><li>IJCAI</li><li>ICML</li><li>SIGIR</li><li>CVPR</li><li>WWW</li><li>ICLR</li><li>AAAI</li><li>SIGMOD</li><li>NDSS</li><li>ESEC/FSE</li><li>ICSE</li><li>ASE</li><li>ISSRE</li></ul><h2 id="ğŸŒŸ-åˆ¶ä½œæ–°é¡µé¢çš„è§„èŒƒ"><a href="#ğŸŒŸ-åˆ¶ä½œæ–°é¡µé¢çš„è§„èŒƒ" class="headerlink" title="ğŸŒŸ åˆ¶ä½œæ–°é¡µé¢çš„è§„èŒƒ"></a>ğŸŒŸ åˆ¶ä½œæ–°é¡µé¢çš„è§„èŒƒ</h2><p>âš ï¸âš ï¸âš ï¸ ç”±äºé…ç½®åŸå› ï¼Œæœ€å¥½å°†æ–‡ç« ä¸­çš„<strong>å…¨è§’ç¬¦å·</strong>æ›¿æ¢ä¸º<strong>åŠè§’ç¬¦å·+ç©ºæ ¼</strong>ï¼Œä»¥è·å¾—æœ€ä½³çš„å­—ä½“æ•ˆæœ</p><h3 id="é¡µé¢é…ç½®"><a href="#é¡µé¢é…ç½®" class="headerlink" title="é¡µé¢é…ç½®"></a>é¡µé¢é…ç½®</h3><ol><li>å¯¹äºæ–°çš„æ ‡ç­¾é¡µ</li></ol><ul><li>å¯ä»¥ç›´æ¥åœ¨<code>&lt;hexo_path&gt;</code>ä½¿ç”¨å‘½ä»¤<code>hexo new page &lt;new_page_name&gt;</code></li><li>å¯ä»¥åœ¨<code>&lt;hexo_path&gt;/source</code>ä¸‹æ–°å»ºç›¸åº”çš„æ–‡ä»¶å¤¹ï¼Œå¹¶æ‰‹åŠ¨åœ¨å…¶ä¸­æ–°å»º<code>index.md</code>æ–‡ä»¶</li></ul><pre class="line-numbers language-console" data-language="console"><code class="language-console">$ cd &lt;hexo_path&gt;&#x2F;source$ mkdir &lt;new_folder&gt;$ cd &lt;new_folder&gt;$ vim index.md<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>éšååœ¨<code>&lt;hexo_path&gt;/_config.mashiro.yml</code>ä¸­çš„<code>menu</code>å­—æ®µè¿›è¡Œé…ç½®ï¼š<code>&lt;display_name&gt;: /&lt;new_page_name&gt;</code></p><ol start="2"><li><code>index.md</code>æ–‡ä»¶çš„ä¹¦å†™è§„èŒƒ</li></ol><p>âš ï¸ ä»…æœ‰åœ¨<code>&lt;hexo_path&gt;/source/_post</code>ä¸­çš„æ–‡æ¡£ï¼Œå…¶ä¸­çš„<code>tag</code>å­—æ®µå¯ä»¥è¢«æ­£ç¡®ç´¢å¼•ï¼Œå…¶ä»–ç›®å½•ä¸­è‹¥åŒ…å«<code>tag</code>å­—æ®µåˆ™ä¼šå¯¼è‡´éƒ¨ç½²å¤±è´¥ã€‚</p><ol start="3"><li>æ™®é€šmarkdownæ–‡ä»¶çš„ä¹¦å†™è§„èŒƒ</li></ol><p>æ–‡ä»¶å¤´ç¤ºä¾‹å¦‚ä¸‹ï¼Œåç»­çš„æ–‡ç« æ ‡é¢˜ä»<strong>äºŒçº§ç›®å½•</strong>å¼€å§‹ä¹¦å†™ã€‚</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">title</span><span class="token punctuation">:</span> Some Notes<span class="token key atrule">updated</span><span class="token punctuation">:</span> <span class="token datetime number">2022-04-12 20:00:00</span><span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2022-04-12 16:07:32</span><span class="token key atrule">tag</span><span class="token punctuation">:</span> <span class="token punctuation">-</span> note<span class="token punctuation">-</span> 5G<span class="token punctuation">-</span> 4G<span class="token punctuation">-</span> 5G NR<span class="token punctuation">---</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="4"><li><p>åœ¨é€‚å½“çš„ä½ç½®æ·»åŠ <code>&lt;--! more --&gt;</code>ä»¥æ§åˆ¶æ–‡ç« åœ¨é¦–é¡µæ˜¾ç¤ºçš„æ‘˜è¦å†…å®¹ã€‚</p></li><li><p>æ–‡æ¡£å†…éƒ¨å¼•ç”¨</p></li></ol><p>âš ï¸ éœ€è¦å‚è€ƒ<code>&lt;hexo_path&gt;/public</code>ä¸­ç”Ÿæˆçš„æ–‡æ¡£ç»“æ„ã€‚ä¸€èˆ¬çš„ï¼Œ<code>&lt;hexo_path&gt;/source/_post</code>ä¸­<code>&lt;dir_name&gt;/&lt;file_name&gt;.md</code>æ–‡æ¡£å°†å­˜å‚¨åœ¨<code>&lt;hexo_path&gt;/public/uncategorized/&lt;dir_name&gt;/&lt;file_name&gt;/index.html</code></p><p>âš ï¸âš ï¸ hexoç°å·²é…ç½®ä¸ºæ— é¡»<code>/index.html</code>åç¼€ï¼Œæ–‡ç« å†…å¼•ç”¨è¯·ä½¿ç”¨<code>/uncategorized/&lt;dir_name&gt;/&lt;file_name&gt;</code></p><h3 id="é¡µé¢éƒ¨ç½²"><a href="#é¡µé¢éƒ¨ç½²" class="headerlink" title="é¡µé¢éƒ¨ç½²"></a>é¡µé¢éƒ¨ç½²</h3><ol><li>é¡µé¢æœ¬åœ°æµ‹è¯•</li></ol><p>è®¿é—®<a href="https://localhost:4000ä»¥æµ‹è¯•">https://localhost:4000ä»¥æµ‹è¯•</a></p><pre class="line-numbers language-console" data-language="console"><code class="language-console">$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo s<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="2"><li>é¡µé¢éƒ¨ç½²</li></ol><p>å®˜æ–¹æ–¹æ³•ï¼šä»£ç å¦‚ä¸‹ã€‚æ³¨æ„<code>font-spider</code>é‚£ä¸€è¡Œï¼Œæ‚¨çš„<code>public</code>æ–‡ä»¶å¤¹ä¸­æœ€æ·±çš„htmlæ–‡ä»¶åµŒå¥—äº†å‡ å±‚ï¼Œå°±åº”å½“åœ¨åé¢å†™å‡ å±‚çš„é€šé…ç¬¦ï¼Œå¯ä»¥å°†ä¸Šé¢è¿™äº›è¯­å¥ä¿å­˜ä¸ºä¸€ä¸ªè„šæœ¬æ–‡ä»¶ï¼Œéƒ¨ç½²æ—¶è¿è¡Œä¸€ä¸‹å°±è¡Œäº†ã€‚</p><pre class="line-numbers language-console" data-language="console"><code class="language-console">$ hexo clean$ hexo g$ font-spider public&#x2F;*.html public&#x2F;*&#x2F;*.html public&#x2F;*&#x2F;*&#x2F;*.html public&#x2F;*&#x2F;*&#x2F;*&#x2F;*.html public&#x2F;*&#x2F;*&#x2F;*&#x2F;*&#x2F;*.html$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>âš ï¸âš ï¸ ä¸Šè¿°ä»£ç å·²å†™å…¥<code>&lt;hexo_path&gt;/mydeploy.sh</code>ï¼Œè¿è¡Œ<code>sudo ./mydeploy.sh</code></p><h2 id="åŸºäºæ¨¡ç‰ˆçš„å°ä¿®æ”¹"><a href="#åŸºäºæ¨¡ç‰ˆçš„å°ä¿®æ”¹" class="headerlink" title="åŸºäºæ¨¡ç‰ˆçš„å°ä¿®æ”¹"></a>åŸºäºæ¨¡ç‰ˆçš„å°ä¿®æ”¹</h2><h3 id="æ·»åŠ æ–‡ç« æ›´æ–°æ—¶é—´-å¹¶æŒ‰æ›´æ–°æ—¶é—´å€’åºæ’åˆ—æ–‡ç« "><a href="#æ·»åŠ æ–‡ç« æ›´æ–°æ—¶é—´-å¹¶æŒ‰æ›´æ–°æ—¶é—´å€’åºæ’åˆ—æ–‡ç« " class="headerlink" title="æ·»åŠ æ–‡ç« æ›´æ–°æ—¶é—´, å¹¶æŒ‰æ›´æ–°æ—¶é—´å€’åºæ’åˆ—æ–‡ç« "></a>æ·»åŠ æ–‡ç« æ›´æ–°æ—¶é—´, å¹¶æŒ‰æ›´æ–°æ—¶é—´å€’åºæ’åˆ—æ–‡ç« </h3><p>æœ¬ç«™æ¨¡ç‰ˆä¸­æ²¡æœ‰å†…ç½®æ›´æ–°æ—¶é—´çš„æ˜¾ç¤ºï¼Œéœ€è¦æŒ‰éœ€å¯¹æ¨¡ç‰ˆåšä»¥ä¸‹ä¿®æ”¹</p><h4 id="æ›´æ”¹Hexoç­–ç•¥"><a href="#æ›´æ”¹Hexoç­–ç•¥" class="headerlink" title="æ›´æ”¹Hexoç­–ç•¥"></a>æ›´æ”¹Hexoç­–ç•¥</h4><ol><li>è°ƒæ•´Hexoä¸­postçš„é»˜è®¤ç”Ÿæˆæ ¼å¼ï¼šåœ¨<code>&#123;hexo_path&#125;/scaffolds/post.md</code>ä¸­æŒ‡å®šé»˜è®¤æ¨¡ç‰ˆæ ·å¼</li></ol><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">title</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#123;</span> title <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#123;</span> date <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token key atrule">updated</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#123;</span> date <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token punctuation">---</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>å¼€å¯<code>updated</code>æ’åº: åœ¨<code>.md</code>æ–‡ä»¶çš„å¤´éƒ¨åŠ å…¥<code>updatedå­—æ®µ</code>, å¹¶å®Œå–„è¯¥å­—æ®µ</li><li>è°ƒæ•´ Hexo ä¸»é…ç½®æ–‡ä»¶: åœ¨<code>&lt;hexo_path&gt;/_config.yml</code>ä¸­æ›´æ–°æ–‡ç« æ’åºä¸º<u>æŒ‰ç…§æ›´æ–°æ—¶é—´æ’åº</u></li></ol><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token comment"># Home page setting</span><span class="token comment"># path: Root path for your blogs index page. (default = '')</span><span class="token comment"># per_page: Posts displayed per page. (0 = disable pagination)</span><span class="token comment"># order_by: Posts order. (Order by date descending by default)</span><span class="token key atrule">index_generator</span><span class="token punctuation">:</span>  <span class="token key atrule">path</span><span class="token punctuation">:</span> <span class="token string">''</span>  <span class="token key atrule">per_page</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">order_by</span><span class="token punctuation">:</span> <span class="token punctuation">-</span>updated<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="æ›´æ”¹æ¨¡ç‰ˆæ ·å¼"><a href="#æ›´æ”¹æ¨¡ç‰ˆæ ·å¼" class="headerlink" title="æ›´æ”¹æ¨¡ç‰ˆæ ·å¼"></a>æ›´æ”¹æ¨¡ç‰ˆæ ·å¼</h4><p>åŸä¸»é¢˜é»˜è®¤æ²¡æœ‰æ›´æ–°æ—¶é—´æ˜¾ç¤ºçš„ï¼Œæ‰€ä»¥éœ€è¦è‡ªå·±æ–°å¢æ›´æ–°æ—¶é—´æ˜¾ç¤ºã€‚è®°ä¸»é¢˜æ–‡ä»¶å¤¹ä¸º<code>&#123;theme_path&#125;</code>, ä¸€èˆ¬ä½äº<code>&#123;hexo_path&#125;/themes/&#123;theme_name&#125;</code></p><ol><li>ç¡®å®šä¿®æ”¹èŒƒå›´</li></ol><p>æŸ¥çœ‹äº†<code>&#123;theme_path&#125;/layout</code>æ–‡ä»¶å¤¹ä¸‹çš„<code>post.ejs</code>æ–‡ä»¶ï¼Œé‡Œé¢å¼•å…¥çš„æ˜¯ <code>_partial/article</code>çš„å†…å®¹ã€‚æ‰¾åˆ°<code>&#123;theme_path&#125;/layout/_partial</code>æ–‡ä»¶å¤¹ä¸‹çš„ <code>article.ejs</code>ã€‚å…¶ä¸­å…³äºæ—¶é—´çš„å†…å®¹å¦‚ä¸‹ï¼Œè¯´æ˜æ—¥æœŸéƒ¨åˆ†å¼•ç”¨äº†<code>post/date</code>ï¼Œæ ·å¼æ–‡ä»¶ä¸º<code>article-date</code></p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"article-meta"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'post/date'</span><span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token literal-property property">class_name</span><span class="token operator">:</span> <span class="token string">'article-date'</span><span class="token punctuation">,</span> <span class="token literal-property property">date_format</span><span class="token operator">:</span> <span class="token keyword">null</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'post/category'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>ä¿®æ”¹èŒƒå›´å³ä¸º:</p><ul><li><code>&#123;theme_path&#125;/layout/_partial/article.ejs</code></li><li><code>&#123;theme_path&#125;/layout/_partial/post/date.ejs</code></li><li><code>&#123;theme_path&#125;/source/css/_partial/article.styl</code></li></ul><ol start="2"><li>æ–°å»º<code>updated.ejs</code></li></ol><p>è€ƒè™‘å…¼å®¹æ€§ï¼Œæ¨¡ä»¿<code>&#123;theme_path&#125;/layout/_partial/post/date.ejs</code>æ–°å»ºä¸€ä¸ª<code>updated.ejs</code>æ–‡ä»¶ã€‚</p><p>ä¿®æ”¹<code>date.ejs</code>å¦‚ä¸‹ï¼Œå…¶ä¸­ä»…æ·»åŠ äº†<code>&lt;%= __(&#39;published&#39;) %&gt;</code>å­—æ®µï¼Œç”¨äºå®šä¹‰è¯­è¨€ã€‚</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token operator">&lt;</span>a href<span class="token operator">=</span><span class="token string">"&lt;%- url_for(post.path) %>"</span> <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"&lt;%= class_name %>"</span><span class="token operator">></span>  <span class="token operator">&lt;</span><span class="token operator">%=</span> <span class="token function">__</span><span class="token punctuation">(</span><span class="token string">'published'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span> <span class="token operator">&lt;</span>time <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"dt-published"</span> datetime<span class="token operator">=</span><span class="token string">"&lt;%= date_xml(post.date) %>"</span> itemprop<span class="token operator">=</span><span class="token string">"datePublished"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%=</span> <span class="token function">date</span><span class="token punctuation">(</span>post<span class="token punctuation">.</span>date<span class="token punctuation">,</span> date_format<span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>time<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>a<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>æ–°å»ºç›¸ä¼¼çš„<code>updated.ejs</code>å¦‚ä¸‹ï¼Œå…¶ä¸­å®šä¹‰äº†<code>&lt;%= __(&#39;updated&#39;) %&gt;</code>å­—æ®µï¼Œç”¨äºå®šä¹‰è¯­è¨€ã€‚</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token operator">&lt;</span>a href<span class="token operator">=</span><span class="token string">"&lt;%- url_for(post.path) %>"</span> <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"&lt;%= class_name %>"</span><span class="token operator">></span>  <span class="token operator">&lt;</span><span class="token operator">%=</span> <span class="token function">__</span><span class="token punctuation">(</span><span class="token string">'updated'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span> <span class="token operator">&lt;</span>time <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"dt-published"</span> datetime<span class="token operator">=</span><span class="token string">"&lt;%= date_xml(post.updated) %>"</span>   itemprop<span class="token operator">=</span><span class="token string">"dateUpdated"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%=</span> <span class="token function">date</span><span class="token punctuation">(</span>post<span class="token punctuation">.</span>updated<span class="token punctuation">,</span> date_format<span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>time<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>a<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ol start="3"><li>å¢åŠ å­—æ®µè¯­è¨€å®šä¹‰</li></ol><p>ç”±äº<code>date.ejs</code>å’Œ<code>updated.ejs</code>ä¸­åˆ†åˆ«å¼•å…¥äº†<code>published</code>å’Œ<code>updated</code>å­—æ®µï¼Œå› æ­¤éœ€è¦åœ¨è¯­è¨€æ–‡ä»¶ä¸­æ–°å¢å¯¹åº”çš„å­—æ®µã€‚<br>è¯­è¨€æ–‡ä»¶åœ¨ <code>&#123;theme_path&#125;/languages</code>ä¸­ï¼ŒæŒ‰ç…§ä¸»é¢˜çš„è¯­è¨€è®¾ç½®ï¼Œä¿®æ”¹å¯¹åº”çš„è¯­è¨€æ–‡ä»¶ï¼Œæ²¡æŒ‡å®šå°±ä¿®æ”¹<code>default.yml</code>æ–‡ä»¶ã€‚</p><p>åœ¨<code>&#123;theme_path&#125;/languages/zh-CN.yml</code>ä¸­å¢åŠ ä¸‹è¿°å†…å®¹ï¼Œæ ¼å¼ä¸º<code>å­—æ®µå: å­—æ®µå€¼</code></p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">published</span><span class="token punctuation">:</span> å‘å¸ƒäº<span class="token key atrule">updated</span><span class="token punctuation">:</span> æ›´æ–°äº<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="4"><li>ä¿®æ”¹<code>article.ejs</code></li></ol><p>è¯¥æ–‡ä»¶ä¸­è®°å½•äº†æ–‡ç« æ—¥æœŸåˆ—çš„æ˜¾ç¤ºå†…å®¹ï¼Œåœ¨å·²æœ‰çš„å‘å¸ƒæ—¶é—´åå¢åŠ æ›´æ–°æ—¶é—´ã€‚</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"article-meta"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'post/date'</span><span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token literal-property property">class_name</span><span class="token operator">:</span> <span class="token string">'article-date'</span><span class="token punctuation">,</span> <span class="token literal-property property">date_format</span><span class="token operator">:</span> <span class="token keyword">null</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'post/updated'</span><span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token literal-property property">class_name</span><span class="token operator">:</span> <span class="token string">'article-date'</span><span class="token punctuation">,</span> <span class="token literal-property property">date_format</span><span class="token operator">:</span> <span class="token keyword">null</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'post/category'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="5"><li>å¯¹æ—¥æœŸçš„æ ·å¼è¿›è¡Œå¾®è°ƒ</li></ol><p>ç”±äºå‘å¸ƒæ—¶é—´äºæ›´æ–°æ—¶é—´çš„é—´éš”å¤ªå°ï¼Œå½±å“ç¾è§‚ï¼Œå› æ­¤åœ¨<code>&#123;theme_path&#125;/source/css/_partial/article.styl</code>ä¸­æ‰¾åˆ°æ ·å¼<code>article-date</code>ï¼Œè¿›è¡Œæ ·å¼å¾®è°ƒã€‚</p><p>åŸæ ·å¼ï¼š</p><pre class="line-numbers language-css" data-language="css"><code class="language-css">.article-date  @extend $block-caption  <span class="token property">float</span><span class="token punctuation">:</span> left<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>åœ¨æ ·å¼åå¢åŠ ä¸€ä¸ªç©ºæ ¼ï¼Œæ›´æ–°ä¸º</p><pre class="line-numbers language-css" data-language="css"><code class="language-css">.article-date  @extend $block-caption  <span class="token property">float</span><span class="token punctuation">:</span> left  &amp;<span class="token punctuation">:</span>after    <span class="token property">content</span><span class="token punctuation">:</span> <span class="token string">"\00a0"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Â©ï¸ æœ¬èŠ‚å†…å®¹éƒ¨åˆ†å‚è€ƒ <a href="https://blog.vanxnf.top/2018/09/03/Hexo-%E5%8D%9A%E5%AE%A2-Hiker-%E4%B8%BB%E9%A2%98%E5%A2%9E%E5%8A%A0%E6%96%87%E7%AB%A0%E6%9C%80%E5%90%8E%E7%BC%96%E8%BE%91%E6%97%B6%E9%97%B4%EF%BC%8C%E5%B9%B6%E6%8C%89%E7%85%A7%E6%9C%80%E5%90%8E%E7%BC%96%E8%BE%91%E6%97%B6%E9%97%B4%E6%8E%92%E5%BA%8F/">link</a></p><h2 id="å‚è€ƒ"><a href="#å‚è€ƒ" class="headerlink" title="å‚è€ƒ"></a>å‚è€ƒ</h2><ol><li>æœ¬ç«™ä½¿ç”¨çš„æ¨¡ç‰ˆï¼š<a href="https://github.com/bill-xia/hexo-theme-mashiro">hexo-theme-mashiro</a></li><li>TODOï¼šç«™å†…æœç´¢ <a href="https://liam.page/2017/09/21/local-search-engine-in-Hexo-site/">link</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in ICLR 2022 (2022.04.25)</title>
      <link href="/uncategorized/paperlistfile/ICLR2022/"/>
      <url>/uncategorized/paperlistfile/ICLR2022/</url>
      
        <content type="html"><![CDATA[<p><a href="https://openreview.net/group?id=ICLR.cc/2022/Conference">Accept paper list</a></p><span id="more"></span><h2 id="anomaly-detection-anomaly-outlier-out-of-distribution-one-class"><a href="#anomaly-detection-anomaly-outlier-out-of-distribution-one-class" class="headerlink" title="anomaly detection [anomaly, outlier, out-of-distribution, one-class]"></a>anomaly detection [anomaly, outlier, out-of-distribution, one-class]</h2><ul><li><p>Anomaly Detection for Tabular Data with Internal Contrastive Learning</p><p>Tom Shenkar, Lior Wolf</p><p><strong>æ‘˜è¦</strong>ï¼š æˆ‘ä»¬è€ƒè™‘åœ¨è¡¨æ ¼æ•°æ®ä¸­å¯»æ‰¾ç±»å¤–æ ·æœ¬çš„ä»»åŠ¡ï¼Œå…¶ä¸­å‡ ä¹ä¸èƒ½å‡è®¾æ•°æ®çš„ç»“æ„ã€‚<br>ä¸ºäº†æ•æ‰å•ä¸ªè®­ç»ƒç±»æ ·æœ¬çš„ç»“æ„ï¼Œæˆ‘ä»¬å­¦ä¹ äº†æœ€å¤§åŒ–æ¯ä¸ªæ ·æœ¬ä¸è¢«å±è”½éƒ¨åˆ†ä¹‹é—´çš„äº’ä¿¡æ¯çš„æ˜ å°„ã€‚é€šè¿‡ä½¿ç”¨å¯¹æ¯”æŸå¤±æ¥å­¦ä¹ æ˜ å°„ï¼Œè¯¥æŸå¤±ä¸€æ¬¡åªè€ƒè™‘ä¸€ä¸ªæ ·æœ¬ã€‚ä¸€æ—¦å­¦ä¹ ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨è¯¥æ ·æœ¬çš„æ©ç éƒ¨åˆ†æµ‹é‡å­¦ä¹ çš„æ˜ å°„æ˜¯å¦å¯¼è‡´å°çš„å¯¹æ¯”æŸå¤±æ¥å¯¹æµ‹è¯•æ ·æœ¬è¿›è¡Œè¯„åˆ†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸æ–‡çŒ®ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å­˜åœ¨ç›¸å½“å¤§çš„å‡†ç¡®æ€§å·®è·ï¼Œå¹¶ä¸”ç›¸åŒçš„é»˜è®¤è¶…å‚æ•°é›†åœ¨åŸºå‡†æµ‹è¯•ä¸­æä¾›äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</p><p><strong>ä¸€å¥è¯æ€»ç»“</strong>ï¼š ä¸€ç§åŸºäºé¢„æµ‹å‘é‡ä¸­è¢«å±è”½éƒ¨åˆ†çš„èƒ½åŠ›çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚</p></li><li><p>Igeood: An Information Geometry Approach to Out-of-Distribution Detection </p><p>Eduardo Dadalto Camara Gomes, Florence Alberge, Pierre Duhamel, Pablo Piantanida</p><p><strong>æ‘˜è¦</strong>ï¼šå¯é çš„åˆ†å¸ƒå¤– (OOD) æ£€æµ‹æ˜¯å®ç°æ›´å®‰å…¨çš„ç°ä»£æœºå™¨å­¦ä¹  (ML) ç³»ç»Ÿçš„åŸºç¡€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† Igeoodï¼Œä¸€ç§æ£€æµ‹ OOD æ ·æœ¬çš„æœ‰æ•ˆæ–¹æ³•ã€‚Igeood é€‚ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„ç¥ç»ç½‘ç»œï¼Œåœ¨å¯¹ ML æ¨¡å‹çš„ä¸åŒç¨‹åº¦çš„è®¿é—®ä¸‹å·¥ä½œï¼Œä¸éœ€è¦ OOD æ ·æœ¬æˆ–å¯¹ OOD æ•°æ®çš„å‡è®¾ï¼Œä½†ä¹Ÿå¯ä»¥ä» OOD æ ·æœ¬ä¸­å—ç›Šï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ã€‚é€šè¿‡å»ºç«‹åŸºç¡€æ•°æ®åˆ†å¸ƒä¹‹é—´çš„æµ‹åœ°çº¿ï¼ˆFisher-Raoï¼‰è·ç¦»ï¼Œæˆ‘ä»¬çš„é‰´åˆ«å™¨å¯ä»¥ç»“åˆæ¥è‡ª logits è¾“å‡ºçš„ç½®ä¿¡åº¦åˆ†æ•°å’Œæ·±åº¦ç¥ç»ç½‘ç»œçš„å­¦ä¹ ç‰¹å¾ã€‚æ ¹æ®ç»éªŒï¼Œæˆ‘ä»¬è¡¨æ˜ Igeood åœ¨å„ç§ç½‘ç»œæ¶æ„å’Œæ•°æ®é›†ä¸Šä¼˜äºç«äº‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p><p><strong>ä¸€å¥è¯æ€»ç»“</strong>ï¼š æˆ‘ä»¬é€šè¿‡å»ºç«‹æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„ Fisher-Rao è·ç¦»ï¼Œæå‡ºäº†ä¸€ç§çµæ´»æœ‰æ•ˆçš„åˆ†å¸ƒå¤–æ£€æµ‹æ–¹æ³•ã€‚</p></li></ul><ul><li><p>VOS: Learning What You Donâ€™t Know by Virtual Outlier Synthesis </p><p>Xuefeng Du, Zhaoning Wang, Mu Cai, Yixuan Li</p><p><strong>æ‘˜è¦</strong>ï¼š ç”±äºå…¶åœ¨ç¥ç»ç½‘ç»œçš„å®‰å…¨éƒ¨ç½²ä¸­çš„é‡è¦æ€§ï¼Œåˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ£€æµ‹æœ€è¿‘å—åˆ°äº†å¾ˆå¤šå…³æ³¨ã€‚ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯æ¨¡å‹ç¼ºä¹æ¥è‡ªæœªçŸ¥æ•°æ®çš„ç›‘ç£ä¿¡å·ï¼Œå› æ­¤å¯èƒ½ä¼šå¯¹ OOD æ•°æ®äº§ç”Ÿè¿‡åº¦è‡ªä¿¡çš„é¢„æµ‹ã€‚ä»¥å‰çš„æ–¹æ³•ä¾èµ–äºçœŸæ­£çš„å¼‚å¸¸æ•°æ®é›†æ¥è¿›è¡Œæ¨¡å‹æ­£åˆ™åŒ–ï¼Œè¿™åœ¨å®è·µä¸­å¯èƒ½ä»£ä»·é«˜æ˜‚ï¼Œæœ‰æ—¶ç”šè‡³ä¸å¯è¡Œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† VOSï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ OOD æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”åˆæˆè™šæ‹Ÿå¼‚å¸¸å€¼ï¼Œå¯ä»¥åœ¨è®­ç»ƒæœŸé—´æœ‰æ„ä¹‰åœ°è§„èŒƒæ¨¡å‹çš„å†³ç­–è¾¹ç•Œã€‚å…·ä½“æ¥è¯´ï¼ŒVOS ä»ç‰¹å¾ç©ºé—´ä¸­ä¼°è®¡çš„ç±»æ¡ä»¶åˆ†å¸ƒçš„ä½ä¼¼ç„¶åŒºåŸŸé‡‡æ ·è™šæ‹Ÿå¼‚å¸¸å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„æœªçŸ¥æ„ŸçŸ¥è®­ç»ƒç›®æ ‡ï¼Œå®ƒå¯¹æ¯”åœ°å¡‘é€ äº† ID æ•°æ®å’Œåˆæˆå¼‚å¸¸å€¼æ•°æ®ä¹‹é—´çš„ä¸ç¡®å®šæ€§ç©ºé—´ã€‚VOS åœ¨ç‰©ä½“æ£€æµ‹å’Œå›¾åƒåˆ†ç±»æ¨¡å‹ä¸Šéƒ½å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸ä¹‹å‰åœ¨ç‰©ä½“æ£€æµ‹å™¨ä¸Šçš„æœ€ä½³æ–¹æ³•ç›¸æ¯”ï¼ŒFPR95 é™ä½äº†é«˜è¾¾ 7.87%ã€‚ä»£ç å¯åœ¨ <a href="https://github.com/deeplearning-wisc/vos">https://github.com/deeplearning-wisc/vos</a> è·å¾—ã€‚</p></li></ul><h2 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h2><ul><li><p>Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting </p><p>Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X. Liu, Schahram Dustdar</p></li><li><p>CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting </p><p>Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, Steven Hoi</p></li><li><p>Huber Additive Models for Non-stationary Time Series Analysis </p><p>Yingjie Wang, Xianrui Zhong, Fengxiang He, Hong Chen, Dacheng Tao</p></li><li><p>DEPTS: Deep Expansion Learning for Periodic Time Series Forecasting </p><p>Wei Fan, Shun Zheng, Xiaohan Yi, Wei Cao, Yanjie Fu, Jiang Bian, Tie-Yan Liu</p></li><li><p>Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift </p><p>Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, Jaegul Choo</p></li><li><p>Omni-Scale CNNs: a simple and effective kernel size configuration for time series classification </p><p>Wensi Tang, Guodong Long, Lu Liu, Tianyi Zhou, Michael Blumenstein, Jing Jiang</p></li><li><p>T-WaveNet: A Tree-Structured Wavelet Neural Network for Time Series Signal Analysis </p><p>Minhao LIU, Ailing Zeng, Qiuxia LAI, Ruiyuan Gao, Min Li, Jing Qin, Qiang Xu</p></li><li><p>Graph-Guided Network for Irregularly Sampled Multivariate Time Series </p><p>Xiang Zhang, Marko Zeman, Theodoros Tsiligkaridis, Marinka Zitnik</p></li><li><p>Heteroscedastic Temporal Variational Autoencoder For Irregularly Sampled Time Series </p><p>Satya Narayan Shukla, Benjamin Marlin</p></li><li><p>Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks </p><p>Andrea Cini, Ivan Marisca, Cesare Alippi</p></li></ul><ul><li><p>Coherence-based Label Propagation over Time Series for Accelerated Active Learning </p><p>Yooju Shin, Susik Yoon, Sundong Kim, Hwanjun Song, Jae-Gil Lee, Byung Suk Lee</p></li><li><p>PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series </p><p>Paul Jeha, Michael Bohlke-Schneider, Pedro Mercado, Shubham Kapoor, Rajbir Singh Nirwan, Valentin Flunkert, Jan Gasthaus, Tim Januschowski</p></li></ul><h2 id="Sequence-learning"><a href="#Sequence-learning" class="headerlink" title="Sequence learning"></a>Sequence learning</h2><ul><li><p>Efficiently Modeling Long Sequences with Structured State Spaces </p><p>Albert Gu, Karan Goel, Christopher Re</p></li><li><p>Long Expressive Memory for Sequence Modeling </p><p>T. Konstantin Rusch, Siddhartha Mishra, N. Benjamin Erichson, Michael W. Mahoney</p></li><li><p><strong>ã€éœ€è¦çœ‹çœ‹ã€‘</strong> On the approximation properties of recurrent encoder-decoder architectures<br>Zhong Li, Haotian Jiang, Qianxiao Li</p><p><strong>æ‘˜è¦</strong>ï¼š ç¼–ç å™¨-è§£ç å™¨æ¶æ„æœ€è¿‘åœ¨åºåˆ—åˆ°åºåˆ—å»ºæ¨¡æ–¹é¢è·å¾—äº†æ™®åŠï¼Œåœ¨æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ˆå¦‚è½¬æ¢å™¨ï¼‰ä¸­å…·æœ‰ç‰¹è‰²ã€‚ç„¶è€Œï¼Œå¯¹å…¶å·¥ä½œåŸç†çš„æ•°å­¦ç†è§£ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¾ªç¯ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„è¿‘ä¼¼ç‰¹æ€§ã€‚å…ˆå‰çš„å·¥ä½œä¸ºçº¿æ€§è®¾ç½®ä¸­çš„ RNN å»ºç«‹äº†ç†è®ºç»“æœï¼Œå…¶ä¸­è¿‘ä¼¼èƒ½åŠ›å¯èƒ½ä¸ç›®æ ‡æ—¶é—´å…³ç³»çš„å¹³æ»‘åº¦å’Œè®°å¿†æœ‰å…³ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å‘ç°ç¼–ç å™¨å’Œè§£ç å™¨ä¸€èµ·å½¢æˆäº†ä¸€ä¸ªç‰¹å®šçš„â€œæ—¶é—´ç§¯ç»“æ„â€ï¼Œå®ƒå†³å®šäº†é€¼è¿‘æ•ˆç‡ã€‚æ­¤å¤–ï¼Œç¼–ç å™¨-è§£ç å™¨æ¶æ„æ³›åŒ–äº†å…·æœ‰å­¦ä¹ æ—¶é—´éå‡åŒ€å…³ç³»çš„èƒ½åŠ›çš„ RNNã€‚</p><p><strong>ä¸€å¥è¯æ€»ç»“</strong>ï¼š ç»™å‡ºäº†å¾ªç¯ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„è¿‘ä¼¼å±æ€§ï¼Œå…¶ä¸­å½¢æˆçš„æ—¶é—´ç§¯ç»“æ„è¿›ä¸€æ­¥è¡¨å¾äº†èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ <br>çš„æ—¶é—´å…³ç³»ã€‚</p></li><li><p>Temporal Alignment Prediction for Supervised Representation Learning and Few-Shot Sequence Classification </p><p>Bing Su, Ji-Rong Wen</p></li></ul><h1 id="interpretable-interpretability"><a href="#interpretable-interpretability" class="headerlink" title="interpretable/interpretability"></a>interpretable/interpretability</h1><ul><li><p>Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset </p><p>Leon Sixt, Martin Schuessler, Oana-Iuliana Popescu, Philipp WeiÃŸ, Tim Landgraf</p></li><li><p>Toward Faithful Case-based Reasoning through Learning Prototypes in a Nearest Neighbor-friendly Space. </p><p>Seyed Omid Davoudi, Majid Komeili</p></li><li><p>Explaining Point Processes by Learning Interpretable Temporal Logic Rules </p><p>Shuang Li, Mingquan Feng, Lu Wang, Abdelmajid Essofi, Yufeng Cao, Junchi Yan, Le Song</p></li><li><p>Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions </p><p>Arda Sahiner, Tolga Ergen, Batu Ozturkler, Burak Bartan, John M. Pauly, Morteza Mardani, Mert Pilanci</p></li><li><p>Model Agnostic Interpretability for Multiple Instance Learning </p><p>Joseph Early, Christine Evers, SArvapali Ramchurn</p></li><li><p>POETREE: Interpretable Policy Learning with Adaptive Decision Trees </p><p>AlizÃ©e Pace, Alex Chan, Mihaela van der Schaar</p></li><li><p>NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning </p><p>Chun-Hao Chang, Rich Caruana, Anna Goldenberg</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in AAAI 2022 (2022.2.22)</title>
      <link href="/uncategorized/paperlistfile/AAAI2022/"/>
      <url>/uncategorized/paperlistfile/AAAI2022/</url>
      
        <content type="html"><![CDATA[<p><a href="https://aaai.org/Conferences/AAAI-22/wp-content/uploads/2021/12/AAAI-22_Accepted_Paper_List_Main_Technical_Track.pdf">accept paper list</a></p><p><a href="https://mp.weixin.qq.com/s/N2tQOBnYOuC9ZwHG7PIx4w">æ—¶åºç›¸å…³è®ºæ–‡ä¸€è§ˆ</a></p><span id="more"></span><h2 id="Anomaly-detection-anomaly-outlier-out-of-distribution-one-class"><a href="#Anomaly-detection-anomaly-outlier-out-of-distribution-one-class" class="headerlink" title="Anomaly detection [anomaly, outlier, out-of-distribution, one-class]"></a>Anomaly detection [anomaly, outlier, out-of-distribution, one-class]</h2><ul><li><p>A Causal Inference Look at Unsupervised Video Anomaly Detection </p><p>Xiangru Lin, Yuyang Chen, Guanbin Li, Yizhou Yu</p></li><li><p>Comprehensive Regularization in a Bi-Directional Predictive Network for Video Anomaly Detection</p><p>Chengwei Chen, Yuan Xie, Shaohui Lin, Angela Yao, Guannan Jiang, Wei Zhang, Yanyun Qu, Ruizhi Qiao, Bo Ren, Lizhuang Ma</p></li></ul><ul><li><p>Towards a Rigorous Evaluation of Time-Series Anomaly Detection</p><p>Siwon Kim, Kukjin Choi, Hyun-Soo Choi, Byunghan Lee, Sungroh Yoon</p></li><li><p>Unsupervised Anomaly Detection by Robust Density Estimation</p><p>Boyang Liu, Pang-Ning Tan, Jiayu Zhou</p></li></ul><ul><li><p>Self-Training Multi-Sequence Learning with Transformer for Weakly Supervised Video Anomaly Detection</p><p>Shuo Li, Fang Liu, Licheng Jiao</p></li><li><p>Transferring the Contamination Factor between Anomaly Detection Domains by Shape Similarity</p><p>Lorenzo Perini, Vincent Vercruyssen, Jesse Davis</p></li></ul><h2 id="Heterogeneous-data"><a href="#Heterogeneous-data" class="headerlink" title="Heterogeneous data"></a>Heterogeneous data</h2><ul><li><p>Heterogeneous Facility Location with Limited Resources</p><p>Argyrios Deligkas, Aris Filos Ratsikas, Alexandros A. Voudouris</p></li><li><p>H^2-MIL: Exploring Hierarchical Representation with Heterogeneous Multiple Instance Learning for Whole Slide Image Analysis</p><p>Wentai Hou, Lequan Yu, Chengxuan Lin, Helong Huang, Rongshan Yu, Jing Qin, Liansheng Wang</p></li><li><p>FedProto: Federated Prototype Learning Across Heterogeneous Clients</p><p>Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, Chengqi Zhang</p></li></ul><h2 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h2><ul><li><p>Conditional Loss and Deep Euler Scheme for Time Series Generation</p><p>Carl Remlinger, Joseph Mikael, Romuald Elie</p></li></ul><ul><li><p>Training Robust Deep Models for Time-Series Domain: Novel Algorithms and Theoretical Analysis</p><p>Taha Belkhouja, Yan Yan, Janardhan Rao Doppa</p></li></ul><ul><li><p>CATN: Cross Attentive Tree-Aware Network for Multivariate Time Series Forecasting</p><p>Hui He, Qi Zhang, Simeng Bai, Kun Yi, Zhendong Niu</p></li></ul><ul><li>Reinforcement Learning based Dynamic Model Combination for Time Series ForecastingYuwei Fu, Di Wu, Benoit Boulet</li></ul><ul><li><p><strong>ã€éœ€è¦çœ‹çœ‹ã€‘</strong> TS2Vec: Towards Universal Representation of Time Series</p><p>Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, Bixiong Xu</p></li></ul><ul><li><p><strong>ã€éœ€è¦çœ‹çœ‹ã€‘</strong> I-SEA: Importance Sampling and Expected Alignment-Based Deep Distance Metric Learning for Time Series Analysis and Embedding</p><p>Sirisha Rambhatla, Zhengping Che, Yan Liu</p></li></ul><ul><li><p>Clustering Interval-Censored Time-Series for Disease Phenotyping</p><p>Irene Y. Chen, Rahul G. Krishnan, David Sontag</p></li><li><p><strong>ã€éœ€è¦çœ‹çœ‹ã€‘</strong> Learning Temporal Point Processes for Efficient Retrieval of Continuous Time Event Sequences</p><p>Vinayak Gupta, Srikanta Bedathur, Abir De</p></li></ul><h2 id="Sequence-learning"><a href="#Sequence-learning" class="headerlink" title="Sequence learning"></a>Sequence learning</h2><ul><li><p>Post-OCR Document Correction with Large Ensembles of Character Sequence-to-Sequence Models</p><p>Juan Ramirez-Orta, Eduardo Xamena, Ana Maguitman, Evangelos Milios, Axel J. Soto</p></li><li><p>Symbolic Brittleness in Sequence Models: On Systematic Generalization in Symbolic Mathematics</p><p>Sean Welleck, Peter West, Jize Cao, Yejin Choi</p></li></ul><h2 id="interpretable-interpretability"><a href="#interpretable-interpretability" class="headerlink" title="interpretable/interpretability"></a>interpretable/interpretability</h2><ul><li><p>Optimal Local Explainer Aggregation for Interpretable Prediction</p><p>Qiaomei Li, Rachel Cummings, Yonatan Mintz</p></li><li><p>LIMREF: Local Interpretable Model Agnostic Rule-Based Explanations for Forecasting, with an Application to Electricity Smart Meter Data</p><p>Dilini Rajapaksha, Christoph Bergmeir</p></li><li><p>Social Interpretable Tree for Pedestrian Trajectory Prediction</p><p>Liushuai Shi, Le Wang, Chengjiang Long, Sanping Zhou, Fang Zheng, Nanning Zheng, Gang Hua</p></li><li><p><strong>ã€éœ€è¦çœ‹çœ‹ã€‘</strong> Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation</p><p>Hanjie Chen, Yangfeng Ji</p></li><li><p>Interpretable Clustering via Multi-Polytope Machines</p><p>Connor Lawless, Jayant Kalagnanam, Lam M. Nguyen, Dzung Phan, Chandra Reddy</p></li></ul><ul><li><p><strong>ã€éœ€è¦çœ‹çœ‹ã€‘</strong> Interpretable Generative Adversarial Networks</p><p>Chao Li, Kelu Yao, Jin Wang, Boyu Diao, Yongjun Xu, Quanshi Zhang</p></li><li><p><strong>ã€éœ€è¦çœ‹çœ‹ã€‘</strong> Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding</p><p>Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan â€€. Arik, Tomas Pfister</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5G NRï¼ˆNew Radioï¼‰è§„èŒƒ</title>
      <link href="/uncategorized/notes/5G_NR/"/>
      <url>/uncategorized/notes/5G_NR/</url>
      
        <content type="html"><![CDATA[<h2 id="åŸºç¡€æ¦‚å¿µè§£æ"><a href="#åŸºç¡€æ¦‚å¿µè§£æ" class="headerlink" title="åŸºç¡€æ¦‚å¿µè§£æ"></a>åŸºç¡€æ¦‚å¿µè§£æ</h2><h3 id="LTEä¸NR"><a href="#LTEä¸NR" class="headerlink" title="LTEä¸NR"></a>LTEä¸NR</h3><ul><li>LTEæ˜¯4Gæ—¶ä»£çš„ä¸»è¦æŠ€æœ¯ï¼Œä¸”ä¿ç•™å‘åå…¼å®¹çš„ç‰¹æ€§</li><li>NRæ˜¯é’ˆå¯¹5Gçš„æ–°æ— çº¿æ¥å…¥æŠ€æœ¯ï¼Œå…¨ç§°ä¸ºNew Radioï¼ˆæ–°ç©ºå£ï¼‰ï¼ŒNRå€Ÿç”¨äº†è®¸å¤šLTEçš„ç»“æ„ä¸åŠŸèƒ½ï¼Œä½†ä½œä¸ºæ–°çš„æ— çº¿æ¥å…¥æŠ€æœ¯ï¼Œæ— éœ€è€ƒè™‘å‘åå…¼å®¹çš„é—®é¢˜</li></ul><span id="more"></span><h3 id="NRæ ‡å‡†çš„åˆ¶å®š-ä»¥åŠç›¸å…³ç»„ç»‡æ¦‚å¿µ"><a href="#NRæ ‡å‡†çš„åˆ¶å®š-ä»¥åŠç›¸å…³ç»„ç»‡æ¦‚å¿µ" class="headerlink" title="NRæ ‡å‡†çš„åˆ¶å®š, ä»¥åŠç›¸å…³ç»„ç»‡æ¦‚å¿µ"></a>NRæ ‡å‡†çš„åˆ¶å®š, ä»¥åŠç›¸å…³ç»„ç»‡æ¦‚å¿µ</h3><ul><li>ITU-R: å›½é™…ç”µè”çš„æ— çº¿é€šä¿¡éƒ¨é—¨</li><li>IMT: å›½é™…ç§»åŠ¨é€šä¿¡ï¼Œinternational mobile telecommunications</li><li>ITU-R WP5D: ITU-Rçš„å·¥ä½œå°ç»„ï¼Œè´Ÿè´£IMTç³»ç»Ÿçš„æ— çº¿æ–¹é¢çš„å…¨éƒ¨å·¥ä½œï¼›ITU-R WP5Dä¸å„ä¸ªå›½å®¶ä¸åœ°åŒºçš„æ ‡å‡†åŒ–ç»„ç»‡åˆä½œï¼Œå¯¹IMTè¿›è¡Œå®šä¹‰ï¼Œç»´æŠ¤ä¸€ç³»åˆ—<strong>IMTå»ºè®®ä¹¦</strong>ä¸<strong>æŠ¥å‘Š</strong>ã€<strong>æ— çº¿æ¥å£è§„èŒƒ</strong>ï¼ˆ<strong>RSPC</strong>ï¼Œratio interface specificationï¼‰ã€‚<ul><li>IMTå»ºè®®ä¹¦: åŒ…æ‹¬æ¯ä¸€ä»£IMTæ— çº¿æ¥å£æŠ€æœ¯ï¼ˆRITï¼Œradio interface technologiesï¼‰</li><li>RSPC: å¯¹æ¯ä¸ªRITä½œå‡ºæ¦‚è¿°ã€å¯¹è¯¦ç»†è§„èŒƒåˆ—å‡ºå¼•ç”¨åˆ—è¡¨</li></ul></li></ul><h3 id="å‡ ä¸ªé‡è¦çš„RSPCå»ºè®®ä¹¦"><a href="#å‡ ä¸ªé‡è¦çš„RSPCå»ºè®®ä¹¦" class="headerlink" title="å‡ ä¸ªé‡è¦çš„RSPCå»ºè®®ä¹¦"></a>å‡ ä¸ªé‡è¦çš„RSPCå»ºè®®ä¹¦</h3><ul><li>IMT-2000: åŒ…å«å…­ä¸ªRITï¼Œä¸»è¦åŒ…æ‹¬WCDMA/HSPAç­‰3GæŠ€æœ¯</li><li>IMT-Advanced: åŒ…å«ä¸¤ä¸ªRITï¼Œä¸»è¦åŒ…æ‹¬4G/LTEæŠ€æœ¯</li><li>IMT-2020: æ–°çš„å»ºè®®ä¹¦ï¼Œåœ¨2019-2020åˆ¶å®šï¼Œä¸»è¦åŒ…å«5GæŠ€æœ¯</li></ul><h3 id="IMT-2020ä½¿ç”¨åœºæ™¯"><a href="#IMT-2020ä½¿ç”¨åœºæ™¯" class="headerlink" title="IMT-2020ä½¿ç”¨åœºæ™¯"></a>IMT-2020ä½¿ç”¨åœºæ™¯</h3><ul><li>å¢å¼ºçš„ç§»åŠ¨å®½å¸¦é€šä¿¡ï¼ˆenhanced mobile broadbandï¼ŒeMBBï¼‰: <em>ä»¥äººä¸ºä¸­å¿ƒçš„é€šä¿¡åœºæ™¯</em>â€”â€”å»¶ç»­3G/4Gçš„ä¸»è¦é©±åŠ¨åŠ›â€”â€”æ— çº¿å®½å¸¦ï¼›æ–°çš„æŒ‘æˆ˜åŒ…æ‹¬: <strong>çƒ­ç‚¹è¦†ç›–</strong>ï¼ˆé«˜é€Ÿç‡ã€é«˜ç”¨æˆ·å¯†åº¦ã€é«˜å®¹é‡éœ€æ±‚ï¼‰ã€<strong>å¹¿åŸŸè¦†ç›–</strong>ï¼ˆä½é€Ÿç‡ã€ä½ç”¨æˆ·å¯†åº¦ã€ç§»åŠ¨æ€§ã€æ— ç¼ç”¨æˆ·ä½“éªŒï¼‰</li><li>è¶…å¯é ä½æ—¶å»¶é€šä¿¡ï¼ˆultra-reliable and low-latency communicationsï¼ŒURLLCï¼‰: <em>ä»¥äººã€æœºå™¨ä¸ºä¸­å¿ƒçš„é€šä¿¡åœºæ™¯</em>â€”â€”ç‰¹ç‚¹åŒ…æ‹¬: ä½æ—¶å»¶ã€é«˜å¯é æ€§ã€é«˜å¯ç”¨æ€§ï¼›å…¸å‹åœºæ™¯: 3Dæ¸¸æˆã€è§¦è§‰äº’è”ç½‘</li><li>å¤§è§„æ¨¡æœºå™¨ç±»å‹é€šä¿¡ï¼ˆmassive machine type communicationsï¼ŒmMTCï¼‰: <em>ä»¥æœºå™¨ä¸ºä¸­å¿ƒçš„é€šä¿¡åœºæ™¯</em>â€”â€”ç‰¹ç‚¹åŒ…æ‹¬: ç»ˆç«¯è§„æ¨¡å·¨å¤§ã€æ•°æ®é‡å°ã€ä¼ è¾“ä¸é¢‘ç¹ã€å»¶è¿Ÿä¸æ•æ„Ÿï¼›æ–°çš„æŒ‘æˆ˜åŒ…æ‹¬: <strong>ä¸€ä¸ªç³»ç»Ÿä¸­èƒ½å®¹çº³çš„æ€»ç»ˆç«¯æ•°é‡ï¼Œä»¥åŠå¦‚ä½•é™ä½ç»ˆç«¯æˆæœ¬</strong></li></ul><h3 id="IMT-2020èƒ½åŠ›é›†"><a href="#IMT-2020èƒ½åŠ›é›†" class="headerlink" title="IMT-2020èƒ½åŠ›é›†"></a>IMT-2020èƒ½åŠ›é›†</h3><p>è§„èŒƒå®šä¹‰äº†13ç§èƒ½åŠ›ï¼Œå…¶ä¸­8ç§ä¸º<strong>å…³é”®èƒ½åŠ›</strong></p><ul><li>å…³é”®èƒ½åŠ›ï¼ˆé’ˆå¯¹eMBBåœºæ™¯é‡è¦çš„ï¼‰<ul><li>å³°å€¼æ•°æ®é€Ÿç‡ï¼ˆpeak data rateï¼‰: ç†è®ºè®®é¢˜ï¼Œä¸¥é‡ä¾èµ–é¢‘è°±èµ„æºï¼Œ$å³°å€¼æ•°æ®é€Ÿç‡=ç³»ç»Ÿå¸¦å®½ * å³°å€¼é¢‘è°±é€Ÿç‡$</li><li>ç”¨æˆ·ä½“éªŒé€Ÿç‡ï¼ˆuser experienced data rateï¼‰: é’ˆå¯¹<em>å¤§å¤šæ•°ç”¨æˆ·ï¼ˆ95%ï¼‰ã€åœ¨å¤§èŒƒå›´å†…</em>å¯å®ç°çš„é€Ÿç‡ï¼›åŸåŒº/éƒŠåŒº: 100Mbit/sï¼Œå®¤å†…/çƒ­ç‚¹: 1Gbit/s</li><li>é¢‘è°±æ•ˆç‡ï¼ˆspectrum efficiencyï¼‰: æ¯å•ä½æ— çº¿è®¾å¤‡çš„å¹³å‡æ•°æ®ååé‡ï¼Œç›®æ ‡ç¡®å®šä¸º4Gçš„ä¸‰å€</li><li>åŒºåŸŸè¯åŠ¡å®¹é‡ï¼ˆarea traffic capacityï¼‰: ä¾èµ–é¢‘è°±æ•ˆç‡ã€å¸¦å®½ã€ç½‘ç»œéƒ¨ç½²å¯†åº¦ï¼Œ$åŒºåŸŸè¯åŠ¡å®¹é‡=é¢‘è°±æ•ˆç‡ * å¸¦å®½ * TRPå¯†åº¦$</li><li>ç½‘ç»œèƒ½æ•ˆï¼ˆnetwork energy efficiencyï¼‰: ä¸ä¸Šä»£æŒå¹³</li></ul></li><li>å…³é”®èƒ½åŠ›ï¼ˆå…¶ä»–ï¼‰<ul><li>æ—¶å»¶ï¼ˆlatencyï¼‰: é’ˆå¯¹URLLCåœºæ™¯é‡è¦ï¼Œæ—¶å»¶ç›¸æ¯”å‰ä»£å‡å°‘10å€</li><li>ç§»åŠ¨æ€§ï¼ˆmobilityï¼‰: é’ˆå¯¹URLLCåœºæ™¯é‡è¦ï¼Œç›®æ ‡åœºæ™¯500km/hï¼ŒåŒæ—¶è¦æ±‚ä½æ—¶å»¶ï¼ˆä¸è¦æ±‚é«˜ç”¨æˆ·é€Ÿç‡ï¼‰</li><li>è¿æ¥å¯†åº¦ï¼ˆconnection densityï¼‰: é’ˆå¯¹mMTCåœºæ™¯é‡è¦ï¼Œæ¯å•ä½é¢ç§¯å¯æ¥å…¥çš„ç»ˆç«¯æ€»æ•°</li></ul></li><li>å…¶ä»–èƒ½åŠ›<ul><li>é¢‘è°±å’Œå¸¦å®½çµæ´»æ€§ï¼ˆspectrum and bandwidth flexibilyï¼‰: ç³»ç»Ÿåœ¨ä¸åŒé¢‘æ®µä¸Šçš„å·¥ä½œèƒ½åŠ›</li><li>å¯é æ€§ï¼ˆreliabilityï¼‰: æœåŠ¡å¯ç”¨æ€§</li><li><strong>å¯æ¢å¤æ€§ï¼ˆresilienceï¼‰</strong>: åœ¨è‡ªç„¶ã€äººä¸ºç ´åæœŸé—´ã€ä¹‹åï¼Œç½‘ç»œèƒ½ç»§ç»­æ­£å¸¸è¿è¡Œçš„èƒ½åŠ›</li><li>å®‰å…¨ä¸éšç§ï¼ˆsecurity and privacyï¼‰: æ•°æ®ã€ä¿¡ä»¤çš„åŠ å¯†/å®Œæ•´æ€§ä¿æŠ¤ï¼Œæ‹’ç»æœªç»æˆæƒçš„è·Ÿè¸ª</li><li>è¿è¡Œå¯¿å‘½ï¼ˆoperational lifetimeï¼‰: æ¯å•ä½å‚¨å­˜èƒ½é‡çš„è¿è¡Œæ—¶é—´</li></ul></li></ul><h3 id="IMT-2020æ€§èƒ½è¯„ä¼°"><a href="#IMT-2020æ€§èƒ½è¯„ä¼°" class="headerlink" title="IMT-2020æ€§èƒ½è¯„ä¼°"></a>IMT-2020æ€§èƒ½è¯„ä¼°</h3><p>å…¸å‹æµ‹è¯•ç¯å¢ƒ</p><ul><li>å®¤å†…çƒ­ç‚¹ï¼ˆindoor hotspotï¼‰-eMBB: åŠå…¬å®¤/è´­ç‰©ä¸­å¿ƒçš„å®¤å†…éš”ç¦»ç¯å¢ƒï¼Œé’ˆå¯¹å¯†åº¦å¾ˆé«˜çš„é™æ­¢äººç¾¤</li><li>å¯†é›†å¸‚åŒºï¼ˆdense urbanï¼‰-eMBB: é«˜ç”¨æˆ·å¯†åº¦å’Œæµé‡çš„åŸå¸‚ç¯å¢ƒï¼Œé’ˆå¯¹è¡Œäºº/è½¦è¾†ç”¨æˆ·</li><li>éƒŠåŒºï¼ˆRuralï¼‰-eMBB: å†œæ‘ç¯å¢ƒï¼Œé’ˆå¯¹å¤§è¦†ç›–é¢ç§¯å†…çš„è¡Œäººã€è½¦è¾†ã€é«˜é€Ÿè½¦è¾†</li><li>å¸‚åŒºå®ç«™ï¼ˆurban macroï¼‰-mMTC: å…·æœ‰è¿ç»­è¦†ç›–èŒƒå›´çš„åŸå¸‚å®åŸºç«™ç¯å¢ƒï¼Œé’ˆå¯¹å¤§é‡æœºå™¨ç»ˆç«¯</li><li>å¸‚åŒºå®ç«™ï¼ˆurban macroï¼‰- URLLC: å…·æœ‰è¿ç»­è¦†ç›–èŒƒå›´çš„åŸå¸‚å®åŸºç«™ç¯å¢ƒï¼Œé’ˆå¯¹è¶…å¯é ã€ä½æ—¶å»¶é€šä¿¡</li></ul><p>æ¯ä¸ªæŠ€æœ¯åœ¨æ¯ä¸ªå…¸å‹æµ‹è¯•ç¯å¢ƒä¸­è¿›è¡Œæ€§èƒ½è¯„ä¼°çš„ä¸‰ä¸ªåŸºæœ¬æ–¹æ³•</p><ul><li>ä»¿çœŸ: æ— çº¿æ¥å£çš„ç³»ç»Ÿçº§ã€é“¾è·¯çº§ä»¿çœŸ</li><li>åˆ†æ: åŸºäºæ— çº¿æ¥å£å‚æ•°çš„è®¡ç®—ï¼Œæˆ–å…¶ä»–KPIå€¼æ¥è¯„ä¼°</li><li>æ£€æŸ¥: å®¡æ ¸æ— çº¿æ¥å£çš„åŠŸèƒ½ç­‰</li></ul><h2 id="LTEæ¦‚è¿°"><a href="#LTEæ¦‚è¿°" class="headerlink" title="LTEæ¦‚è¿°"></a>LTEæ¦‚è¿°</h2><h3 id="LTEçš„èµ„æºé…ç½®"><a href="#LTEçš„èµ„æºé…ç½®" class="headerlink" title="LTEçš„èµ„æºé…ç½®"></a>LTEçš„èµ„æºé…ç½®</h3><p>LTEåœ¨æ—¶åŸŸä¸Šçš„ä¼ è¾“ä»¥10msä¸ºä¸€å¸§ï¼ˆframeï¼‰ï¼Œæ¯å¸§åŒ…æ‹¬10ä¸ª1msçš„å­å¸§ï¼ˆsubframeï¼‰ï¼Œæ¯ä¸ªå­å¸§åˆ†ä¸ºä¸¤ä¸ªé•¿åº¦ä¸º0.5msçš„æ—¶éš™ï¼ˆslotï¼‰ï¼Œ<br>æ¯ä¸ªslotåœ¨æ—¶åŸŸä¸Šï¼ˆåœ¨æ™®é€šCPæ¨¡å¼ä¸‹ï¼‰åˆ†æˆ7ä¸ªOFDMç¬¦å·ï¼Œæˆ–ï¼ˆåœ¨æ‰©å±•CPæ¨¡å¼ä¸‹ï¼‰åˆ†æˆ6ä¸ªOFDMç¬¦å·ï¼Œè¿™é‡Œçš„ä¸€ä¸ªOFDMç¬¦å·æ˜¯LTEèµ„æºè°ƒåº¦çš„æœ€å°å•å…ƒã€‚ç”±æ­¤å»¶ä¼¸å‡ºTTI/RG/RB/REçš„æ¦‚å¿µ: </p><ul><li>TTIï¼ˆtransmission time-intervalï¼‰: subframeä½œä¸ºLTEçš„ä¸€ä¸ªè°ƒåº¦æ—¶é—´å•ä½ï¼Œç§°ä¸ºä¸€ä¸ªTTI<ul><li>æ—¶åŸŸ: ä¸€ä¸ªsubframeï¼ˆ1msï¼‰</li><li>é¢‘åŸŸ: /ï¼ˆè¿™ä»…ä»…æ˜¯ä¸ªæ—¶åŸŸå®šä¹‰ï¼‰</li></ul></li><li>RGï¼ˆresource gridï¼‰: ä¸€ä¸ªslotä¸­çš„ä¼ è¾“ä¿¡å·å¯ä»¥ç”¨ä¸€ä¸ªèµ„æºæ ¼ï¼ˆRGï¼‰æè¿°ã€‚<ul><li>æ—¶åŸŸ: ä¸€ä¸ªSlotï¼ˆ0.5msï¼‰</li><li>é¢‘åŸŸ: å…¨éƒ¨å­è½½æ³¢</li></ul></li><li>RBï¼ˆResource blockï¼‰: ä¸€ä¸ªslotä¸­çš„æ¯ä¸ªå­è½½æ³¢ç§°ä¸ºä¸€ä¸ªèµ„æºå—ï¼ˆRBï¼‰ï¼ˆè¿™å¯ä»¥è¢«è§†ä¸ºèµ„æºçš„ç²—ç²’åº¦åˆ†å‰²æ–¹æ³•ï¼‰ï¼Œ<strong>RBæ˜¯åˆ†é…èµ„æºåˆ°UEçš„æœ€å°å•ä½</strong><ul><li>æ—¶åŸŸ: ä¸€ä¸ªSlotï¼ˆ0.5msï¼‰</li><li>é¢‘åŸŸ: è¿ç»­12ä¸ªå­è½½æ³¢</li></ul></li><li>REï¼ˆResource elementsï¼‰: ä¸€ä¸ªRBä¸­çš„ä¸€ä¸ªOFDM symbolï¼Œå…¶åœ¨RGä¸Šçš„ä½ç½®å¯ç”±$(k,l)$å”¯ä¸€æ ‡æ³¨ï¼Œå…¶ä¸­$l$è¡¨æ—¶åŸŸï¼Œ$k$è¡¨é¢‘åŸŸï¼ˆè¿™å¯ä»¥è¢«è§†ä¸ºèµ„æºçš„ç»†ç²’åº¦åˆ†å‰²æ–¹æ³•ï¼‰ï¼Œ<strong>REæ—¶LTEèµ„æºè°ƒåº¦çš„æœ€å°å•ä½</strong><ul><li>æ—¶åŸŸ: OFDM symbolï¼ˆslotä¸­çš„1/7æˆ–1/6ï¼‰</li><li>é¢‘åŸŸ: ä¸€ä¸ªå­è½½æ³¢</li></ul></li><li>RG/RB/REçš„å…³ç³»: ä¸€ä¸ªRGå¯åœ¨é¢‘åŸŸä¸Šåˆ†ä¸ºå¤šä¸ªRBï¼›ä¸€ä¸ªRBå¯åœ¨æ—¶åŸŸå’Œé¢‘åŸŸä¸Šåˆ†ä¸ºå¤šä¸ªREï¼ˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼‰ã€‚</li></ul><div align="center">    <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/20220106185157.png" width = "70%" /></div><h3 id="LTEæ•°æ®ä¼ è¾“æ¦‚è¿°"><a href="#LTEæ•°æ®ä¼ è¾“æ¦‚è¿°" class="headerlink" title="LTEæ•°æ®ä¼ è¾“æ¦‚è¿°"></a>LTEæ•°æ®ä¼ è¾“æ¦‚è¿°</h3><p>ç”±äº5G NRåœ¨è®¾è®¡æ—¶å‚è€ƒäº†å¹¶å¤ç”¨äº†è®¸å¤šLTEæŠ€æœ¯æ„å»ºï¼Œæ­¤å¤–5G NRä¸LTEå‡ç”±3GPPåˆ¶å®šï¼Œå› æ­¤åœ¨æ­¤æ€»ç»“LTEå±‚çš„ç›¸å…³çŸ¥è¯†ã€‚</p><p>LTEæ ¹æ®ä¸åŒçš„å¸§æ ¼å¼ï¼Œå¯ä»¥é…ç½®å¸§ä¸ºFDDæˆ–TDDï¼ŒåŒæ—¶å¯ä»¥å¯¹å…¨åŒå·¥å’ŒåŠåŒå·¥æ¨¡å¼è¿›è¡Œé…ç½®ã€‚åœ¨æ­¤é¦–å…ˆå¿½ç•¥LTEä¸­å…·ä½“çš„å¸§æ ¼å¼ï¼Œå¯¹ä¸Šä¸‹è¡Œé“¾è·¯çš„ä¸­çš„å…³é”®ä¿¡å·ä½œå‡ºè§£é‡Šã€‚</p><h4 id="ç‰©ç†å±‚ä¸Šè¡Œé“¾è·¯"><a href="#ç‰©ç†å±‚ä¸Šè¡Œé“¾è·¯" class="headerlink" title="ç‰©ç†å±‚ä¸Šè¡Œé“¾è·¯"></a>ç‰©ç†å±‚ä¸Šè¡Œé“¾è·¯</h4><p>ç”¨æˆ·çš„ä¸Šè¡Œé“¾è·¯ä¼ è¾“åŒ…æ‹¬: </p><ul><li>RSï¼ˆReference signalsï¼Œå‚è€ƒä¿¡å·ï¼‰: åŒ…æ‹¬SRSï¼ˆSounding Reference Signalï¼Œæ¢æµ‹å‚è€ƒä¿¡å·ï¼‰ã€DMRSï¼ˆDemodulation Reference Signalï¼Œè§£è°ƒå‚è€ƒä¿¡å·ï¼‰ï¼Œå‚è€ƒä¿¡å·ç”¨äºä¿¡é“ä¼°è®¡æˆ–å‡è¡¡ã€‚<ul><li>DMRS: BSä½¿ç”¨UEå‘é€çš„DMRSæ¥å‡è¡¡å’Œè§£è°ƒUEçš„ä¼ è¾“</li><li>SRS: åŸºç«™äº†è§£è¯¥UEçš„ä¸Šè¡Œä¿¡é“ç‰¹æ€§ã€‚åŸºç«™å¯ä»¥ä½¿ç”¨è¯¥ä¿¡æ¯æ¥ä¸ºUEåˆ†é…å¥½çš„ä¸Šè¡Œé“¾è·¯ä»¥è¿›è¡Œä¼ è¾“</li></ul></li><li>ç‰©ç†ä¿¡é“<ul><li>PUSCHï¼ˆphysical uplink share channelï¼Œç‰©ç†ä¸Šè¡Œå…±äº«ä¿¡é“ï¼‰: è¯¥ä¿¡é“ä¼ è¾“ç”¨æˆ·çš„ä¸Šè¡Œæ•°æ®ï¼Œè¿™é‡Œçš„ â€œå…±äº«â€ æŒ‡åŒä¸€ç‰©ç†ä¿¡é“å¯ç”±å¤šä¸ªç”¨æˆ·åˆ†æ—¶ä½¿ç”¨ï¼Œæˆ–è€…è¯´ä¿¡é“å…·æœ‰è¾ƒçŸ­çš„æŒç»­æ—¶é—´ã€‚ä¸€ä¸ªUEå¯ä»¥å¹¶è¡Œå­˜åœ¨å¤šæ¡USCHï¼Œè¿™äº›å¹¶è¡Œçš„USCHæ•°æ®å¯ä»¥åœ¨ç‰©ç†å±‚è¿›è¡Œç¼–ç ç»„åˆã€‚</li><li>PUCCHï¼ˆphysical uplink control channelï¼Œç‰©ç†ä¸Šè¡Œæ§åˆ¶ä¿¡é“ï¼‰: è¯¥ä¿¡é“ç”¨äºæ‰¿è½½UCIï¼ˆuplink control informationï¼Œä¸Šè¡Œé“¾è·¯æ§åˆ¶ä¿¡æ¯ï¼‰ï¼ŒPUCCHåŒ…æ‹¬: HARQ ACK/NACKã€CQI ä¿¡é“è´¨é‡æŒ‡æ ‡ã€MIMO åé¦ˆ - RIï¼ˆç§©æŒ‡æ ‡ï¼‰ï¼ŒPMIï¼ˆé¢„ç¼–ç çŸ©é˜µæŒ‡æ ‡ï¼‰ã€ä¸Šè¡Œé“¾è·¯ä¼ è¾“çš„è°ƒåº¦è¯·æ±‚ã€ç”¨äº PUCCH è°ƒåˆ¶çš„ BPSK æˆ– QPSK</li><li>PRACHï¼ˆPhysical Random Access Channelï¼Œç‰©ç†éšæœºæ¥å…¥ä¿¡é“ï¼‰: ä¸Šè¡Œé“¾è·¯ç”¨æˆ·ä½¿ç”¨ç‰©ç†éšæœºæ¥å…¥ä¿¡é“ï¼ˆPRACHï¼‰æ¥å‘èµ·ä¸åŸºç«™çš„è”ç³»ã€‚åŸºç«™å¹¿æ’­ä¸€äº›åŸºæœ¬çš„å°åŒºä¿¡æ¯ï¼ŒåŒ…æ‹¬å¯ä»¥å‘é€éšæœºæ¥å…¥è¯·æ±‚çš„ä½ç½®ã€‚ç„¶åï¼ŒUE è¿›è¡Œ PRACH ä¼ è¾“ï¼Œè¯·æ±‚åˆ†é… PUSCHï¼ŒåŸºç«™ä½¿ç”¨ä¸‹è¡Œé“¾è·¯æ§åˆ¶ä¿¡é“ (PDCCH) æ¥å›å¤ UE å¯ä»¥åœ¨ä½•å¤„ä¼ è¾“ PUSCHã€‚</li><li>æ³¨æ„: ç”¨æˆ·ä¸èƒ½åœ¨åŒä¸€ä¸ªæ—¶éš™ä¸­åŒæ—¶ä¼ è¾“PUCCHå’ŒPUSCHæ•°æ®</li></ul></li><li>å…³äºåŒæ­¥: ä¸Šè¡Œä¿¡å·æ²¡æœ‰ä¸“ç”¨çš„åŒæ­¥ä¿¡å·ã€‚åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œä¸Šè¡Œé“¾è·¯ä¿¡å·å°†ä½¿ç”¨ä¸‹è¡Œé“¾è·¯ä¿¡å·è¿›è¡ŒåŒæ­¥ã€‚ä½†æ˜¯ï¼Œä¸ºäº†åœ¨ä½¿ç”¨ 89600 VSA LTE è§£è°ƒå™¨æ—¶åˆ†æä¸Šè¡Œé“¾è·¯ä¸ä¸‹è¡Œé“¾è·¯åˆ†ç¦»ï¼Œå¯ä»¥ä½¿ç”¨ PUCCH DM-RSã€PUSCH DM -RSã€PRACH æˆ– SRSåŒæ­¥ä¸Šè¡Œé“¾è·¯å¸§ã€‚</li></ul><h4 id="ç‰©ç†å±‚ä¸‹è¡Œé“¾è·¯"><a href="#ç‰©ç†å±‚ä¸‹è¡Œé“¾è·¯" class="headerlink" title="ç‰©ç†å±‚ä¸‹è¡Œé“¾è·¯"></a>ç‰©ç†å±‚ä¸‹è¡Œé“¾è·¯</h4><p>åŸºç«™çš„ä¸‹è¡Œé“¾è·¯ä¼ è¾“åŒ…æ‹¬: </p><ul><li>SYNï¼ˆSynchronizationï¼ŒåŒæ­¥ä¿¡å·ï¼‰: ä¸‹è¡ŒåŒæ­¥ä¿¡å·æœ‰ä¸¤ä¸ªï¼Œä¸»åŒæ­¥ä¿¡å·ï¼ˆP-SSï¼‰å’Œè¾…åŒæ­¥ä¿¡å·ï¼ˆS-SSï¼‰</li><li>RSï¼ˆReference signalsï¼Œå‚è€ƒä¿¡å·ï¼‰ï¼Œæ ¹æ®ä¸åŒçš„å¸§é…ç½®ï¼Œæ‰€å‘é€çš„å‚è€ƒä¿¡å·å°†æœ‰æ‰€ä¸åŒ: <ul><li>C-RSï¼ˆCell specific Reference Signalï¼‰: BSå‘é€çš„C-RSè¢«UEç”¨äºä¸‹è¡Œç‰©ç†ä¿¡é“çš„ä¿¡é“ä¼°è®¡ã€è·å–ä¿¡é“çŠ¶æ€ä¿¡æ¯ä»¥ä¾¿ä¿¡é“è°ƒåº¦ã€æ‰§è¡Œç»ˆç«¯æµ‹é‡ä»è€Œå†³å®šUEçš„åˆå§‹æ¥å…¥/é€‰æ‹©å’Œåˆ‡æ¢ã€ç»ˆç«¯ä¾§é¢‘ç‡è¯¯å·®çš„æ ¡æ­£ã€‚C-RSåœ¨æ¯ä¸ªä¸‹è¡Œå­å¸§ï¼Œæ•´ä¸ªä¸‹è¡Œä¼ è¾“å¸¦å®½å†…çš„æ¯ä¸ªRBä¸Šéƒ½ä¼šå‘é€ï¼Œæ— è®ºæ˜¯å¦ä¸‹è¡Œé“¾è·¯æœ‰æ•°æ®å‘é€ã€‚</li><li>UE-RSï¼ˆUE specific Reference Signalï¼‰: BSå¯ä»¥åœ¨åˆ†é…ç»™UEçš„PDSCHçš„RBä¸­å‘é€UE-RSã€‚</li><li>P-RSï¼ˆPositioning Reference Signalï¼Œå®šä½å‚è€ƒä¿¡å·): ç”¨äºå¢å¼ºUEåœ°ç†å®šä½ç²¾åº¦</li><li>MBSFN-RSï¼ˆMulticast/Broadcast Single Frequency Network Reference Signalï¼Œç»„æ’­/å¹¿æ’­å•é¢‘ç½‘ç»œå‚è€ƒä¿¡å· ï¼‰: ç”¨äºè¡¥å¿åœ¨ç‰©ç†å¤šæ’­ä¿¡é“ï¼ˆä¸‹è¡Œé“¾è·¯ä¿¡é“çš„å½±å“PMCHï¼‰ï¼Œå…¶ä¸­åŒ…å«çš„å¤šæ’­/å¹¿æ’­æ•°æ®</li></ul></li><li>ç‰©ç†ä¿¡é“<ul><li>æ§åˆ¶ä¿¡é“ï¼ˆControl channelsï¼‰: æ§åˆ¶ä¿¡é“æä¾›ç®¡ç†ç”¨æˆ·ä¿¡é“ä¸Šæ•°æ®ä¼ è¾“æ‰€éœ€çš„ä¿¡æ¯ï¼Œå¹¶ä¿ƒè¿›ä¸åŸºç«™çš„è¿æ¥ã€‚è¿™äº›é€šé“æ”¾ç½®åœ¨å¸§ä¸­çš„ç‰¹å®šä½ç½®ã€‚<ul><li>PBCH: ç‰©ç†å¹¿æ’­é¢‘é“ï¼Œæºå¸¦ç‰¹å®šäºcellçš„ä¿¡æ¯ã€‚</li><li>PCFICH: ç‰©ç†æ§åˆ¶æ ¼å¼æŒ‡ç¤ºé€šé“ï¼ŒåŒ…å«æœ‰å…³å­å¸§ä¸­ç”¨äº PDCCH çš„ OFDM ç¬¦å·æ•°é‡çš„ä¿¡æ¯ã€‚</li><li>PDCCH: ç‰©ç†ä¸‹è¡Œæ§åˆ¶ä¿¡é“ï¼ŒåŒ…å«è°ƒåº¦ä¿¡æ¯ã€‚</li><li>PHICH: ç‰©ç†æ··åˆARQæŒ‡ç¤ºé€šé“ï¼Œæºå¸¦æ··åˆ ARQ ACK / NACKã€‚</li></ul></li><li>å…±äº«ä¿¡é“ï¼ˆShared channelï¼‰: PDSCHï¼ˆphysical downlink share channelï¼‰åŒ…å«å‘é€ç»™ç”¨æˆ·çš„æ•°æ®ã€‚æ‰€æœ‰èµ„æºå—éƒ½å¯ç”¨äºåˆ†é…ï¼Œä½†åªæœ‰æœªä¸ºæ§åˆ¶ä¿¡é“é¢„ç•™çš„å­è½½æ³¢å¯ç”¨äºæ‰¿è½½æ•°æ®ã€‚</li><li>ç»„æ’­ä¿¡é“ï¼ˆMulticast channelï¼‰: ç‰©ç†å¤šæ’­ä¿¡é“ (Physical Multicast Channelï¼ŒPMCH) æ”¯æŒMBMSï¼ˆMultimedia Broadcast/Multicast Serviceï¼Œå¤šåª’ä½“å¹¿æ’­/å¤šæ’­æœåŠ¡ï¼‰ï¼Œå¹¶æ‰¿è½½ä¾›å¤šä¸ªç”¨æˆ·ä½¿ç”¨çš„æ•°æ®ã€‚å•ä¸ªå°åŒºï¼ˆå¹¿æ’­ï¼‰æˆ–å¤šä¸ªå°åŒºï¼ˆå¤šæ’­ï¼‰éƒ½å¯ä»¥å‚ä¸ä¼ è¾“æ•°æ®ã€‚æ¥è‡ªä¸åŒå°åŒºçš„ä¿¡å·åœ¨UEå¤„æ±‡åˆï¼Œä»¥èƒ½å¤Ÿæä¾›æ›´é«˜çš„ä¿¡å·åŠŸç‡ã€‚MBMS ä¿¡å·åœ¨æ‰©å±• CP æ¨¡å¼ä¸‹ä¼ è¾“ï¼Œä»¥å‡è½»ç”±äºæ¯ä¸ªå°åŒºåˆ° UE çš„è·ç¦»ä¸åŒè€Œå¯¼è‡´çš„å¤šå¾„æ•ˆåº”ã€‚</li></ul></li></ul><p>ï¼ˆåç»­æ›´æ–°: 5G NRæ¥å…¥æµç¨‹ï¼Œç«äº‰æœºåˆ¶ï¼‰</p><hr><h2 id="å‚è€ƒæ–‡çŒ®"><a href="#å‚è€ƒæ–‡çŒ®" class="headerlink" title="å‚è€ƒæ–‡çŒ®"></a>å‚è€ƒæ–‡çŒ®</h2><p>[1] <a href="https://rfmw.em.keysight.com/wireless/helpfiles/89600B/WebHelp/subsystems/lte/content/lte_overview.htm">https://rfmw.em.keysight.com/wireless/helpfiles/89600B/WebHelp/subsystems/lte/content/lte_overview.htm</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> 5G </tag>
            
            <tag> 4G </tag>
            
            <tag> 5G NR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformers - Survey</title>
      <link href="/uncategorized/notes/x_formers/"/>
      <url>/uncategorized/notes/x_formers/</url>
      
        <content type="html"><![CDATA[<p>A Survey of Transformers</p><p>TIANYANG LIN, YUXIN WANG, XIANGYANG LIU, and XIPENG QIU</p><p>School of Computer Science, Fudan University, China and Shanghai Key Laboratory of Intelligent Information Processing, Fudan<br>University, China</p><span id="more"></span><h2 id="Motivations"><a href="#Motivations" class="headerlink" title="Motivations"></a>Motivations</h2><p>Vanilla Transformerçš„ä¸»è¦å¼Šç«¯ä¸æ”¹è¿›æ–¹å‘</p><ul><li><strong>Model Efficiency</strong>ï¼šTransformeå¤„ç†é•¿åºåˆ—æ—¶æ•ˆç‡ä½ä¸‹ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºself-attentionçš„è®¡ç®—å’Œå†…å­˜å¤æ‚æ€§é€ æˆçš„ã€‚æ”¹è¿›æ–¹æ³•åŒ…æ‹¬è½»é‡çº§attentionï¼Œä¾‹å¦‚sparse attention<br>variantsã€å’Œåˆ†æ²»æ–¹æ³•ï¼ˆDivide-and-conquerï¼‰ï¼Œä¾‹å¦‚recurrent and hierarchical mechanismã€‚</li><li><strong>Model Generalization</strong>ï¼šTransformerçš„ç»“æ„ä»ç†è®ºä¸Šæ¥è¯´æ˜¯éå¸¸çµæ´»çš„ï¼Œå‡ ä¹ä¸å¯¹è¾“å…¥æ•°æ®çš„ç»“æ„æ€§åå·®è¿›è¡Œå‡è®¾ï¼Œå› æ­¤å¾ˆéš¾å¯¹<strong>å°è§„æ¨¡æ•°æ®</strong>è¿›è¡Œè®­ç»ƒã€‚<br>æ”¹è¿›æ–¹æ³•åŒ…æ‹¬å¼•å…¥ç»“æ„æ€§åå·®ï¼ˆstructural biasï¼‰æˆ–æ­£åˆ™åŒ–ï¼ˆregularization,ï¼‰ã€å¯¹å¤§è§„æ¨¡æœªæ ‡è®°æ•°æ®è¿›è¡Œé¢„è®­ç»ƒç­‰ã€‚</li><li><strong>Model Adaptation</strong>ï¼šè¿™ç±»å·¥ä½œæ—¨åœ¨ä½¿Transformeré€‚åº”ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡å’Œåº”ç”¨ã€‚</li></ul><p>è¿™ç¯‡æ–‡ç« ä¸»è¦æ ¹æ®æ”¹è¿›vanilla Transformerçš„æ–¹å¼æ¥ç»„ç»‡ç›¸å…³çš„å·¥ä½œï¼Œå³ï¼š<strong>æ¶æ„ä¿®æ”¹</strong>ã€<strong>é¢„è®­ç»ƒ</strong>ã€<strong>åº”ç”¨</strong>ã€‚ä¸”æœ¬æ–‡ä¸»è¦å…³æ³¨æ¶æ„å˜ä½“ï¼Œå¹¶ç®€è¦è®¨è®ºé¢„è®­ç»ƒå’Œé¢å‘åº”ç”¨çš„å˜ä½“ã€‚</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="Vanilla-Transformer"><a href="#Vanilla-Transformer" class="headerlink" title="Vanilla Transformer"></a>Vanilla Transformer</h3><div align="center">    <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/20220120210749.png" width = "70%" /></div><p>é¦–å…ˆï¼Œtransformeréµå¾ªseq2seqç»“æ„ï¼Œå…¶ä¸­encoder decoderéƒ½ç”±$L$ä¸ªå•ç‹¬çš„æ¨¡å—å †å è€Œæˆã€‚è¦ç‚¹åŒ…æ‹¬ï¼š</p><ul><li><strong>encoder</strong>: multi-head self-attention, position-wise feed-forward network (FFN), residual connection, Layer Normalization.</li><li><strong>decoder</strong>: ä¸Šè¿°æ¨¡å— + cross-attention (between the multi-head self-attention modules and the position-wise FFNs), decoderä¸­çš„attention matrixè®¡ç®—æ˜¯æœ‰ä½ç½®é™åˆ¶çš„<br>ï¼ˆè€ƒè™‘åˆ°åç»­æ—¶åˆ»è¾“å‡ºä¸èƒ½ä¸ºå‰åºæ—¶åˆ»çš„è¾“å‡ºæä¾›å‚è€ƒï¼‰</li></ul><h4 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h4><div align="center">  $\operatorname{Attention}(\mathrm{Q}, \mathrm{K}, \mathrm{V})=\operatorname{softmax}\left(\frac{\mathrm{QK}^{\top}}{\sqrt{D_{k}}}\right) \mathrm{V}=\mathrm{AV}$</div><p>ä¸Šå¼æ˜¯attentionçš„åŸºæœ¬åŸç†ï¼Œå…¶ä¸­query $Q \in \mathbb{R}^{N \times D_{k}}$ï¼Œkey $\mathrm{K} \in \mathbb{R}^{M \times D_{k}}$ï¼Œvalue $\mathbf{V} \in \mathbb{R}^{M \times D_{v}}$ã€‚$N,M$åˆ†åˆ«ä¸ºqueryå’Œkeyï¼ˆvalueï¼‰çš„é•¿åº¦ï¼Œ$D_k, D_v$ä¸ºkeyï¼ˆqueryï¼‰ä¸valueçš„ç»´åº¦ã€‚$\mathrm{A}=\operatorname{softmax}\left(\frac{\mathrm{QK}^{\top}}{\sqrt{D_{k}}}\right)$ä¹Ÿè¢«ç§°ä¸ºattention matrixã€‚é™¤ä»¥$\sqrt{D_{k}}$æ˜¯ä¸ºäº†ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ã€‚</p><p>å°†æ•°æ®ç»´åº¦å‹ç¼©ä¸º1ï¼Œåˆ™ä¸Šè¿°ä¸‰ä¸ªå¯¹è±¡å¯ä»¥ç†è§£ä¸ºï¼Œ$\mathbf{v}=[v_1, \cdots, v_N]$ä»£è¡¨moduleåœ¨æœªç»è¿‡ç­›é€‰æ—¶è¦è¾“å‡ºçš„å€¼ï¼Œæˆ‘ä»¬æœŸæœ›çš„è¾“å‡ºæ˜¯$\mathbf{w} * \mathbf{v}$ï¼Œå…¶ä¸­æƒé‡$\mathbf{w}$çš„è®¡ç®—å³ä¸ºä¸Šå¼ä¸­$softmax(\cdot)$çš„ç»“æœã€‚</p><p>  <strong>ä¸è½¯å¯»å€ä¹‹é—´çš„è”ç³»ï¼ˆå¦‚ä¸‹å›¾ï¼‰</strong>ï¼šä»¤Source $\mathbf{S}=[&lt;k_1, v_1&gt;, \cdots, &lt;k_n, v_n&gt;]$è§†ä¸ºå­˜å‚¨å™¨ä¸­çš„å…¨éƒ¨å†…å®¹ï¼Œå½“å‰æœ‰ä¸€ä¸ªæŸ¥è¯¢$q=k_i$ï¼Œç›®çš„æ˜¯å–å‡ºsourceä¸­åŒ¹é…é”®å€¼çš„å€¼$v_i$ã€‚<br>  æˆ‘ä»¬è®°$\mathbf{k} = [k_1, \cdots, k_n]$ï¼Œæˆ‘ä»¬é€šè¿‡Query $q$å’Œå­˜å‚¨å™¨å†…å…ƒç´ çš„åœ°å€$\mathbf{k}$è¿›è¡Œç›¸ä¼¼æ€§æ¯”è¾ƒæ¥å¯»å€ï¼Œä¹‹æ‰€ä»¥è¯´æ˜¯è½¯å¯»å€ï¼ŒæŒ‡çš„ä¸åƒä¸€èˆ¬å¯»å€åªä»å­˜å‚¨å†…å®¹é‡Œé¢æ‰¾å‡ºä¸€æ¡å†…å®¹$k_i$ï¼Œ<br>è€Œæ˜¯å¯èƒ½ä»$\mathbf{k}$ä¸­çš„æ¯ä¸€é¡¹éƒ½ä¼šå–å‡ºå†…å®¹ï¼Œå–å‡ºå†…å®¹çš„é‡è¦æ€§æ ¹æ®$q$å’Œ$\mathbf{k}$çš„ç›¸ä¼¼æ€§æ¥å†³å®šï¼Œç›¸ä¼¼æ€§è®°ä¸º$\mathbf{w} = [w_1, \cdots, w_n]$ï¼Œ<br>  ä¹‹åå¯¹å­˜å‚¨å™¨ä¸­çš„æ¯ä¸€é¡¹å¯¹åº”çš„å€¼è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå³$v = w_1 v_1 + \cdots + w_n v_n$ï¼Œå¾—åˆ°æœ€ç»ˆçš„Valueå€¼ï¼Œä¹Ÿå³Attentionçš„ç»“æœå€¼ã€‚<br>  æ‰€ä»¥ä¸å°‘ç ”ç©¶äººå‘˜å°†Attentionæœºåˆ¶çœ‹ä½œè½¯å¯»å€çš„ä¸€ç§ç‰¹ä¾‹ï¼Œè¿™ä¹Ÿæ˜¯éå¸¸æœ‰é“ç†çš„ã€‚</p><div align="center">    <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/20220120203102.png" width = "50%" /></div><div align="center">    \begin{aligned}        \text { MultiHeadAttn }(Q, K, V) &=\text { Concat }\left(\text { head }_{1}, \cdots, \text { head }_{H}\right) \mathrm{W}^{O}, \\        \text { where head }_{i} &=\operatorname{Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) .    \end{aligned}</div><p>ä¸Šå¼æ˜¯multi-head attentionçš„åŸºæœ¬è¡¨è¾¾å¼ï¼Œå…¶ä¸­$Q, K, V$çš„ç»´åº¦å‡ä¸º$D_m$ï¼Œä»–ä»¬åˆ†åˆ«ç”±å‡ ä¸ªçº¿æ€§æ˜ å°„ï¼ˆ$W_{i}^{Q}, W_{i}^{K} W_{i}^{V}$ï¼‰æŠ•å½±åˆ°ç»´åº¦ä¸º$D_k, D_k, D_v$çš„ç©ºé—´ä¸­ï¼Œå¹¶è¿›è¡Œattentionè®¡ç®—ï¼Œæœ€åæ¨¡å‹å°†æ‰€æœ‰è¾“å‡ºè¿æ¥å¹¶å°†å…¶æŠ•å½±åˆ°$D_m$ç»´ç©ºé—´ã€‚</p><ul><li><strong>Self-attention</strong>ï¼š$Q=K=V=X$ï¼Œ$X$æ˜¯å‰ä¸€å±‚çš„è¾“å‡º</li><li><strong>Masked Self-attention</strong>ï¼šåœ¨Transformerè§£ç å™¨ä¸­ï¼Œself-attentionç”Ÿæˆçš„weightå—åˆ°ä½ç½®é™åˆ¶ï¼Œå…¶ç”Ÿæˆçš„attention matrixåªåº¦é‡æŸä¸ªä½ç½®iå’Œjä¹‹é—´çš„æƒé‡ï¼Œä¸”$i&gt;=j$ã€‚å…·ä½“åœ°ï¼Œå…¶å®ç°è¿‡ç¨‹æ˜¯ä¸ºattention matrixçš„æŸäº›ä½ç½®èµ‹äºˆmaskã€‚$\hat{A}=\exp \left(\frac{Q K^{\top}}{\sqrt{D_{k}}}\right)$, $\hat{A}_{i j}=-\infty \text { if } i&lt;j$ã€‚è¿™ç§è‡ªæˆ‘æ³¨æ„é€šå¸¸è¢«ç§°ä¸ºè‡ªå›å½’æ³¨æ„æˆ–å› æœæ³¨æ„ã€‚</li><li><strong>Cross-attention</strong>ï¼šqueryç”±ä¸Šä¸€å±‚decoderçš„è¾“å‡ºæŠ•å½±è€Œæ¥ï¼Œkey/valueåˆencoderçš„è¾“å‡ºæŠ•å½±è€Œæ¥</li></ul><h4 id="Position-wise-FFN"><a href="#Position-wise-FFN" class="headerlink" title="Position-wise FFN"></a>Position-wise FFN</h4><p>åŸºäºä½ç½®çš„FFNæ˜¯ä¸€ä¸ªå…¨è¿æ¥çš„å‰é¦ˆç½‘ç»œï¼Œå®ƒåœ¨æ¯ä¸€ä¸ªä½ç½®ä¸Šè¿›è¡Œç‹¬ç«‹è¿ç®—ï¼Œæ³¨æ„ï¼šå‰å‘ç½‘ç»œçš„å‚æ•°åœ¨ä¸åŒä½ç½®ä¸Šæ˜¯å…±äº«çš„ï¼Œå› æ­¤Position-wise FFNä¹Ÿå¯ä»¥ç†è§£ä¸ºä¸¤å±‚kernel sizeä¸º1çš„å·ç§¯å±‚ã€‚</p><div align="center">    $\operatorname{FFN}\left(\mathbf{H}^{\prime}\right)=\operatorname{ReLU}\left(\mathbf{H}^{\prime} \mathbf{W}^{1}+\mathbf{b}^{1}\right) \mathbf{W}^{2}+\mathbf{b}^{2}$</div><p>å…¶ä¸­$\mathbf{H}^{\prime}$ä¸ºä¸Šä¸€å±‚çš„è¾“å‡ºï¼Œ$\mathbf{W}^{1} \in \mathbb{R}^{D_{m} \times D_{f}}, \mathbf{W}^{2} \in \mathbb{R}^{D_{f} \times D_{m}}, \mathbf{b}^{1} \in \mathbb{R}^{D_{f}}, \mathbf{b}^{2} \in \mathbb{R}^{D_{m}}$ï¼Œä¸€èˆ¬æ¥è®²FFNçš„ç»´åº¦å‚æ•°è®¾ç½®ä¸º$D_f &gt; D_m$</p><h4 id="Residual-connection-and-normalization"><a href="#Residual-connection-and-normalization" class="headerlink" title="Residual connection and normalization"></a>Residual connection and normalization</h4><div align="center">    \begin{aligned}        \mathrm{H}^{\prime} &=\text { LayerNorm }(\text { SelfAttention }(\mathrm{X})+\mathrm{X}) \\        \mathrm{H} &=\text { LayerNorm }\left(\mathrm{FFN}\left(\mathrm{H}^{\prime}\right)+\mathrm{H}^{\prime}\right)    \end{aligned}</div><h4 id="Position-Encodings"><a href="#Position-Encodings" class="headerlink" title="Position Encodings"></a>Position Encodings</h4><p>å› ä¸ºTransformeræ²¡æœ‰å¼•å…¥é€’å½’ç»“æ„æˆ–å·ç§¯æ“ä½œï¼Œæ‰€ä»¥å¯¹äºæ¯ä¸ªattentionæ¥è¯´ï¼Œå®ƒä»¬ä¸çŸ¥é“æ•°æ®çš„å‰åä½ç½®ä¿¡æ¯ï¼ˆç‰¹åˆ«æ˜¯å¯¹äºç¼–ç å™¨æ¥è¯´ï¼‰ã€‚å› æ­¤éœ€è¦å¯¹æ•°æ®çš„ä½ç½®åšé¢å¤–çš„è¡¨å¾</p><h3 id="æ¨¡å‹çš„æ‹†è§£ç”¨æ³•"><a href="#æ¨¡å‹çš„æ‹†è§£ç”¨æ³•" class="headerlink" title="æ¨¡å‹çš„æ‹†è§£ç”¨æ³•"></a>æ¨¡å‹çš„æ‹†è§£ç”¨æ³•</h3><ul><li><strong>encoder-decoder</strong>ï¼šç”¨äºseq2seq modeling</li><li><strong>encoder only</strong>ï¼šrepresentation learningï¼Œç”¨äºæ”¯æŒclassificationï¼Œsequence labeling</li><li><strong>decoder only</strong>ï¼šï¼ˆæ­¤æ—¶encoder-decoder cross-attention moduleä¹Ÿè¢«ç§»é™¤ï¼‰ï¼Œsequence generationï¼Œç”¨äºæ”¯æŒlanguage modeling</li></ul><p>ï¼ˆåç»­æ›´æ–°ï¼šä¸»è¦transformerçš„æ€»ç»“ï¼Œä»¥åŠäº®ç‚¹ç»“æ„ï¼‰</p>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> transformers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4GWANä¸­çš„å‘Šè­¦ï¼ˆalarmï¼‰åˆ†æ</title>
      <link href="/uncategorized/notes/alarm_in_4GWAN/"/>
      <url>/uncategorized/notes/alarm_in_4GWAN/</url>
      
        <content type="html"><![CDATA[<h1 id="å‘Šè­¦ä¸æ€§èƒ½æŒ‡æ ‡ä¹‹é—´çš„å…³ç³»"><a href="#å‘Šè­¦ä¸æ€§èƒ½æŒ‡æ ‡ä¹‹é—´çš„å…³ç³»" class="headerlink" title="å‘Šè­¦ä¸æ€§èƒ½æŒ‡æ ‡ä¹‹é—´çš„å…³ç³»"></a>å‘Šè­¦ä¸æ€§èƒ½æŒ‡æ ‡ä¹‹é—´çš„å…³ç³»</h1><p>æœ¬æ–‡ä¸»è¦ä»¥caseä¸ºå•ä½ï¼Œåˆ†ææ¯ä¸ªå‘Šè­¦å¯èƒ½å¯¹æ€§èƒ½æŒ‡æ ‡é€ æˆçš„å½±å“ï¼Œå›é¡¾ä¸“å®¶åœ¨åˆ¤å®šå‘Šè­¦å¯¹æµé‡æŸå¤±æ—¶çš„æ€è·¯: </p><ol><li>ä»¥expert labelä¸­æ ‡æ³¨çš„<code>Time of Interest (TOI)</code>ç¡®å®šcaseä¸­çš„target alarm: target alarmä¸ºTOIä¸­å‘ç”Ÿè¿‡çš„å‘Šè­¦ã€ä»æœªæ¸…é™¤çš„å‘Šè­¦</li><li>ä»¥alarmä¸ºä¸­å¿ƒï¼Œåˆ†æä¸å‘Šè­¦ç›¸å…³çš„ä¸Šä¸‹æ¸¸çŸ¥è¯†ï¼ˆå¦‚ç‰©ç†å±‚ã€é“¾è·¯å±‚ç­‰ï¼‰æ˜¯å¦å‘ç”Ÿå¼‚å¸¸ï¼ˆè¯„ä»·æ‰€ç”¨çš„å‚è€ƒæŒ‡æ ‡: ç¯æ¯”KPI - ä¸€å¤©ç¯æ¯”æˆ–ä¸€å‘¨ç¯æ¯”ï¼‰</li><li>åˆ†æå¼‚å¸¸é“¾è·¯æ˜¯å¦å®Œæ•´ï¼ŒåŒ…æ‹¬: ï¼ˆ1ï¼‰å‘Šè­¦æ‰€å½±å“çš„KPIæ˜¯å¦å‘ç”Ÿå¼‚å¸¸ï¼ˆ2ï¼‰ç›®æ ‡KPIï¼ˆä¸€èˆ¬ä¸ºæµé‡ï¼‰æ˜¯å¦å‘ç”Ÿå¼‚å¸¸</li></ol><span id="more"></span><h2 id="case-2-RRUçº§å‘Šè­¦-æ¥å£å¼‚å¸¸-å‘Šè­¦æ— æŸå¤±æ¡ˆä¾‹"><a href="#case-2-RRUçº§å‘Šè­¦-æ¥å£å¼‚å¸¸-å‘Šè­¦æ— æŸå¤±æ¡ˆä¾‹" class="headerlink" title="case 2 (RRUçº§å‘Šè­¦ æ¥å£å¼‚å¸¸ å‘Šè­¦æ— æŸå¤±æ¡ˆä¾‹)"></a>case 2 (RRUçº§å‘Šè­¦ æ¥å£å¼‚å¸¸ å‘Šè­¦æ— æŸå¤±æ¡ˆä¾‹)</h2><p><code>alarm: RF Unit CPRI Interface Error</code> (Minor) - å°„é¢‘å•å…ƒé“¾è·¯ä¸Šæ‰¿è½½çš„ä¸šåŠ¡è´¨é‡å¯èƒ½ä¼šç•¥æœ‰ä¸‹é™ï¼ˆè¯¦ç»†è¯´æ˜è§ä¸‹å›¾ï¼‰</p><p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/20220104134155.png"></p><p><strong>ä¸“å®¶æ€è·¯</strong>: </p><ol><li>å‘Šè­¦æ—¶é—´å†…ï¼Œå‘Šè­¦å°åŒºçš„ç‰©ç†å±‚ä¸­ï¼Œä¸Šä¸‹è¡ŒRSRPæŒ‡æ ‡ä¸ç¯æ¯”å‚è€ƒæŒ‡æ ‡ç›¸æ¯”æ²¡æœ‰æ˜æ˜¾å˜åŒ–ï¼Œè¯´æ˜å‘Šè­¦å¯¹å°åŒºæ— å½±å“ã€‚</li><li>è¿›ä¸€æ­¥çš„æ¯”è¾ƒå‘Šè­¦å°åŒºä¸ç¯æ¯”æµé‡æ˜¯å¦æœ‰æŸå¤±ã€‚</li></ol><p>å¼ºç›¸å…³çš„KPIæŒ‡æ ‡ï¼ˆå‘Šè­¦å°åŒºï¼‰: </p><ul><li>L.MeasRpts.RSRP.Index0~2 (RSRP&lt;-110dbm)  Ratio (%)ï¼ˆMRæµ‹é‡ä¸ŠæŠ¥RSRP&lt;-110dbmæ¯”ä¾‹(%)ï¼‰</li><li>UL PUCCH RSRP low than-120dBm ratioï¼ˆä¸Šè¡ŒPUCCH RSRPä½äº-120dBmçš„æ¯”ä¾‹ï¼‰</li><li>UL PUSCH RSRP low than-120dBm ratioï¼ˆä¸Šè¡ŒPUSCH RSRPä½äº-120dBmçš„æ¯”ä¾‹ï¼‰</li></ul><h2 id="case-3-å…¨ç«™çº§å‘Šè­¦-æ—¶é’Ÿç±»å¼‚å¸¸-å‘Šè­¦æ— æŸå¤±æ¡ˆä¾‹"><a href="#case-3-å…¨ç«™çº§å‘Šè­¦-æ—¶é’Ÿç±»å¼‚å¸¸-å‘Šè­¦æ— æŸå¤±æ¡ˆä¾‹" class="headerlink" title="case 3 (å…¨ç«™çº§å‘Šè­¦ æ—¶é’Ÿç±»å¼‚å¸¸ å‘Šè­¦æ— æŸå¤±æ¡ˆä¾‹)"></a>case 3 (å…¨ç«™çº§å‘Šè­¦ æ—¶é’Ÿç±»å¼‚å¸¸ å‘Šè­¦æ— æŸå¤±æ¡ˆä¾‹)</h2><p><code>alarm: External Clock Reference Problem</code>ï¼ˆMajorï¼‰- Minor å½±å“åˆ‡æ¢ï¼Œå¯èƒ½å¯¼è‡´æ–­ç«™ï¼›Major å¯¼è‡´è·¨ç«™å¹²æ‰°ï¼Œå½±å“é€Ÿç‡ã€æ¥å…¥æˆåŠŸç‡ã€æ‰è¯ç‡ç­‰æŒ‡æ ‡ï¼Œä¸¥é‡æ—¶ä¼šå¼•å‘æ–­ç«™ï¼ˆè¯¦ç»†è¯´æ˜è§ä¸‹å›¾ï¼‰</p><p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/20220104134040.png"></p><p><strong>ä¸“å®¶æ€è·¯</strong>: å‘Šè­¦å°åŒºçš„ç‰©ç†å±‚ä¸­çš„å¹²æ‰°æŒ‡æ ‡ã€ç”¨æˆ·å±‚çš„é€Ÿç‡æŒ‡æ ‡å‡æ— æ˜æ˜¾å˜åŒ–ï¼Œè¯´æ˜å‘Šè­¦æ— å½±å“ã€‚</p><p>å¼ºç›¸å…³çš„KPIæŒ‡æ ‡ï¼ˆå‘Šè­¦å°åŒºï¼‰: </p><ul><li>L.UL.Interference.Avg(dBm)ï¼ˆä¸Šè¡Œå¹³å‡å¹²æ‰°ï¼‰</li><li>ï¼ˆä¸Šè¡Œç”¨æˆ·ä½“éªŒé€Ÿç‡(Mbps)ï¼‰</li><li>LTE_User UL Average Throughput(Mbps)ä¸‹è¡Œç”¨æˆ·ä½“éªŒé€Ÿç‡(Mbps)ï¼ˆä¸‹è¡Œç”¨æˆ·ä½“éªŒé€Ÿç‡(Mbps)ï¼‰</li></ul><h2 id="case-4-å°åŒºçº§å‘Šè­¦-æ–­ç«™å¼‚å¸¸-å‘Šè­¦æœ‰æŸå¤±æ¡ˆä¾‹"><a href="#case-4-å°åŒºçº§å‘Šè­¦-æ–­ç«™å¼‚å¸¸-å‘Šè­¦æœ‰æŸå¤±æ¡ˆä¾‹" class="headerlink" title="case 4 (å°åŒºçº§å‘Šè­¦ æ–­ç«™å¼‚å¸¸ å‘Šè­¦æœ‰æŸå¤±æ¡ˆä¾‹)"></a>case 4 (å°åŒºçº§å‘Šè­¦ æ–­ç«™å¼‚å¸¸ å‘Šè­¦æœ‰æŸå¤±æ¡ˆä¾‹)</h2><p><code>alarm: Cell Unavailable</code></p><p>å¼ºç›¸å…³çš„KPIæŒ‡æ ‡ï¼ˆå‘Šè­¦å°åŒºï¼‰: ï¼ˆæœªåˆ—å‡ºï¼‰</p><p>è¿™ä¸ªæ¡ˆä¾‹ç›´æ¥æ¯”è¾ƒäº†å‘Šè­¦å°åŒºçš„æµé‡ç¯æ¯”æ˜¯å¦æŸå¤±ï¼ˆå®é™…æƒ…å†µæ˜¯æŸå¤±çš„ï¼‰ï¼Œå’Œé‚»åŒºå°åŒºçš„æµé‡ç´¯åŠ ç¯æ¯”æ˜¯å¦å¢åŠ ï¼ˆå®é™…æƒ…å†µä¸“å®¶è®¤ä¸ºæ˜¯æ— å¢åŠ çš„ï¼‰</p><h2 id="case-5-6-RRUçº§å‘Šè­¦-æ–­ç«™å¼‚å¸¸-å‘Šè­¦æ— æŸå¤±æ¡ˆä¾‹"><a href="#case-5-6-RRUçº§å‘Šè­¦-æ–­ç«™å¼‚å¸¸-å‘Šè­¦æ— æŸå¤±æ¡ˆä¾‹" class="headerlink" title="case 5/6 (RRUçº§å‘Šè­¦ æ–­ç«™å¼‚å¸¸ å‘Šè­¦æ— æŸå¤±æ¡ˆä¾‹)"></a>case 5/6 (RRUçº§å‘Šè­¦ æ–­ç«™å¼‚å¸¸ å‘Šè­¦æ— æŸå¤±æ¡ˆä¾‹)</h2><p><code>alarm: RF Unit VSWR Threshold Crossed (Minor, Major)</code> - å°„é¢‘å•å…ƒçš„è¾“å‡ºåŠŸç‡é™ä½ï¼Œå°åŒºè¦†ç›–ç¼©å°ï¼ˆè¯¦ç»†è¯´æ˜è§ä¸‹å›¾ï¼‰</p><p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/20220104141701.png"></p><p> <strong>ä¸“å®¶æ€è·¯</strong> : </p><ol><li>ç‰©ç†å±‚æŒ‡æ ‡ä¸­ï¼ŒRSRP&lt;-110dbmæ¯”ä¾‹æ— æ˜æ˜¾å˜åŒ–ï¼Œè¯´æ˜å‘Šè­¦å°åŒºçš„ä¿¡å·å¼ºåº¦åœ¨å‘Šè­¦æœŸé—´æœªå‘ç”Ÿæ˜æ˜¾å˜åŒ– </li><li>ä¸‹è¡Œå¹³å‡MCSæ— æ˜æ˜¾å˜åŒ–ï¼Œè¯´æ˜å‘Šè­¦å°åŒºçš„ä¿¡é“è´¨é‡åœ¨å‘Šè­¦æœŸé—´æœªå‘ç”Ÿæ˜æ˜¾å˜åŒ–ï¼›ä¸‹è¡Œ64QAMçš„æ¯”ä¾‹æ— æ˜æ˜¾å˜åŒ–ï¼Œï¼ˆQAMæŒ‡ç›¸æ­£äº¤æŒ¯å¹…è°ƒåˆ¶ï¼Œå¸¸ç”¨çš„QAMæœ‰16QAMã€64QAMã€256QAMï¼Œè¶Šå¤§å¯¹ä¿¡é“è´¨é‡çš„è¦æ±‚è¶Šé«˜ï¼‰</li><li>åŒé¢‘å’Œå¼‚é¢‘åˆ‡æ¢å‡ºæˆåŠŸæ¬¡æ•°æ— æ˜æ˜¾å˜åŒ–ï¼Œè¯´æ˜å‘Šè­¦å°åŒºçš„ç”¨æˆ·æœªå› å°åŒºè¦†ç›–èŒƒå›´å˜å°è€Œåˆ‡å‡ºå‘Šè­¦å°åŒº</li></ol><p>å¼ºç›¸å…³çš„KPIæŒ‡æ ‡ï¼ˆå‘Šè­¦å°åŒºï¼‰: </p><ul><li>L.MeasRpts.RSRP.Index0~2 (RSRP&lt;-110dbm)  Ratio (%) ï¼ˆMRæµ‹é‡ä¸ŠæŠ¥RSRP&lt;-110dbmæ¯”ä¾‹(%)ï¼Œç‰©ç†å±‚æŒ‡æ ‡ï¼ŒRSRPæµ‹é‡æä¾›å°åŒºä¿¡å·å¼ºåº¦çš„æµ‹é‡ï¼‰</li><li>DL MCS AVG/DL64QAM RATIO ï¼ˆä¸‹è¡Œå¹³å‡MCS/ä¸‹è¡Œ64QAMçš„æ¯”ä¾‹ï¼ŒMACå±‚æŒ‡æ ‡ï¼ŒMCS:è°ƒåˆ¶ç¼–ç æ–¹æ¡ˆï¼Œå½“ä¿¡é“è´¨é‡å¥½æ—¶é‡‡ç”¨é«˜é˜¶çš„è°ƒåˆ¶æ–¹å¼å’Œæ›´é«˜çš„ç¼–ç æ•ˆç‡ï¼ŒMCSè¶Šé«˜ï¼Œç ç‡è¶Šå¤§ï¼Œä¼ è¾“æ•ˆç‡è¶Šé«˜ï¼‰</li><li>InterFreq HO Succ Times/IntraFreq HO Succ Timesï¼ˆåŒé¢‘åˆ‡æ¢å‡ºæˆåŠŸæ¬¡æ•°/å¼‚é¢‘åˆ‡æ¢å‡ºæˆåŠŸæ¬¡æ•°ï¼Œç”¨æˆ·å±‚æŒ‡æ ‡ï¼‰</li></ul><h2 id="case-13-RRUçº§å‘Šè­¦-æ–­ç«™å¼‚å¸¸-å‘Šè­¦æœ‰æŸå¤±æ¡ˆä¾‹"><a href="#case-13-RRUçº§å‘Šè­¦-æ–­ç«™å¼‚å¸¸-å‘Šè­¦æœ‰æŸå¤±æ¡ˆä¾‹" class="headerlink" title="case 13  (RRUçº§å‘Šè­¦ æ–­ç«™å¼‚å¸¸ å‘Šè­¦æœ‰æŸå¤±æ¡ˆä¾‹)"></a>case 13  (RRUçº§å‘Šè­¦ æ–­ç«™å¼‚å¸¸ å‘Šè­¦æœ‰æŸå¤±æ¡ˆä¾‹)</h2><p>TOIæ—¶é—´æ®µå†…223196ä¸Šå‘ç”Ÿè¿‡çš„å‘Šè­¦ï¼Œé»‘ä½“ä¸ºè¿˜æœªæ¸…é™¤çš„å‘Šè­¦: </p><ul><li><strong>RF Unit Optical Module Fault</strong></li><li>External Clock Reference Problem</li><li>Remote Maintenance Link Failure</li><li><strong>RF Unit Maintenance Link Failure</strong></li><li>Cell Capability Degraded</li><li>BBU CPRI Interface Error</li><li>BBU CPRI Line Rate Negotiation Abnormal</li><li><strong>Cell Unavailable</strong></li></ul><p>å‘Šè­¦è§£æå¦‚ä¸‹è¡¨</p><table><thead><tr><th>å‘Šè­¦å</th><th>å‘Šè­¦ç±»å‹</th><th>å‘Šè­¦æè¿°</th><th>å½±å“</th></tr></thead><tbody><tr><td>RF Unit Optical Module Fault</td><td>å°„é¢‘ç±»</td><td>å°„é¢‘å•å…ƒå…‰æ¨¡å—æ•…éšœ</td><td>å…‰æ¨¡å—çš„æ”¶å‘æ€§èƒ½ä¸‹é™ =&gt;å°„é¢‘å•å…ƒé“¾è·¯ä¸Šæ‰¿è½½çš„ä¸šåŠ¡è´¨é‡ä¸‹é™=&gt;å°„é¢‘å•å…ƒä¸Šæ‰¿è½½çš„ä¸šåŠ¡å¯èƒ½ä¼šä¸­æ–­</td></tr><tr><td>External Clock Reference Problem</td><td>æ—¶é’Ÿç±»</td><td>æ—¶é’Ÿå‚è€ƒæºå¼‚å¸¸</td><td>å¯èƒ½å¯¼è‡´TDDç½‘å…ƒç³»ç»Ÿæ—¶é’Ÿä¸å¯ç”¨ï¼Œå¯èƒ½å‡ºç°å°åŒºæ¥å…¥å¤±è´¥ã€æ‰è¯ç­‰ä¸šåŠ¡å¼‚å¸¸æˆ–æ— æ³•æä¾›ä¸šåŠ¡</td></tr><tr><td>Remote Maintenance   Link Failure</td><td>æ–­ç«™ç±»</td><td>Operation and Maintenance CHannel (OMCH)ä¸­æ–­è¶…è¿‡äº”åˆ†é’Ÿå¯¼è‡´æ— æ³•ç»´æŠ¤</td><td>ç”¨æˆ·æ— æ³•ç»´æŠ¤è¿œç«¯è®¾å¤‡ã€‚åŸºç«™åªèƒ½åœ¨ç°åœºç»´æŠ¤ã€‚</td></tr><tr><td>RF Unit Maintenance Link Failure</td><td>å°„é¢‘ç±»</td><td>BBUä¸å°„é¢‘å•å…ƒä¹‹é—´çš„ç»´æŠ¤é“¾è·¯æ•…éšœï¼Œä¸”æ•…éšœæŒç»­ä¸€æ®µæ—¶é—´</td><td>å°„é¢‘å•å…ƒä¸Šæ‰¿è½½çš„ä¸šåŠ¡ä¸­æ–­ã€‚</td></tr><tr><td>Cell Capability Degrade</td><td>æ–­ç«™ç±»</td><td>å°åŒºæœåŠ¡èƒ½åŠ›ä¸‹é™</td><td>å‘Šè­¦å°åŒºæä¾›ç»™å®¢æˆ·å¯ç”¨çš„æ— çº¿ç©ºå£èƒ½åŠ›ä¼šä¸‹é™ï¼Œå¯èƒ½å‡ºç°ç”¨æˆ·æ¥å…¥å¼‚å¸¸</td></tr><tr><td>BBU CPRI Interface Error</td><td>å°„é¢‘ç±»</td><td>BBU CPRIå…‰æ¨¡å—æ¥å£å¼‚å¸¸å‘Šè­¦</td><td>BBUä¸ä¸‹çº§å°„é¢‘å•å…ƒçš„å…‰æ¨¡å—çš„è¿æ¥é“¾è·¯ä¸­æ–­ï¼Œæˆ–æ”¶å‘æ€§èƒ½è½»å¾®æ¶åŒ–ï¼ŒMACå±‚é”™å¸§ç‡è¶…è¿‡æŒ‡å®šé—¨é™=&gt;ä¸šåŠ¡è´¨é‡å˜å·®</td></tr><tr><td>BBU CPRI Line Rate Negotiation Abnormal</td><td>å°„é¢‘ç±»</td><td>BBUä¸ä¸‹çº§å°„é¢‘å•å…ƒé—´çº¿é€Ÿç‡ä¸ä¸€è‡´</td><td>å¼•å‘BBUä¸ä¸‹çº§å°„é¢‘å•å…ƒçš„é—´çš„å¸¦å®½ä¸è¶³ï¼Œå¯¼è‡´å°åŒºå»ºç«‹å¤±è´¥ï¼Œç³»ç»Ÿå®¹é‡é™ä½</td></tr></tbody></table><p>è¿™ä¸ªæ¡ˆä¾‹ç›´æ¥æ¯”è¾ƒäº†å‘Šè­¦å°åŒºçš„æµé‡ç¯æ¯”æ˜¯å¦æŸå¤±ï¼ˆå®é™…æƒ…å†µæ˜¯æŸå¤±çš„ï¼‰ï¼Œå’Œé‚»åŒºå°åŒºçš„æµé‡ç´¯åŠ ç¯æ¯”æ˜¯å¦å¢åŠ ï¼ˆå®é™…æƒ…å†µä¸“å®¶è®¤ä¸ºæ˜¯æ— å¢åŠ çš„ï¼‰</p>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> 5G </tag>
            
            <tag> 4G </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linuxæœåŠ¡å™¨å¸¸ç”¨é…ç½®</title>
      <link href="/uncategorized/notes/linux_server_notes/"/>
      <url>/uncategorized/notes/linux_server_notes/</url>
      
        <content type="html"><![CDATA[<p>ä»…è®°å½•ä¸å¸¸ç”¨çš„å‘½ä»¤ï¼Œå¯èƒ½ä¼šä¸å…¨å“¦ğŸ‘ğŸ»</p><span id="more"></span><ol><li>æœåŠ¡å™¨<code>conda</code>åˆ‡æ¢åŒ—å¤–é•œåƒæº</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ conda config <span class="token parameter variable">--add</span> channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/free/ $ conda config <span class="token parameter variable">--add</span> channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/main/ $ conda config <span class="token parameter variable">--add</span> channels https://mirrors.bfsu.edu.cn/anaconda/cloud/conda-forge $ conda config <span class="token parameter variable">--add</span> channels https://mirrors.bfsu.edu.cn/anaconda/cloud/msys2/$ conda config <span class="token parameter variable">--set</span> show_channel_urls <span class="token function">yes</span> $ conda config <span class="token parameter variable">--add</span> channels https://mirrors.bfsu.edu.cn/anaconda/cloud/pytorch/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li><p><code>kill -STOP 1234</code> å°†è¿›ç¨‹æš‚åœï¼Œå¦‚æœè¦è®©å®ƒæ¢å¤åˆ°åå°ï¼Œç”¨<code>kill -CONT 1234</code>ã€‚<a href="https://www.cnblogs.com/kexinxin/p/9939119.html">ref1</a>, <a href="https://www.jianshu.com/p/d4190447736e">ref2</a> </p></li><li><p>linuxè§£å‹ã€å®‰è£…raræ–‡ä»¶</p></li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ <span class="token function">wget</span> http://rarsoft.com/rar/rarlinux-x64-5.5.0.tar.gz$ <span class="token function">sudo</span> <span class="token function">tar</span> <span class="token parameter variable">-zxvf</span> rarlinux-x64-5.5.0.tar.gz<span class="token comment"># ç¼–è¯‘</span>$ <span class="token builtin class-name">cd</span> <span class="token function">rar</span>$ <span class="token function">sudo</span> <span class="token function">make</span><span class="token comment"># æµ‹è¯•</span>$ <span class="token function">rar</span><span class="token comment"># è§£å‹, æŒ‰å‹ç¼©åŒ…å†…çš„æ–‡ä»¶ç»“æ„è§£å‹</span>$ <span class="token function">rar</span> x <span class="token operator">&lt;</span>file_path<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="4"><li>æŸ¥çœ‹ç›®å½•çš„æ–‡ä»¶å¤§å°ä½¿ç”¨<code>du -h &lt;dir_path&gt;</code>ï¼Œ<code>-h</code>ä»£è¡¨humanï¼Œå°†è¾“å‡ºäººç±»æ˜“è¯»çš„æ–‡ä»¶å•ä½ï¼ˆMB/GB/â€¦ï¼‰</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdownå¸¸ç”¨è¯­æ³•</title>
      <link href="/uncategorized/notes/markdown_notes/"/>
      <url>/uncategorized/notes/markdown_notes/</url>
      
        <content type="html"><![CDATA[<p><a href="http://www.w3chtml.com/html/html-basic-grammar.html">Remark: HTMLå¸¸ç”¨è¯­æ³•</a></p><span id="more"></span><ol><li>æ’å…¥å¹¶è°ƒæ•´å›¾ç‰‡å¤§å°</li></ol><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">align</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>center<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>fig.png<span class="token punctuation">"</span></span> <span class="token attr-name">width</span> <span class="token attr-value"><span class="token punctuation attr-equals">=</span> <span class="token punctuation">"</span>70%<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="2"><li>è®¾ç½®é¡µé¢æ”¯æŒlatexå…¬å¼ï¼Œåœ¨é¡µé¢å¤´éƒ¨æ’å…¥ä»¥ä¸‹å†…å®¹</li></ol><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>head</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML<span class="token punctuation">"</span></span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>text/javascript<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token script"></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>text/x-mathjax-config<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token script"><span class="token language-javascript">        MathJax<span class="token punctuation">.</span>Hub<span class="token punctuation">.</span><span class="token function">Config</span><span class="token punctuation">(</span><span class="token punctuation">&#123;</span>            <span class="token literal-property property">tex2jax</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>            <span class="token literal-property property">skipTags</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">'script'</span><span class="token punctuation">,</span> <span class="token string">'noscript'</span><span class="token punctuation">,</span> <span class="token string">'style'</span><span class="token punctuation">,</span> <span class="token string">'textarea'</span><span class="token punctuation">,</span> <span class="token string">'pre'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token literal-property property">inlineMath</span><span class="token operator">:</span> <span class="token punctuation">[</span> <span class="token punctuation">[</span><span class="token string">'$'</span><span class="token punctuation">,</span><span class="token string">'$'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">"\\("</span><span class="token punctuation">,</span><span class="token string">"\\)"</span><span class="token punctuation">]</span> <span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token punctuation">&#125;</span>        <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    </span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>head</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="3"><li><p>ä¸ºæ–‡ç« åŠ å…¥é”šç‚¹</p><ol><li>markdownçš„æ ‡é¢˜è¡Œæœ¬èº«è‡ªå¸¦é”šç‚¹ï¼Œç›´æ¥ä½¿ç”¨<code>[some_text](#title)</code>å³å¯å®ç°é¡µå†…è·³è½¬ï¼Œæ³¨æ„ï¼Œ<code>title</code>ä¸­ä¸å¯ä»¥åŒ…å«ç¬¦å·ï¼Œå¦åˆ™ä¼šè·³è½¬å¤±è´¥ã€‚</li><li>ä½¿ç”¨<code>&lt;a&gt;</code>æ ‡ç­¾ï¼Œæ³¨æ„ï¼ŒåŒ…å›´åœ¨æ ‡ç­¾ä¸­çš„éƒ¨åˆ†ï¼Œä¸é€‚ç”¨äºmarkdownè¯­æ³•ï¼Œä»¥åŠå…¬å¼è¯­å¥ã€‚</li></ol></li></ol><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token comment">&lt;!-- HTMLæ–¹æ³• --></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>tag<span class="token punctuation">"</span></span><span class="token punctuation">></span></span> æ·»åŠ é”šç‚¹ <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>#tag<span class="token punctuation">"</span></span><span class="token punctuation">></span></span> é“¾æ¥é”šç‚¹ <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown"><span class="token comment">&lt;!-- markdownæ–¹æ³• --></span><span class="token url">[<span class="token content">é“¾æ¥é”šç‚¹</span>](<span class="token url">#tag</span>)</span><span class="token url">[<span class="token content">é“¾æ¥åˆ°å¦ä¸€ä¸ªæ–‡ç« çš„é”šç‚¹</span>](<span class="token url">other_file.md#tag</span>)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in Other Top Conference (2022)</title>
      <link href="/uncategorized/paperlistfile/otherconf_2022/"/>
      <url>/uncategorized/paperlistfile/otherconf_2022/</url>
      
        <content type="html"><![CDATA[<h2 id="WSDM-2022"><a href="#WSDM-2022" class="headerlink" title="WSDM 2022"></a>WSDM 2022</h2><p><a href="https://mp.weixin.qq.com/s/ewdkeq39RODHSsKABNh6cA">æ—¶åºè®ºæ–‡ä¸€è§ˆ</a></p><span id="more"></span>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>å…³è”åˆ†æä¸å› æœåˆ†æè°ƒç ”</title>
      <link href="/uncategorized/surveys/correlation/"/>
      <url>/uncategorized/surveys/correlation/</url>
      
        <content type="html"><![CDATA[<p>æœ¬æ–‡æ—¨åœ¨è°ƒç ”åœ¨æ™ºèƒ½è¿ç»´é¢†åŸŸä¸­çš„å…³è”åˆ†æä¸å› æœåˆ†ææ–¹æ³•ï¼Œæœ¬æ–‡çš„ç»„ç»‡å¦‚ä¸‹ï¼š</p><ol><li>é¦–å…ˆæˆ‘ä»¬å°†ç»™å‡ºä¸€ä¸ªæºè‡ªçœŸå®ä¸šç•Œéœ€æ±‚çš„æ¡ˆä¾‹åœºæ™¯ã€‚</li><li>å›´ç»•è¯¥ä¸Šè¿°åœºæ™¯ï¼Œæˆ‘ä»¬å°†ä»‹ç»è¯¥åœºæ™¯ä¸­å¯ç”¨çš„å…³è”åˆ†æã€å› æœåˆ†ææ–¹æ³•ï¼Œå¹¶ç»™å‡ºç›¸å…³çš„æ–¹æ³•åˆ†ç±»</li><li>å¯¹äºç»å…¸æ–¹æ³•ï¼ˆåŸºäºé¢‘ç¹é¡¹æŒ–æ˜ï¼‰å’Œä¸æˆ‘ä»¬çš„ç ”ç©¶ç›¸å…³çš„æ–¹æ³•ï¼ˆåŸºäºå›¾åµŒå…¥ï¼‰ï¼Œæœ¬æ–‡ä¹Ÿå°†åˆ—å‡ºç›¸å…³çš„ç»¼è¿°ä¸è®ºæ–‡åˆ—è¡¨ã€‚</li><li>åœ¨æœ¬æ–‡çš„ç ”ç©¶èŒƒå›´ä¹‹å¤–ï¼Œæœ¬æ–‡ä¹Ÿåˆ—å‡ºäº†â€œäº‹ä»¶åºåˆ—-äº‹ä»¶åºåˆ—â€å…³è”åˆ†æã€â€œäº‹ä»¶åºåˆ—-æ—¶é—´åºåˆ—â€å…³è”åˆ†æçš„ç›¸å…³æ–¹æ³•ã€‚</li></ol><span id="more"></span><h2 id="æ¡ˆä¾‹åœºæ™¯ä¸é—®é¢˜è½¬åŒ–"><a href="#æ¡ˆä¾‹åœºæ™¯ä¸é—®é¢˜è½¬åŒ–" class="headerlink" title="æ¡ˆä¾‹åœºæ™¯ä¸é—®é¢˜è½¬åŒ–"></a>æ¡ˆä¾‹åœºæ™¯ä¸é—®é¢˜è½¬åŒ–</h2><p>è¯¥åœºæ™¯æºäºç§»åŠ¨é€šä¿¡ç½‘ç»œä¸­çš„ç½‘ç»œä¼˜åŒ–ä»»åŠ¡ã€‚åœ¨ç°ç½‘ç½‘ç»œä¼˜åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œå¾€å¾€éœ€è¦è¿›è¡Œ<strong>ç½‘ç»œå‚æ•°</strong>çš„è°ƒæ•´ï¼Œå‚æ•°è°ƒæ•´åå°†æ”¶é›†ç½‘ç»œä¸­çš„<strong>æ€§èƒ½æŒ‡æ ‡æ•°æ®</strong>ï¼ˆKPIï¼‰ç”¨äºéªŒè¯å‚æ•°è°ƒæ•´çš„æ•ˆæœã€‚å®é™…ä¸Šï¼Œç½‘ç»œKPIå—å¤šç§é…ç½®å‚æ•°çš„è”åˆå½±å“ï¼Œä¸”é’ˆå¯¹æŸä¸ªå°åŒºçš„å‚æ•°è°ƒæ•´ä¹Ÿä¼šå½±å“è¯¥å°åŒºçš„é‚»åŒºï¼Œè¿›è€Œé€ æˆé‚»åŒºç½‘ç»œæ€§èƒ½çš„å˜åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›äº†è§£ï¼š</p><ol><li>ç»™å®šéœ€è¦ä¼˜åŒ–çš„ç½‘ç»œæ€§èƒ½æŒ‡æ ‡ï¼Œç¡®å®šæœ‰å“ªäº›å‚æ•°ï¼ˆç»„ï¼‰å°†å½±å“è¯¥æŒ‡æ ‡ï¼›</li><li>ç»™å®šè¦è°ƒæ•´çš„ç½‘ç»œå‚æ•°ï¼Œåˆ†æè¯¥å‚æ•°å°†å½±å“å“ªäº›æŒ‡æ ‡ã€‚</li></ol><p>ç»¼ä¸Šæ‰€è¿°ï¼Œæœ¬åœºæ™¯çš„æœ¬è´¨æ˜¯è¦å»ºç«‹â€œå‚æ•°â€åˆ°â€œç½‘ç»œæ€§èƒ½æŒ‡æ ‡â€ä¹‹é—´çš„å…³è”å…³ç³»ï¼ˆè¾ƒä½é˜¶ï¼‰æˆ–å› æœå…³ç³»ï¼ˆè¾ƒé«˜é˜¶ï¼‰ã€‚å…¶ä¸­KPIæ•°æ®æœ¬è´¨ä¸Šæ˜¯<strong>æ—¶é—´åºåˆ—</strong>ï¼Œè€Œå‚æ•°è°ƒæ•´å¯ä»¥è§†ä¸º<strong>æ—¶é—´åºåˆ—</strong>æˆ–<strong>äº‹ä»¶åºåˆ—</strong>ã€‚å› æ­¤æœ¬ä»»åŠ¡å¯¹åº”çš„ç§‘å­¦é—®é¢˜è¢«ç®€åŒ–ä¸º</p><blockquote><p>ç»™å®šä¸€å¯¹æ—¶é—´åºåˆ—ï¼ˆæˆ–ä¸€ä¸ªæ—¶é—´åºåˆ—ä¸ä¸€ä¸ªäº‹ä»¶åºåˆ—ï¼‰ï¼Œå¦‚ä½•å»ºæ¨¡äºŒè€…ä¹‹é—´çš„å…³è”å…³ç³»ï¼ˆæˆ–å› æœå…³ç³»ï¼‰</p></blockquote><p><strong>åœ¨åé¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦è°ƒç ”â€œæ—¶é—´åºåˆ—-æ—¶é—´åºåˆ—â€ä¹‹é—´çš„å…³è”åˆ†ææ–¹æ³•ã€‚</strong></p><p>ä¸€äº›èƒŒæ™¯çŸ¥è¯†ï¼š</p><ul><li>ç§»åŠ¨ç½‘ç»œçš„å»ºè®¾è¿‡ç¨‹åˆ†ä¸ºï¼šè§„åˆ’ã€å»ºè®¾ã€ç»´æŠ¤ã€ä¼˜åŒ–ï¼Œå››ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å±äºä¼˜åŒ–ç¯èŠ‚ã€‚</li><li>ç½‘ç»œé…ç½®å‚æ•°åˆ†ä¸ºï¼šéååŒç±»å‚æ•°ï¼ˆå‚æ•°è°ƒæ•´ååªå½±å“æœ¬å°åŒºçš„ç½‘ç»œæ€§èƒ½ï¼‰ã€ååŒç±»å‚æ•°ï¼ˆå‚æ•°è°ƒæ•´åä¼šå½±å“é‚»åŒºç½‘ç»œæ€§èƒ½ï¼‰</li><li>è°ƒæ•´ååŒç±»å‚æ•°çš„å…·ä½“åœºæ™¯åŒ…æ‹¬ï¼šåˆ‡æ¢ç±»ä¸è´Ÿè½½å‡è¡¡ç±»çš„ç½‘ç»œæ€§èƒ½ä¼˜åŒ–ï¼ŒRFå‚æ•°ä¼˜åŒ–</li></ul><h2 id="åˆ†ç±»"><a href="#åˆ†ç±»" class="headerlink" title="åˆ†ç±»"></a>åˆ†ç±»</h2><div align="center">    <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/20220216193335.png" width = "90%" /></div><p><strong>æ³¨ï¼š</strong></p><ol><li>ä¸Šè¿°åˆ†ç±»æ˜¯é’ˆå¯¹æœ¬æ–‡çš„ç ”ç©¶ç›®æ ‡è¿›è¡Œçš„åˆ†ç±»ï¼Œäº‹å®ä¸Šâ€œåŸºäºç›¸ä¼¼åº¦â€ã€â€œåŸºäºç›¸å…³æ€§â€ã€â€œåŸºäºå›¾æ¨¡å‹ - åŸºäºå›å½’æ¨¡å‹â€çš„æ–¹æ³•å±äºå±äºå…³è”åˆ†æï¼Œâ€œåŸºäºå›¾æ¨¡å‹â€åˆ†ç±»ä¸‹çš„â€œåŸºäºæ¡ä»¶çº¦æŸâ€ã€â€œåŸºäºå¾—åˆ†â€ã€â€œåŸºäºå‡½æ•°å¼æ¨¡å‹â€çš„æ–¹æ³•å±äºå› æœåˆ†æã€‚</li><li>å› æœåˆ†æçš„åˆ†ç±»ä¾æ®ã€ä»¥åŠä»‹ç»å¯å‚ç…§<a href="#refer12">[12]</a><a href="#refer13">[13]</a><a href="#refer13">[14]</a></li></ol><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><blockquote><div></div>[1] Su, Y., Zhao, Y., Xia, W., Liu, R., Bu, J., Zhu, J., ... & Pei, D. (2019, June). CoFlux: robustly correlating KPIs by fluctuations for service troubleshooting. In Proceedings of the International Symposium on Quality of Service (pp. 1-10).<div></div>[2] Niennattrakul, V., & Ratanamahatana, C. A. (2007, April). On clustering multimedia time series data using k-means and dynamic time warping. In 2007 International Conference on Multimedia and Ubiquitous Engineering (MUE'07) (pp. 733-738). IEEE.<div></div>[3] è´¾æµ·æ¶›. (2018). åŸºäºæ•°æ®æŒ–æ˜çš„åŠ¨ç¯ç›‘æ§ç³»ç»Ÿå‘Šè­¦ç›¸å…³æ€§ç ”ç©¶ (Doctoral dissertation, åŒ—äº¬: åŒ—äº¬äº¤é€šå¤§å­¦).<div></div>[4] Luo, C., Lou, J. G., Lin, Q., Fu, Q., Ding, R., Zhang, D., & Wang, Z. (2014, August). Correlating events with time series for incident diagnosis. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1583-1592).<div></div>[5] Jiang, G., Chen, H., & Yoshihira, K. (2006, June). Discovering likely invariants of distributed transaction systems for autonomic system management. In 2006 IEEE International Conference on Autonomic Computing (pp. 199-208). IEEE.<div></div>[6] Gerhardus, A., & Runge, J. (2020). High-recall causal discovery for autocorrelated time series with latent confounders. Advances in Neural Information Processing Systems, 33, 12615-12625.<div></div>[7] Nauta, M., Bucur, D., & Seifert, C. (2019). Causal discovery with attention-based convolutional neural networks. Machine Learning and Knowledge Extraction, 1(1), 312-340.<div></div>[8] Chu, Y., Wang, X., Ma, J., Jia, K., Zhou, J., & Yang, H. (2020, November). Inductive Granger Causal Modeling for Multivariate Time Series. In 2020 IEEE International Conference on Data Mining (ICDM) (pp. 972-977). IEEE.<div></div>[9] Brouillard, P., Lachapelle, S., Lacoste, A., Lacoste-Julien, S., & Drouin, A. (2020). Differentiable causal discovery from interventional data. Advances in Neural Information Processing Systems, 33, 21865-21877.<div></div>[10] Ogarrio, J. M., Spirtes, P., & Ramsey, J. (2016, August). A hybrid causal search algorithm for latent variable models. In Conference on probabilistic graphical models (pp. 368-379). PMLR.<div></div>[11] Ding, C., Gong, M., Zhang, K., & Tao, D. (2019). Likelihood-free overcomplete ICA and applications in causal discovery. Advances in Neural Information Processing Systems, 32.<div id="refer12">[12] Glymour, C., Zhang, K., & Spirtes, P. (2019). Review of causal discovery methods based on graphical models. Frontiers in genetics, 10, 524.</div> <div id="refer13">[13] Yao, L., Chu, Z., Li, S., Li, Y., Gao, J., & Zhang, A. (2021). A survey on causal inference. ACM Transactions on Knowledge Discovery from Data (TKDD), 15(5), 1-46.</div> <div id="refer14">[14] Malinsky, D., & Danks, D. (2018). Causal discovery algorithms: A practical guide. Philosophy Compass, 13(1), e12470.</div> </blockquote><h2 id="åŸºäºé¢‘ç¹é¡¹æŒ–æ˜çš„å·¥ä½œè°ƒç ”"><a href="#åŸºäºé¢‘ç¹é¡¹æŒ–æ˜çš„å·¥ä½œè°ƒç ”" class="headerlink" title="åŸºäºé¢‘ç¹é¡¹æŒ–æ˜çš„å·¥ä½œè°ƒç ”"></a>åŸºäºé¢‘ç¹é¡¹æŒ–æ˜çš„å·¥ä½œè°ƒç ”</h2><h3 id="Apriori"><a href="#Apriori" class="headerlink" title="Apriori"></a>Apriori</h3><p><strong>Apriori</strong>æœ€æ—©ç”± Agrawal æå‡ºï¼Œé€šè¿‡å¤šæ¬¡è¿­ä»£å»ºç«‹å€™é€‰é›†æŸ¥æ‰¾é¢‘ç¹é¡¹ã€‚åœ¨<a href="#refer15">[15]</a>ä¸­ï¼Œä½œè€…å¯¹å¤§è§„æ¨¡æ“ä½œç³»ç»Ÿä¸­äº‹ä»¶åºåˆ—é—´çš„ç›¸å…³æ€§è¿›è¡Œäº†ç ”ç©¶ï¼Œä¸ºäº†æ–¹ä¾¿å¯¹äº‹ä»¶åºåˆ—çš„ç›¸å…³æ€§è¿›è¡Œç ”ç©¶ï¼Œä½œè€…é¦–å…ˆå°†å†—é•¿çš„äº‹ä»¶è½¬åŒ–ä¸ºä¸åŒçš„äº‹ä»¶ç±»å‹ï¼Œå¹¶æ ¹æ®äº‹ä»¶ç±»å‹åºåˆ—æ•°æ®å®šä¹‰äº†â€œepisodeâ€ã€‚è®°åœ¨æ—¶é—´çª—å£ TW å†…å‘ç”Ÿçš„æ‰€æœ‰äº‹ä»¶ç±»å‹å³ä¸ºä¸€ä¸ª episodeï¼Œå¹¶è®°ä¸º$E_{eA}, T_{eA}$ï¼Œå…¶ä¸­$E_{eA}$ä¸ºä¸äº‹ä»¶ğ‘’Aç›¸é‚»çš„æ‰€æœ‰äº‹ä»¶çš„é›†åˆï¼Œ$T_{eA}$ä¸ºå¯¹åº”çš„æ—¶é—´çª—å£ã€‚ç„¶åï¼Œæ–‡ä¸­ç”¨é¢‘ç¹é¡¹æŸ¥æ‰¾ç®—æ³• Apriori å¯¹ episode åºåˆ—ä¸­é¢‘ç¹é¡¹é›†è¿›è¡Œæœç´¢ï¼Œè€Œè¿™ç§é¢‘ç¹é¡¹é›†çš„å½¢å¼å°±è¢«è®¤å®šä¸ºäº‹ä»¶ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æ–‡ä¸­åˆ©ç”¨ä¸åŒ h-ç½®ä¿¡åº¦ä¸ Apriori ä¸­ä¿®å‰ªå‹ç¼©æ¯”çš„å˜åŒ–æ›²çº¿å…³ç³»ï¼Œè‡ªåŠ¨æ±‚å‡ºæœ€é€‚å®œçš„æœ€å°æ”¯æŒåº¦ï¼Œä»è€Œå¯¹é¢‘ç¹é¡¹é›†è¿›è¡Œä¿®å‰ªã€‚</p><p>HJ Lu<a href="#refer16">[16]</a>è®¤ä¸ºç»å…¸çš„å…³è”è§„åˆ™æŒ–æ˜å¿½ç•¥äº†äº‹ç‰©å‘ç”Ÿçš„è¯­å¢ƒï¼Œå¦‚æ—¶é—´ã€åœ°ç‚¹ç­‰ã€‚ä½œè€…è®¤ä¸ºé¡¹ç›®å…³è”æœ‰ä¸¤ç§ï¼š<br>1ï¼‰äº‹ç‰©å†…çš„é¢‘ç¹é¡¹å…³è”ï¼ˆå¦‚ï¼ŒåŒä¸€å¤©å†…ï¼Œä¸¤ä¸ªè‚¡ç¥¨ä¸€èµ·æ¶¨ï¼‰ï¼›<br>2ï¼‰ä¸åŒäº‹ç‰©é—´çš„é¢‘ç¹é¡¹å…³è”ï¼ˆå¦‚ A è‚¡ç¥¨ç¬¬ä¸€å¤©æ¶¨äº†åï¼ŒB è‚¡ç¥¨åœ¨ç¬¬å››å¤©æœ‰è¾ƒå¤§æ¦‚ç‡ä¹Ÿæ¶¨ï¼‰ã€‚<br>è€Œä¼ ç»Ÿçš„é¢‘ç¹é¡¹å…³è”ç®—æ³•åªå±€é™äºæŸ¥æ‰¾å…³è”1ã€‚å› æ­¤æ–‡ä¸­å¯¹Aprioriç®—æ³•è¿›è¡Œäº†æ‰©å±•ï¼Œä»¥æå‡ºä¸€ç§Extension-Aprioriç®—æ³•ï¼Œå°†å…³è”è§„åˆ™æŒ–æ˜çš„èŒƒå›´ä»ä¼ ç»Ÿçš„å•ç»´åº¦å…³è”æ‰©å±•åˆ°å¤šç»´çš„äº‹ç‰©é—´å…³è”ï¼Œå¹¶ç ”ç©¶äº†å¤šç»´äº‹ä»¶å…³è”è§„åˆ™ä¸­æ”¯æŒåº¦ä¸ç½®ä¿¡åº¦çš„è®¡ç®—æ–¹å¼ã€‚åŒæ—¶ç®—æ³•åˆ©ç”¨å“ˆå¸ŒæŠ€æœ¯è¿‡æ»¤æ‰ä¸å¿…è¦çš„å€™é€‰äºŒé¡¹é›†ï¼Œå°†æ‰€æœ‰å¯èƒ½çš„äºŒé¡¹é›†æ•£åˆ—åˆ°ä¸€ä¸ªå“ˆå¸Œè¡¨ä¸­ï¼Œå‡å°‘äº†æ•°æ®åº“æ‰«æçš„å¤æ‚åº¦ã€‚</p><p>ä½†ç”±äº Apriori ç®—æ³•æ¯ä¸€æ¬¡å¢åŠ é¢‘ç¹é¡¹é›†å¤§å°æ—¶éƒ½éœ€è¦é‡æ–°æ‰«ææ•´ä¸ªæ•°æ®é›†ï¼Œæ‰€ä»¥å½“æ•°æ®é›†å¾ˆå¤§æ—¶ï¼Œç®—æ³•æ•ˆç‡è¾ƒä½ï¼Œå› æ­¤æœ‰è®¸å¤šç ”ç©¶æ˜¯é’ˆå¯¹å¦‚ä½•å°†Apriori ç®—æ³•è¿›è¡Œä¼˜åŒ–æé€Ÿã€‚</p><p>å¦‚<a href="#refer17">[17]</a>ç ”ç©¶äº†é¢‘ç¹é¡¹æŒ–æ˜ç®—æ³•åœ¨ MapReduce æ¡†æ¶ä¸­çš„å®ç°ã€‚æ–‡çŒ®<a href="#refer17">[17]</a>å°†ä¸²è¡Œçš„Aprioriç®—æ³•è½¬åŒ–ä¸ºåˆ†å¸ƒå¼çš„MapReduceç‰ˆæœ¬ï¼Œåœ¨æ¯ä¸€æ¬¡æŸ¥æ‰¾é¢‘ç¹é¡¹é›†æ—¶ï¼Œä½¿ç”¨mapç”Ÿæˆå€™é€‰æ”¯æŒï¼Œå¹¶ç”¨reduceæ”¶é›†å…¨å±€çš„å€™é€‰æ”¯æŒã€‚å¹¶ä¸”ç®—æ³•å¯ä»¥æ ¹æ®å€™é€‰å¯¹è±¡çš„æ•°é‡ä¸å‰ä¸€ä¸ªMapReduceé˜¶æ®µçš„æ‰§è¡Œæ—¶é—´ï¼ŒåŠ¨æ€çš„æ”¶é›†å¯å˜é•¿åº¦çš„å€™é€‰å¯¹è±¡ï¼Œæå¤§åœ°ç¼©çŸ­äº†Aprioriç”Ÿæˆé¢‘ç¹é¡¹é›†çš„æ—¶é—´ã€‚ </p><h3 id="FP-growth"><a href="#FP-growth" class="headerlink" title="FP-growth"></a>FP-growth</h3><p>é¢‘ç¹æ¨¡å¼ç”Ÿé•¿ç®—æ³•ï¼ˆ<strong>FP-growth</strong>ï¼‰æ˜¯æœ€æ—©ç”±éŸ©å®¶ç‚œç­‰äººæå‡ºçš„åˆ©ç”¨é¢‘ç¹æ¨¡å¼æ ‘è¿›è¡Œé¢‘ç¹é¡¹æŒ–æ˜çš„ç®—æ³•ã€‚ç›¸æ¯”Aprioriï¼ŒFP-growthåªç”¨éå†ä¸¤éæ•°æ®ï¼Œä¸”ä¸éœ€è¦äº§ç”Ÿå€™é€‰åºåˆ—ï¼Œæå¤§æé«˜äº†æŒ–æ˜æ•ˆç‡ã€‚å› æ­¤ä¹Ÿæœ‰è®¸å¤šç ”ç©¶äººå‘˜é€šè¿‡ FP-growth æ¥æŒ–æ˜åºåˆ—ä¸­çš„é¢‘ç¹é¡¹ã€‚</p><p>æ–‡çŒ®<a href="#refer18">[18]</a>ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¹¶è¡ŒåŒ–FP-Growthç®—æ³•çš„MapReduceæ–¹æ³•ï¼Œå°†å¤§è§„æ¨¡çš„é¢‘ç¹é¡¹æŒ–æ˜ä»»åŠ¡è‡ªåŠ¨åˆ†å‰²æˆç‹¬ç«‹çš„è®¡ç®—ä»»åŠ¡ï¼Œå¹¶å°†å…¶æ˜ å°„åˆ°MapReduceæ¡†æ¶ä¸­ã€‚ä½œè€…ç”¨æå‡ºçš„æ–¹æ³•å¯¹åŒ…å«ç½‘é¡µä¸æ ‡ç­¾çš„äº‹ä»¶åºåˆ—æ•°æ®åº“è¿›è¡Œç½‘é¡µé—´ç›¸å…³åº¦çš„è®¡ç®—ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„ç›¸å…³ç½‘é¡µæ¨èã€‚</p><p>è€Œä¼ ç»Ÿçš„Aprioriå’ŒFP-growth ç®—æ³•éƒ½æ˜¯åŸºäºæœ€å°æ”¯æŒåº¦çš„é¢‘ç¹é¡¹æœç´¢ç®—æ³•ï¼Œå› æ­¤å­˜åœ¨ä»¥ä¸‹ä¸¤ä¸ªé—®é¢˜ï¼š<br>1ï¼‰å½“æœ€å°æ”¯æŒåº¦è®¾ç½®è¾ƒå¤§æ—¶ï¼ŒåŒ…å«ç¨€æœ‰é¡¹çš„é¢‘ç¹é¡¹ä¼šè¢«è¿‡æ»¤æ‰ï¼›<br>2ï¼‰å½“æœ€å°æ”¯æŒåº¦è®¾ç½®è¾ƒå°æ—¶ï¼Œåˆ™ä¼šç”Ÿæˆè¿‡å¤šçš„é¢‘ç¹é¡¹å¯¼è‡´è®¡ç®—é‡çˆ†ç‚¸ã€‚<br>å¯¹äº FP-Growth ç®—æ³•ï¼Œç”±äºæ¯ä¸ªé¡¹ç›®éƒ½æœ‰æœ€ä½æ”¯æŒåº¦ï¼Œå› æ­¤ç”¨æˆ·å¾ˆéš¾ä¸€æ¬¡ä¸ºæ‰€æœ‰çš„é¡¹ç›®è®¾ç½®é€‚å½“çš„é˜ˆå€¼ï¼Œæ‰€ä»¥ç”¨æˆ·é€šå¸¸éœ€è¦å¤šæ¬¡ä¼˜åŒ–ç®—æ³•çš„å‚æ•°ï¼Œç›´åˆ°è¾¾åˆ°æ»¡æ„çš„ç»“æœã€‚</p><p>åŸºäºä¸Šè¿°é—®é¢˜ï¼Œæ–‡çŒ®<a href="#refer19">[19]</a>æå‡ºä¸€ç§MIS-Treeç»“æ„å’Œåä¸ºCFP-Growthçš„ç®—æ³•ï¼Œä¸FP-growthä¸åŒï¼ŒCFP-Growthä¸­æ‰€æœ‰çš„è¾“å…¥é¡¹ç›®ä»¥æœ€å°æ”¯æŒåº¦è¿›è¡Œé™åºæ’åˆ—ï¼Œç„¶åå°†æ‰€æœ‰çš„é¡¹ç›®è¾“å…¥åˆ°æ ‘ç»“æ„ä¸­ï¼Œæ„å»ºä¸€ä¸ªç±»ä¼¼FP-treeçš„æ ‘çŠ¶ç»“æ„ç§°ä¸ºMIS-Treeï¼ŒåŒæ—¶æµ‹é‡æ ‘ä¸­å„é¡¹ç›®çš„æ”¯æŒåº¦ã€‚ç„¶åå¯¹æ ‘ä¸­æ‰€æœ‰æ”¯æŒåº¦å°äºå…¨éƒ¨é¡¹ç›®ä¸­æœ€å°æ”¯æŒåº¦çš„é¡¹è¿›è¡Œä¿®å‰ªï¼Œå¹¶å¯¹å«æœ‰ç›¸åŒçˆ¶èŠ‚ç‚¹çš„é¡¹è¿›è¡Œåˆå¹¶ï¼Œä»¥ä¿è¯æ ‘ç»“æ„çš„ç´§å‡‘æ€§ï¼Œå¾—åˆ°çš„å³ä¸ºæœ€å°æ”¯æŒåº¦é¡¹ç›®æ ‘(MIS-Tree)ã€‚<br>æœ€ååªéœ€è¦å°†MIS-Treeä¸­æ¯ä¸ªé¡¹ç›®ä½œä¸ºåç¼€é¡¹å¹¶è¿›è¡Œæ‰«æå°±å¯ä»¥å‘ç°å®Œæ•´çš„é¢‘ç¹æ¨¡å¼é›†ã€‚æ–‡ä¸­ä½œè€…åˆ†åˆ«åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œç»“æœè¡¨æ˜è¯¥ç®—æ³•æ›´åŠ æœ‰æ•ˆä¸å¿«é€Ÿã€‚</p><h3 id="Reference-1"><a href="#Reference-1" class="headerlink" title="Reference"></a>Reference</h3><blockquote><div id="refer15">[15] Gupta, C. (2012, September). Event correlation for operations management of largescale it systems. In Proceedings of the 9th international conference on Autonomic computing (pp. 91-96).</div><div id="refer16">[16] Lu, H., Feng, L., & Han, J. (2000). Beyond intratransaction association analysis: mining multidimensional intertransaction association rules. ACM Transactions on Information Systems (TOIS), 18(4), 423-454.</div><div id="refer17">[17] Lin, M. Y., Lee, P. Y., & Hsueh, S. C. (2012, February). Apriori-based frequent itemset mining algorithms on MapReduce. In Proceedings of the 6th international conference on ubiquitous information management and communication (pp. 1-8).</div><div id="refer18">[18] Li, H., Wang, Y., Zhang, D., Zhang, M., & Chang, E. Y. (2008, October). Pfp: parallel fp-growth for query recommendation. In Proceedings of the 2008 ACM conference on Recommender systems (pp. 107-114).</div><div id="refer19">[19] Hu, Y. H., & Chen, Y. L. (2006). Mining association rules with multiple minimum supports: a new mining algorithm and a support tuning mechanism. Decision support systems, 42(1), 1-24.</div></blockquote><h2 id="åŸºäºå›¾çš„æ–¹æ³•"><a href="#åŸºäºå›¾çš„æ–¹æ³•" class="headerlink" title="åŸºäºå›¾çš„æ–¹æ³•"></a>åŸºäºå›¾çš„æ–¹æ³•</h2><p><a href="#refer20">[20]</a>æè¿°äº†åœ¨ä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿä¸­æ„å»ºçš„å¼‚å¸¸æ£€æµ‹ç³»ç»ŸåŠå…¶åŸå› åˆ†æå¹³å°ï¼Œåœ¨å›¾ç»“æ„éƒ¨åˆ†ï¼Œæœ¬æ–‡æ„å»ºå›¾çš„æ–¹å¼æ˜¯å€¼å¾—å€Ÿé‰´çš„ã€‚<br>é¦–å…ˆï¼Œæœ¬æ–‡çš„ç³»ç»Ÿå›¾ç»“æ„æ˜¯å·²çŸ¥çš„<strong>ç³»ç»Ÿæ¶æ„å›¾</strong>ï¼Œé’ˆå¯¹æ¢æµ‹åˆ°çš„å¼‚å¸¸ï¼Œæœ¬æ–‡æå‡ºæ–¹æ³•ç”¨äºåœ¨å›¾ä¸Šåˆ†æå¼‚å¸¸æ‰€é€ æˆçš„å½±å“ï¼Œå¹¶å®šä½æ ¹å› ï¼Œå›¾çš„æ„é€ æ–¹å¼ï¼š</p><ol><li>å‘ç”Ÿå¼‚å¸¸æ—¶çš„å›¾anomaly graphä½¿ç”¨$G=(V, E)$æ¥è¡¨ç¤ºï¼Œå…¶ä¸­$C \cup A$ã€‚</li><li>$C$æ˜¯ç³»ç»Ÿçš„ç»„ä»¶æ„æˆçš„é›†åˆï¼ŒåŒ…å«ç³»ç»Ÿä¸­çš„é€»è¾‘ç»„ä»¶ã€ç‰©ç†ç»„ä»¶ç­‰ï¼Œ$A$æ˜¯åº•å±‚å¼‚å¸¸æ£€æµ‹ç»“æœæŠ¥å‘Šçš„å¼‚å¸¸ï¼ˆåŒ…æ‹¬ç”±è§„åˆ™å®šä¹‰çš„å’Œç”±å®æ—¶ç›‘æ§ç³»ç»Ÿå¾—åˆ°çš„ï¼‰ï¼Œ$E$æ˜¯anomaly graphä¸­çš„è¾¹é›†åˆï¼Œ</li><li>å›¾ä¸­å­˜åœ¨ä¸¤ç§è¾¹ï¼š1ï¼‰è¿æ¥ç»„ä»¶çš„è¾¹ï¼Œä»£è¡¨ç»„ä»¶ä¹‹é—´çš„ä»å±å…³ç³»ï¼›2ï¼‰è¿æ¥ç»„ä»¶ä¸å¼‚å¸¸çš„è¾¹ï¼Œä»£è¡¨æŸä¸ªç»„ä»¶äº§ç”Ÿäº†æŸä¸ªå¼‚å¸¸</li><li>æœ¬æ–‡è¿˜ä¸ºalarm edgeè®¾ç½®äº†åˆ†æ•°ï¼Œä»£è¡¨äº†æŸä¸ªç»„ä»¶å‘ç”ŸæŸä¸ªå¼‚å¸¸çš„ä¸¥é‡ç¨‹åº¦ï¼Œè¿™ä¸ªåˆ†æ•°ç”±ï¼š1ï¼‰æŸä¸ªæ—¶é—´æ®µå†…è¯¥å¼‚å¸¸æ¯æ¬¡å‘ç”Ÿçš„ä¸¥é‡ç¨‹åº¦ï¼›2ï¼‰è¯¥å¼‚å¸¸çš„å‘ç”Ÿé¢‘ç‡ï¼Œå…±åŒå†³å®šã€‚</li></ol><p><em>æœ¬æ–‡åœ¨å¼‚å¸¸å‘ç”Ÿçš„æ—¶å€™ï¼Œå»ºç«‹ä¸€ä¸ªå¼‚å¸¸æ—¶çš„çŠ¶æ€å›¾ï¼Œç„¶ååœ¨å›¾ä¸Šé’ˆå¯¹å¼‚å¸¸åŠå…¶è¿æ¥çš„è¾¹è®¡ç®—å¼‚å¸¸ä¸¥é‡ç¨‹åº¦åˆ†æ•°ï¼Œåˆ†æ•°é«˜çš„è¾¹æ‰€è¿æ¥çš„ç»„ä»¶å¯èƒ½å°±æ˜¯å¼‚å¸¸ã€‚</em></p><p><a href="#refer21">[21]</a>ä»äº‘æœåŠ¡è®¾æ–½æä¾›å•†çš„è§’åº¦ï¼Œå»ºç«‹äº†ä¸€ç§å¼‚å¸¸å®šä½çš„æ–¹æ³•ï¼Œå…¶ä¸­ä½¿ç”¨äº†æœ‰å‘æ— ç¯å›¾$G=(V,E)$æ¥å»ºæ¨¡å¼‚å¸¸çš„ä¼ æ’­ï¼Œå…¶ä¸­ï¼š</p><ol><li>èŠ‚ç‚¹è¡¨ç¤ºè™šæ‹ŸæœºVM</li><li>è¾¹è¡¨ç¤ºèŠ‚ç‚¹ä¹‹é—´çš„ä¸¤ç§å…³ç³»<ol><li>ç”±service callå¼•èµ·çš„ä¸šåŠ¡å…³è”</li><li>ç”±äºåœ¨åŒä¸€ä¸ªç‰©ç†ä¸»æœºä¸Šè€Œå¯èƒ½äº§ç”Ÿèµ„æºç«äº‰çš„</li></ol></li></ol><p>è¯¥æ–‡ç« å‡è®¾å¯¹äºæŸä¸ªå¼‚å¸¸$a$ï¼Œå®ƒå‘ç”Ÿçš„æ—¶å€™ï¼ˆä¸€èˆ¬æŒ‡è¿™ä¸ªå¼‚å¸¸æ˜¯æŸä¸ªæœåŠ¡çš„ç­‰å¾…æ—¶é—´è¿‡é•¿ï¼‰ï¼ŒæŸä¸ªVMä¸Šçš„ä¸€ç»„ç›¸å…³æŒ‡æ ‡ä¸€å®šä¹Ÿæ˜¯ç¹å¿™çš„ï¼ˆå¦‚CPUç­‰ï¼Œæ„æ€æ˜¯è¿™ä¸ªæŒ‡æ ‡çš„ç¹å¿™å¯¼è‡´äº†å¼‚å¸¸çš„å‘ç”Ÿï¼‰ã€‚</p><p>å› æ­¤ï¼Œä¸ºäº†æ‰¾åˆ°è¿™æ ·çš„ä¸€ç»„æŒ‡æ ‡ä½œä¸ºå¼‚å¸¸çš„åŸå› ï¼Œä½œè€…æå‡ºåœ¨$G$ä¸Šè¿›è¡Œéšæœºæ¸¸èµ°ï¼Œæ¸¸èµ°çš„è¿‡ç¨‹ä¸­æ¥è®¡ç®—æ¯ä¸ªvmçš„æŒ‡æ ‡ä¸å¼‚å¸¸æœåŠ¡çš„å“åº”æ—¶é—´ä¹‹é—´çš„å…³è”å…³ç³»ï¼ˆåˆ©ç”¨similarityï¼Œå…¶ä¸­similarityçš„è®¡ç®—ä¸ºï¼šæé«˜æŸä¸ªVMçš„ç‰©ç†æŒ‡æ ‡å ç”¨ï¼Œå¦‚cpuï¼Œç„¶åæµ‹é‡serviceçš„å“åº”æ—¶é—´ï¼Œè®¡ç®—äºŒè€…åœ¨è¿™æ®µæ—¶é—´å†…çš„ç›¸å…³ç³»æ•°ï¼‰ï¼Œæ¸¸èµ°è§„åˆ™å¦‚ä¸‹ï¼š</p><ol><li>walkeræ€»æ˜¯æ›´å€¾å‘äºå¾€å…·æœ‰æ›´é«˜similarityçš„èŠ‚ç‚¹å»æ¸¸èµ°</li><li>walkeræ¸¸èµ°åˆ°ä½similarityçš„èŠ‚ç‚¹çš„æ—¶å€™ï¼Œå¯ä»¥é€‰æ‹©è¿”å›</li><li>walkerçš„é¢†åŸŸå‡ä¸ºä½similarityçš„èŠ‚ç‚¹æ—¶ï¼Œå¯ä»¥é€‰æ‹©å¾…ç€ä¸åŠ¨</li></ol><p>ç»¼ä¸Šæ‰€è¿°ï¼Œåœ¨æ¸¸èµ°çš„è¿‡ç¨‹ä¸­ï¼Œè¢«è®¿é—®æœ€å¤šçš„èŠ‚ç‚¹å°†è¢«è§†ä¸ºæ˜¯é«˜å¯èƒ½æ€§çš„å¼‚å¸¸æ ¹å› ã€‚<em>æœ¬æ–‡åœ¨å¼‚å¸¸å‘ç”Ÿçš„æ—¶å€™ï¼Œå»ºç«‹ä¸€ä¸ªåŒ…å«äº†ç‰©ç†å…³ç³»ä¸æœåŠ¡è°ƒç”¨å…³ç³»çš„ç½‘ç»œæ‹“æ‰‘å›¾ï¼Œç„¶ååœ¨å›¾ä¸Šä»¥éšæœºæ¸¸èµ°çš„æ–¹å¼è®¡ç®—èŠ‚ç‚¹çš„å¼‚å¸¸ç¨‹åº¦åˆ†æ•°ï¼Œæ¸¸èµ°å®Œæ¯•åï¼Œå³å¯è¯†åˆ«å‡ºå¼‚å¸¸çš„æ ¹å› ã€‚</em></p><p><a href="#refer22">[22]</a>åˆ©ç”¨<strong>å¤šç»´æ—¶åºæŒ‡æ ‡</strong>æ¥åŠ¨æ€ç”ŸæˆæœåŠ¡å…³è”ï¼Œè¯Šæ–­æ ¹å› ã€‚é’ˆå¯¹å¤šç»´æ—¶é—´åºåˆ—ï¼Œè¯¥å·¥ä½œåˆ†ææ—¶åºä¹‹é—´çš„å¼‚å¸¸å…³è”ï¼Œæ¨æ–­<strong>å¼‚å¸¸è¡Œä¸ºå›¾</strong>æ¥æè¿°ä¸åŒæœåŠ¡ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æ ¹æ®è¡Œä¸ºå›¾ï¼Œè¯¥å·¥ä½œä½¿ç”¨å‰å‘ã€è‡ªå‘å’Œåå‘éšæœºæ¸¸èµ°ç®—æ³•è®¾è®¡å¯å‘å¼æ¨¡å‹ï¼Œç”¨ä»¥è¯†åˆ«æœåŠ¡æ•…éšœçš„æ ¹æœ¬åŸå› ã€‚è§£æå¯ä»¥çœ‹ï¼š<a href="https://blog.csdn.net/weixin_53741275/article/details/111973738">link</a></p><p><a href="#refer23">[23]</a>è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼ŒMicroCauseï¼Œå¯ä»¥å‡†ç¡®åœ°å®šä½å¯¼è‡´å¾®æœåŠ¡æ•…éšœçš„æ ¹æœ¬åŸå› çš„<strong>ç›‘æ§æŒ‡æ ‡ï¼ˆæ—¶é—´åºåˆ—ï¼‰</strong>ã€‚MicroCauseç»“åˆäº†ç®€å•è€Œæœ‰æ•ˆçš„è·¯å¾„æ¡ä»¶æ—¶é—´åºåˆ—ï¼ˆPCTSï¼‰ç®—æ³•ä»¥å‡†ç¡®åœ°æ•è·æ—¶é—´åºåˆ—ä¹‹é—´çš„<strong>å› æœå…³ç³»</strong>ï¼Œä»¥åŠé¢å‘æ—¶é—´å› æœçš„æ–°å‹éšæœºæ¸¸èµ°æ–¹æ³•ï¼ˆTCORWï¼‰</p><p><a href="#refer24">[24]</a>ä¸­æåˆ°ï¼Œ<strong>ä¸å˜ç½‘ç»œ</strong>å·²è¢«è¯æ˜æ˜¯è¡¨å¾å¤æ‚ç³»ç»Ÿè¡Œä¸ºçš„æœ‰æ•ˆæ–¹æ³•ã€‚åœ¨ä¸å˜ç½‘ç»œä¸­ï¼ŒèŠ‚ç‚¹è¡¨ç¤ºç³»ç»Ÿç»„ä»¶ï¼Œè¾¹ç¼˜è¡¨ç¤ºä¸¤ä¸ªç»„ä»¶ä¹‹é—´çš„ç¨³å®šï¼Œé‡è¦çš„äº¤äº’ä½œç”¨ã€‚ä¸å˜æ€§ç½‘ç»œçš„ç»“æ„å’Œæ¼”åŒ–ï¼Œå°¤å…¶æ˜¯<strong>æ¶ˆå¤±çš„ç›¸å…³æ€§</strong>ï¼Œå¯ä»¥ä¸ºå®šä½å› æœå¼‚å¸¸å’Œæ‰§è¡Œè¯Šæ–­æä¾›é‡è¦çš„å¯ç¤ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„åˆ©ç”¨ä¸å˜ç½‘ç»œæ£€æµ‹å› æœå¼‚å¸¸çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨æ¶ˆå¤±çš„ç›¸å…³æ€§ç™¾åˆ†æ¯”æ¥å¯¹å¯èƒ½çš„å¶ç„¶åˆ†é‡è¿›è¡Œæ’åï¼Œè¿™æœ‰å‡ ä¸ªå±€é™æ€§ï¼š1ï¼‰ç½‘ç»œä¸­çš„æ•…éšœä¼ æ’­è¢«å¿½ç•¥ï¼›2ï¼‰æ ¹æºå¶ç„¶å¼‚å¸¸å¯èƒ½å¹¶ä¸æ€»æ˜¯é‚£äº›æ¶ˆå¤±ç‡å¾ˆé«˜çš„èŠ‚ç‚¹ï¼›3ï¼‰æ¶ˆå¤±çš„ç›¸å…³æ€§çš„æ—¶é—´æ¨¡å¼æœªç”¨äºé²æ£’æ£€æµ‹ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ª<strong>åŸºäºç½‘ç»œæ‰©æ•£çš„æ¡†æ¶</strong>ï¼Œä»¥è¯†åˆ«é‡å¤§çš„å› æœå¼‚å¸¸å¹¶å¯¹å®ƒä»¬è¿›è¡Œæ’åã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å¯¹æ•´ä¸ªä¸å˜ç½‘ç»œä¸Šçš„æ•…éšœä¼ æ’­å»ºæ¨¡ï¼Œå¹¶ä¸”å¯ä»¥å¯¹ç»“æ„å’Œéšæ—¶é—´å˜åŒ–çš„ç ´ç¢ä¸å˜æ¨¡å¼è¿›è¡Œè”åˆæ¨æ–­ã€‚</p><h3 id="Reference-2"><a href="#Reference-2" class="headerlink" title="Reference"></a>Reference</h3><blockquote><div id="refer20">[20] Wang, H., Nguyen, P., Li, J., Kopru, S., Zhang, G., Katariya, S., & Ben-Romdhane, S. (2019). GRANO: Interactive graph-based root cause analysis for cloud-native distributed data platform. Proceedings of the VLDB Endowment, 12(12), 1942-1945.</div><div id="refer21">[21] Weng, J., Wang, J. H., Yang, J., & Yang, Y. (2018). Root cause analysis of anomalies of multitier services in public clouds. IEEE/ACM Transactions on Networking, 26(4), 1646-1659.</div><div id="refer22">[22] Ma, M., Xu, J., Wang, Y., Chen, P., Zhang, Z., & Wang, P. (2020, April). Automap: Diagnose your microservice-based web applications automatically. In Proceedings of The Web Conference 2020(pp. 246-258).</div><div id="refer23">[23] Meng, Y., Zhang, S., Sun, Y., Zhang, R., Hu, Z., Zhang, Y., ... & Pei, D. (2020, June). Localizing failure root causes in a microservice through causality inference. InÂ 2020 IEEE/ACM 28th International Symposium on Quality of Service (IWQoS)Â (pp. 1-10). IEEE.</div><div id="refer24">[24] Cheng, W., Zhang, K., Chen, H., Jiang, G., Chen, Z., & Wang, W. (2016, August). Ranking causal anomalies via temporal and dynamical analysis on vanishing correlations. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 805-814).</div></blockquote><h2 id="â€œäº‹ä»¶åºåˆ—-æ—¶é—´åºåˆ—â€å…³è”åˆ†æè®ºæ–‡åˆ—è¡¨"><a href="#â€œäº‹ä»¶åºåˆ—-æ—¶é—´åºåˆ—â€å…³è”åˆ†æè®ºæ–‡åˆ—è¡¨" class="headerlink" title="â€œäº‹ä»¶åºåˆ—-æ—¶é—´åºåˆ—â€å…³è”åˆ†æè®ºæ–‡åˆ—è¡¨"></a>â€œäº‹ä»¶åºåˆ—-æ—¶é—´åºåˆ—â€å…³è”åˆ†æè®ºæ–‡åˆ—è¡¨</h2><ul><li><p>Luo, C., Lou, J. G., Lin, Q., Fu, Q., Ding, R., Zhang, D., &amp; Wang, Z. (2014, August). Correlating events with time series for incident diagnosis. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1583-1592).</p></li><li><p>Chi, L., Sathe, S., Han, B., &amp; Wang, Y. (2016, December). A Novel Method for Assessing Event Impacts on Event-Driven Time Series. In 2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW) (pp. 519-526). IEEE.</p><p>  <strong>æ¢—æ¦‚</strong>ï¼šæœ¬æ–‡æå‡ºä½¿ç”¨è‡ªé€‚åº”çª—å£çš„$t_score$æ¥åŠ¨æ€çš„åº¦é‡äº‹ä»¶å¯¹æ—¶é—´åºåˆ—çš„å½±å“ï¼Œå¹¶æå‡ºäº†$A_score$ï¼Œç”¨äºå®šé‡è¯„ä¼°äº‹ä»¶å¯¹æ—¶é—´åºåˆ—çš„å½±å“ç¨‹åº¦ã€‚</p></li><li><p>Wu, C., Zhao, N., Wang, L., Yang, X., Li, S., Zhang, M., â€¦ &amp; Pei, D. Identifying Root-Cause Metrics for Incident Diagnosis in Online Service Systems. In The 32nd International Symposium on Software Reliability Engineering (ISSRE 2021). IEEE.</p><p>  <strong>æ¢—æ¦‚</strong>ï¼šæœ¬æ–‡è´¡çŒ®åœ¨äºä¸‰ä¸ªæ–¹é¢ï¼Œé¦–å…ˆæ ¹æ®å‡è®¾æ£€éªŒï¼Œè¯†åˆ«å‘ç”Ÿå¼‚å¸¸æ—¶æœ‰å“ªäº›æŒ‡æ ‡è¢«å½±å“ã€‚ç„¶åå¯¹å¼‚å¸¸è¿›è¡Œåˆ†ç±»ï¼Œå»é™¤é‚£äº›è¿ç»´äººå‘˜ä¸ä¼šè€ƒè™‘çš„å¼‚å¸¸æ¨¡å¼ã€‚æœ€åæ ¹æ®ä¸Šè¿°ä¸¤ä¸ªè¾“å‡ºï¼Œè¾“å‡ºä¸€ä¸ªç»è¿‡æ’åºçš„å¯ç–‘æŒ‡æ ‡åˆ—è¡¨ï¼Œä»¥è¾…åŠ©å·¥ç¨‹å¸ˆè¯†åˆ«å¼‚å¸¸æ ¹å› ã€‚</p></li><li><p>Xun, P., Zhu, P. D., Li, C. L., &amp; Zhu, H. Y. (2016, December). Discovering multi-type correlated events with time series for exception detection of complex systems. In 2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW) (pp. 21-28). IEEE.</p></li><li><p>Krosman, K., &amp; Sosnowski, J. (2021). Correlating Time Series Signals and Event Logs in Embedded Systems. Sensors, 21(21), 7128.</p></li><li><p>Minaei-Bidgoli, B., &amp; Lajevardi, S. B. (2008, September). Correlation mining between time series stream and event stream. In 2008 Fourth International Conference on Networked Computing and Advanced Information Management (Vol. 2, pp. 333-338). IEEE.</p></li></ul><h3 id="ä¸å…·å¤‡å¾ˆå¼ºå‚è€ƒæ„ä¹‰ä½†å¾ˆæœ‰è¶£çš„è®ºæ–‡"><a href="#ä¸å…·å¤‡å¾ˆå¼ºå‚è€ƒæ„ä¹‰ä½†å¾ˆæœ‰è¶£çš„è®ºæ–‡" class="headerlink" title="ä¸å…·å¤‡å¾ˆå¼ºå‚è€ƒæ„ä¹‰ä½†å¾ˆæœ‰è¶£çš„è®ºæ–‡"></a>ä¸å…·å¤‡å¾ˆå¼ºå‚è€ƒæ„ä¹‰ä½†å¾ˆæœ‰è¶£çš„è®ºæ–‡</h3><ul><li><p>Chi, L., Han, B., &amp; Wang, Y. (2016). Open Problem: Accurately Measuring Event Impacts on Time Series. In KDD MiLeTS Workshop.</p><p>  KDD workshopä¸­æå‡ºçš„open problemï¼Œæœ‰æŒ‡å¯¼æ€§æ„è§</p></li><li><p>Van Dortmont, M. A. M. M., van den Elzen, S., &amp; van Wijk, J. J. (2019, June). ChronoCorrelator: Enriching events with time series. In Computer Graphics Forum (Vol. 38, No. 3, pp. 387-399).</p><p>  ä¸€ç§å¯è§†åŒ–çš„æ–¹æ³•ï¼Œç”¨äºå°†äº‹ä»¶å…³è”åˆ°æ—¶é—´åºåˆ—çš„å˜åŒ–ä¸Šï¼Œä¹Ÿæ˜¯é‡‡ç”¨äº†ç±»ä¼¼two sample testçš„æ–¹æ³•</p></li><li><p>Xiao, S., Yan, J., Farajtabar, M., Song, L., Yang, X., &amp; Zha, H. (2019). Learning time series associated event sequences with recurrent point process networks. IEEE transactions on neural networks and learning systems, 30(10), 3124-3136.</p><p>  è¿™ç¯‡æ­ç¤ºäº†å¤šç§äº‹ä»¶ç±»å‹ä¹‹é—´çš„ç›¸å…³å…³ç³»ï¼ˆåŸºäºattentionï¼‰ï¼Œä½†æ²¡æœ‰è¯´æ˜äº‹ä»¶å¯¹æ—¶é—´åºåˆ—çš„å½±å“ç¨‹åº¦ï¼Œæœ¬æ–‡çš„ä¸»è¦ç›®æ ‡æ˜¯äº‹ä»¶é¢„æµ‹</p></li></ul><h2 id="â€œäº‹ä»¶åºåˆ—-äº‹ä»¶åºåˆ—â€å…³è”åˆ†æè®ºæ–‡åˆ—è¡¨ï¼ˆå¾…ç­›é€‰ï¼‰"><a href="#â€œäº‹ä»¶åºåˆ—-äº‹ä»¶åºåˆ—â€å…³è”åˆ†æè®ºæ–‡åˆ—è¡¨ï¼ˆå¾…ç­›é€‰ï¼‰" class="headerlink" title="â€œäº‹ä»¶åºåˆ—-äº‹ä»¶åºåˆ—â€å…³è”åˆ†æè®ºæ–‡åˆ—è¡¨ï¼ˆå¾…ç­›é€‰ï¼‰"></a>â€œäº‹ä»¶åºåˆ—-äº‹ä»¶åºåˆ—â€å…³è”åˆ†æè®ºæ–‡åˆ—è¡¨ï¼ˆå¾…ç­›é€‰ï¼‰</h2><ul><li><p>Noel, S., Robertson, E., &amp; Jajodia, S. (2004, December). Correlating intrusion events and building attack scenarios through attack graph distances. In 20th Annual Computer Security Applications Conference (pp. 350-359). IEEE.</p></li><li><p>Bayomie, D., Awad, A., &amp; Ezat, E. (2016, June). Correlating unlabeled events from cyclic business processes execution. In International Conference on Advanced Information Systems Engineering (pp. 274-289). Springer, Cham.</p></li><li><p>Dindar, N., Fischer, P. M., Soner, M., &amp; Tatbul, N. (2011, July). Efficiently correlating complex events over live and archived data streams. In Proceedings of the 5th ACM international conference on Distributed event-based system (pp. 243-254).</p></li><li><p>Steiger, E., Resch, B., de Albuquerque, J. P., &amp; Zipf, A. (2016). Mining and correlating traffic events from human sensor observations with official transport data using self-organizing-maps. Transportation Research Part C: Emerging Technologies, 73, 91-104.</p></li><li><p>Vlachos, M., Wu, K. L., Chen, S. K., &amp; Philip, S. Y. (2008). Correlating burst events on streaming stock market data. Data Mining and Knowledge Discovery, 16(1), 109-133.</p></li><li><p>Koch, G. G., Koldehofe, B., &amp; Rothermel, K. (2010, July). Cordies: Expressive event correlation in distributed systems. In Proceedings of the Fourth ACM International Conference on Distributed Event-Based Systems (pp. 26-37).</p></li><li><p>Cheng, L., Van Dongen, B. F., &amp; Van Der Aalst, W. M. (2017, May). Efficient event correlation over distributed systems. In 2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID) (pp. 1-10). IEEE.</p></li><li><p>Gruschke, B. (1998, October). Integrated event management: Event correlation using dependency graphs. In Proceedings of the 9th IFIP/IEEE International Workshop on Distributed Systems: Operations &amp; Management (DSOM 98) (pp. 130-141).</p></li><li><p>Jiang, G., &amp; Cybenko, G. (2004, June). Temporal and spatial distributed event correlation for network security. In Proceedings of the 2004 American Control Conference (Vol. 2, pp. 996-1001). IEEE.</p></li><li><p>Liu, G., Mok, A. K., &amp; Yang, E. J. (1999, May). Composite events for network event correlation. In Integrated Network Management VI. Distributed Management for the Networked Millennium. Proceedings of the Sixth IFIP/IEEE International Symposium on Integrated Network Management.(Cat. No. 99EX302) (pp. 247-260). IEEE.</p></li><li><p>Rozsnyai, S., Slominski, A., &amp; Lakshmanan, G. T. (2011, July). Discovering event correlation rules for semi-structured business processes. In Proceedings of the 5th ACM international conference on Distributed event-based system (pp. 75-86).</p></li><li><p>Wu, G., Zhang, H., Qiu, M., Ming, Z., Li, J., &amp; Qin, X. (2013). A decentralized approach for mining event correlations in distributed system monitoring. Journal of parallel and Distributed Computing, 73(3), 330-340.</p></li><li><p>Kotenko, I., Fedorchenko, A., Saenko, I., &amp; Kushnerevich, A. (2018, March). Parallelization of security event correlation based on accounting of event type links. In 2018 26th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP) (pp. 462-469). IEEE.</p></li><li><p>Xuewei, F., Dongxia, W., Jiemei, Z., Guoqing, M., &amp; Jin, L. (2010, June). Analyzing and correlating security events using state machine. In 2010 10th IEEE International Conference on Computer and Information Technology (pp. 2849-2854). IEEE.</p></li><li><p>Hasan, M., Sugla, B., &amp; Viswanathan, R. (1999, May). A conceptual framework for network management event correlation and filtering systems. In Integrated Network Management VI. Distributed Management for the Networked Millennium. Proceedings of the Sixth IFIP/IEEE International Symposium on Integrated Network Management.(Cat. No. 99EX302) (pp. 233-246). IEEE.</p></li><li><p>Skopik, F., &amp; Fiedler, R. (2013). Intrusion detection in distributed systems using fingerprinting and massive event correlation. INFORMATIK 2013â€“Informatik angepasst an Mensch, Organisation und Umwelt.</p></li><li><p>Flammini, F., Mazzocca, N., Pappalardo, A., Pragliola, C., &amp; Vittorini, V. (2011, August). Augmenting surveillance system capabilities by exploiting event correlation and distributed attack detection. In International Conference on Availability, Reliability, and Security (pp. 191-204). Springer, Berlin, Heidelberg.</p></li><li><p>Martin-Flatin, J. P. (2004, June). Distributed event correlation and self-managed systems. In Proceedings of the 1st International Workshop on Self-* Properties in Complex Information Systems (Self-Star 2004) (pp. 61-64).</p></li><li><p>Griffith, R., Hellerstein, J., Kaiser, G., &amp; Diao, Y. (2006, June). Dynamic adaptation of temporal event correlation for qos management in distributed systems. In 200614th IEEE International Workshop on Quality of Service (pp. 290-294). IEEE.</p></li><li><p>Steinert, R., Gestrelius, S., &amp; Gillblad, D. (2011, December). A distributed spatio-temporal event correlation protocol for multi-layer virtual networks. In 2011 IEEE Global Telecommunications Conference-GLOBECOM 2011 (pp. 1-5). IEEE.</p></li><li><p>Griffith, R., Hellerstein, J. L., Diao, Y., &amp; Kaiser, G. E. (2005). Dynamic Adaptation of Rules for Temporal Event Correlation in Distributed Systems.</p></li><li><p>Fu, X., Ren, R., Zhan, J., Zhou, W., Jia, Z., &amp; Lu, G. (2012, October). Logmaster: Mining event correlations in logs of large-scale cluster systems. In 2012 IEEE 31st Symposium on Reliable Distributed Systems (pp. 71-80). IEEE.</p></li><li><p>Myers, J., Grimaila, M., &amp; Mills, R. (2010, April). Insider threat detection using distributed event correlation of web server logs. In International Conference on Cyber Warfare and Security (p. 251). Academic Conferences International Limited.</p></li><li><p>Myers, J., Grimaila, M., &amp; Mills, R. (2010, April). Insider threat detection using distributed event correlation of web server logs. In International Conference on Cyber Warfare and Security (p. 251). Academic Conferences International Limited.</p></li><li><p>Myers, J., Grimaila, M. R., &amp; Mills, R. F. (2011, January). Log-based distributed security event detection using simple event correlator. In 2011 44th Hawaii International Conference on System Sciences (pp. 1-7). IEEE.</p></li><li><p>Parekh, J. J. (2005). Privacy-Preserving Distributed Event Correlation.</p></li><li><p>Kato, N., Ohta, K., Ika, T., Mansfield, G., &amp; Nemoto, Y. (1999). A proposal of event correlation for distributed network fault management and its evaluation. IEICE Transactions on Communications, 82(6), 859-867.</p></li><li><p>Cerullo, G., Coppolino, L., Dâ€™Antonio, S., Formicola, V., Papale, G., &amp; Ragucci, B. (2016). Enabling convergence of physical and logical security through intelligent event correlation. In Intelligent Distributed Computing IX (pp. 427-437). Springer, Cham.</p></li><li><p>Alves, P. G. (2014). A Distributed Security Event Correlation Platform for SCADA (Doctoral dissertation, Universidade de Coimbra).</p></li><li><p>Zhang, B., &amp; Al-Shaer, E. (2007, October). Self-organizing monitoring agents for hierarchical event correlation. In International Workshop on Distributed Systems: Operations and Management (pp. 13-24). Springer, Berlin, Heidelberg.</p></li><li><p>Katker, S. (1996, March). A modeling framework for integrated distributed systems fault management. In Proceedings of IFIP/IEEE International Conference on Distributed Platforms (pp. 186-198). IEEE.</p></li><li><p>Teufl, P., Payer, U., &amp; Fellner, R. (2010, February). Event correlation on the basis of activation patterns. In 2010 18th Euromicro Conference on Parallel, Distributed and Network-based Processing (pp. 631-640). IEEE.</p></li><li><p>Yoneki, E. (2010). Time/space aware event correlation. In Principles and Applications of Distributed Event-Based Systems (pp. 43-74). IGI Global.</p></li><li><p>Guo, N., Gao, T., Zhang, B., &amp; Zhao, H. (2007, October). Distributed and scalable event correlation based on causality graph. In Asia-Pacific Network Operations and Management Symposium (pp. 567-570). Springer, Berlin, Heidelberg.</p></li><li><p>Tai, W., OSullivan, D., &amp; Keeney, J. (2008, April). Distributed fault correlation scheme using a semantic publish/subscribe system. In NOMS 2008-2008 IEEE Network Operations and Management Symposium (pp. 835-838). IEEE.</p></li><li><p>Marwede, N., Rohr, M., van Hoorn, A., &amp; Hasselbring, W. (2009, March). Automatic failure diagnosis support in distributed large-scale software systems based on timing behavior anomaly correlation. In 2009 13th European Conference on Software Maintenance and Reengineering (pp. 47-58). IEEE.</p></li><li><p>Fu, S., &amp; Xu, C. Z. (2007, October). Quantifying temporal and spatial correlation of failure events for proactive management. In 2007 26th IEEE International Symposium on Reliable Distributed Systems (SRDS 2007) (pp. 175-184). IEEE.</p></li></ul><hr><h2 id="å…¶ä»–å·¥ä½œ"><a href="#å…¶ä»–å·¥ä½œ" class="headerlink" title="å…¶ä»–å·¥ä½œ"></a>å…¶ä»–å·¥ä½œ</h2><p>è¿™éƒ¨åˆ†è®°å½•åœ¨è°ƒç ”è¿‡ç¨‹ä¸­ï¼Œå®ç°å…³è”åˆ†æçš„å…¶ä»–å·¥ä½œï¼Œå°¤å…¶åœ¨è¿ç»´é¢†åŸŸ</p><ul><li><p>Liu, P., Chen, Y., Nie, X., Zhu, J., Zhang, S., Sui, K., â€¦ &amp; Pei, D. (2019, October). FluxRank: A Widely-Deployable Framework to Automatically Localizing Root Cause Machines for Software Service Failure Mitigation. In 2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE) (pp. 35-46). IEEE.</p><p>  ç”¨äºå·²ç»ç¡®å®šäº†å¼‚å¸¸çš„å‘ç”Ÿåï¼Œå¿«é€Ÿçš„å®šä½å¼‚å¸¸çš„æ ¹æœ¬åŸå› ï¼Œå³å®šä½åˆ°å¯¼è‡´å¼‚å¸¸çš„æœåŠ¡å™¨ï¼ˆè€Œéå¯¼è‡´å¼‚å¸¸çš„ä»£ç æ®µæˆ–è€…æ›´å…·ä½“çš„åŸå› ï¼‰</p><p>  FluxRankçš„è®¾è®¡æ€è·¯åœ¨äºï¼šåœ¨ç³»ç»Ÿå‘ç”Ÿæ•…éšœçš„æ—¶å€™ï¼Œé€šå¸¸éœ€è¦å…ˆç¡®è®¤æ•…éšœï¼Œç„¶ååœ¨å°½å¯èƒ½çŸ­çš„æ—¶é—´å†…å°†ä¸šåŠ¡è½¬ç§»åˆ°å…¶ä»–ä¸å—å¼‚å¸¸å½±å“çš„æœåŠ¡å™¨ä¸Šä»¥ä¾¿å°½å¿«æ¢å¤ä»»åŠ¡ã€‚æœ€åå°†ä½¿ç”¨å¾ˆé•¿çš„æ—¶é—´æ¥åˆ†æå¯¼è‡´å¼‚å¸¸çš„åŸå› ï¼šå¦‚ä»£ç é”™è¯¯ç­‰ã€‚æœ¬æ–‡æ—¨åœ¨å®šä½å¼‚å¸¸åˆ°æœåŠ¡å™¨çº§åˆ«ï¼Œä¸“æ³¨äºå¼‚å¸¸çš„å¿«é€Ÿç¼“è§£ï¼Œè€Œéæ‰¾åˆ°æ ¹æœ¬åŸå› ã€‚</p></li><li><p>Jayathilaka, H., Krintz, C., &amp; Wolski, R. (2017, April). Performance monitoring and root cause analysis for cloud-hosted web applications. In Proceedings of the 26th International Conference on World Wide Web (pp. 469-478).</p><p>  è¿™ä¸ªå·¥ä½œä¸å…³è”åˆ†æå®Œå…¨æ— å…³ï¼Œä¸»è¦æ˜¯å¼€å‘äº†ä¸€ç§åœ¨PaaSå¹³å°ä¸Šçš„ç›‘æ§ç³»ç»Ÿï¼Œç”¨äºå¿«é€Ÿå¼‚å¸¸å®šä½äºæ ¹å› åˆ†æ</p></li><li><p>Arzani, B., Ciraci, S., Loo, B. T., Schuster, A., &amp; Outhred, G. (2016, August). Taking the blame game out of data centers operations with netpoirot. In Proceedings of the 2016 ACM SIGCOMM Conference (pp. 440-453).</p><p>  è¿™ä¸ªå·¥ä½œæ¶‰åŠåˆ°äº†å…³è”åˆ†æï¼Œä½†ä¸åºåˆ—æ•°æ®å®Œå…¨æ— å…³</p></li><li><p>Gao, J., Yaseen, N., MacDavid, R., Frujeri, F. V., Liu, V., Bianchini, R., â€¦ &amp; Arzani, B. (2020, July). Scouts: Improving the Diagnosis Process Through Domain-customized Incident Routing. In Proceedings of the Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication (pp. 253-269).</p><p>  å®Œå…¨æ— å…³ï¼Œä¸»è¦åšå¼‚å¸¸å®šä½çš„ï¼Œä¹Ÿä¸æ˜¯åºåˆ—æ•°æ®</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
            <tag> survey </tag>
            
            <tag> causal discovery </tag>
            
            <tag> correlation analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in NeuralIPS 2021 (2021.12.06 - 2021.12.14)</title>
      <link href="/uncategorized/paperlistfile/NeuralIPS2021/"/>
      <url>/uncategorized/paperlistfile/NeuralIPS2021/</url>
      
        <content type="html"><![CDATA[<!--tags:æŒ‰ä»»åŠ¡åˆ†ç±»### Anomaly detection / Outlier / Out-of-distribution### Interpretable / Explainable### Causal discovery### Data augmentation æŒ‰æ•°æ®åˆ†ç±»### Time series### Missing value / Irregular sampled / Imputation### Sequence### HeterogeneousæŒ‰æ·±åº¦å­¦ä¹ æ¶æ„åˆ†ç±»### Recurrent neural network / RNN / LSTM / GRU ### Autoencoder--><p><a href="https://neurips.cc/Conferences/2021/Schedule?type=Poster">Accept paper lists</a></p><p><a href="https://nips.cc/Conferences/2021/DatasetsBenchmarks/AcceptedPapers">Benchmarks papers</a></p><span id="more"></span><h2 id="Main-Track"><a href="#Main-Track" class="headerlink" title="Main Track"></a>Main Track</h2><h3 id="Anomaly-detection-Outlier-Out-of-distribution"><a href="#Anomaly-detection-Outlier-Out-of-distribution" class="headerlink" title="Anomaly detection / Outlier / Out-of-distribution"></a>Anomaly detection / Outlier / Out-of-distribution</h3><ul><li><p>Online false discovery rate control for anomaly detection in time series </p><p>  Quentin Rebjock, Baris Kurt, Tim Januschowski, Laurent Callot</p></li><li><p>Detecting Anomalous Event Sequences with Temporal Point Processes </p><p>  Oleksandr Shchur, Ali Caner Turkmen, Tim Januschowski, Jan Gasthaus, Stephan GÃ¼nnemann</p></li><li><p>Learned Robust PCA: A Scalable Deep Unfolding Approach for High-Dimensional Outlier Detection </p><p>  HanQin Cai, Jialin Liu, Wotao Yin</p></li><li><p>Task-Agnostic Undesirable Feature Deactivation Using Out-of-Distribution Data</p><p>  Dongmin Park Â· Hwanjun Song Â· Minseok Kim Â· Jae-Gil Lee</p></li><li><p>ReAct: Out-of-distribution Detection With Rectified Activations </p><p>  Yiyou Sun, Chuan Guo, Yixuan Li</p></li><li><p>STEP: Out-of-Distribution Detection in the Presence of Limited In-Distribution Labeled Data </p><p>  Zhi Zhou, Lan-Zhe Guo, Zhanzhan Cheng, Yu-Feng Li, Shiliang Pu</p></li><li><p>Locally Most Powerful Bayesian Test for Out-of-Distribution Detection using Deep Generative Models </p><p>  Keunseo Kim, JunCheol Shin, Heeyoung Kim</p></li></ul><ul><li><p>Single Layer Predictive Normalized Maximum Likelihood for Out-of-Distribution Detection </p><p>  Koby Bibas, Meir Feder, Tal Hassner</p></li></ul><h3 id="Interpretable-Explainable"><a href="#Interpretable-Explainable" class="headerlink" title="Interpretable / Explainable"></a>Interpretable / Explainable</h3><ul><li><p>Self-Interpretable Model with Transformation Equivariant Interpretation </p><p>  Yipei Wang, Xiaoqian Wang</p></li><li><p>Physics-Integrated Variational Autoencoders for Robust and Interpretable Generative Modeling </p><p>  Naoya Takeishi, Alexandros Kalousis</p></li><li><p>Scalable Rule-Based Representation Learning for Interpretable Classification </p><p>  Zhuo Wang, Wei Zhang, Ning Liu, Jianyong Wang</p></li><li><p>Dynamic Inference with Neural Interpreters </p><p>  Nasim Rahaman, Muhammad Waleed Gondal, Shruti Joshi, Peter Vincent Gehler, Yoshua Bengio, Francesco Locatello, Bernhard SchÃ¶lkopf</p></li><li><p>Understanding Instance-based Interpretability of Variational Auto-Encoders </p><p>  Zhifeng Kong, Kamalika Chaudhuri</p></li><li><p>Reliable Post hoc Explanations: Modeling Uncertainty in Explainability </p><p>  Dylan Z Slack, Sophie Hilgard, Sameer Singh, Himabindu Lakkaraju</p></li><li><p>Explaining Latent Representations with a Corpus of Examples </p><p>  Jonathan CrabbÃ©, Zhaozhi Qian, Fergus Imrie, Mihaela van der Schaar</p></li></ul><h3 id="Causal-discovery"><a href="#Causal-discovery" class="headerlink" title="Causal discovery"></a>Causal discovery</h3><p>æœ‰46ç¯‡æ–‡ç« ï¼Œæš‚æ—¶ä¸æ•´ç†</p><h3 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h3><ul><li><p>Learning Debiased Representation via Disentangled Feature Augmentation </p><p>  Jungsoo Lee, Eungyeup Kim, Juyoung Lee, Jihyeon Lee, Jaegul Choo</p></li><li><p>Data Augmentation Can Improve Robustness </p><p>  Sylvestre-Alvise Rebuffi, Sven Gowal, Dan Andrei Calian, Florian Stimberg, Olivia Wiles, Timothy Mann</p></li><li><p>Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics </p><p>  Bhavin Choksi, Milad Mozafari, Callum Biggs Oâ€™May, B. ADOR, Andrea Alamia, Rufin VanRullen</p></li><li><p>How Data Augmentation affects Optimization for Linear Regression </p><p>  Boris Hanin, Yi Sun</p></li><li><p>AugMax: Adversarial Composition of Random Augmentations for Robust Training </p><p>  Haotao Wang, Chaowei Xiao, Jean Kossaifi, Zhiding Yu, Anima Anandkumar, Zhangyang Wang</p></li><li><p>Deceive D: Adaptive Pseudo Augmentation for GAN Training with Limited Data </p><p>  Liming Jiang, Bo Dai, Wayne Wu, Chen Change Loy</p></li><li><p>Self-Supervised GANs with Label Augmentation </p><p>  Liang Hou, Huawei Shen, Qi Cao, Xueqi Cheng</p></li><li><p>Explanation-based Data Augmentation for Image Classification </p><p>  Sandareka Wickramanayake, Wynne Hsu, Mong-Li Lee</p></li></ul><hr><h3 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h3><ul><li><p>SurvITE: Learning Heterogeneous Treatment Effects from Time-to-Event Data </p><p>  Alicia Curth, Changhee Lee, Mihaela van der Schaar</p></li><li><p>CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation </p><p>  YUSUKE TASHIRO, Jiaming Song, Yang Song, Stefano Ermon</p></li><li><p>Coresets for Time Series Clustering </p><p>  Lingxiao Huang, K. Sudhir, Nisheeth K Vishnoi</p></li><li><p>MixSeq: Connecting Macroscopic Time Series Forecasting with Microscopic Time Series Data </p><p>  Zhibo Zhu, Ziqi Liu, Ge Jin, Zhiqiang Zhang, Lei Chen, JUN ZHOU, Jianyong Zhou</p></li><li><p>Deep Explicit Duration Switching Models for Time Series </p><p>  Abdul Fatir Ansari, Konstantinos Benidis, Richard Kurle, Ali Caner Turkmen, Harold Soh, Alex Smola, Bernie Wang, Tim Januschowski</p></li><li><p>Online false discovery rate control for anomaly detection in time series </p><p>  Quentin Rebjock, Baris Kurt, Tim Januschowski, Laurent Callot</p></li><li><p>Topological Attention for Time Series Forecasting </p><p>  Sebastian Zeng, Florian Graf, Christoph Hofer, Roland Kwitt</p></li><li><p>Adjusting for Autocorrelated Errors in Neural Networks for Time Series </p><p>  Fan-Keng Sun, Chris Lang, Duane S Boning</p></li><li><p>Probabilistic Transformer For Time Series Analysis </p><p>  Binh Tang, David S. Matteson</p></li><li><p>Dynamical Wasserstein Barycenters for Time-series Modeling </p><p>  Kevin C Cheng, Shuchin Aeron, Michael C Hughes, Eric Miller</p></li><li><p>Conformal Time-series Forecasting </p><p>  KamilÄ— StankeviÄiÅ«tÄ—, Ahmed Alaa, Mihaela van der Schaar</p></li><li><p>Time-series Generation by Contrastive Imitation </p><p>  Daniel Jarrett, Ioana Bica, Mihaela van der Schaar</p></li></ul><hr><h3 id="Missing-value-Irregular-sampled-Imputation"><a href="#Missing-value-Irregular-sampled-Imputation" class="headerlink" title="Missing value / Irregular sampled / Imputation"></a>Missing value / Irregular sampled / Imputation</h3><ul><li><p>Identifiable Generative models for Missing Not at Random Data Imputation </p><p>  Chao Ma, Cheng Zhang</p></li><li><p>Whatâ€™s a good imputation to predict with missing values? </p><p>  Marine Le Morvan, Julie Josse, Erwan Scornet, Gael Varoquaux</p></li><li><p>MIRACLE: Causally-Aware Imputation via Learning Missing Data Mechanisms </p><p>  Trent Kyono, Yao Zhang, Alexis Bellot, Mihaela van der Schaar</p></li><li><p>Assessing Fairness in the Presence of Missing Data </p><p>  Yiliang Zhang, Qi Long</p></li><li><p>Coresets for Clustering with Missing Values </p><p>  Vladimir Braverman, Shaofeng H.-C. Jiang, Robert Krauthgamer, Xuan Wu</p></li><li><p>Time-series Generation by Contrastive Imitation </p><p>  Daniel Jarrett, Ioana Bica, Mihaela van der Schaar</p></li><li><p>CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation </p><p>  YUSUKE TASHIRO, Jiaming Song, Yang Song, Stefano Ermon</p></li></ul><hr><h3 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h3><ul><li><p>Duplex Sequence-to-Sequence Learning for Reversible Machine Translation </p><p>  Zaixiang Zheng, Hao Zhou, Shujian Huang, Jiajun Chen, Jingjing Xu, Lei Li</p></li><li><p>Sequence-to-Sequence Learning with Latent Neural Grammars </p><p>  Yoon Kim</p></li><li><p>A Constant Approximation Algorithm for Sequential Random-Order No-Substitution k-Median Clustering </p><p>  Tom Hess, Michal Moshkovitz, Sivan Sabato</p></li><li><p>Pay Better Attention to Attention: Head Selection in Multilingual and Multi-Domain Sequence Modeling </p><p>  Hongyu Gong, Yun Tang, Juan Pino, Xian Li</p></li><li><p>Contrastively Disentangled Sequential Variational Autoencoder </p><p>  Junwen Bai, Weiran Wang, Carla P Gomes</p></li><li><p>Detecting Anomalous Event Sequences with Temporal Point Processes </p><p>  Oleksandr Shchur, Ali Caner Turkmen, Tim Januschowski, Jan Gasthaus, Stephan GÃ¼nnemann</p></li></ul><hr><h3 id="Heterogeneous"><a href="#Heterogeneous" class="headerlink" title="Heterogeneous"></a>Heterogeneous</h3><ul><li><p>RelaySum for Decentralized Deep Learning on Heterogeneous Data </p><p>  Thijs Vogels, Lie He, Anastasia Koloskova, Sai Praneeth Karimireddy, Tao Lin, Sebastian U Stich, Martin Jaggi</p></li><li><p>Distilling Meta Knowledge on Heterogeneous Graph for Illicit Drug Trafficker Detection on Social Media </p><p>  Yiyue Qian, Yiming Zhang, Yanfang Ye, Chuxu Zhang</p></li><li><p>Distributed Machine Learning with Sparse Heterogeneous Data </p><p>  Dominic Richards, Sahand Negahban, Patrick Rebeschini</p></li><li><p>FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout </p><p>  Samuel HorvÃ¡th, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos Venieris, Nicholas Donald Lane</p></li></ul><hr><h3 id="Recurrent-neural-network-RNN-LSTM-GRU"><a href="#Recurrent-neural-network-RNN-LSTM-GRU" class="headerlink" title="Recurrent neural network / RNN / LSTM / GRU"></a>Recurrent neural network / RNN / LSTM / GRU</h3><ul><li><p>Charting and Navigating the Space of Solutions for Recurrent Neural Networks </p><p>  Elia Turner, Kabir Vinay Dabholkar, Omri Barak</p></li><li><p>Self-Instantiated Recurrent Units with Dynamic Soft Recursion </p><p>  Aston Zhang, Yi Tay, Yikang Shen, Alvin Chan, Shuai Zhang</p></li><li><p>SBO-RNN: Reformulating Recurrent Neural Networks via Stochastic Bilevel Optimization </p><p>  Ziming Zhang, Yun Yue, Guojun Wu, Yanhua Li, Haichong Zhang</p></li><li><p><strong>(éœ€è¦çœ‹çœ‹)</strong> On the Provable Generalization of Recurrent Neural Networks </p><p>  Lifu Wang, Bo Shen, Bo Hu, Xing Cao</p></li><li><p>Recurrence along Depth: Deep Convolutional Neural Networks with Recurrent Layer Aggregation </p><p>  Jingyu Zhao, Yanwen Fang, Guodong Li</p></li><li><p>Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems </p><p>  Jimmy T.H. Smith, Scott Linderman, David Sussillo</p></li><li><p>Noisy Recurrent Neural Networks </p><p>  Soon Hoe Lim, N. Benjamin Erichson, Liam Hodgkinson, Michael W. Mahoney</p></li><li><p>Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers </p><p>  Albert Gu, Isys Johnson, Karan Goel, Khaled Kamal Saab, Tri Dao, Atri Rudra, Christopher Re</p></li><li><p>Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks </p><p>  Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, Tom Goldstein</p></li><li><p><strong>(éœ€è¦çœ‹çœ‹)</strong> Learning and Generalization in RNNs </p><p>  Abhishek Panigrahi, Navin Goyal</p></li><li><p>Framing RNN as a kernel method: A neural ODE approach </p><p>  Adeline Fermanian, Pierre Marion, Jean-Philippe Vert, GÃ©rard Biau</p></li><li><p>Structured in Space, Randomized in Time: Leveraging Dropout in RNNs for Efficient Training </p><p>  Anup Sarma, Sonali Singh, Huaipan Jiang, Rui Zhang, Mahmut Kandemir, Chita Das</p></li></ul><hr><h3 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h3><ul><li><p>Statistical Regeneration Guarantees of the Wasserstein Autoencoder with Latent Space Consistency </p><p>  Anish Chakrabarty, Swagatam Das</p></li><li><p>Physics-Integrated Variational Autoencoders for Robust and Interpretable Generative Modeling </p><p>  Naoya Takeishi, Alexandros Kalousis</p></li><li><p>On the Value of Infinite Gradients in Variational Autoencoder Models </p><p>  Bin Dai, Li Kevin Wenliang, David Wipf</p></li><li><p><strong>(éœ€è¦çœ‹çœ‹)</strong> Shape your Space: A Gaussian Mixture Regularization Approach to Deterministic Autoencoders </p><p>  Amrutha Saseendran, Kathrin Skubch, Stefan Falkner, Margret Keuper</p></li><li><p>Neighborhood Reconstructing Autoencoders </p><p>  Yonghyeon LEE, Hyeokjun Kwon, Frank C. Park</p></li><li><p>Model Selection for Bayesian Autoencoders </p><p>  Ba-Hien Tran, Simone Rossi, Dimitrios Milios, Pietro Michiardi, Edwin V Bonilla, Maurizio Filippone</p></li><li><p>Clockwork Variational Autoencoders </p><p>  Vaibhav Saxena, Jimmy Ba, Danijar Hafner</p></li><li><p>A Contrastive Learning Approach for Training Variational Autoencoder Priors </p><p>  Jyoti Aneja, Alex Schwing, Jan Kautz, Arash Vahdat</p></li><li><p>Contrastively Disentangled Sequential Variational Autoencoder </p><p>  Junwen Bai, Weiran Wang, Carla P Gomes</p></li><li><p>Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning </p><p>  Robin Winter, Frank Noe, Djork-Arne Clevert</p></li></ul><hr><h3 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h3><ul><li><p>Robust and Fully-Dynamic Coreset for Continuous-and-Bounded Learning (With Outliers) Problems </p><p>  Zixiu Wang, Yiwen Guo, Hu Ding</p></li><li><p>Automatic Unsupervised Outlier Model Selection </p><p>  Yue Zhao, Ryan Rossi, Leman Akoglu</p></li><li><p>Improving Self-supervised Learning with Automated Unsupervised Outlier Arbitration </p><p>  Yu Wang, Jingyang Lin, Jingjing Zou, Yingwei Pan, Ting Yao, Tao Mei</p></li><li><p>Drop-DTW: Aligning Common Signal Between Sequences While Dropping Outliers </p><p>  Nikita Dvornik, Isma Hadji, Konstantinos G. Derpanis, Animesh Garg, Allan Douglas Jepson</p></li><li><p>Consistent Estimation for PCA and Sparse Regression with Oblivious Outliers </p><p>  Tommaso dâ€™Orsi, Chih-Hung Liu, Rajai Nasser, Gleb Novikov, David Steurer, Stefan Tiegel</p></li><li><p>Approximate optimization of convex functions with outlier noise </p><p>  Anindya De, Sanjeev Khanna, Huan Li, Hesam Nikpey</p></li></ul><hr><h2 id="NeurIPS-2021-Datasets-and-Benchmarks-Accepted-Papers"><a href="#NeurIPS-2021-Datasets-and-Benchmarks-Accepted-Papers" class="headerlink" title="NeurIPS 2021 Datasets and Benchmarks Accepted Papers"></a>NeurIPS 2021 Datasets and Benchmarks Accepted Papers</h2><h3 id="Anomaly-detection-Outlier-Out-of-distribution-1"><a href="#Anomaly-detection-Outlier-Out-of-distribution-1" class="headerlink" title="Anomaly detection / Outlier / Out-of-distribution"></a>Anomaly detection / Outlier / Out-of-distribution</h3><ul><li><p>SegmentMeIfYouCan: A Benchmark for Anomaly Segmentation</p><p>  Robin Chan Â· Krzysztof Lis Â· Svenja Uhlemeyer Â· Hermann Blum Â· Sina Honari Â· Roland Siegwart Â· Pascal Fua Â· Mathieu Salzmann Â· Matthias Rottmann</p></li><li><p><strong>(è¦çœ‹ä¸€ä¸‹)</strong> Revisiting Time Series Outlier Detection: Definitions and Benchmarks</p><p>  Henry Lai Â· Daochen Zha Â· Junjie Xu Â· Yue Zhao Â· Guanchu Wang Â· Xia Hu</p></li></ul><h3 id="Interpretable-Explainable-1"><a href="#Interpretable-Explainable-1" class="headerlink" title="Interpretable / Explainable"></a>Interpretable / Explainable</h3><ul><li><p>Chaos as an interpretable benchmark for forecasting and data-driven modelling</p><p>  William Gilpin</p></li><li><p>Synthetic Benchmarks for Scientific Research in Explainable Machine Learning</p><p>  Yang Liu Â· Sujay Khandagale Â· Colin White Â· Willie Neiswanger</p></li><li><p>FFA-IR: Towards an Explainable and Reliable Medical Report Generation Benchmark </p><p>  Mingjie Li Â· Wenjia Cai Â· Rui Liu Â· Yuetian Weng Â· Xiaoyun Zhao Â· Cong Wang Â· Xin Chen Â· Zhong Liu Â· Caineng Pan Â· Mengke Li Â· yingfeng zheng Â· Yizhi Liu Â· Flora Salim Â· Karin Verspoor Â· Xiaodan Liang Â· Xiaojun Chang</p></li><li><p>Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing</p><p>  Sarah Wiegreffe Â· Ana Marasovic</p></li></ul><h3 id="causal-discovery"><a href="#causal-discovery" class="headerlink" title="causal discovery"></a>causal discovery</h3><ul><li>Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning  Nan Rosemary Ke Â· Aniket Didolkar Â· Sarthak Mittal Â· Anirudh Goyal ALIAS PARTH GOYAL Â· Guillaume Lajoie Â· Stefan Bauer Â· Danilo Jimenez Rezende Â· Michael Mozer Â· Yoshua Bengio Â· Chris Pal</li></ul><h3 id="Time-series-1"><a href="#Time-series-1" class="headerlink" title="Time series"></a>Time series</h3><ul><li><p>Revisiting Time Series Outlier Detection: Definitions and Benchmarks</p><p>  Henry Lai Â· Daochen Zha Â· Junjie Xu Â· Yue Zhao Â· Guanchu Wang Â· Xia Hu</p></li><li><p>Monash Time Series Forecasting Archive</p><p>  Rakshitha Godahewa Â· Christoph Bergmeir Â· Geoffrey Webb Â· Rob Hyndman Â· Pablo Montero-Manso</p></li><li><p>Benchmarking the Robustness of Spatial-Temporal Models Against Corruptions</p><p>  Chenyu Yi Â· SIYUAN YANG Â· Haoliang Li Â· Yap-peng Tan Â· Alex Kot</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Causal Discovery</title>
      <link href="/uncategorized/surveys/causal_discovery/"/>
      <url>/uncategorized/surveys/causal_discovery/</url>
      
        <content type="html"><![CDATA[<ul><li>è°ƒç ”è¿‘å¹´å› æœå‘ç°çš„æœ€æ–°ç ”ç©¶</li><li>å°†â€å› æœå‘ç°â€ä¸â€æ ¹å› åˆ†æâ€ä»¥åŠâ€åŸºäºå› æœå›¾çš„å› æœå‘ç°â€ç­‰æ¦‚å¿µç†æ¸…å…³ç³»</li><li>ä»ä¼˜åŒ–çš„è§’åº¦é‡æ–°çœ‹å¾…ä¸‰ç±»å› æœå‘ç°ç®—æ³•, è¿™å¯ä»¥å¸®åŠ©æˆ‘ä»¬å€Ÿé‰´å·²æœ‰æ–¹æ³•åˆ°è‡ªå·±çš„å·¥ä½œä¸­ã€‚</li><li>æ€»ç»“ç”¨äºå› æœå‘ç°çš„å„ç§åŒ…, ä»¥åŠGithubä¸Šstarè¾ƒå¤šçš„Repositories</li></ul><span id="more"></span><h2 id="1-å› æœä¸‰é˜¶æ¢¯"><a href="#1-å› æœä¸‰é˜¶æ¢¯" class="headerlink" title="1. å› æœä¸‰é˜¶æ¢¯"></a>1. å› æœä¸‰é˜¶æ¢¯</h2><p><strong>Causal discovery(CD)</strong> åŒ…å«ä¸‰ä¸ªå±‚æ¬¡ <a href="#ref1">[1]</a>: </p><ul><li>å…³è” <em>Association</em>: Aã€Bç›¸å…³</li><li>å¹²é¢„ <em>Intervention</em>: æ”¹å˜Açš„æ—¶å€™, Bå¦‚ä½•æ”¹å˜</li><li>åäº‹å® <em>Counterfactuals</em>: è¦æƒ³æ”¹å˜B, åº”è¯¥å¦‚ä½•æ”¹å˜A</li></ul><p>äº‹å®ä¸Š, è®¸å¤šCDçš„æ–¹æ³•éƒ½ä»…èƒ½åšåˆ°<strong>å‰ä¸¤ä¸ªé˜¶æ®µ</strong>ã€‚â€åäº‹å®â€é€šå¸¸æ˜¯ä¸å¯å®ç°çš„, å› ä¸ºâ€åäº‹å®â€è¦æ±‚åœ¨ä¸<u>å†å²å®Œå…¨ç›¸åŒçš„ç¯å¢ƒå› ç´ </u>ä¸‹<strong>æ¢ç©¶å†³ç­–å¯¹ä¸ªä½“çš„å½±å“</strong>ã€‚</p><h2 id="2-å› æœçš„æœ¬è´¨ã€æ•°å­¦è¡¨è¾¾-ä¸-å› æœç»“æ„å­¦ä¹ "><a href="#2-å› æœçš„æœ¬è´¨ã€æ•°å­¦è¡¨è¾¾-ä¸-å› æœç»“æ„å­¦ä¹ " class="headerlink" title="2. å› æœçš„æœ¬è´¨ã€æ•°å­¦è¡¨è¾¾ ä¸ å› æœç»“æ„å­¦ä¹ "></a>2. å› æœçš„æœ¬è´¨ã€æ•°å­¦è¡¨è¾¾ ä¸ å› æœç»“æ„å­¦ä¹ </h2><p>è¿™ä¸€èŠ‚çš„åˆ†ææºè‡ªä¸‹é¢å¯¹ä¸‰ç±»causal structure learningæ–¹æ³•æŒ–æ˜çš„å› æœæœ¬è´¨çš„æ€è€ƒã€‚åæ–‡ä¸­å·²ç»åˆ†æå¾—å‡ºï¼Œä¸‰ç±»causal structure learningæ–¹æ³•å‘ç°çš„å› æœéƒ½åªåœç•™åœ¨ç”±â€œæ¡ä»¶ä¾èµ–â€æˆ–è€…â€œå›å½’â€æ‰€åˆ»ç”»çš„â€œ<strong>å…³è”å…³ç³»</strong>â€ï¼Œè€ŒéçœŸæ­£çš„å› æœå…³ç³»ï¼Œå› æ­¤æœ¬èŠ‚çš„ä¸»é¢˜æ˜¯</p><ul><li>è°ƒç ”casual discoveryå’Œcausal structure learningä¹‹é—´çš„åŒºåˆ«ï¼ˆè¿™ä¸ªé—®é¢˜å’Œå“²å­¦ä¸Šçš„å› æœå®šä¹‰æœ‰å…³ï¼‰</li><li>é™¤äº†DAGè¡¨ç¤ºçš„å› æœå›¾å¤–ï¼Œè¿˜æœ‰å“ªäº›è¡¨å¾å› æœçš„æ¨¡å‹èŒƒå¼ï¼ˆDAGå› æœå›¾å°±æ˜¯èŠ‚ç‚¹ä»£è¡¨å˜é‡ï¼Œè¾¹ä»£è¡¨ç›´æ¥åŸå› çš„å›¾æ¨¡å‹ï¼‰</li></ul><p>è¦åˆ†æcausal discoveryå’Œcausal structure learningä¹‹é—´çš„åŒºåˆ«ï¼Œæœ‰ä»¥ä¸‹æ€è·¯ï¼š</p><ul><li><p>å·²çŸ¥causal structure learningä¸­ï¼Œå·²ç»ç”¨ä¸åŒçš„æ¨¡å‹å¯¹å› æœè¿›è¡Œäº†å®šä¹‰ï¼Œè¿™ç§å®šä¹‰æˆ–æ˜¯å®šæ€§çš„ã€æˆ–æ˜¯å®šé‡çš„ï¼Œå› æ­¤è¿™ç±»æ–¹æ³•æœ¬è´¨ä¸Šå°±æ˜¯åœ¨æ ¹æ®æ¯ç§æ–¹æ³•çš„å®šä¹‰ï¼Œä»æ•°æ®ä¸­æ‹Ÿåˆæ‰€è°“çš„â€œå› æœâ€ã€‚å› æ­¤causal structure learningå¯ä»¥è§†ä¸ºâ€œå­¦ä¹ ä»¥æ¨¡å‹å®šä¹‰çš„å› æœâ€çš„ä¸€ç±»æ–¹æ³•</p></li><li><p>é‚£é™¤äº†ç”¨æ¨¡å‹å®šä¹‰çš„å› æœï¼Œè¿˜æœ‰å“ªäº›å®šä¹‰å‘¢ã€‚</p></li></ul><h3 id="2-1-å› æœçš„æœ¬è´¨"><a href="#2-1-å› æœçš„æœ¬è´¨" class="headerlink" title="2.1 å› æœçš„æœ¬è´¨"></a>2.1 å› æœçš„æœ¬è´¨</h3><p>æ–‡çŒ®<a href="#ref14">[14]</a>ä»å“²å­¦ã€ç¤¾ä¼šå­¦ç­‰å±‚é¢æ¢ç©¶äº†å› æœåœ¨AIé¢†åŸŸçš„å®šä¹‰ï¼Œåœ¨Section 6.5ä¸­æåˆ°äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šâ€œå› æœæ˜¯<strong>ç¡®å®šæ€§</strong>çš„è¿˜æ˜¯<strong>æ¦‚ç‡æ€§</strong>çš„ï¼Ÿâ€ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›ç»“è®ºï¼šè‡ªå¤ä»¥æ¥ï¼Œå› æœè¿™ä¸ªæ¦‚å¿µéƒ½<strong>ä¸å…·å¤‡ä¸€ä¸ªç»Ÿä¸€çš„å®šä¹‰</strong>ã€‚è‡ªå¤ä»¥æ¥ï¼Œå› æœæ€»æ˜¯ä¸å†³å®šè®ºã€ç‰©ç†å¿…ç„¶æ€§å’Œé€»è¾‘å¿…ç„¶æ€§è”ç³»åœ¨ä¸€èµ·ï¼ˆå³ï¼š<strong>ç¡®å®šæ€§</strong>ï¼‰ã€‚ä½†åœ¨ç°ä»£ç§‘å­¦æ‰€ç ”ç©¶çš„å› æœç†è®ºå‡ ä¹éƒ½æ˜¯<strong>æ¦‚ç‡æ€§</strong>çš„ï¼Œå³ä»¥æ¦‚ç‡è®ºå’Œç»Ÿè®¡å­¦è®ºä¸ºåŸºç¡€çš„ã€‚ä½†ä¹Ÿæœ‰éƒ¨åˆ†ç¡®å®šæ€§çš„è¡¨è¿°ï¼Œè¿™ç§è¡¨è¿°å¤§å¤šæºè‡ªäºå·¥ç¨‹å­¦ï¼Œæ˜¯ä¸€ç§ä¸è¯è‡ªæ˜æ–¹æ³•ï¼ˆè¿™é‡Œæˆ‘ç†è§£ï¼Œåœ¨å·¥ç¨‹å­¦é‡Œæ€»æœ‰â€œæ”¹å˜Aï¼ŒåŸºäºè®¾è®¡åŸç†ï¼ŒBå°±ä¸€å®šä¼šæ”¹å˜ï¼Œå¹¶ä¸”Båªå—Aæ§åˆ¶ï¼Œå› æ­¤ABæ˜¯å› æœå…³ç³»â€çš„è¡¨è¿°ï¼Œè¿™ç§å·¥ç¨‹å­¦ä¸Šçš„å› æœæ˜¾ç„¶æ˜¯ç¡®å®šæ€§çš„ï¼‰ã€‚</p><p>å…¶å®ï¼Œå› æœåœ¨ç»Ÿè®¡å­¦ä¸Šçš„å®šä¹‰ä¹Ÿå¹¶æœªç»Ÿä¸€ï¼Œæ–‡çŒ®<a href="#ref16">[16]</a>ä¸­åˆ—å‡ºäº†ä¸‰ç§ä¸»è¦çš„è§‚ç‚¹ï¼Œä¸‹æ–‡è¯´æ˜ä¸€ç§è¾ƒå¸¸è§çš„è§‚ç‚¹ï¼Œæ¥è‡ªäºGood<a href="#ref17">[17]</a>ï¼š</p><blockquote><p>äº‹ä»¶ $F$ æ˜¯äº‹ä»¶ $E$ çš„åŸå› éœ€è¦æ»¡è¶³ï¼š</p><ol><li><p>äº‹ä»¶ $E$ å’Œ $F$ åŒæ—¶æˆç«‹ï¼ˆå¯¹æœ‰æ—¶é—´çº¦æŸçš„äº‹ä»¶åˆ™æ˜¯$F_t \leq E_t$ï¼‰</p></li><li><p>$P(E|H)&lt;1, P(F|H)&lt;1$ ï¼Œå…¶ä¸­$H$åŒ…å«ä¸¤éƒ¨åˆ†ï¼š</p><ul><li><p>$H1$ : æ‰€æœ‰çš„è‡ªç„¶æ³•åˆ™</p></li><li><p>$H2$ : æ‰€æœ‰è¢«è®¤ä¸ºæ˜¯ç†æ‰€å½“ç„¶çš„çœŸå®èƒŒæ™¯æ¡ä»¶</p></li></ul></li><li><p>$P(E|FH) &gt; P(E|\bar{F}H)$</p></li></ol></blockquote><p>Goodåœ¨åç»­çš„ç ”ç©¶<a href="#ref18">[18]</a><a href="#ref19">[19]</a>ä¸­è¯•å›¾ç»™å‡ºå› æœå…³ç³»çš„ä¸€ç§å®šé‡æè¿°ã€‚$F$ å¯¹$E$ çš„æ½œåœ¨å› æœè¶‹åŠ¿å¯ä»¥ç”±ä¸‹å¼åº¦é‡</p><p>$$\log \frac{P(\bar{E}|\bar{F}H)}{P(\bar{E}|FH)}$$</p><blockquote><p>where $H$ consists of all laws of nature and the background conditions before $F$ started. Thus for $F$ to be a potential cause of $E$, the <strong>two must be probabilistically dependent conditional on $H$</strong>. The (actual causal) degree to which $F$ caused $E$ is the <strong>limit</strong>, as the sizes of the events tend uniformly to zero, of the strength of the network of causal connections between $E$ and $F$. Here the strength of a link from $F$ to $E$ is measured by the tendency of $F$ to cause $E$; the strength of the network as a whole is a function of these link strengths which takes into account interactions amongst causes.</p></blockquote><p>æ ¹æ®ä»¥ä¸Šæ€è·¯ï¼Œå¯ä»¥åˆ†æå¾—åˆ°ï¼š</p><ul><li>å› æœåœ¨å“²å­¦ä¸Šçš„å®šä¹‰ä¸€èˆ¬éƒ½æ˜¯ç¡®å®šæ€§çš„ï¼Œä½†åœ¨ç°ä»£ç§‘å­¦ä¸­ï¼Œé€šå¸¸è¢«å®šä¹‰ä¸ºæ¦‚ç‡æ€§çš„ã€‚ä¹‹æ‰€ä»¥è¢«å¦‚æ­¤å®šä¹‰ï¼Œä¸€æ–¹é¢æ˜¯æœ‰éƒ¨åˆ†å‰äººçš„å·¥ä½œä¹Ÿå¦‚æ­¤å®šä¹‰ï¼›å¦ä¸€æ–¹é¢ï¼Œç»å…¸çš„æ¦‚ç‡è¶³ä»¥æ¨¡æ‹Ÿäººç±»æ¨ç†çš„å‡ ä¹æ‰€æœ‰æ–¹é¢<a href="#ref15">[15]</a>ã€‚ä»è¿™ä¸ªè§’åº¦ä¸Šæ¥è®²ï¼Œ<strong>æ¦‚ç‡æ€§å› æœæ˜¯å¯¹ç¡®å®šæ€§å› æœçš„ä¸€ä¸ªæ›´å¹¿ä¹‰çš„æè¿°</strong>ã€‚</li><li>ç«™åœ¨æ¦‚ç‡æ€§å› æœçš„è§’åº¦ï¼Œå› æœå‘ç°å¯ä»¥è¢«ç§°ä¹‹ä¸ºï¼Œåœ¨æ•°æ®ä¸­å‘ç°æ¦‚ç‡ç»“æ„çš„å·¥ä½œï¼ˆå³å› ç´ Açš„æ¦‚ç‡åˆ†å¸ƒå˜åŒ–æ—¶ï¼Œå› ç´ Bçš„æ¦‚ç‡åˆ†å¸ƒæ˜¯å¦å˜åŒ–ï¼‰ï¼Œå…¶æœ¬è´¨éƒ½æ˜¯åœ¨æ‹Ÿåˆä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ã€‚ä»è¿™ä¸ªè§’åº¦æ¥è¯´ï¼Œ<strong>æ¦‚ç‡æ¨¡å‹</strong>ä¹Ÿå¯ä»¥ç”¨äºå› æœå‘ç°ã€‚</li></ul><h3 id="2-2-å› æœç»“æ„å­¦ä¹ "><a href="#2-2-å› æœç»“æ„å­¦ä¹ " class="headerlink" title="2.2 å› æœç»“æ„å­¦ä¹ "></a>2.2 å› æœç»“æ„å­¦ä¹ </h3><p>è¿‘ä¸‰åå¹´æ¥, å› æœå­¦ä¹ çš„å·¥ä½œä¸€èˆ¬èšç„¦äºâ€å› æœç»“æ„å­¦ä¹ (casual structure learning)â€, æ‰€å¾—åˆ°çš„<strong>ç»“æ„å› æœæ¨¡å‹(structural causal model, SCM)</strong> å¯¹å› æœè¿›è¡Œäº†æ¦‚ç‡ã€ç»Ÿè®¡ä¸Šçš„å®šä¹‰ï¼Œå¹¶ç”¨æ°å½“çš„æ¨¡å‹æ¥æè¿°å› æœç»“æ„ã€‚SCMåŒ…å«ä¸¤ä¸ªéƒ¨åˆ†: </p><ul><li>Graphical models: ç”±å›¾æ¨¡å‹è¡¨ç¤ºçš„å› æœå…³ç³», å…¶ä¸­èŠ‚ç‚¹è¡¨ç¤ºéšæœºå˜é‡, æœ‰å‘è¾¹è¡¨ç¤ºå› æœæ–¹å‘</li><li>Structural equations: åœ¨å›¾æ¨¡å‹ä¸­, æœ‰å‘è¾¹ä¸Šçš„å› æœæ•ˆåº”, ç”±å‡½æ•°å¼è¡¨ç¤º</li></ul><p>graphical modelåˆ™æ˜¯structure causal modelçš„ä¸€ç§ï¼Œä¹Ÿæ˜¯å—å¹¿æ³›ç ”ç©¶çš„ä¸€ç§ã€‚</p><p><a href="#ref1">[1]</a>ä¸­å¯¹SCMæœ‰ä»¥ä¸‹è®ºè¿°</p><blockquote><p><strong>â€œstructural causal modelsâ€</strong> (SCM), which consists of three parts: <em>graphical models</em>, <em>structural equations,</em> and <em>counterfactual and interventional logic</em>. </p><p>Graphical models serve as a language for representing what agents know about the world. Counterfactuals help them articulate what they wish to know. And structural equations serve to tie the two together in a solid semantics.</p></blockquote><p><a href="#ref3">[3]</a>ä¸­åˆ™ç€é‡æ¨å´‡äº†å›¾æ¨¡å‹ä½œä¸ºå› æœæ¨¡å‹çš„è¡¨è¾¾å½¢å¼</p><blockquote><p>Methods for extracting causal conclusions from observational studies are on the <strong>middle</strong> rung of Pearlâ€™s Ladder of Causation, and they can be expressed in a mathematical language that extends classical statistics and <strong>emphasizes graphical models</strong>.</p><p>Various options exist for causal models: causal diagrams, structural equations, logical statements, and so forth. I am strongly sold on causal diagrams for nearly all applications, primarily due to their transparency but also due to the explicit answers they provide to many of the questions we wish to ask.</p><p>â€¦â€¦ </p><p>Pearl defines a causal model to be <strong>a directed acyclic graph</strong> that can be paired with data to produce quantitative causal estimates. The graph embodies the structural relationships that a researcher assumes are driving empirical results. The structure of the graphical model, including the identification of vertices as mediators, confounders, or colliders, can guide experimental design through the identification of minimal sets of control variables. Modern expositions on graphical cause and effect models are Pearl (2009) and Spirtes et al. (2000).</p></blockquote><p>ç»¼ä¸Šæ‰€è¿°ï¼Œcasual discovery / probabilistic model / structure casual model / casual graph ä¹‹é—´çš„å…³ç³»ä¸º</p><div align="center">    <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/20221024103453.png" width = "65%" /></div>**Remark**: (1) æ ¼å…°æ°å› æœåº”è¯¥å±äºstructure casual modelï¼Œä½†ä¸ä¸€å®šå±äºå›¾æ¨¡å‹ï¼›(2) å›¾æ¨¡å‹ä¸ä»…åŒ…æ‹¬DAGï¼Œè¿˜æœ‰å…¶ä»–ç±»å‹çš„å›¾ã€‚<h3 id="2-3-å› æœä¸ç»Ÿè®¡æ¨¡å‹çš„å…³è”ä¸åŒºåˆ«"><a href="#2-3-å› æœä¸ç»Ÿè®¡æ¨¡å‹çš„å…³è”ä¸åŒºåˆ«" class="headerlink" title="2.3 å› æœä¸ç»Ÿè®¡æ¨¡å‹çš„å…³è”ä¸åŒºåˆ«"></a>2.3 å› æœä¸ç»Ÿè®¡æ¨¡å‹çš„å…³è”ä¸åŒºåˆ«</h3><p>ä¸Šæ–‡ä¸­æåˆ°ï¼Œå¹¿ä¹‰ä¸Šè®²ï¼Œå› æœå­¦ä¹ çš„æœ¬è´¨æ˜¯å­¦ä¹ ä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ï¼ˆæ¦‚ç‡æ¨¡å‹å¯ä»¥è¢«è§†ä¸ºç»Ÿè®¡æ¨¡å‹ï¼‰ã€‚ä½†<strong>æ¦‚ç‡æ¨¡å‹ ï¼ˆæˆ–ç»Ÿè®¡æ¨¡å‹ï¼‰ä¸ å› æœ æ˜¯ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„æ¦‚å¿µ</strong>ã€‚</p><p>é¦–å…ˆï¼Œæ˜ç¡®ä¸€ä¸ªå…³ç³»ï¼Œå³ï¼šæ ¹æ®å› æœä¸‰é˜¶æ¢¯ï¼Œä»ä¸Šåˆ°ä¸‹åˆ†åˆ«ä¸ºâ€œå…³è”â€â€œå¹²é¢„â€â€œåäº‹å®â€ï¼Œå…¶ä¸­å…³è”æ˜¯ä¸Šè¿°ä¸¤ä¸ªæ¦‚å¿µçš„åŸºç¡€ï¼Œè€Œå¹²é¢„å’Œåäº‹å®æ‰æ˜¯å› æœç‰¹æœ‰çš„å±‚æ¬¡ã€‚<a href="#ref20">[20]</a>å¯¹å…³è”å’Œå› æœç»™å‡ºäº†æ˜ç¡®çš„åˆ†ç•Œçº¿ã€‚</p><ul><li>å…³è”ï¼šå¯ä»¥ç”¨åˆ†å¸ƒå‡½æ•°æ¥å®šä¹‰ï¼Œä¾‹å­ï¼šç›¸å…³æ€§ã€å›å½’ã€ä¾èµ–æ€§ã€æ¡ä»¶ç‹¬ç«‹æ€§ç­‰</li><li>å› æœï¼šä¸èƒ½åªç”¨åˆ†å¸ƒå‡½æ•°æ¥å®šä¹‰ï¼Œä¾‹å­ï¼šå½±å“ã€æ··æ·†ã€å¹²æ‰°ç­‰ã€‚<strong>å› æœä¸èƒ½ä»å…³è”ä¸­æ¨å¯¼å‡º</strong>ï¼Œç”šè‡³<strong>ä¸å¯ä»¥åªä»ç»Ÿè®¡å…³è”çš„è§’åº¦ä¸Šå®šä¹‰</strong>ã€‚</li></ul><p>ä»æ•°å­¦çš„è§’åº¦ä¸Šï¼Œä»»ä½•å› æœåˆ†æçš„æ•°å­¦æ–¹æ³•éƒ½<strong>å¿…é¡»è·å¾—æ–°çš„ç¬¦å·</strong>æ¥è¡¨ç¤ºå› æœå…³ç³»ã€‚æ¦‚ç‡ç¬¦å·å¯¹äºè¡¨ç¤ºå› æœå…³ç³»æ¥è¯´æ˜¯ä¸å¤Ÿçš„ã€‚</p><ul><li>ä¸¾ä¾‹æ¥è¯´ï¼Œæ¦‚ç‡ç›¸å…³çš„æ•°å­¦è¯­è¨€ä¸å…è®¸æˆ‘ä»¬è¡¨è¾¾â€œç—‡çŠ¶ä¸å¯¼è‡´ç–¾ç—…â€è¿™ä¸€ç®€å•äº‹å®ï¼Œæ›´ä¸å…è®¸æˆ‘ä»¬ä»è¿™äº›äº‹å®ä¸­å¾—å‡ºæ•°å­¦ç»“è®ºã€‚æˆ‘ä»¬åªèƒ½è¯´ï¼Œä¸¤ä¸ªäº‹ä»¶æ˜¯ç›¸äº’ä¾èµ–çš„â€”â€”è¿™æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬æ‰¾åˆ°ä¸€ä¸ªï¼Œæˆ‘ä»¬å°±å¯ä»¥é¢„æœŸé‡åˆ°å¦ä¸€ä¸ªï¼Œä½†æˆ‘ä»¬æ— æ³•<strong>åŒºåˆ†ç»Ÿè®¡ä¾èµ–æ€§</strong>ï¼ˆç”±æ¡ä»¶æ¦‚ç‡ P é‡åŒ–çš„ç–¾ç—…ç—‡çŠ¶ï¼‰å’Œ<strong>å› æœä¾èµ–æ€§</strong>ï¼Œæˆ‘ä»¬åœ¨æ ‡å‡†æ¦‚ç‡æ¼”ç®—ä¸­æ²¡æœ‰å¯¹æ­¤å…³ç³»çš„è¡¨è¾¾å¼<a href="#ref20">[20]</a>ã€‚</li></ul><p>è¿™æ˜¯æˆ‘ä»¬ä¸ºå› æœå…³ç³»å»ºç«‹ä¸€å¥—æ–°çš„æ•°å­¦è¡¨è¾¾çš„åŠ¨æœºã€‚</p><h3 id="2-4-å› æœåˆ†æä¸­çš„æ•°å­¦è¡¨ç¤º"><a href="#2-4-å› æœåˆ†æä¸­çš„æ•°å­¦è¡¨ç¤º" class="headerlink" title="2.4 å› æœåˆ†æä¸­çš„æ•°å­¦è¡¨ç¤º"></a>2.4 å› æœåˆ†æä¸­çš„æ•°å­¦è¡¨ç¤º</h3><p><strong>Structural Causal Models</strong> </p><ul><li><p>Structure Equationsï¼šå˜é‡-ä¸‹æ ‡å½¢å¼ / å¸¦ $do$ æ“ä½œç¬¦çš„å½¢å¼ï¼š</p>  <p>$$  ğ‘ƒ(ğ‘Œ_ğ‘¥  = ğ‘¦)âŸºğ‘ƒ(ğ‘Œ=ğ‘¦|ğ‘ ğ‘’ğ‘¡(ğ‘‹=ğ‘¥))âŸºğ‘ƒ(ğ‘Œ=ğ‘¦|ğ‘‘ğ‘œ(ğ‘‹=ğ‘¥))  $$</p><ul><li><p>$P(Y_x = y)$ï¼š$Y$ æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œå½“éšæœºå˜é‡ $X=x$ æ—¶ $Y$ åœ¨æ€»ä½“ä¸­è¾¾åˆ° $y$ å€¼çš„æ¦‚ç‡ã€‚</p></li><li><p>$P(Y=y|do(X=x))$ï¼šå½“ $X=x$ å‡åŒ€çš„æ–½åŠ åœ¨æ¯ä¸€ä¸ªä¸ªä½“ä¸Šæ—¶ï¼Œ$Y=y$ å‘ç”Ÿçš„æ¦‚ç‡ï¼Œå¸¸ç”¨äºæ§åˆ¶å®éªŒã€‚</p></li></ul></li><li><p>Graphical Modelsï¼šå› æœå‡è®¾ç¼–ç åœ¨<strong>ç¼ºå¤±çš„é“¾æ¥</strong>ä¸­â€”â€”ä¸¤ä¸ªå˜é‡ä¹‹é—´æœ‰è¿æ¥åªä»£è¡¨å­˜åœ¨å› æœå…³ç³»çš„<strong>å¯èƒ½æ€§</strong>ï¼Œè€Œæ²¡æœ‰è¿æ¥åˆ™ä»£è¡¨æ²¡æœ‰å› æœã€‚å¯ä»¥è¾…ä»¥æ–¹ç¨‹å¼æ¥è¡¨è¾¾å› æœä¹‹é—´çš„å®šé‡å…³ç³»ã€‚</p></li></ul><p><strong>è¡¨è¾¾å› æœæ•ˆåº”å¤§å°</strong></p><ul><li>$Cov(X, Y)$ï¼šç»™å®šç”±ä»¥ä¸Šæ¨¡å‹è¡¨å¾çš„å› æœå‡è®¾åï¼Œå¯ä»¥ç”¨åæ–¹å·®è¡¨ç¤ºå› æœæ•ˆåº”çš„å¤§å°ï¼Œè¾…åŠ©ä»è§‚å¯Ÿåˆ°çš„éå®éªŒæ•°æ®ä¸­ä¼°è®¡å› æœå‚æ•°ã€‚</li></ul><p><strong>å¹²é¢„â€”â€”æ•°å­¦è¡¨è¾¾</strong></p><p>å¹²é¢„çš„åŸºç¡€æ˜¯ç»“æ„æ–¹ç¨‹ï¼ˆSEMï¼‰ï¼Œå¹¶é€šè¿‡$do$è¿ç®—ç¬¦å®Œæˆã€‚</p><ul><li>$do(x)$ï¼šæ¨¡æ‹Ÿäº†ä¸€ç§ç‰©ç†ä¸–ç•Œçš„äº¤äº’â€”â€”åœ¨ç»™å®šçš„SEMä¸­ï¼Œ<strong>åˆ é™¤</strong>åŸæ¨¡å‹ä¸­Xå˜é‡çš„å‡½æ•°ï¼Œå¹¶äººä¸º<strong>æ›¿æ¢</strong>å˜é‡ä¸ºä¸€ä¸ªå¸¸æ•° $X=x$ã€‚å¦‚ä¸‹å›¾æ‰€ç¤º<ul><li>$P(z, y | do(x))$ï¼šå˜é‡ $Y$ å’Œ $Z$ çš„å¹²é¢„ååˆ†å¸ƒï¼Œå³è¡¨ç¤ºå¯¹æ‰€æœ‰ä¸ªä½“å‡å®æ–½äº†$X=x$ åï¼Œå˜é‡ $Y$ å’Œ $Z$ çš„è”åˆåˆ†å¸ƒ</li><li>$P(z, y, x)$ï¼šå˜é‡çš„å¹²é¢„å‰åˆ†å¸ƒ</li></ul></li><li>è®°å¹²é¢„å‰çš„å› æœæ¨¡å‹ä¸º$M$ï¼Œå¹²é¢„ $X=x$ åçš„å› æœæ¨¡å‹ä¸º$M_x$<ul><li>$P_M (y|do(x)) â‰œ P_{M_x}(y)$ï¼šåœ¨å¹²é¢„å‰çš„æ¨¡å‹ $M$ ä¸­å®æ–½å¹²é¢„ $do(x)$ å $Y$ çš„åˆ†å¸ƒï¼Œç­‰ä»·äºåœ¨å¹²é¢„åæ¨¡å‹ $M_x$ ä¸­$ Y$ çš„åˆ†å¸ƒã€‚</li></ul></li></ul><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20221117170455716.png" alt="image-20221117170455716" style="zoom:50%;" /><p>å¹²é¢„çš„æ•ˆæœè¡¨ç¤º</p><ul><li>Causal effectï¼š$P(Y=y|do(x))= \sum_z P(z, y|do(x))$</li><li>Measurements<ul><li>Average Differenceï¼š$E(Y|do(x_0^{â€˜})) -E(Y|do(x_0))$</li><li>Experimental Risk Ratioï¼š$\frac{E(Y|do(x_0^â€²))}{E(Y|do(x_0))}$</li></ul></li></ul><p><strong>åäº‹å®â€”â€”æ•°å­¦è¡¨è¾¾</strong></p><ul><li>$Y_x (u)$ï¼šåœ¨å˜é‡æ›¾è¢«è®¾ç½®ï¼ˆhas been setï¼‰ä¸º $X=x$ çš„æƒ…å†µä¸‹ï¼Œä¸ªä½“ $u$ å¾—åˆ°çš„ $Y$ å˜é‡çš„å€¼</li><li>$Y_x (u)â‰œY_{M_x} (u)$ï¼šUnit-levelçš„åäº‹å®ï¼Œå®šä¹‰ä¸ºåœ¨å¹²é¢„åæ¨¡å‹ $M_x$ ä¸­ï¼Œä¸ªä½“ $u$ å¾—åˆ°çš„ $Y$ å˜é‡çš„å€¼ã€‚<ul><li>åäº‹å®å¯ä»¥è¢«è§†ä¸ºç‰¹å®šæƒ…å¢ƒå› ç´ ï¼ˆä¸è¿‡å»æŸä¸ªæ—¶é—´çš„æ‰€æœ‰å› ç´ ä¸€è‡´ï¼‰ä¸‹çš„å¹²é¢„ã€‚</li><li>åäº‹å®ä¸€èˆ¬ç ”ç©¶ç‰¹å®šæƒ…å¢ƒå› ç´ ä¸‹ï¼Œä¸ªä½“çš„å¹²é¢„ç»“æœã€‚å¹²é¢„åˆ™ä¸€èˆ¬ç ”ç©¶å†³ç­–çš„<strong>å¹³å‡</strong>å½±å“æƒ…å†µ</li></ul></li></ul><h3 id="2-5-åäº‹å®æ¨ç†"><a href="#2-5-åäº‹å®æ¨ç†" class="headerlink" title="2.5 åäº‹å®æ¨ç†"></a>2.5 åäº‹å®æ¨ç†</h3><p>ä»ä¸Šæ–‡å¯ä»¥çœ‹å‡ºï¼Œå› æœåŒºåˆ«ä¸ä¸€èˆ¬çš„ç»Ÿè®¡æ¨¡å‹ï¼Œå…¶ä¸­å¹²é¢„å’Œåäº‹å®èµ·åˆ°äº†å†³å®šæ€§çš„ä½œç”¨ã€‚æœ¬èŠ‚å°†å…ˆåŒºåˆ«å¹²é¢„ä¸åäº‹å®çš„åŒºåˆ«ï¼Œç„¶åé˜è¿°åäº‹å®æ¨ç†çš„åŸºæœ¬æ­¥éª¤ã€‚</p><p>ä»ç›®æ ‡çš„è§’åº¦</p><ul><li>å¹²é¢„ï¼šå›ç­”â€œå¦‚æœ<strong>ç°åœ¨</strong>å¹²é¢„å˜é‡Xï¼Œé‚£ä¹ˆå¯¹Zä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ï¼Ÿâ€</li><li>åäº‹å®ï¼šå›ç­”â€œå¦‚æœåœ¨<strong>è¿‡å»æŸä¸ªæ—¶é—´</strong>å¹²é¢„å˜é‡Xï¼ŒZä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ï¼Ÿâ€</li></ul><p>å…¶åŒºåˆ«åœ¨äº</p><ul><li><p>â€œå¹²é¢„â€æ”¹å˜ä½†ä¸ä¸è§‚å¯Ÿåˆ°çš„ä¸–ç•Œç›¸çŸ›ç›¾ï¼Œå› ä¸ºå¹²é¢„å‰åçš„ä¸–ç•Œå¤„äºä¸åŒçš„æ—¶é—´</p></li><li><p>â€œåäº‹å®â€åˆ™ä¸å·²çŸ¥äº‹å®ç›¸å†²çª</p></li></ul><p>æ³¨ï¼šäºŒè€…å¹¶ä¸æ˜¯è¢«ä¸¥æ ¼åŒºåˆ†çš„ï¼Œä¹Ÿæœ‰ç ”ç©¶ä¸åŒºåˆ†äºŒè€…ï¼Œè¿™æ˜¯ä¸€ç§å“²å­¦/æ–‡åŒ–ä¸Šçš„å·®åˆ«</p><p><strong>åäº‹å®æ¨ç†çš„åŸºæœ¬æ­¥éª¤</strong></p><p><strong>Background</strong>ï¼šSCMåŠå…¶å½“ä¸­çš„å‚æ•°</p><ol><li><p>Step1: å¤–å±•â€”â€”ä¸ºæ¯ä¸ªè§‚æµ‹å˜é‡è®¾ç½®ä¸€ä¸ªåœ¨<strong>è¿‡å»å¯èƒ½æœªè§‚æµ‹</strong>åˆ°çš„å˜é‡é›†åˆ$U$ï¼ŒåŸºäºå†å²è§‚æµ‹å­¦ä¹ é›†åˆ$U$</p></li><li><p>Step2: å¹²é¢„â€”â€”ä¸ºåäº‹å®æ¨ç†å»ºç«‹æ–°çš„å¹²é¢„æ¨¡å‹ï¼Œå…¶ä¸­å¹²é¢„æ“ä½œå°†ä¼šä»£æ›¿SCMä¸­çš„ä¸€ä¸ªï¼ˆæˆ–å¤šä¸ªï¼‰å˜é‡</p></li><li><p>Step3: é¢„æµ‹â€”â€”å°†å†å²æœªè§‚æµ‹å˜é‡$U$ä»¥åŠ<strong>å¹²é¢„æ“ä½œ</strong>ä»£å…¥ä¿®æ”¹åçš„SCMï¼Œè¿›è¡Œåäº‹å®æ¨ç†</p></li></ol><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20221117171058267.png" alt="image-20221117171058267" style="zoom:67%;" /><p><strong>åäº‹å®ï¼ˆCounterfactualï¼‰æ¨ç†â€”â€”ä»¥ç¡®å®šæ€§æ¨¡å‹ä¸ºä¾‹</strong></p><p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20221117171259748.png" alt="image-20221117171259748"></p><p>å¯¹äºæ¦‚ç‡å‹æ¨¡å‹ï¼Œåˆ™éœ€è¦é’ˆå¯¹å˜é‡çš„æ‰€æœ‰æƒ…å†µè¿›è¡Œè€ƒè™‘ã€‚å…¶ä¸­â€œé¢„æµ‹â€åˆ™ç”±â€œæ±‚æ¦‚ç‡â€è½¬ä¸ºâ€œæ±‚æœŸæœ›â€ã€‚</p><p>ä¸‹é¢çš„æŒ‡æ ‡å¯ä»¥ç”¨äº<strong>è¯„ä¼°</strong>æŸä¸ªæ“ä½œæ˜¯å¦å¯¹æœªæ¥æœ‰å½±å“<a href="#ref20">[20]</a>ï¼Œæ­¤å¤–è¿˜æœ‰å¤šç§æŒ‡æ ‡ï¼Œå¦‚</p><p>$$P N(x, y)=P\left(Y_{x^{\prime}}=y^{\prime} \mid X=x, Y=y\right) \\E T T=P\left(Y_x=y \mid X=x^{\prime}\right)$$</p><h3 id="2-6-æ”¯æ’‘ææ–™"><a href="#2-6-æ”¯æ’‘ææ–™" class="headerlink" title="2.6 æ”¯æ’‘ææ–™"></a>2.6 æ”¯æ’‘ææ–™</h3><p>â€œç»“æ„å­¦ä¹ æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæ¨¡å‹é€‰æ‹©é—®é¢˜ï¼Œé€‰æ‹©ä¸€ä¸ªç»™å®šæ•°æ®é›†ä¸Šæœ€èƒ½å¤Ÿæè¿°æ•°æ®ä¾èµ–çš„æ¨¡å‹ã€‚å› æœç»“æ„å­¦ä¹ æ˜¯ç»“æ„å­¦ä¹ ä¸­çš„ä¸€ç§ç‰¹ä¾‹ï¼Œå…¶å­¦ä¹ äº†ä¸€ä¸ªå› æœå›¾â€ã€‚è¿™ä¸ªè§‚ç‚¹æ˜¯è¢«æ™®éæ¥å—çš„ã€‚</p><p><strong>Remark</strong>ï¼šç»“æ„å­¦ä¹ çš„ä¸‰ç§æ–¹æ³•ï¼ˆåŸºäºçº¦æŸã€åŸºäºåˆ†æ•°ã€åŸºäºå‡½æ•°ï¼‰æœ¬è´¨ä¸Šéƒ½å¯ä»¥ç»†åˆ†ä¸ºé€šè¿‡ç»„åˆ/æœç´¢ç®—æ³•ï¼Œæ¥è¯†åˆ«å› æœç»“æ„ <a href="#ref13">[13]</a></p><blockquote><p><strong>Structure learning is a model selection problem</strong> in which one estimates or learns a graph that best describes the dependence structure in a given data set (Drton &amp; Maathuis 2017). <strong>Causal structure learning is the special case</strong> in which one tries to learn the <strong>causal graph</strong> or certain aspects of it, and this is what we focus on in this article.</p><p>â€”â€” Heinze-Deml, C., Maathuis, M. H., &amp; Meinshausen, N. (2018). Causal structure learning. <em>Annual Review of Statistics and Its Application</em>, <em>5</em>, 371-391.</p></blockquote><p><strong>Q1ï¼šé™¤DAGå¤–ï¼Œè¿˜æœ‰å“ªäº›å› æœå›¾</strong> </p><p>æ‘˜è‡ª <a href="#ref13">[13]</a></p><blockquote><p>Other types of graph used to represent causal structure include Partially Oriented Induced Path Graphs (POIPGs)[190, 228], SingleWorld Intervention Graphs (SWIGs) [24, 201, 202], Ïƒ-connection graphs [56], undirected graphs[11], interaction and component graphs for dynamic systems [40], Maximal Almost Ancestral Graphs (MAAGs)[231], psi-ECs [110], Patterns [274], and arid, bow-free, and ancestral ADMGs [19]. </p><p>There are also other types of assumptions relating to the functional form of the structural relationships (e.g., linear or non-linear) as well as the parametric form of the marginals and the errors (e.g., Gaussian or non-Gaussian).</p></blockquote><p><strong>Q2ï¼šå¤§æ•°æ®å¯¹å› æœå‘ç°çš„è´¡çŒ®</strong></p><p>ç”±ä¸‰é˜¶æ¢¯å®šä¹‰çš„å› æœé€šå¸¸æ˜¯éš¾ä»¥æ¨æ–­çš„ï¼Œå› ä¸ºå¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå®éªŒäººå‘˜éƒ½éš¾ä»¥â€œå¹²é¢„â€ï¼Œæ›´åˆ«è¯´â€œåäº‹å®â€ã€‚æ­¤å¤–ï¼Œå¯èƒ½å­˜åœ¨â€œæœªè¢«è§‚æµ‹çš„æ½œåœ¨å› ç´ â€ã€â€œå› æœçŸ¥è¯†é€šå¸¸æ˜¯éå…ˆéªŒçš„ï¼ˆæˆ‘ç†è§£æ˜¯ï¼Œéå…ˆéªŒå¯¼è‡´éš¾ä»¥åäº‹å®ï¼‰â€ä¹Ÿæ˜¯é˜»ç¢å› æœå‘ç°ä¸ä¼°è®¡çš„å› ç´ ä¹‹ä¸€ <a href="#ref13">[13]</a>ã€‚</p><blockquote><p>Unfortunately, in many cases, it may not be possible to undertake such experiments due to prohibitive cost, ethical concerns, or impracticality. For example, to understand the impact of smoking, it would be necessary to force diferent individuals to smoke or not-smoke. <strong>Researchers are therefore often left with non-experimental, observational data.</strong> In the absence of intervention and manipulation, observational data leave researchers facing a number of challenges: Firstly, observational datasets may not contain all relevant variables - <strong>there may exist unobserved/hidden/latent factors</strong> (this is sometimes referred to as the third variable problem). Secondly, observational data may <strong>exhibit selection bias</strong> - for example, younger patients may in general prefer to opt for surgery, whereas older patients may prefer medication. Thirdly, the causal <strong>relationships underlying these data may not be known a priori</strong> - for example, are genetic factors independent causes of a particular outcome, or do they mediate or moderate an outcome? These three challenges afect the discovery and estimation of causal relationships</p></blockquote><p>å› æ­¤ï¼Œå¤§æ•°æ®ã€æˆ–æœºå™¨å­¦ä¹ ç®—æ³•åœ¨å› æœå‘ç°é—®é¢˜ä¸­æ‰®æ¼”çš„è§’è‰²å¯ä»¥æè¿°ä¸º</p><ul><li>å¤§æ•°æ®çš„æ•°æ®é‡çº§å¼¥è¡¥äº†è§‚å¯Ÿä¸å……åˆ†å¯¼è‡´çš„â€œæœªè§‚å¯Ÿã€æ¼è§‚å¯Ÿâ€ã€â€œé€‰æ‹©åå·®â€ï¼Œå³å¤§æ•°æ®ä½¿å¾—æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°æ›´å¤šçš„å˜é‡ï¼Œå½“è§‚å¯Ÿè¶³å¤Ÿå……åˆ†æ—¶ï¼Œæ¨æ–­å‡ºçœŸå®å› æœçš„æ¦‚ç‡å°±è¶Šå¤§ã€‚</li><li>å¤§æ•°æ®ä¹Ÿå¯ä»¥ä»¥æ•°æ®é‡çº§å‡è½»é€‰æ‹©åå·®ï¼ˆæˆ‘ç†è§£æ˜¯ï¼šè™½ç„¶å¹´è½»äººå¯èƒ½æ›´å€¾å‘äºé€‰æ‹©æ•´å½¢æ‰‹æœ¯ï¼Œä½†éšç€æ ·æœ¬å¢å¤šï¼Œä¹Ÿå¯ä»¥æ‰¾åˆ°å€¾å‘äºåšæ•´å½¢æ‰‹æœ¯çš„è€å¹´äººï¼‰ã€‚</li><li>å¤§æ•°æ®å¯èƒ½å¯ä»¥å¸®åŠ©æˆ‘ä»¬è¿›è¡Œåäº‹å®ï¼Œä¾‹å¦‚å¯¹äºä¸€ä¸ªå‘¨æœŸç³»ç»Ÿï¼Œé€šè¿‡è¶³å¤Ÿå¤šå‘¨æœŸçš„è§‚å¯Ÿï¼Œæˆ‘ä»¬ä¹Ÿè®¸å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªæ—¶é—´ç‚¹ï¼Œåªæœ‰ä¸€ä¸ªåŸå› å˜é‡å‘ç”Ÿå˜åŒ–ï¼Œè€Œå…¶ä»–æ‰€æœ‰å˜é‡éƒ½ä¸å†å²ä¿æŒä¸€è‡´ï¼Œè¿™ç§åœºæ™¯æœ‰åˆ©äºæ¨æ–­å› æœåäº‹å®ã€‚</li></ul><p>æ­¤å¤–ï¼Œå¯¹æ¯”å®éªŒæ•°æ®ï¼ˆå³å­˜åœ¨â€œå¹²é¢„-æ•ˆæœâ€ç»“æ„çš„æ•°æ®ï¼‰ä»¥åŠè§‚æµ‹æ•°æ®ï¼ˆå³åªåŒ…å«éä¸»åŠ¨å¹²é¢„ä»¥åŠè¢«åŠ¨è§‚æµ‹çš„æ•°æ®ï¼‰ï¼Œè§‚æµ‹æ•°æ®å¯ä»¥æä¾›æ›´å¥½çš„ç»Ÿè®¡èƒ½åŠ›å’Œå¯æ¨å¹¿æ€§ <a href="#ref13">[13]</a>ã€‚</p><h2 id="3-Casual-Structure-Learning"><a href="#3-Casual-Structure-Learning" class="headerlink" title="3. Casual Structure Learning"></a>3. Casual Structure Learning</h2><h3 id="3-1-Casual-structure-learningçš„ä¸‰ç±»æ–¹æ³•"><a href="#3-1-Casual-structure-learningçš„ä¸‰ç±»æ–¹æ³•" class="headerlink" title="3.1 Casual structure learningçš„ä¸‰ç±»æ–¹æ³•"></a>3.1 Casual structure learningçš„ä¸‰ç±»æ–¹æ³•</h3><p>Casual structure learningçš„ç»å…¸åˆ†ç±»æ–¹æ³•å¯åˆ†ä¸ºä¸‰ä¸ª<strong>ä¸»è¦ç±»åˆ«</strong>ï¼šconstrain-based, score-based, functional casual model <a href="#ref2">[2]</a>ï¼Œè¿˜æœ‰ä¸€äº›hybird methodï¼Œæ­¤å¤„ä¸åˆ—å‡ºã€‚</p><p><strong>Remark</strong>ï¼šä¹Ÿæœ‰æ–‡ç« <a href="#ref13">[13]</a>æå‡ºâ€œconstraint-based, score-based, those exploiting structural asymmetries, and those exploiting various forms of interventionâ€çš„åˆ†ç±»æ–¹æ³•ï¼Œè¿™ç§åˆ†ç±»æ–¹æ³•æ¯”è¾ƒæ–°ï¼Œå¯èƒ½å¯¹è¿‘æœŸï¼ˆ2022ï¼‰å·¥ä½œæœ‰è¾ƒå¥½çš„é€‚åº”æ€§ã€‚</p><ul><li><p><strong>Constraint-based methods</strong>: è¿™ç±»æ–¹æ³•ä¾èµ–éšæœºå˜é‡é—´çš„<strong>æ¡ä»¶ç‹¬ç«‹æ€§æµ‹è¯•(conditional independency test)</strong> æ¢ç©¶å˜é‡é—´çš„å› æœç»“æ„</p><ul><li><p>åœ¨ä¼ ç»Ÿçš„PCç®—æ³•ä¸­, ä¸ºäº†ç®€ä¾¿çš„æ¨å¯¼å‡ºå› æœç»“æœ, åŸºäºCIå®šä¹‰äº†ä¸¤ç§å›¾ä¸Šçš„ç»“æ„, å³ V-structure / D-separation, è¿™ä¸¤ç§ç»“æ„å¯ä»¥è¾…åŠ©æ¨å¯¼å‡ºå› æœçš„ç»“æ„ä¸æ–¹å‘ã€‚å…·ä½“çš„, PCé¦–å…ˆæ„é€ ä¸€ä¸ªå®Œå…¨å›¾, ç„¶åé€šè¿‡ä¸¤ä¸¤å˜é‡é—´çš„independency teståˆ é™¤æŸäº›æ— å‘è¾¹, ç„¶ååŸºäºCI testä»¥åŠV-structure / D-separation, ç¡®å®šå…¶ä½™è¾¹çš„æ–¹å‘æˆ–åˆ é™¤æŸäº›è¾¹ã€‚</p></li><li><p>ç¼ºç‚¹: </p><ol><li>ä¸èƒ½å­˜åœ¨æœªè§‚æµ‹çš„æ··æ‚å˜é‡, è¯¥æ¡ä»¶åœ¨å¤§æ•°æ®çš„æƒ…å†µä¸‹å¾ˆéš¾æ»¡è¶³, ä½†å­˜åœ¨å¦‚FCIçš„ç®—æ³•æ”¾å®½äº†è¯¥é™åˆ¶</li><li>æ ¹æ®å› æœä¿¡å¿µå‡è®¾, åªèƒ½æ ¹æ®æ¡ä»¶ç‹¬ç«‹æ€§æ¥åˆ¤æ–­å› æœå…³ç³», å› æ­¤éœ€è¦éå¸¸å¤šä¸”é«˜è´¨é‡çš„æ•°æ®, å¦‚æœæ•°æ®è¾ƒå°‘, åˆ™æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾æµ‹è¯•å¯èƒ½ä¼šäº’æ–¥</li><li>å¯¹äºåˆ†å‰ç»“æ„ä»¥åŠå¯¹æ’ç»“æ„, è¯¥ç±»ç®—æ³•æ— æ³•æ ¹æ®æ¡ä»¶ç‹¬ç«‹æ€§åˆ†è¾¨<strong>é©¬å°”å¯å¤«ç­‰ä»·ç±»(Markov equivalent class)</strong>, å› æ­¤å¯¹å±€éƒ¨å› æœå…³ç³»çš„åˆ¤åˆ«ä¸è¶³</li></ol><ul><li><strong>Markov equivalent class</strong>: æ‹¥æœ‰ç›¸åŒdåˆ†ç¦»ç»“æ„çš„å› æœå›¾å¹¶ä¸”å…·æœ‰ç›¸åŒæ¡ä»¶ç‹¬ç«‹æ€§å…³ç³»çš„å› æœå›¾è¢«ç§°ä½œé©¬å°”å¯å¤«ç­‰ä»·ç±», æ— æ³•æ ¹æ®æ¡ä»¶ç‹¬ç«‹æ€§åˆ†è¾¨å› æœæ–¹å‘ã€‚</li></ul></li></ul></li><li><p><strong>Score-based methods</strong>: è¿™ç±»æ–¹æ³•é¦–å…ˆæŒ‡å®šå› æœçˆ¶èŠ‚ç‚¹åˆ°å­èŠ‚ç‚¹ä¹‹é—´çš„å‡½æ•°å…³ç³», ç„¶åä»¥æŸä¸ªåˆ†æ•°, å¦‚AIC / BIC, ä¸ºä¼˜åŒ–ç›®æ ‡, ä¼˜åŒ–å¾—åˆ°<strong>å›¾ç»“æ„</strong>ä»¥åŠç›¸å…³å‚æ•°ã€‚</p><ul><li>å¦‚NOTEARSå‡è®¾å‡½æ•°å…³ç³»ä¸º $x=\sum w_x f(P_a(x))$ , å…¶ä¸­ $w_x$ æ˜¯å˜é‡ $x$ çš„æƒé‡, $P_a(x)$ æ˜¯å…¶å› æœçˆ¶èŠ‚ç‚¹</li><li>ç¼ºç‚¹: <ol><li>è¯¥æ–¹æ³•ä¹Ÿä¼šå¾—åˆ°é©¬å°”å¯å¤«ç­‰ä»·ç±»ã€‚</li><li>ç”±äºè¦æ‰¾åˆ°æœ€ä¼˜åˆ†æ•°, å°±è¦æœç´¢å…¨éƒ¨çš„å›¾, è¿™æ˜¯ä¸€ä¸ªNP-hardçš„é—®é¢˜, å¤æ‚åº¦æé«˜ä¸”å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚</li></ol></li></ul></li><li><p><strong>Functional casual model</strong>: è¿™ç±»æ–¹æ³•å¾€å¾€æ¢ç©¶ä¸¤ä¸ªå·²æœ‰å…³è”çš„å˜é‡ä¹‹é—´çš„å› æœæ–¹å‘ã€‚é¦–å…ˆå¯¹æ•°æ®ä¸å› æœå‡½æ•°åšå‡ºå‡è®¾, ç„¶åé€šè¿‡æµ‹è¯•ä¸¤ä¸ªå˜é‡ä¹‹é—´æ˜¯å¦æ»¡è¶³å…³è”å‡½æ•°, æ¥åˆ¤æ–­äºŒè€…ä¹‹é—´çš„å…³è”æ–¹å‘ã€‚</p><ul><li><p>å¦‚LiNGAMå‡è®¾å› æœä¹‹é—´æ»¡è¶³<strong>çº¿æ€§å…³ç³»</strong>, ä¸”æ•°æ®ä¸­çš„å™ªéŸ³ä¸º<strong>é«˜æ–¯å™ªå£°</strong></p></li><li><p>è¯¥ç±»æ–¹æ³•ç”±äºè¿›è¡Œäº†ä¸¥æ ¼çš„å‡è®¾, ä¸”ä¸€èˆ¬ä¼šæ ¹æ®å‡½æ•°çš„æ‹Ÿåˆç¨‹åº¦æ¥æ‰¾åˆ°å”¯ä¸€çš„å› æœæ–¹å‘, å› æ­¤ä¸€èˆ¬ä¸ä¼šå‡ºç°é©¬å°”å¯å¤«ç­‰ä»·ç±»</p></li><li><p>æ³¨æ„, FCMä¸€èˆ¬æ˜¯æ¢ç©¶ä¸¤ä¸ªå˜é‡ä¹‹é—´å› æœå…³ç³»çš„æ–¹æ³•, å¦‚<a href="#ref4">[4]</a>æ‰€è¿°ã€‚è¯¥ç±»æ–¹æ³•çš„ç¼ºç‚¹æ˜¯å¯¹æ•°æ®ç‰¹æ€§ä»¥åŠå› æœæœ‰è¾ƒå¼ºçš„å‡è®¾ã€‚</p><blockquote><p>Determining causal relationships <strong>between two variables</strong> is a fundamental and challenging causal discovery task (Janzing et al., 2012). <strong>Conventional constraint-based and score-based causal discovery methods identify causal structures only up to Markov equivalent classes (Spirtes et al., 2001), in which some causal relationships are undetermined.</strong> To address this challenge, properly constrained functional causal models (<strong>FCMs</strong>) have been proposed. FCMs represent the effect as a function of its cause and independent noise and can help identify the causal direction between two variables by imposing substantial structural constraints on model classes, such as additive noise models (ANMs)</p></blockquote></li></ul></li></ul><div align="center">    <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20221018113112863.png" width = "75%" /></div><p><strong>éœ€è¦æŒ‡å‡ºçš„æ˜¯: å›¾æ¨¡å‹åªæ˜¯ä¸€ç§å› æœå…³ç³»çš„è¡¨ç¤ºæ–¹å¼, ä½†è¯¥è¡¨ç¤ºæ–¹å¼ä¸æ˜¯å¿…è¦çš„ã€‚</strong></p><p>ä¸Šè¿°ä¸‰ç±»æ–¹æ³•, ç”±äºå…¶å‡è®¾ä¸åŒã€æ£€éªŒæ–¹æ³•ä¸åŒ, å› æ­¤æŒ–æ˜å‡ºçš„â€å› æœâ€å…·æœ‰ä¸åŒå«ä¹‰(å³å› æœå›¾ä¸­çš„æœ‰å‘è¾¹å…·æœ‰ä¸åŒçš„å«ä¹‰), åˆ†å±ä¸åŒçš„å› æœé˜¶æ¢¯: </p><ul><li>Constraint-based: è¯¥ç±»æ–¹æ³•é€šè¿‡ä¸€ç³»åˆ—å˜é‡ä¹‹é—´çš„CI, å­¦ä¹ å˜é‡ä¹‹é—´çš„å› æœå…³ç³»ã€‚ è¯¥ç±»æ–¹æ³•æŒ–æ˜å‡ºçš„å› æœæœ¬è´¨æ˜¯â€<strong>æ¡ä»¶ä¾èµ–</strong>â€œ, è¿™ç±»ä¾èµ–å±äºâ€<strong>å…³è”</strong>â€œå±‚é¢ã€‚</li><li>Score-based / Functional causal model: è¯¥ç±»æ–¹æ³•é¦–å…ˆå®šä¹‰äº†å› æœå˜é‡ä¹‹é—´æ»¡è¶³çš„<strong>å‡½æ•°å…³ç³»</strong>, å‰è€…ä¼˜åŒ–å…¨å±€å¾—åˆ†å‡½æ•°æ¥ç¡®å®šå˜é‡é—´çš„å› æœ, åè€…é‡‡ç”¨ç©·ä¸¾ä¼˜åŒ–ç®—æ³•æœç´¢å˜é‡é—´çš„å› æœã€‚è¿™ä¸¤ç±»æ–¹æ³•æŒ–æ˜å‡ºçš„å› æœæœ¬è´¨æ˜¯ç”±â€å›å½’å…³ç³»â€è¡¨ç¤ºçš„â€å…³è”â€ã€‚</li></ul><p><strong>Q3ï¼šCDä¸RCAçš„å…³ç³»</strong></p><p>æ­¤å¤–, å¼ºè°ƒä¸€ä¸‹Root Cause Analysis(RCA)ä¸CDçš„å…³ç³», å³: ä¸€èˆ¬çš„RCAæ›´å…³æ³¨Causal discoveryçš„å‰ä¸¤ä¸ªç­‰çº§, å³æ¢ç©¶â€ä»€ä¹ˆå’Œå¼‚å¸¸ç›¸å…³ï¼Ÿâ€â€ä»€ä¹ˆå¯¼è‡´äº†å¼‚å¸¸ï¼Ÿâ€, è¿™å°±æ˜¯ä¸ºä½•è®¸å¤šRCAçš„æ–¹æ³•éƒ½<strong>åªè€ƒè™‘äº†å…³è”ã€æ¨ç†</strong>, å› æ­¤Causal discoveryå’ŒRCAçš„å…³ç³»å¦‚ä¸‹: </p><div align="center">    <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20221018113441012.png" width = "33%" /></div><h3 id="3-2-ä»ä¼˜åŒ–çš„è§’åº¦åˆ†æä¸‰ç±»æ–¹æ³•"><a href="#3-2-ä»ä¼˜åŒ–çš„è§’åº¦åˆ†æä¸‰ç±»æ–¹æ³•" class="headerlink" title="3.2 ä»ä¼˜åŒ–çš„è§’åº¦åˆ†æä¸‰ç±»æ–¹æ³•"></a>3.2 ä»ä¼˜åŒ–çš„è§’åº¦åˆ†æä¸‰ç±»æ–¹æ³•</h3><p>ä¸ºäº†ç†è§£ä¸‰ç±»Casual structure learningçš„æ–¹æ³•, è¿™é‡Œä»ä¼˜åŒ–çš„è§’åº¦å¯¹ä¸‰ç±»æ–¹æ³•è¿›è¡Œé˜è¿°ã€‚</p><p>é¦–å…ˆç”¨$\mathcal{G}$è¡¨ç¤º$d$ä¸ªèŠ‚ç‚¹æ‰€æ„æˆçš„æœ‰å‘å›¾ç©ºé—´ã€‚å¯¹ $\forall G(\boldsymbol{M}) \in \mathcal{G}$ , ç”¨ $\boldsymbol{M}$ è¡¨ç¤ºå¯¹åº”çš„é‚»æ¥çŸ©é˜µ, åè¿‡æ¥, ç”¨ $G(\boldsymbol{M})$ è¡¨ç¤ºä»¥ $\boldsymbol{M}$ ä¸ºé‚»æ¥çŸ©é˜µçš„æœ‰å‘å›¾ã€‚å…ƒç´  $M_{i,j}=1$ è¡¨ç¤ºå­˜åœ¨å› æœå…³ç³» $\boldsymbol{x}_i \rightarrow \boldsymbol{x}<em>j$ ï¼Œ$M</em>{i,j}=0$ è¡¨ç¤ºä¸å­˜åœ¨å› æœå…³ç³»ã€‚ $\boldsymbol{X}=[\boldsymbol{x}_1, \cdots, \boldsymbol{x}_d] \in \mathbb{R}^{n \times d}$ è¡¨ç¤º $d$ ç»´çš„è§‚æµ‹æ•°æ®, å…¶ä¸­æ¯ä¸ªæ•°æ®è§‚æµ‹ $n$ æ¬¡ã€‚$\mathbb{D}$ è¡¨ç¤ºç”± $d$ ä¸ªèŠ‚ç‚¹ç»„æˆçš„æœ‰å‘æ— ç¯å›¾é›†åˆã€‚</p><h3 id="3-2-1-Constraint-based-Method"><a href="#3-2-1-Constraint-based-Method" class="headerlink" title="3.2.1 Constraint-based Method"></a>3.2.1 Constraint-based Method</h3><p>åŸºäºçº¦æŸçš„ç®—æ³•åˆ©ç”¨ ä»ä¸€ç³»åˆ—ç»Ÿè®¡æµ‹è¯•ä¸­è·å¾—çš„ä¸€ç»„æ¡ä»¶ç‹¬ç«‹æ€§ç»“æœ æ¥æ¢å¤å› æœå›¾ã€‚</p><p>å½“ä»æ•°æ® $\boldsymbol{X}$ ä¸­å·²ç»å­¦ä¹ åˆ°æ¯ä¸€å¯¹å˜é‡é—´æ¡ä»¶ç‹¬ç«‹æ£€éªŒçš„æœ€å°æµ‹è¯•ç»Ÿè®¡é‡ï¼Œå¦‚p-valueï¼Œå› æ­¤å¯ä»¥æ„é€ å‡ºæµ‹è¯•ç»Ÿè®¡é‡çŸ©é˜µ $\boldsymbol{P}$ ï¼Œå…¶ä¸­å¯¹è§’çº¿ä¸Šçš„å…ƒç´ å‡ä¸º0ï¼Œå…ƒç´  $P_{i,j}$ è¡¨ç¤º $\boldsymbol{x}_i , \boldsymbol{x}_j$ é—´çš„æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒçš„æµ‹è¯•ç»Ÿè®¡é‡ï¼Œå‡è®¾æ£€éªŒçš„æ˜¾è‘—æ€§æ°´å¹³ä¸º $\alpha$ ã€‚ $f$ è¡¨ç¤ºæ¡ä»¶ç‹¬ç«‹æ£€éªŒç»Ÿè®¡é‡çš„å‡½æ•°ï¼Œ $Q$ è¡¨ç¤ºç”¨äºè¯„ä»·å¾—åˆ°çš„ç»Ÿè®¡é‡çŸ©é˜µä¸å›¾ $G$ çš„æ‹Ÿåˆç¨‹åº¦çš„å‡½æ•°ã€‚</p><p>è¯¥ç±»æ–¹æ³•çš„ä¼˜åŒ–é—®é¢˜è¡¨è¿°ä¸º: </p><p>$$\begin{array}{}\min : & Q(\boldsymbol{M}, f(\boldsymbol{P}, \alpha)) \\s.t. & G(\boldsymbol{M}) \in \mathbb{D} \\var: & \boldsymbol{M} \in\{0,1\}^{d \times d} \\\end{array}$$</p>**ä¸¾ä¾‹**: å¦‚åœ¨PC<a href="#ref5">[5]</a>ç®—æ³•ä¸­ï¼Œå­˜åœ¨å‡è®¾æ£€éªŒ $\left\{\begin{array}{}H_0: & \boldsymbol{x}_i \perp \boldsymbol{x}_j \\H_1: & \boldsymbol{x}_i \perp / \boldsymbol{x}_j\end{array}\right.$ ï¼Œå½“æµ‹è¯•ç»Ÿè®¡é‡å°äºæ˜¾è‘—æ€§æ°´å¹³æ—¶ï¼Œ $H_1$ æˆç«‹ï¼Œå³æœ‰<p>$$Q(\boldsymbol{M}, f(\boldsymbol{P}, \alpha))=\sum_{i, j=1: \mathrm{d}} M_{i, j} \cdot\left(P_{i, j}-\alpha\right)$$</p>è¡¨ç¤ºä»æ•°æ®ä¸­å¾—åˆ°çš„(æ¡ä»¶)ç‹¬ç«‹çº¦æŸåœ¨å›¾å¾—åˆ°çš„(æ¡ä»¶)ç‹¬ç«‹çº¦æŸé›†åˆä¸­æœªå‡ºç°çš„æ•°é‡ã€‚<h3 id="3-2-2-Score-based-Method"><a href="#3-2-2-Score-based-Method" class="headerlink" title="3.2.2 Score-based Method"></a>3.2.2 Score-based Method</h3><p>åŸºäºå¾—åˆ†çš„ç®—æ³•æœ€å¤§åŒ–å›¾ $G$ ä¸è§‚æµ‹æ•°æ® $\boldsymbol{X}$ ä¹‹é—´çš„é€‚åº”åº¦, æ¥æ„å»ºå› æœç»“æ„ã€‚è¯¥ç±»æ–¹æ³•çš„ä¼˜åŒ–é—®é¢˜è¡¨è¿°ä¸º: </p><p>$$\begin{array}{ll}\max &S\left( \boldsymbol{M} ,\boldsymbol{X} \right)  \\ \text{s.t.} &G\left( \boldsymbol{M} \right)  \in \mathbb{D} \\ \text{var} &\boldsymbol{M} \in \left\{ 0,1\right\}^{d\times d}  \end{array}$$</p>å…¶ä¸­DAGçº¦æŸåœ¨<a href="#ref6">[6]</a>ä¸­è¢«é‡å†™ä¸º $\text{tr} \left( {}e^{\boldsymbol{M} \circ \boldsymbol{M} }\right)  -d=0$ , è¿™ä½¿å¾—ç›®æ ‡å‡½æ•°å¯ä»¥è¢«è¿ç»­ä¼˜åŒ–ã€‚$S(\cdot)$ä¸ºå›¾ä¸è§‚æµ‹åŠ©å±€ä¹‹é—´çš„é€‚åº”åº¦å¾—åˆ†, å¯ç”¨çš„å¾—åˆ†å‡½æ•°åŒ…æ‹¬BIC(GES<a href="#ref7">[7]</a>)ã€Bde<a href="#ref8">[8]</a>ã€Bge<a href="#ref9">[9]</a>ã€‚ä¸åŒçš„æ–¹æ³•å¾€å¾€é‡‡ç”¨ä¸åŒçš„æœç´¢ç®—æ³•åœ¨å›¾ç©ºé—´ä¸­ä¸å“¦è¯ä¸Šè¿°ç›®æ ‡å‡½æ•°, å¦‚: è´ªå¿ƒæœç´¢(greedy search)<a href="#ref8">[8]</a>ã€é¡ºåºæŸ¥æ‰¾(order search)<a href="#ref10">[10]</a>ã€åæ ‡ä¸‹é™<a href="#ref5">[5]</a>ã€‚<p><strong>ä¸¾ä¾‹</strong>: NOTEARS<a href="#ref11">[11]]</a>ä¸­çš„å¾—åˆ†å‡½æ•°ä¸º</p><p>$$\mathcal{S}(\boldsymbol{M}, \boldsymbol{X})=\frac{1}{2 n} \sum_{t=1}^n\left\|\boldsymbol{x}_{t,:}-\boldsymbol{f}\left(\boldsymbol{M}, \boldsymbol{x}_{t,:}\right)\right\|_F^2$$</p>$\boldsymbol{x}_{t,:} \in \mathbb{R}^{1 \times d}$ è¡¨ç¤ºç¬¬ $t$ ä¸ªè§‚æµ‹æ ·æœ¬ï¼Œ$f$ ä¸ºç”Ÿæˆæ¨¡å‹<h3 id="3-2-3-Functional-Causal-Model"><a href="#3-2-3-Functional-Causal-Model" class="headerlink" title="3.2.3 Functional Causal Model"></a>3.2.3 Functional Causal Model</h3><p>åŸºäºFCMçš„ç®—æ³•å‡è®¾å˜é‡é—´çš„å› æœå…³ç³»æ»¡è¶³å‡½æ•° $\boldsymbol{x}_j=f(\boldsymbol{x}_i,\boldsymbol{e}<em>j;\boldsymbol{\theta}</em>{i,j})$  , å…¶ä¸­ $\boldsymbol{x}_i, \boldsymbol{x}_j$ åˆ†åˆ«ä¸ºç›´æ¥åŸå› å˜é‡ã€æœå˜é‡ï¼Œ $\boldsymbol{e}_j \in \mathbb{R}^{n}$ è¡¨ç¤ºä¸€äº›ä¸å¯æµ‹é‡å› ç´ æˆ–å™ªéŸ³ã€‚ $\boldsymbol{\epsilon}=[\boldsymbol{e}_1, \cdots, \boldsymbol{e}_d]$ ï¼Œ $\boldsymbol{\theta}$ ä¸ºæ¨¡å‹å‚æ•°ã€‚ä¸‹å¼ä¸­ç”¨ $L(\cdot)$ è¡¨ç¤ºç”¨äºè¡¡é‡å‚æ•° $\boldsymbol{\theta}$ çš„æ¨¡å‹çš„é¢„æµ‹å€¼ä¸å®é™…è§‚æµ‹çš„æ•°æ® $\boldsymbol{x}_j$ é—´çš„æ‹Ÿåˆç¨‹åº¦çš„å‡½æ•°ã€‚è¯¥ç±»æ–¹æ³•çš„ä¼˜åŒ–é—®é¢˜è¡¨è¿°ä¸º: </p><p>$$\begin{array}{}\min : & \sum_{i, j=1: \mathrm{d}} M_{i, j} \cdot\left(L\left(\boldsymbol{x}_j, f\left(\boldsymbol{x}_i, \theta_{i, j}\right)\right)+Q\left(\boldsymbol{x}_i, \boldsymbol{x}_j-f\left(\boldsymbol{x}_i, \theta_{i, j}\right)\right)\right) \\s.t. & G(M) \in \mathbb{D} \\var: & \boldsymbol{\theta}, \boldsymbol{M} \in\{0,1\}^{d \times d}\end{array}$$</p>è®¾æœ‰å‡è®¾ $H_1: \boldsymbol{x}_i \perp (\boldsymbol{x}_j - f(\boldsymbol{x}_i,\boldsymbol{\theta}_{i,j})$ ï¼Œ$Q\left(\boldsymbol{x}_i, \boldsymbol{x}_j-f\left(\boldsymbol{x}_i, \theta_{i, j}\right)\right)$ è¡¨ç¤º $\boldsymbol{x}_i$ å’Œ $\boldsymbol{x}_j-f\left(\boldsymbol{x}_i, \theta_{i, j}\right)$ ä¹‹é—´çš„ç‹¬ç«‹æ€§æ£€éªŒçš„æµ‹è¯•ç»Ÿè®¡é‡ï¼Œå½“å…¶å°äºæ˜¾è‘—æ€§æ°´å¹³ $\alpha$ æ—¶æ¥å— $H1$ ï¼ˆè¿™ç§æ–¹æ³•å¯¹åº”äºä¸€ç±»å‡è®¾æ£€éªŒæ–¹æ³•ï¼šregression-based independence testï¼‰ã€‚<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a name="ref1">[1]</a> Pearl, J. (2019). The seven tools of causal inference, with reflections on machine learning. <em>Communications of the ACM</em>, <em>62</em>(3), 54-60.</p><p><a name="ref2">[2]</a> Glymour, Clark, Kun Zhang, and Peter Spirtes. â€œReview of causal discovery methods based on graphical models.â€ <em>Frontiers in genetics</em> 10 (2019): 524.</p><p><a name="ref3">[3]Â </a>Goldberg, L. R. (2019). The Book of Why: The New Science of Cause and Effect: by Judea Pearl and Dana Mackenzie, <em>Basic Books</em> (2018). ISBN: 978-0465097609.</p><p><a name="ref4">[4]</a> Tu, R., Zhang, K., KjellstrÃ¶m, H., &amp; Zhang, C. (2022). Optimal transport for causal discovery. In *ICLR 2022-The Tenth International Conference on Learning Representations (Virtual), Apr 25th-29th, 2022.</p><p><a name="ref5">[5]</a> Kalisch, Markus, and Peter BÃ¼hlman. â€œEstimating high-dimensional directed acyclic graphs with the PC-algorithm.â€ Journal of Machine Learning Research 8.3 (2007).</p><p><a name="ref6">[6]</a> Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. 2018. DAGs with NO TEARS: continuous optimization for structure learning. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPSâ€™18). Curran Associates Inc., Red Hook, NY, USA, 9492â€“9503.</p><p><a name="ref7">[7]</a> D. M. Chickering and D. Heckerman. Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables. Machine Learning, 29(2-3):181â€“212, 1997.</p><p><a name="ref8">[8]</a> D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine learning, 20(3):197â€“243, 1995.</p><p><a name="ref9">[9]</a> J. Kuipers, G. Moffa, and D. Heckerman. Addendum on the scoring of gaussian directed acyclic graphical models. The Annals of Statistics, pages 1689â€“1691, 2014.</p><p><a name="ref10">[10]</a> F. Fu and Q. Zhou. Learning sparse causal Gaussian networks with experimental intervention: Regularization and coordinate descent. Journal of the American Statistical Association, 108(501):288â€“300, 2013. </p><p><a name="ref11">[11]</a> Zheng, X., Aragam, B., Ravikumar, P. K., &amp; Xing, E. P. (2018). Dags with no tears: Continuous optimization for structure learning. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>.</p><p><a name="ref12">[12]</a> Shimizu, S., Hoyer, P. O., HyvÃ¤rinen, A., Kerminen, A., &amp; Jordan, M. (2006). A linear non-Gaussian acyclic model for causal discovery. <em>Journal of Machine Learning Research</em>, <em>7</em>(10).</p><p><a name="ref13">[13]</a> Matthew J. Vowels, Necati Cihan Camgoz, and Richard Bowden. 2022. Dâ€™ya Like DAGs? A Survey on Structure Learning and Causal Discovery. ACM Comput. Surv. Just Accepted (March 2022). <a href="https://doi.org/10.1145/3527154">https://doi.org/10.1145/3527154</a></p><p><a name="ref14">[14]</a> Starmans, R. (2020). Prometheus unbound or Paradise regained: the concept of Causality in the contemporary AI-Data Science debate. <em>Journal de la SociÃ©tÃ© FranÃ§aise de Statistique</em>, <em>161</em>(1), 4-41.</p><p><a name="ref15">[15]</a> Cheeseman, P. (1985). In defense of probability. In Proceedings of the Ninth International Joint Conference on AI (IJCAI, 1983).</p><p><a name="ref16">[16]</a> Williamson, J. (2009). Probabilistic theories of causality. <em>The Oxford handbook of causation</em>, 185-212.</p><p><a name="ref17">[17]</a> Good, I. J. (1959). A theory of causality. <em>The British Journal for the Philosophy of Science</em>, <em>9</em>(36), 307-310.</p><p><a name="ref18">[18]</a> Good, I. J. (1961). A causal calculus (I). <em>The British journal for the philosophy of science</em>, <em>11</em>(44), 305-318.</p><p><a name="ref19">[19]</a> Good, I. J. (1961). A causal calculus (II). <em>The British journal for the philosophy of science</em>, <em>12</em>(45), 43-51.</p><p><a name="ref20">[20]</a> Pearl, J. (2009). Causal inference in statistics: An overview. <em>Statistics surveys</em>, <em>3</em>, 96-146.</p><h2 id="2020-2022æœ€æ–°è®ºæ–‡åˆ—è¡¨"><a href="#2020-2022æœ€æ–°è®ºæ–‡åˆ—è¡¨" class="headerlink" title="2020-2022æœ€æ–°è®ºæ–‡åˆ—è¡¨"></a>2020-2022æœ€æ–°è®ºæ–‡åˆ—è¡¨</h2><ol><li>Jalaldoust, A., HlavÃ¡ÄkovÃ¡-Schindler, K., &amp; Plant, C. (2022, June). Causal Discovery in Hawkes Processes by Minimum Description Length. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (Vol. 36, No. 6, pp. 6978-6987).ã€é«˜ç»´Hawkesåºåˆ—ä¸­çš„Grange causal graph learningã€‘</li><li>Zhang, H., Zhang, K., Zhou, S., Guan, J., &amp; Zhang, J. (2021, May). Testing independence between linear combinations for causal discovery. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (Vol. 35, No. 7, pp. 6538-6546).ã€çº¿æ€§éé«˜æ–¯ç»“æ„æ–¹ç¨‹æ¨¡å‹ä¸‹ä¸¤ä¸ªçº¿æ€§ç»„åˆä¹‹é—´çš„ç‹¬ç«‹æ€§â€”â€”æ¡ä»¶ç‹¬ç«‹æ€§æµ‹è¯•ä¸­çš„ä¸€ä¸ªç‰¹æ®Šé—®é¢˜ã€‘</li><li>Lu, N. Y., Zhang, K., &amp; Yuan, C. (2021, May). Improving causal discovery by optimal bayesian network learning. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (Vol. 35, No. 10, pp. 8741-8748).ã€æå‡ºäº†ä¸€ç§åŸºäºåˆ†æ•°çš„æ–¹æ³•ä¸­, ä¸€ç§æ–°çš„ç©·ä¸¾ä¼˜åŒ–æ–¹æ³•ã€‘</li><li>Hyttinen, A., Eberhardt, F., &amp; JÃ¤rvisalo, M. (2014, July). Constraint-based Causal Discovery: Conflict Resolution with Answer Set Programming. In <em>UAI</em> (pp. 340-349).ã€å°†å› æœå›¾æœç´¢é—®é¢˜, è§†ä¸ºå¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜ã€‘</li><li>Dhir, A., &amp; Lee, C. M. (2020, April). Integrating overlapping datasets using bivariate causal discovery. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (Vol. 34, No. 04, pp. 3781-3790).ã€ä»å¤šä¸ªæ•°æ®é›†ä¸­å­¦ä¹ ä¸€è‡´çš„å› æœç»“æ„çš„é—®é¢˜ã€‘</li><li>Huang, B., Zhang, K., Gong, M., &amp; Glymour, C. (2020, April). Causal discovery from multiple data sets with non-identical variable sets. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (Vol. 34, No. 06, pp. 10153-10161).ã€å…·æœ‰ä¸åŒå˜é‡é›†çš„å¤šä¸ªæ•°æ®é›†çš„å› æœå‘ç°ã€‘</li><li>Maeda, T. N., &amp; Shimizu, S. (2020, June). RCD: Repetitive causal discovery of linear non-Gaussian acyclic models with latent confounders. In <em>International Conference on Artificial Intelligence and Statistics</em> (pp. 735-745). PMLR.ã€å—æ½œåœ¨æ··æ‚å› ç´ å½±å“çš„æ•°æ®ä¸­, å‘ç°åˆ©ç”¨å‡½æ•°æ¨¡å‹æ¥å‘ç°å› æœ(ä»¥å¾€é€šå¸¸æ˜¯åŸºäºçº¦æŸ)ã€‘</li><li>Tu, R., Zhang, C., Ackermann, P., Mohan, K., KjellstrÃ¶m, H., &amp; Zhang, K. (2019, April). Causal discovery in the presence of missing data. In <em>The 22nd International Conference on Artificial Intelligence and Statistics</em> (pp. 1762-1770). PMLR.ã€<strong>ç¼ºå¤±æ•°æ®ä¸­çš„å› æœå‘ç°</strong>ã€‘</li><li>Feng, G., Yu, K., Wang, Y., Yuan, Y., &amp; DjuriÄ‡, P. M. (2020, May). Improving convergent cross mapping for causal discovery with Gaussian processes. In <em>ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> (pp. 3692-3696). IEEE.ã€è€¦åˆæ—¶é—´åºåˆ—ä¹‹é—´çš„å› æœå‘ç°ã€‘</li><li>Lippe, P., Cohen, T., &amp; Gavves, E. (2021). Efficient neural causal discovery without acyclicity constraints. <em>arXiv preprint arXiv:2107.10483</em>.ã€ä¸€ä¸ªåŸºäºç¥ç»ç½‘ç»œ+score basedçš„å› æœå‘ç°ã€‘</li><li>Tu, R., Zhang, K., KjellstrÃ¶m, H., &amp; Zhang, C. (2022). Optimal transport for causal discovery. In <em>ICLR 2022-The Tenth International Conference on Learning Representations (Virtual), Apr 25th-29th, 2022</em>.ã€ç”¨<strong>æœ€ä¼˜ä¼ è¾“</strong>ç†è®ºé‡å†™äº†FCMçš„æ–¹æ³•, å¹¶ä»¥ä¼˜åŒ–çš„å½¢å¼åšä¼˜åŒ–, å®ç°å› æœå‘ç°ã€‘</li><li>Zhu, S., Ng, I., &amp; Chen, Z. (2019, September). Causal Discovery with Reinforcement Learning. In <em>International Conference on Learning Representations</em>.ã€è¿™ä¸ªæ–‡ç« é‡Œå¯¹<strong>å„ç±»ä¼˜åŒ–æ–¹æ³•</strong>æœ‰æ¯”è¾ƒå¥½çš„è°ƒç ”ã€‘</li><li>Huang, B., Zhang, K., Gong, M., &amp; Glymour, C. (2019, May). Causal discovery and forecasting in nonstationary environments with state-space models. In <em>International conference on machine learning</em> (pp. 2901-2910). PMLR.ã€éå¹³ç¨³æ—¶é—´åºåˆ—ä¸­çš„å› æœå‘ç°ã€‘</li><li>Empirical Bayesian Approaches for Robust Constraint-based Causal Discovery under Insufficient Dataã€å°æ•°æ®ã€éå¹³ç¨³ã€‘</li><li>Brouillard, P., Lachapelle, S., Lacoste, A., Lacoste-Julien, S., &amp; Drouin, A. (2020). Differentiable causal discovery from interventional data. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 21865-21877.ã€å¼±å¿ è¯šå‡è®¾çš„å› æœå‘ç°ã€‘</li><li>Mokhtarian, E., Akbari, S., Ghassami, A., &amp; Kiyavash, N. (2021, August). A recursive markov boundary-based approach to causal structure learning. In The KDDâ€™21 Workshop on Causal Discovery (pp. 26-54). PMLR.ã€åŸºäºçº¦æŸçš„æ–¹æ³•, ç”¨é€’å½’çš„ä¼˜åŒ–æ–¹æ³•ã€‘</li></ol><h2 id="å€¼å¾—å…³æ³¨çš„æœ€æ–°å·¥ä½œ"><a href="#å€¼å¾—å…³æ³¨çš„æœ€æ–°å·¥ä½œ" class="headerlink" title="å€¼å¾—å…³æ³¨çš„æœ€æ–°å·¥ä½œ"></a>å€¼å¾—å…³æ³¨çš„æœ€æ–°å·¥ä½œ</h2><ol><li>Bhattacharya, R., Nagarajan, T., Malinsky, D., &amp; Shpitser, I. (2021, March). Differentiable causal discovery under unmeasured confounding. In <em>International Conference on Artificial Intelligence and Statistics</em> (pp. 2314-2322). PMLR.ã€confounded systemsä¸­çš„å› æœå›¾æ¨¡å‹å‘ç°, å…¶ä¸­èŠ‚ç‚¹çš„å®šä¹‰å¯èƒ½ä¸ä¸€æ ·ã€‘</li><li>Brouillard, P., Lachapelle, S., Lacoste, A., Lacoste-Julien, S., &amp; Drouin, A. (2020). Differentiable causal discovery from interventional data. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 21865-21877.ã€å’Œä¸Šé¢çš„æœ‰ç‚¹åƒã€‘</li><li>S. Ren, H. Yin, M. Sun and P. Li, â€œCausal Discovery with Flow-based Conditional Density Estimation,â€ <em>2021 IEEE International Conference on Data Mining (ICDM)</em>, 2021, pp. 1300-1305, doi: 10.1109/ICDM51629.2021.00161.ã€æµæ¨¡å‹æ¥ä¼°è®¡å˜é‡çš„è”åˆæ¦‚ç‡å¯†åº¦, æ ¹æ®æ¡ä»¶å¯†åº¦ä¼°è®¡çš„æ–¹å·®æ¨æ–­æ¯ä¸ªæ½œåœ¨å› æœæ–¹å‘çš„åˆ†æ•°, æˆ‘ä»¬çš„å› æœå‘ç°æ–¹æ³•å‡è½»äº†ä¼ ç»Ÿæ–¹æ³•æ‰€åšçš„é™åˆ¶æ€§å‡è®¾, æ›´å¥½åœ°æ•æ‰å„ç§é—®é¢˜é¢†åŸŸä¸­ä»¥ä»»æ„å½¢å¼å‡ºç°çš„æ•°æ®ä¹‹é—´çš„å¤æ‚å› æœå…³ç³»ã€‘</li><li>Zhang, H., Zhou, S., Zhang, K., &amp; Guan, J. (2022, June). Residual Similarity Based Conditional Independence Test and Its Application in Causal Discovery. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (Vol. 36, No. 5, pp. 5942-5949).ã€CIè½¬ä¼˜åŒ–é—®é¢˜ã€‘</li></ol><hr><h2 id="ToolBox"><a href="#ToolBox" class="headerlink" title="ToolBox"></a>ToolBox</h2><ul><li>gCastle <a href="https://github.com/huawei-noah/trustworthyAI/tree/master/gcastle">URL</a></li></ul><blockquote><p>gCastleæ˜¯åä¸ºè¯ºäºšæ–¹èˆŸå®éªŒå®¤è‡ªç ”çš„å› æœç»“æ„å­¦ä¹ å·¥å…·é“¾, ä¸»è¦çš„åŠŸèƒ½å’Œæ„¿æ™¯åŒ…æ‹¬: </p><ol><li>æ•°æ®ç”ŸæˆåŠå¤„ç†: åŒ…å«å„ç§æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆç®—å­, æ•°æ®è¯»å–ç®—å­, æ•°æ®å¤„ç†ç®—å­(å¦‚å…ˆéªŒçŒå…¥, å˜é‡é€‰æ‹©, CRAM)ã€‚</li><li>å› æœå›¾æ„å»º: æä¾›äº†ä¸€ä¸ªå› æœç»“æ„å­¦ä¹ pythonç®—æ³•åº“, åŒ…å«äº†ä¸»æµçš„å› æœå­¦ä¹ ç®—æ³•ä»¥åŠæœ€è¿‘å…´èµ·çš„åŸºäºæ¢¯åº¦çš„å› æœç»“æ„å­¦ä¹ ç®—æ³•ã€‚</li><li>å› æœè¯„ä»·: æä¾›äº†å¸¸ç”¨çš„å› æœç»“æ„å­¦ä¹ æ€§èƒ½è¯„ä»·æŒ‡æ ‡, åŒ…æ‹¬F1, SHD, FDR, TPR, FDR, NNZç­‰</li></ol></blockquote><ul><li>Causal Discovery Toolbox <a href="https://github.com/FenTechSolutions/CausalDiscoveryToolbox">URL</a></li></ul><blockquote><p>The Causal Discovery Toolbox is a package for causal inference in graphs and in the pairwise settings for Python&gt;=3.5.<br>Tools for graph structure recovery and dependencies are included. The package is based on Numpy, Scikit-learn, Pytorch and R.</p></blockquote><ul><li>Tigramite <a href="https://github.com/jakobrunge/tigramite">URL</a></li></ul><blockquote><p>Tigramite æ˜¯ä¸€ä¸ªå› æœæ—¶é—´åºåˆ—åˆ†æ python åŒ…ã€‚å®ƒå…è®¸ä»é«˜ç»´æ—¶é—´åºåˆ—æ•°æ®é›†æœ‰æ•ˆåœ°é‡å»ºå› æœå›¾, å¹¶å¯¹è·å¾—çš„å› æœä¾èµ–è¿›è¡Œå»ºæ¨¡,<br>ä»¥è¿›è¡Œå› æœä¸­ä»‹å’Œé¢„æµ‹åˆ†æã€‚å› æœå‘ç°åŸºäºé€‚ç”¨äºç¦»æ•£æˆ–è¿ç»­å€¼æ—¶é—´åºåˆ—çš„çº¿æ€§å’Œéå‚æ•°æ¡ä»¶ç‹¬ç«‹æ€§æµ‹è¯•ã€‚</p><ul><li>åŒ…å«çš„å› æœå‘ç°æ–¹æ³•: PCMCIã€PCMCIplusã€LPCMCI</li><li>åŒ…å«çš„ç‹¬ç«‹æ€§æµ‹è¯•æ–¹æ³•: ParCorrã€GPDC / GPDCtorchã€CMIknnã€CMIsymb</li></ul></blockquote><ul><li>causalDisco: an R package with tools for causal discovery on observational data <a href="https://github.com/annennenne/causalDisco">URL</a></li></ul><blockquote><p>causalDisco åŒ…æ‹¬temporal PCçš„å®ç°</p></blockquote><ul><li>Causal Discovery Tools for Time Series Applications - A Collection of Tutorials <a href="https://github.com/savinims/DATAS_Causal_Discovery">URL</a></li></ul><blockquote><p>ä¸ºå¤§æ°”ç§‘å­¦å®¶æ•°æ®åˆ†æå·¥å…· (DATAS) ç½‘å…³çš„ä¸€éƒ¨åˆ†, ç¼–å†™çš„æ•™ç¨‹ä¾§é‡äºå¤§æ°”ç§‘å­¦åº”ç”¨ã€‚æ•°æ®: <a href="https://datasgateway.colostate.edu/">https://datasgateway.colostate.edu/</a></p><p>æœ¬èµ„æ–™åº“ä¸­è§£é‡Šçš„æ–¹æ³•ä¾§é‡äºè§‚å¯Ÿæ€§ç ”ç©¶, å…¶ä¸­ä¸è¿›è¡Œå—æ§å®éªŒ(ä¾‹å¦‚, æ°”å€™ä¸­çš„æœ‰é’ˆå¯¹æ€§çš„å»ºæ¨¡ç ”ç©¶)æ¥ç¡®å®šåŸå› å’Œå½±å“ã€‚<br>è¿™äº›æ–¹æ³•å…è®¸æ‚¨è¯†åˆ«éœ€è¦ä½¿ç”¨æˆ‘ä»¬ç°æœ‰çš„ç‰¹å®šåº”ç”¨é¢†åŸŸçŸ¥è¯†è¿›ä¸€æ­¥éªŒè¯çš„â€æ½œåœ¨â€å…³ç³»ã€‚</p><p>æ–¹æ³•åŒ…æ‹¬: äºŒå…ƒæ ¼å…°æ°å› æœæ£€éªŒã€PCç¨³å®šç®—æ³•çš„æ—¶é—´åºåˆ—æ‰©å±•</p></blockquote><h2 id="Related-work-with-code"><a href="#Related-work-with-code" class="headerlink" title="Related work with code"></a>Related work with code</h2><p>[1] TCDF: Causal Discovery with Attention-Based Convolutional Neural Networks <a href="https://github.com/M-Nauta/TCDF">URL</a></p><blockquote><p>æ—¶é—´å› æœå‘ç°æ¡†æ¶ (TCDF) æ˜¯åœ¨ PyTorch ä¸­å®ç°çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚ç»™å®šå¤šä¸ªæ—¶é—´åºåˆ—ä½œä¸ºè¾“å…¥, TCDF å‘ç°è¿™äº›æ—¶é—´åºåˆ—ä¹‹é—´çš„å› æœå…³ç³»å¹¶è¾“å‡ºå› æœå›¾ã€‚<br>å®ƒè¿˜å¯ä»¥æ ¹æ®å…¶ä»–æ—¶é—´åºåˆ—é¢„æµ‹ä¸€ä¸ªæ—¶é—´åºåˆ—ã€‚TCDF ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„å·ç§¯ç¥ç»ç½‘ç»œç»“åˆå› æœéªŒè¯æ­¥éª¤ã€‚é€šè¿‡è§£é‡Šå·ç§¯ç½‘ç»œçš„å†…éƒ¨å‚æ•°, TCDF è¿˜å¯ä»¥å‘ç°å› æœä¹‹é—´çš„æ—¶é—´å»¶è¿Ÿã€‚</p></blockquote><p>[2] Amortize Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data <a href="https://github.com/loeweX/AmortizedCausalDiscovery">URL</a></p><blockquote><p>é€šè¿‡ Amortized Causal Discovery, æˆ‘ä»¬å­¦ä¹ ä»å…·æœ‰ä¸åŒæ½œåœ¨å› æœå›¾ä½†å…±äº«åŠ¨æ€çš„æ ·æœ¬ä¸­æ¨æ–­å› æœå…³ç³»ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè·¨æ ·æœ¬è¿›è¡Œæ³›åŒ–, ä»è€Œé€šè¿‡å¢åŠ è®­ç»ƒæ•°æ®å¤§å°æ¥æé«˜æˆ‘ä»¬çš„æ€§èƒ½ã€‚</p></blockquote><p>[3] Causal Discovery from Nonstationary/Heterogeneous Data: Skeleton Estimation and Orientation Determination. IJCAI 2017. <a href="https://github.com/Biwei-Huang/Causal-Discovery-from-Nonstationary-Heterogeneous-Data">URL</a></p><p>[4] Causal Discovery in Heavy-Tailed Models <a href="https://github.com/nicolagnecco/causalXtreme">URL</a></p><p>[5] Differentiable Causal Discovery from Interventional Data <a href="https://github.com/slachapelle/dcdi">URL</a></p><p>[6] Generalized Score Functions for Causal Discovery. KDD, 2018 <a href="https://github.com/Biwei-Huang/Generalized-Score-Functions-for-Causal-Discovery">URL</a></p><blockquote><p>å…·æœ‰å¹¿ä¹‰å¾—åˆ†å‡½æ•°çš„è´ªå©ªç­‰ä»·æœç´¢çš„å› æœç»“æ„å­¦ä¹ (é€‚ç”¨äºæ··åˆè¿ç»­å’Œç¦»æ•£æ•°æ®ã€å…·æœ‰é«˜æ–¯æˆ–éé«˜æ–¯åˆ†å¸ƒçš„æ•°æ®ã€çº¿æ€§æˆ–éçº¿æ€§å› æœæœºåˆ¶ä»¥åŠå…·æœ‰å¤šç»´çš„å˜é‡ã€‚)</p></blockquote><p>[7] Learning the Causal Structure of Copula Models with Latent Variables. UAI. 2018 <a href="https://github.com/cuiruifei/CopulaFactorModel">URL</a></p><p>[8] Data Generating Process to Evaluate Causal Discovery Techniques for Time Series Data, at the Causal Discovery &amp; Causality-Inspired Machine Learning Workshop at NeurIPS 2020. <a href="https://github.com/causalens/cdml-neurips2020">URL</a></p><p>[9] Process Mining Meets Causal Machine Learning: Discovering Causal Rules from Event Logs <a href="https://github.com/zahradbozorgi/CausalRulesDiscovery">URL</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> paper list </tag>
            
            <tag> survey </tag>
            
            <tag> causal discovery </tag>
            
            <tag> structural causal model </tag>
            
            <tag> toolbox </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in IJCAI 2021 (2021.08.19-2021.08.26)</title>
      <link href="/uncategorized/paperlistfile/IJCAI2021/"/>
      <url>/uncategorized/paperlistfile/IJCAI2021/</url>
      
        <content type="html"><![CDATA[<p>Accept papers: <a href="https://ijcai-21.org/program-main-track/">link</a></p><span id="more"></span><h2 id="anomaly-detection-anomaly-outlier-out-of-distribution-one-class-Malware-detection-Fraud-Detection-Fake-News-Detection"><a href="#anomaly-detection-anomaly-outlier-out-of-distribution-one-class-Malware-detection-Fraud-Detection-Fake-News-Detection" class="headerlink" title="anomaly detection (anomaly, outlier, out-of-distribution, one-class, Malware detection, Fraud Detection, Fake News Detection)"></a>anomaly detection (anomaly, outlier, out-of-distribution, one-class, Malware detection, Fraud Detection, Fake News Detection)</h2><ul><li><p>Masked Contrastive Learning for Anomaly Detection</p><p>Hyunsoo Cho (Seoul National University)<br>Jinseok Seol (Seoul National University)<br>Sang-goo Lee (Seoul National University)</p></li><li><p>Weakly-Supervised Spatio-Temporal Anomaly Detection in Surveillance Video</p><p>Jie Wu (Sun Yat-sen University ByteDance Inc.)<br>Wei Zhang (Baidu Inc.)<br>Guanbin Li (Sun Yat-sen University)<br>Wenhao Wu (Baidu Inc.)<br>Xiao Tan (Baidu Inc.)<br>Yingying Li (Baidu Inc.)<br>Errui Ding (Baidu Inc.)<br>Liang Lin (Sun Yat-sen University)</p></li><li><p>RCA: A Deep Collaborative Autoencoder Approach for Anomaly Detection</p><p>Boyang Liu (Michigan State University)<br>Ding Wang (Michigan State University)<br>Kaixiang Lin (Michigan State University)<br>Pang-Ning Tan (Michigan State University)<br>Jiayu Zhou (Michigan State University)</p></li><li><p>Understanding the Effect of Bias in Deep Anomaly Detection</p><p>Ziyu Ye (University of Chicago)<br>Yuxin Chen (University of Chicago)<br>Haitao Zheng (University of Chicago)</p></li><li><p>Likelihood-free Out-of-Distribution Detection with Invertible Generative Models</p><p>Amirhossein Ahmadian (Division of Statistics and Machine Learning, Department of Computer and Information Science, LinkÃ¶ping University)<br>Fredrik Lindsten (Division of Statistics and Machine Learning, Department of Computer and Information Science, LinkÃ¶ping University)</p></li><li><p>MG-DVD: A Real-time Framework for Malware Variant Detection Based on Dynamic Heterogeneous Graph Learning</p><p>Chen Liu (School of Computer Science and Engineering, Beihang University, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China)<br>Bo Li (School of Computer Science and Engineering, Beihang University, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China)<br>Jun Zhao (School of Computer Science and Engineering, Beihang University, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China)<br>Ming Su (School of Computer Science and Engineering, Beihang University, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China)<br>Xu-Dong Liu (School of Computer Science and Engineering, Beihang University, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China)</p></li><li><p>Online Credit Payment Fraud Detection via Structure-Aware Hierarchical Recurrent Neural Network</p><p>Wangli Lin (Alibaba Group, Hangzhou, China)<br>Li Sun (Alibaba Group, Hangzhou, China)<br>Qiwei Zhong (Alibaba Group, Hangzhou, China)<br>Can Liu (Alibaba Group, Hangzhou, China)<br>Jinghua Feng (Alibaba Group, Hangzhou, China)<br>Xiang Ao (Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China)<br>Hao Yang (Alibaba Group, Hangzhou, China)</p></li></ul><h2 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h2><ul><li><p>Time-Series Representation Learning via Temporal and Contextual Contrasting</p><p>Emadeldeen Eldele (School of Computer Science and Engineering, Nanyang Technological University, Singapore)<br>Mohamed Ragab (School of Computer Science and Engineering, Nanyang Technological University, Singapore)<br>Zhenghua Chen (Institute for Infocomm Research, A<em>STAR, Singapore)<br>Min Wu (Institute for Infocomm Research, A</em>STAR, Singapore)<br>Chee Keong Kwoh (School of Computer Science and Engineering, Nanyang Technological University, Singapore)<br>Xiaoli Li (Institute for Infocomm Research, A*STAR, Singapore)<br>Cuntai Guan (School of Computer Science and Engineering, Nanyang Technological University, Singapore)</p></li></ul><ul><li><p>TE-ESN: Time Encoding Echo State Network for Prediction Based on Irregularly Sampled Time Series Data</p><p>Chenxi Sun (Key Laboratory of Machine Perception (Ministry of Education), Peking University, Beijing, China. School of Electronics Engineering and Computer Science, Peking University, Beijing, China.)<br>Shenda Hong (National Institute of Health Data Science, Peking University, Beijing, China. Institute of Medical Technology, Health Science Center of Peking University, Beijing, China.)<br>Moxian Song (Key Laboratory of Machine Perception (Ministry of Education), Peking University, Beijing, China. School of Electronics Engineering and Computer Science, Peking University, Beijing, China.)<br>Yen-Hsiu Chou (Key Laboratory of Machine Perception (Ministry of Education), Peking University, Beijing, China. School of Electronics Engineering and Computer Science, Peking University, Beijing, China.)<br>Yongyue Sun (Key Laboratory of Machine Perception (Ministry of Education), Peking University, Beijing, China. School of Electronics Engineering and Computer Science, Peking University, Beijing, China.)<br>Derun Cai (Key Laboratory of Machine Perception (Ministry of Education), Peking University, Beijing, China. School of Electronics Engineering and Computer Science, Peking University, Beijing, China.)<br>Hongyan Li (Key Laboratory of Machine Perception (Ministry of Education), Peking University, Beijing, China. School of Electronics Engineering and Computer Science, Peking University, Beijing, China.)</p></li><li><p>Time-Aware Multi-Scale RNNs for Time Series Modeling</p><p>Zipeng Chen (School of Computer Science and Engineering, South China University of Technology, Guangzhou, China)<br>Qianli Ma (School of Computer Science and Engineering, South China University of Technology, Guangzhou, China Key Laboratory of Big Data and Intelligent Robot (South China University of Technology), Ministry of Education)<br>Zhenxi Lin (School of Computer Science and Engineering, South China University of Technology, Guangzhou, China)</p></li><li><p>Adversarial Spectral Kernel Matching for Unsupervised Time Series Domain Adaptation</p><p>Qiao Liu (School of Computer Science and Engineering, Southeast University, Nanjing, 210096, China MOE Key Laboratory of Computer Network and Information Integration (Southeast University))<br>Hui Xue (School of Computer Science and Engineering, Southeast University, Nanjing, 210096, China MOE Key Laboratory of Computer Network and Information Integration (Southeast University))</p></li><li><p>Two Birds with One Stone: Series Saliency for Accurate and Interpretable Multivariate Time Series Forecasting</p><p>Qingyi Pan (High Performance Computing Group, Dept. of Comp. Sci. and Tech., BNRist Center, Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University, Beijing, 100084 China)<br>Wenbo Hu (RealAI)<br>Ning Chen (High Performance Computing Group, Dept. of Comp. Sci. and Tech., BNRist Center, Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University, Beijing, 100084 China)</p></li></ul><h2 id="heterogeneous-multi-source"><a href="#heterogeneous-multi-source" class="headerlink" title="heterogeneous (multi-source)"></a>heterogeneous (multi-source)</h2><ul><li><p>Adapting Meta Knowledge with Heterogeneous Information Network for COVID-19 Themed Malicious Repository Detection</p><p>Yiyue Qian (Department of Computer and Data Sciences, Case Western Reserve University, USA)<br>Yiming Zhang (Department of Computer and Data Sciences, Case Western Reserve University, USA)<br>Yanfang Ye (Department of Computer and Data Sciences, Case Western Reserve University, USA)<br>Chuxu Zhang (Department of Computer Science, Brandeis University, USA)</p></li><li><p>Temporal Heterogeneous Information Network Embedding</p><p>Hong Huang (National Engineering Research Center for Big Data Technology and System Service Computing Technology and Systems Laboratory Huazhong University of Science and Technology, China)<br>Ruize Shi (National Engineering Research Center for Big Data Technology and System Service Computing Technology and Systems Laboratory Huazhong University of Science and Technology, China)<br>Wei Zhou (Huazhong University of Science and Technology, China)<br>Xiao Wang (Beijing University of Posts and Telecommunications, China)<br>Hai Jin (National Engineering Research Center for Big Data Technology and System Service Computing Technology and Systems Laboratory Huazhong University of Science and Technology, China)<br>Xiaoming Fu (University of Goettingen, Germany)</p></li></ul><h2 id="Graph-Representation-Learning"><a href="#Graph-Representation-Learning" class="headerlink" title="Graph Representation Learning"></a>Graph Representation Learning</h2><ul><li><p>MG-DVD: A Real-time Framework for Malware Variant Detection Based on Dynamic Heterogeneous Graph Learning</p><p>Chen Liu (School of Computer Science and Engineering, Beihang University, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China)<br>Bo Li (School of Computer Science and Engineering, Beihang University, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China)<br>Jun Zhao (School of Computer Science and Engineering, Beihang University, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China)<br>Ming Su (School of Computer Science and Engineering, Beihang University, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China)<br>Xu-Dong Liu (School of Computer Science and Engineering, Beihang University, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, China)</p></li><li><p>Heterogeneous Graph Information Bottleneck</p><p>Liang Yang (Hebei University of Technology, Tianjin, China Institute of Information Engineering, CAS, Beijing, China)<br>Fan Wu (Hebei University of Technology, Tianjin, China)<br>Zichen Zheng (Hebei University of Technology, Tianjin, China)<br>Bingxin Niu (Hebei University of Technology, Tianjin, China)<br>Junhua Gu (Hebei University of Technology, Tianjin, China)<br>Chuan Wang (Institute of Information Engineering, CAS, Beijing, China)<br>Xiaochun Cao (Institute of Information Engineering, CAS, Beijing, China)<br>Yuanfang Guo (Beihang University, Beijing, China)</p></li><li><p>Learning Attributed Graph Representation with Communicative Message Passing Transformer</p><p>Jianwen Chen (School of Computer Science and Engineering, Sun Yat-sen University)<br>Shuangjia Zheng (School of Computer Science and Engineering, Sun Yat-sen University Galixir Technologies Ltd, Beijing)<br>Ying Song (School of System Science and Engineering, Sun Yat-sen University)<br>Jiahua Rao (School of Computer Science and Engineering, Sun Yat-sen University Galixir Technologies Ltd, Beijing)<br>Yuedong Yang (School of Computer Science and Engineering, Sun Yat-sen University Key Laboratory of Machine Intelligence and Advanced Computing, Sun Yat-sen University)</p></li><li><p>CuCo: Graph Representation with Curriculum Contrastive Learning</p><p>Guanyi Chu (Beijing University of Posts and Telecommunications)<br>Xiao Wang (Beijing University of Posts and Telecommunications)<br>Chuan Shi (Beijing University of Posts and Telecommunications)<br>Xunqiang Jiang (Beijing University of Posts and Telecommunications)</p></li><li><p>Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning</p><p>Ming Jin (Monash University)<br>Yizhen Zheng (Monash University)<br>Yuan-Fang Li (Monash University)<br>Chen Gong (Nanjing University of Science and Technology)<br>Chuan Zhou (Chinese Academy of Sciences)<br>Shirui Pan (Monash University)</p></li></ul><h2 id="sequence"><a href="#sequence" class="headerlink" title="sequence"></a>sequence</h2><ul><li><p>k-Nearest Neighbors by Means of Sequence to Sequence Deep Neural Networks and Memory Networks</p><p>Yiming Xu (Northwestern University)<br>Diego Klabjan (Northwestern University)</p></li><li><p>A Novel Sequence-to-Subgraph Framework for Diagnosis Classification</p><p>Jun Chen (Baidu Inc, Beijing 100193, China)<br>Quan Yuan (Baidu Inc, Beijing 100193, China)<br>Chao Lu (Baidu Inc, Beijing 100193, China)<br>Haifeng Huang (Baidu Inc, Beijing 100193, China)</p></li><li><p>Multi-series Time-aware Sequence Partitioning for Disease Progression Modeling</p><p>Xi Yang (Department of Computer Science, North Carolina State University)<br>Yuan Zhang (Department of Computer Science, North Carolina State University)<br>Min Chi (Department of Computer Science, North Carolina State University)</p></li></ul><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><ul><li><p>Regularizing Variational Autoencoder with Diversity and Uncertainty Awareness</p><p>Dazhong Shen (School of Computer Science and Technology, University of Science and Technology of China Baidu Talent Intelligence Center)<br>Chuan Qin (Baidu Talent Intelligence Center)<br>Chao Wang (School of Computer Science and Technology, University of Science and Technology of China Baidu Talent Intelligence Center)<br>Hengshu Zhu (Baidu Talent Intelligence Center)<br>Enhong Chen (School of Computer Science and Technology, University of Science and Technology of China)<br>Hui Xiong (Rutgers, The State University of New Jersey)</p></li></ul><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><ul><li><p>Change Matters: Medication Change Prediction with Recurrent Residual Networks</p><p>Chaoqi Yang (University of Illinois at Urbana-Champaign)<br>Cao Xiao (IQVIA)<br>Lucas Glass (IQVIA)<br>Jimeng Sun (University of Illinois at Urbana-Champaign)</p></li><li><p>State-Based Recurrent SPMNs for Decision-Theoretic Planning under Partial Observability</p><p>Layton Hayes (Institute for AI, University of Georgia, Athens GA 30602)<br>Prashant Doshi (Institute for AI, University of Georgia, Athens GA 30602 Department of Computer Science, University of Georgia, Athens GA 30602)<br>Swaraj Pawar (Dept. of Computer Science, University of Georgia, Athens GA 30602)<br>Hari Teja Tatavarti (Institute for AI, University of Georgia, Athens GA 30602)</p></li><li><p>Online Credit Payment Fraud Detection via Structure-Aware Hierarchical Recurrent Neural Network</p><p>Wangli Lin (Alibaba Group, Hangzhou, China)<br>Li Sun (Alibaba Group, Hangzhou, China)<br>Qiwei Zhong (Alibaba Group, Hangzhou, China)<br>Can Liu (Alibaba Group, Hangzhou, China)<br>Jinghua Feng (Alibaba Group, Hangzhou, China)<br>Xiang Ao (Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China)<br>Hao Yang (Alibaba Group, Hangzhou, China)</p></li></ul><h2 id="causal-analysis"><a href="#causal-analysis" class="headerlink" title="causal analysis"></a>causal analysis</h2><ul><li><p>Causal Discovery with Multi-Domain LiNGAM for Latent Factors</p><p>Yan Zeng (Guangdong University of Technology RIKEN)<br>Shohei Shimizu (Shiga University RIKEN)<br>Ruichu Cai (Guangdong University of Technology)<br>Feng Xie (Peking University)<br>Michio Yamamoto (Okayama University RIKEN)<br>Zhifeng Hao (Guangdong University of Technology Foshan University)</p></li><li><p>Inferring Time-delayed Causal Relations in POMDPs from the Principle of Independence of Cause and Mechanism</p><p>Junchi Liang (Department of Computer Science, Rutgers University, New Jersey, USA)<br>Abdeslam Boularias (Department of Computer Science, Rutgers University, New Jersey, USA)</p></li><li><p>User Retention: A Causal Approach with Triple Task Modeling</p><p>Yang Zhang (Ant Group Beihang University)<br>Dong Wang (Ant Group)<br>Qiang Li (Ant Group)<br>Yue Shen (Ant Group)<br>Ziqi Liu (Ant Group)<br>Xiaodong Zeng (Ant Group)<br>Zhiqiang Zhang (Ant Group)<br>Jinjie Gu (Ant Group)<br>Derek F. Wong (University of Macau)</p></li><li><p>Ordering-Based Causal Discovery with Reinforcement Learning</p><p>Xiaoqiang Wang (State Key Laboratory for Manufacturing Systems Engineering, School of Automation Science and Engineering, Xiâ€™an Jiaotong University)<br>Yali Du (University College London)<br>Shengyu Zhu (Huawei Noahâ€™s Ark Lab)<br>Liangjun Ke (State Key Laboratory for Manufacturing Systems Engineering, School of Automation Science and Engineering, Xiâ€™an Jiaotong University)<br>Zhitang Chen (Huawei Noahâ€™s Ark Lab)<br>Jianye Hao (Huawei Noahâ€™s Ark Lab College of Intelligence and Computing, Tianjin University)<br>Jun Wang (University College London)</p></li><li><p>Dependent Multi-Task Learning with Causal Intervention for Image Captioning</p><p>Wenqing Chen (MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University State Key Lab of Advanced Optical Communication System and Network, Shanghai Jiao Tong University)<br>Jidong Tian (MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University State Key Lab of Advanced Optical Communication System and Network, Shanghai Jiao Tong University)<br>Caoyun Fan (MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University State Key Lab of Advanced Optical Communication System and Network, Shanghai Jiao Tong University)<br>Hao He (MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University State Key Lab of Advanced Optical Communication System and Network, Shanghai Jiao Tong University)<br>Yaohui Jin (MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University State Key Lab of Advanced Optical Communication System and Network, Shanghai Jiao Tong University)</p></li><li><p>Provable Guarantees on the Robustness of Decision Rules to Causal Interventions</p><p>Benjie Wang (University of Oxford)<br>Clare Lyle (University of Oxford)<br>Marta Kwiatkowska (University of Oxford)</p></li><li><p>A Ladder of Causal Distances</p><p>Maxime Peyrard (EPFL)<br>Robert West (EPFL)</p></li></ul><h2 id="correlation-analysis-association-analysis"><a href="#correlation-analysis-association-analysis" class="headerlink" title="correlation analysis (association analysis)"></a>correlation analysis (association analysis)</h2><ul><li><p>Differentially Private Correlation Alignment for Domain Adaptation</p><p>Kaizhong Jin (State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China)<br>Xiang Cheng (State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China)<br>Jiaxi Yang (State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China)<br>Kaiyuan Shen (State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China)</p></li><li><p>Learning Unknown from Correlations: Graph Neural Network for Inter-novel-protein Interaction Prediction</p><p>Guofeng Lv (SenseTime Research)<br>Zhiqiang Hu (SenseTime Research)<br>Yanguang Bi (SenseTime Research)<br>Shaoting Zhang (SenseTime Research)</p></li><li><p> Correlation-Guided Representation for Multi-Label Text Classification</p></li></ul><p>  Qian-Wen Zhang (Tencent Cloud Xiaowei, Beijing 100080, China)<br>  Ximing Zhang (Beijing University of Posts and Telecommunications, Beijing 100876, China)<br>  Zhao Yan (Tencent Cloud Xiaowei, Beijing 100080, China)<br>  Ruifang Liu (Beijing University of Posts and Telecommunications, Beijing 100876, China)<br>  Yunbo Cao (Tencent Cloud Xiaowei, Beijing 100080, China)<br>  Min-Ling Zhang (School of Computer Science and Engineering, Southeast University, Nanjing 210096, China Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China)</p><ul><li><p>Location Predicts You: Location Prediction via Bi-direction Speculation and Dual-level Association</p><p>Xixi Li (National Engineering Research Center for Multimedia Software (NERCMS), School of Computer Science, Wuhan University)<br>Ruimin Hu (National Engineering Research Center for Multimedia Software (NERCMS), School of Computer Science, Wuhan University)<br>Zheng Wang (Research Institute for an Inclusive Society through Engineering (RIISE), The University of Tokyo Department of Information and Communication Engineering, The University of Tokyo)<br>Toshihiko Yamasaki (Research Institute for an Inclusive Society through Engineering (RIISE), The University of Tokyo Department of Information and Communication Engineering, The University of Tokyo)</p></li></ul><h2 id="clustering"><a href="#clustering" class="headerlink" title="clustering"></a>clustering</h2><h2 id="About-distribution"><a href="#About-distribution" class="headerlink" title="About distribution"></a>About distribution</h2><h2 id="missing-value-amp-irregularly-sampled-time-series-Incomplete-imputation-â€¦"><a href="#missing-value-amp-irregularly-sampled-time-series-Incomplete-imputation-â€¦" class="headerlink" title="missing value &amp; irregularly sampled time series [Incomplete, imputation, â€¦]"></a>missing value &amp; irregularly sampled time series [Incomplete, imputation, â€¦]</h2><h2 id="interpretable-Understanding-explanation-Attribution-â€¦"><a href="#interpretable-Understanding-explanation-Attribution-â€¦" class="headerlink" title="interpretable [Understanding, explanation, Attribution â€¦]"></a>interpretable [Understanding, explanation, Attribution â€¦]</h2>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in SIGKDD 2021 (2021.08.14-2021.08.18)</title>
      <link href="/uncategorized/paperlistfile/KDD2021/"/>
      <url>/uncategorized/paperlistfile/KDD2021/</url>
      
        <content type="html"><![CDATA[<p>Accept papers: <a href="https://kdd.org/kdd2021/accepted-papers/index">link</a></p><span id="more"></span><h2 id="anomaly-detection-anomaly-outlier-out-of-distribution-one-class-Malware-detection-Fraud-Detection-Fake-News-Detectionâ€¦"><a href="#anomaly-detection-anomaly-outlier-out-of-distribution-one-class-Malware-detection-Fraud-Detection-Fake-News-Detectionâ€¦" class="headerlink" title="anomaly detection (anomaly, outlier, out-of-distribution, one-class, Malware detection, Fraud Detection, Fake News Detectionâ€¦)"></a>anomaly detection (anomaly, outlier, out-of-distribution, one-class, Malware detection, Fraud Detection, Fake News Detectionâ€¦)</h2><ul><li><p>ELITE : Robust Deep Anomaly Detection with Meta Gradient</p><p>Authors: Huayi Zhang (WPI); Lei Cao (MIT)$^{\star}$; Peter VanNostrand (WPI); Samuel Madden (MIT); Elke A Rundensteiner (WPI)</p></li><li><p>Joint Optimization of Known and Unknown Anomaly Detection</p><p>Authors: Guansong Pang (University of Adelaide)$^{\star}$; Anton van den Hengel (University of Adelaide); Chunhua Shen (University of Adelaide); Longbing Cao (University of Technology Sydney)</p></li><li><p>Multi-Scale One-Class Recurrent Neural Networks for Discrete Event Sequence Anomaly Detection</p><p>Authors: Zhiwei Wang (Michigan State University)$^{\star}$; Zhengzhang Chen (NEC Laboratories America, Inc.); Jingchao Ni ( NEC Laboratories America); Hui Liu (Michigan State University); Haifeng Chen (NEC Labs); Jiliang Tang (Michigan State University)</p></li><li><p>Multivariate Time Series Anomaly Detection and Interpretation using Hierarchical Inter-Metric and Temporal Embedding</p><p>Authors: Zhihan Li (Tsinghua University)$^{\star}$; Youjian Zhao (Tsinghua University); Jiaqi Han (Tsinghua University); Ya Su (Tsinghua University); Rui Jiao (Tsinghua University); Xidao Wen (Tsinghua University); Dan Pei (Tsinghua University)</p></li><li><p>Practical Approach to Asynchronous Multivariate Time Series Anomaly Detection and Localization</p><p>Authors: Ahmed Abdulaal (eBay)$^{\star}$; Zhuanghua Liu (eBay); Tomer Lancewicki (EBay)</p></li><li><p>Time Series Anomaly Detection for Cyber-physical Systems via Neural System Identification and Bayesian Filtering</p><p>Authors: Cheng Feng (Siemens)$^{\star}$; Pengwei Tian (Siemens)</p></li><li><p>Deep Clustering-based Fair Outlier Detection</p><p>Authors: Hanyu Song (Brandeis University)$^{\star}$; Peizhao Li (Brandeis University); Hongfu Liu (Brandeis University)</p></li><li><p>Fast One-class Classification using Class Boundary-preserving Random Projections</p><p>Authors: Arindam Bhattacharya (IIT DELHI)$^{\star}$; Sumanth Varambally (IIT Delhi); Amitabha Bagchi (IIT Delhi); Srikanta Bedathur (IIT Delhi)</p></li><li><p>Heterogeneous Temporal Graph Transformer: An Intelligent System for Evolving Android Malware Detection</p><p>Authors: Yujie Fan (Case Western Reserve University); Mingxuan Ju (Case Western Reserve University); Shifu Hou (Case Western Reserve University); Yanfang Ye (Case Western Reserve University)$^{\star}$; Wenqiang Wan (Tencent Security Lab); Kui Wang (Tencent Security Lab); Yinming Mei (Tencent Security Lab); Qi Xiong (Tencent Security Lab)</p></li><li><p>Live-Streaming Fraud Detection: A Heterogeneous Graph Neural Network Approach</p><p>Authors: Zhao Li (Alibaba Group); Haishuai Wang (Fairfield Universityï¼ŒDepartment of Computer Science and Engineering); Peng Zhang (Guangzhou University)$^{\star}$; Pengrui Hui ( Alibaba Group); Jiaming Huang (Alibaba Group); Jian Liao (Alibaba Group); Ji Zhang (The University of Southern Queensland); Jiajun Bu (Zhejiang University)</p></li><li><p>Intention-aware Heterogeneous Graph Attention Networks for Fraud Transactions Detection</p><p>Authors: Can Liu (Alibaba Group)$^{\star}$; Li Sun (Alibaba Group); Xiang Ao (Institute of Computing Technology, CAS); Jinghua Feng (Ailbaba Group ); Qing He (Institute of Computing Technology, Chinese Academy of Sciences); Hao Yang (Alibaba Group)</p></li><li><p>Multi-modal Emergent Fake News Detection via Meta Neural Process Networks</p><p>Authors: Yaqing Wang (Purdue University)$^{\star}$; Fenglong Ma (Pennsylvania State University); Haoyu Wang (SUNY Buffalo); Kishlay Jha (University of Virginia); Jing Gao (University at Buffalo)</p></li><li><p>Automated Testing of Graphics Units by Deep-Learning Detection of Visual Anomalies</p><p>Authors: Lev Faivishevsky (Intel)$^{\star}$; Adi Szeskin (Intel); Ashwin k Muppalla (Intel); Ravid Ziv (Intel); Ronen Laperdon (Intel); Benjamin Melloul (intel); Tahi Hollander (Intel); Tom Hope (Intel); Amitai Armon (Intel)</p></li></ul><h2 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h2><ul><li><p>Apriori Convolutions for Highly Efficient and Accurate Time Series Classification</p><p>Authors: Angus Dempster (Monash University)$^{\star}$; Daniel F Schmidt (Monash University); Geoffrey I Webb (Monash)</p></li><li><p>Fast and Accurate Partial Fourier Transform for Time Series Data</p><p>Authors: Yong-chan Park (Seoul National University)$^{\star}$; Jun-gi Jang (Seoul National University); U Kang (Seoul National University)</p></li><li><p>Representation Learning of Multivariate Time Series using a Transformer Framework</p><p>Authors: George Zerveas (Brown University)$^{\star}$; Srideepika Jayaraman (IBM); Dhaval Patel (IBM TJ Watson Research Center); Anuradha Bhamidipaty (IBM Watson Research Center); Carsten Eickhoff (Brown University)</p></li><li><p>ST-Norm: Spatial and Temporal Normalization for Multi-variate Time Series Forecasting</p><p>Authors: Jinliang Deng (University of Technology Sydney); Xiusi Chen (University of California, Los Angeles); Renhe Jiang (The University of Tokyo); Xuan Song (Southern University of Science and Technology); Ivor Tsang (University of Technology Sydney)$^{\star}$</p></li><li><p>Statistical models coupling allows for complex localmultivariate time series analysis</p><p>Authors: Veronica Tozzo (Massachusets General Hospital - Harvard Medical School)$^{\star}$; Federico Ciech (University of Genoa); Davide Garbarino (University of Genoa); Alessandro Verri (University of Genova, Italy)</p></li><li><p>Causal and Interpretable Rules for Time Series Analysis</p><p>Authors: Amin Dhaou (Total)$^{\star}$; Josselin Garnier (Ã‰cole Polytechnique); Antoine Bertoncello (Total); Erwann LE PENNEC (Polytechnique)</p></li></ul><h2 id="Graph-Representation-Learning"><a href="#Graph-Representation-Learning" class="headerlink" title="Graph Representation Learning"></a>Graph Representation Learning</h2><ul><li><p>Are we really making much progress? Revisiting, benchmarking and refining the Heterogeneous Graph Neural Networks</p><p>Authors: Qingsong Lv (Tsinghua University); Ming Ding (Tsinghua University); Qiang Liu (Institute of Information Engineering, Chinese Academy of Sciences); Yuxiang Chen (Tsinghua University); Wenzheng Feng (Tsinghua University); Siming He (University of Pennsylvania); Chang Zhou (Alibaba Group); Jian-guo Jiang (Institute of Information Engineering , Chinese Academy of Sciences); Yuxiao Dong (Facebook AI); Jie Tang (Tsinghua University)$^{\star}$</p></li><li><p>Attentive Heterogeneous Graph Embedding for Job Mobility Prediction</p><p>Authors: Le Zhang (University of Science and Technology of China)$^{\star}$; Ding Zhou (University of Science and Technology of China); Hengshu Zhu (Baidu Talent Intelligence Center, Baidu Inc.); Tong Xu (University of Science and Technology of China); Rui Zha ( University of Science and Technology of China); Enhong Chen (University of Science and Technology of China); Hui Xiong (Rutgers University)</p></li><li><p>DiffMG: Differentiable Meta Graph Search for Heterogeneous Graph Neural Networks</p><p>Authors: Yuhui Ding (The Hong Kong University of Science and Technology)$^{\star}$; Quanming Yao (4Paradigm); Huan Zhao (4Paradigm Inc.); Tong Zhang (Hong Kong University of Science and Technology)</p></li><li><p>HGK-GNN: Heterogeneous Graph Kernel based Graph Neural Networks</p><p>Authors: Qingqing Long (Peking University)$^{\star}$; Lingjun Xu (Peking University); Zheng Fang (pku); Guojie Song (Peking University)</p></li><li><p>Pre-training on Large-Scale Heterogeneous Graph</p><p>Authors: Xunqiang Jiang (Beijing University of Posts and Telecommunications)$^{\star}$; Tianrui Jia (Beijing University of Posts and Telecommunications); Chuan Shi (Beijing University of Posts and Telecommunications); Yuan Fang (Singapore Management University); Zhe Lin (Peng Cheng Laboratory); Hui Wang (Peng Cheng Laboratory)</p></li><li><p>Scalable Heterogeneous Graph Neural Networks for Predicting High-potential Early-stage Startups</p><p>Authors: SHENGMING ZHANG (Rutgers University)$^{\star}$; Hao Zhong (ESCP Business School); Zixuan Yuan (Rutgers University); Hui Xiong (the State University of New Jersey)</p></li><li><p>Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning</p><p>Authors: Xiao Wang (Beijing University of Posts and Telecommunications); Nian Liu (Beijing University of Posts and Telecommunications)$^{\star}$; Hui Han (Beijing University of Posts and Telecommunications); Chuan Shi (Beijing University of Posts and Telecommunications)</p></li><li><p>Heterogeneous Temporal Graph Transformer: An Intelligent System for Evolving Android Malware Detection</p><p>Authors: Yujie Fan (Case Western Reserve University); Mingxuan Ju (Case Western Reserve University); Shifu Hou (Case Western Reserve University); Yanfang Ye (Case Western Reserve University)$^{\star}$; Wenqiang Wan (Tencent Security Lab); Kui Wang (Tencent Security Lab); Yinming Mei (Tencent Security Lab); Qi Xiong (Tencent Security Lab)</p></li><li><p>HGAMN: Heterogeneous Graph Attention Matching Network for Multilingual POI Retrieval at Baidu Maps</p><p>Authors: Jizhou Huang (Baidu)$^{\star}$; Haifeng Wang (Baidu); Yibo Sun (Baidu); Miao Fan (Baidu); Zhengjie Huang (Baidu); Chunyuan Yuan (Baidu); Yawen Li (BUPT)</p></li></ul><h2 id="sequence"><a href="#sequence" class="headerlink" title="sequence"></a>sequence</h2><ul><li>PETGEN: Personalized Text Generation Attack on Deep User Sequence Classification ModelsAuthors: Bing He (Georgia Institute of Technology)$^{\star}$; Dr.Mustaque Ahamad (Georgia Institute of Technology); Srijan Kumar (Georgia Institute of Technology)</li><li>TimeSHAP: Explaining Recurrent Models through Sequence PerturbationsAuthors: Joao Bento (Feedzai); Pedro Saleiro (Feedzai)$^{\star}$; AndrÃ© F. Cruz (Feedzai); Mario Figueiredo (University of Lisbon); Pedro Bizarro (Feedzai)</li></ul><h2 id="causal-analysis"><a href="#causal-analysis" class="headerlink" title="causal analysis"></a>causal analysis</h2><ul><li><p>Causal models for Real Time Bidding with repeated user interactions</p><p>Authors: Martin Bompaire (Criteo)$^{\star}$; Benjamin Heymann (Criteo); Alexandre Gilotte (Criteo)</p></li><li><p>DARING: Differentiable Causal Discovery with Residual Independence</p><p>Authors: Yue He (Tsinghua University)$^{\star}$; Peng Cui (Tsinghua University); Zheyan Shen (Tsinghua University); Renzhe Xu (Tsinghua University); Furui Liu (Huawei Noahâ€™s Ark Lab); Yong Jiang (Tsinghua University)</p></li><li><p>MPCSL - A Modular Pipeline for Causal Structure Learning</p><p>Authors: Johannes Huegle (Hasso Plattner Institute)$^{\star}$; Christopher Hagedorn (Hasso Plattner Institute); Michael Perscheid (Hasso Plattner Institute); Hasso Plattner (Hasso Plattner Institute)</p></li></ul><hr><h2 id="clustering"><a href="#clustering" class="headerlink" title="clustering"></a>clustering</h2><h2 id="About-distribution"><a href="#About-distribution" class="headerlink" title="About distribution"></a>About distribution</h2><h2 id="interpretable-Understanding-explanation-Attribution-â€¦"><a href="#interpretable-Understanding-explanation-Attribution-â€¦" class="headerlink" title="interpretable [Understanding, explanation, Attribution â€¦]"></a>interpretable [Understanding, explanation, Attribution â€¦]</h2><h2 id="missing-value-amp-irregularly-sampled-time-series-Incomplete-imputation-â€¦"><a href="#missing-value-amp-irregularly-sampled-time-series-Incomplete-imputation-â€¦" class="headerlink" title="missing value &amp; irregularly sampled time series [Incomplete, imputation, â€¦]"></a>missing value &amp; irregularly sampled time series [Incomplete, imputation, â€¦]</h2>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AIOps 2021æŒ‘æˆ˜èµ›ç­”è¾©è®°å½•</title>
      <link href="/uncategorized/notes/AIOps2021%E6%8C%91%E6%88%98%E8%B5%9B/"/>
      <url>/uncategorized/notes/AIOps2021%E6%8C%91%E6%88%98%E8%B5%9B/</url>
      
        <content type="html"><![CDATA[<h1 id="1-æ—¶ç©ºæ•°æ®å¤šæŒ‡æ ‡é¢„æµ‹"><a href="#1-æ—¶ç©ºæ•°æ®å¤šæŒ‡æ ‡é¢„æµ‹" class="headerlink" title="1. æ—¶ç©ºæ•°æ®å¤šæŒ‡æ ‡é¢„æµ‹"></a>1. æ—¶ç©ºæ•°æ®å¤šæŒ‡æ ‡é¢„æµ‹</h1><p>æ—¶ç©ºæ•°æ®å“ªæœ‰å‡ ç§</p><ul><li>å›¾ç‰‡+æ—¶é—´è½´</li><li>æ—¶é—´åºåˆ—+ç©ºåŸŸå…³ç³»</li></ul><span id="more"></span><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p><img src="https://user-images.githubusercontent.com/16149619/118065808-2e857600-b3d0-11eb-9d1b-90659d92c37f.png" alt="da83bb35b2ee1661e1c9a674721a092"><br><img src="https://user-images.githubusercontent.com/16149619/118066874-11ea3d80-b3d2-11eb-8fc6-d193e9e90d1b.png" alt="2afb0700c4e39369712fbb8ea34e0aa"><br><img src="https://user-images.githubusercontent.com/16149619/118066884-157dc480-b3d2-11eb-868b-6971c9d0cc3d.png" alt="092efde5fcc1ead4c548ea92808792d"><br><img src="https://user-images.githubusercontent.com/16149619/118066889-17478800-b3d2-11eb-9cc5-158e6496540c.png" alt="482d0beee98602dae12b360f6031d18"><br><img src="https://user-images.githubusercontent.com/16149619/118066898-1a427880-b3d2-11eb-96a2-1bd2a807d5c7.png" alt="3e27d036423732fd15c7a62c1e1128c"><br><img src="https://user-images.githubusercontent.com/16149619/118066914-20385980-b3d2-11eb-8019-9a7e6e2cb35f.png" alt="58898959d166daba42e0ebb387b2f79"><br><img src="https://user-images.githubusercontent.com/16149619/118066921-23cbe080-b3d2-11eb-8f48-6659212f580f.png" alt="dd47545c4bd922d7b84c960ee7d4833"><br><img src="https://user-images.githubusercontent.com/16149619/118066931-262e3a80-b3d2-11eb-9cbc-996f792bb8c6.png" alt="88c7451c8cc1e57e4d2eeb486c469df"><br><img src="https://user-images.githubusercontent.com/16149619/118066951-2cbcb200-b3d2-11eb-923e-83d27bd4d36e.png" alt="30d0135c1f07ed84151f625d3d57a2e"><br><img src="https://user-images.githubusercontent.com/16149619/118066956-2fb7a280-b3d2-11eb-83b6-b15bbc07d8d8.png" alt="6930d622e79c8636b9190138728649b"></p><h3 id="æ•°æ®é›†"><a href="#æ•°æ®é›†" class="headerlink" title="æ•°æ®é›†"></a>æ•°æ®é›†</h3><p><img src="https://user-images.githubusercontent.com/16149619/118065786-2594a480-b3d0-11eb-9c39-f4690cc4673a.png" alt="2b64a3575b3878a9420d6f72c5b7f68"></p><h3 id="æŒ‘æˆ˜"><a href="#æŒ‘æˆ˜" class="headerlink" title="æŒ‘æˆ˜"></a>æŒ‘æˆ˜</h3><ol><li>node edgeå‡æœ‰å¼‚è´¨æ€§ï¼š</li></ol><ul><li>nodeï¼šåŸºç«™ä¸šåŠ¡ä¸åŒï¼Œæ•°æ®åˆ†å¸ƒä¸åŒ</li><li>edgeï¼šnodeé—´æœ‰ä¸åŒçš„å…³ç³»ï¼ˆä¸¾ä¾‹ï¼šæ¥å‘ä¸å»å‘çš„é«˜é“èŠ‚ç‚¹ã€å°åŒºé—´çš„èŠ‚ç‚¹ï¼‰</li></ul><p><img src="https://user-images.githubusercontent.com/16149619/118066346-24b04280-b3d1-11eb-920d-04e16840a234.png" alt="87bc8f9f29b296c3f76e60b2802642e"></p><ol start="2"><li><p>ç¼ºä¹è¿ç»­æ€§çš„å‡è®¾ï¼šç”¨æˆ·æ¥å…¥ä¸å—ç‰©ç†ç©ºé—´çº¦æŸï¼Œå¯ä»¥ç¬é—´ä»å°åŒºAæ¥å…¥åˆ°å°åŒºB</p></li><li><p>çªå‘æ€§</p></li></ol><p><img src="https://user-images.githubusercontent.com/16149619/118066497-6b05a180-b3d1-11eb-9629-c2f33b294cfa.png" alt="2e14a03c01af3b1caf0a662ee8a7146"></p><ol start="4"><li>ç½‘ç»œç»“æ„å¤æ‚æ€§ï¼ˆé¢†åŒºå…³ç³»ï¼‰ï¼šä¸åŒåŒºåŸŸï¼ŒåŸºç«™ä¹‹é—´çš„è¿æ¥å¯†é›†ç¨‹åº¦æœ‰å¾ˆå¤§ä¸åŒ</li></ol><p><img src="https://user-images.githubusercontent.com/16149619/118066700-c041b300-b3d1-11eb-832e-d075536509e7.png" alt="175124e6e9fd50b498402605498cb46"></p><h3 id="è‡ªå·±çš„å·¥ä½œ"><a href="#è‡ªå·±çš„å·¥ä½œ" class="headerlink" title="è‡ªå·±çš„å·¥ä½œ"></a>è‡ªå·±çš„å·¥ä½œ</h3><p>å¼ºè°ƒè‡ªé€‚åº”æ„Ÿå—é‡ï¼Œå¼ºè°ƒå¼‚è´¨æ€§</p><p><img src="https://user-images.githubusercontent.com/16149619/118066996-43630900-b3d2-11eb-985d-f46dcef6ef33.png" alt="70fcdc172c8cedf6935d33ac839464a"><br><img src="https://user-images.githubusercontent.com/16149619/118066999-452ccc80-b3d2-11eb-8d9a-12dc0f79a6d1.png" alt="37a65aed9e33e8fa09b12b8be7683c6"><br><img src="https://user-images.githubusercontent.com/16149619/118067004-46f69000-b3d2-11eb-84cb-9d979621749f.png" alt="02bb2f207040d2afe6de2757b8c1e72"><br><img src="https://user-images.githubusercontent.com/16149619/118067008-4958ea00-b3d2-11eb-9f61-2ccde47c3644.png" alt="a0a14ff67f978b4ad952b64ff14c559"><br><img src="https://user-images.githubusercontent.com/16149619/118067012-4bbb4400-b3d2-11eb-9e42-491317cf8528.png" alt="c16290875c6b0455c241f091633377a"></p><hr><h1 id="2-äººæœºç‰©èåˆæ™ºèƒ½è¿ç»´ï¼šæ„ŸçŸ¥ã€è¯Šæ–­ã€äº¤äº’"><a href="#2-äººæœºç‰©èåˆæ™ºèƒ½è¿ç»´ï¼šæ„ŸçŸ¥ã€è¯Šæ–­ã€äº¤äº’" class="headerlink" title="2. äººæœºç‰©èåˆæ™ºèƒ½è¿ç»´ï¼šæ„ŸçŸ¥ã€è¯Šæ–­ã€äº¤äº’"></a>2. äººæœºç‰©èåˆæ™ºèƒ½è¿ç»´ï¼šæ„ŸçŸ¥ã€è¯Šæ–­ã€äº¤äº’</h1><p>é‡ç‚¹ï¼šéœ€è¦èåˆäººç±»çŸ¥è¯†åˆ°å·²æœ‰çš„æ—¶é—´åºåˆ—ã€æ—¥å¿—åˆ†æä¸­</p><p><img src="https://user-images.githubusercontent.com/16149619/118067350-ef0c5900-b3d2-11eb-9899-4b577997d00c.png" alt="17bde0bb13da6f7b437d75aefb74218"></p><p>ä¸»è¦åˆ†ææ—¥å¿—æ•°æ®ï¼Œæ•°æ®ç‰¹ç‚¹ï¼šåŠç»“æ„åŒ–ã€å¤šè¨€ä¸”å¤æ‚ã€ä¸åŒç»„ä»¶ã€ç¬¬ä¸‰æ–¹ç»„ä»¶æ—¥å¿—çš„å¼‚è´¨æ€§ã€‚</p><h3 id="éš¾ç‚¹"><a href="#éš¾ç‚¹" class="headerlink" title="éš¾ç‚¹"></a>éš¾ç‚¹</h3><p><img src="https://user-images.githubusercontent.com/16149619/118067374-00edfc00-b3d3-11eb-8e6d-938d3f9183cf.png" alt="83e9edce68a0e8f224e3abf3e4536c4"><br><img src="https://user-images.githubusercontent.com/16149619/118067397-0ba89100-b3d3-11eb-8e92-cc80bb72edc6.png" alt="ba672144e6d23c3ab8da3696835ddc1"></p><h3 id="åˆ†å¸ƒå¼ç³»ç»Ÿçš„ç‰¹ç‚¹"><a href="#åˆ†å¸ƒå¼ç³»ç»Ÿçš„ç‰¹ç‚¹" class="headerlink" title="åˆ†å¸ƒå¼ç³»ç»Ÿçš„ç‰¹ç‚¹"></a>åˆ†å¸ƒå¼ç³»ç»Ÿçš„ç‰¹ç‚¹</h3><p>åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œç»Ÿä¸€è¯·æ±‚æ—¥å¿—åœ¨æ•°æ®ä¸­äº¤æ›¿å‡ºç°ï¼Œè€Œä¸å”¯ä¸€</p><p>â€¦</p><p><img src="https://user-images.githubusercontent.com/16149619/118067606-7b1e8080-b3d3-11eb-85db-c757c38269b5.png" alt="29fb6e68dc5cf6736622b00546c8985"></p><h3 id="ä»–ä»¬çš„å·¥ä½œ"><a href="#ä»–ä»¬çš„å·¥ä½œ" class="headerlink" title="ä»–ä»¬çš„å·¥ä½œ"></a>ä»–ä»¬çš„å·¥ä½œ</h3><p>æ¦‚è¿°</p><p><img src="https://user-images.githubusercontent.com/16149619/118067701-a7d29800-b3d3-11eb-9f81-b0095b575551.png" alt="e09e7e6521aabee6c58e6aa477b81cb"></p><p>é‡ç‚¹1ï¼šæ¨¡å‹é€‚åº”æ€§é—®é¢˜ï¼Œè‡ªå­¦ä¹ ã€è‡ªæ›´æ–°çš„æ•…éšœé¢„æµ‹æ¨¡å‹ï¼Œhuman-in-loop</p><p><img src="https://user-images.githubusercontent.com/16149619/118067887-0435b780-b3d4-11eb-8890-a73e3d0bfbd9.png" alt="5f23f647ca6efef99e4caa69ee2a015"></p><p>é‡ç‚¹2ï¼šæœºå™¨å­¦ä¹ ç»“æœå¦‚ä½•æé«˜äººç±»çš„è¿ç»´æ°´å¹³ï¼ŒæŒ‡å¯¼ç¨‹åºå‘˜è®¾ç½®meaningfulçš„æ—¥å¿—æ‰“å°ç‚¹ï¼ˆæ‰“å°ç‚¹è¶Šå¤šï¼Œæ—¥å¿—åŒ…å«çš„ä¿¡æ¯è¶Šæ— ç”¨ï¼‰ï¼Œå®é™…æ•ˆæœå¤§å¹…å‡å°‘æ‰“å°ç‚¹ã€ä¸”æå‡äº†æ•…éšœè¯Šæ–­å‡†ç¡®åº¦</p><p><img src="https://user-images.githubusercontent.com/16149619/118068263-a6559f80-b3d4-11eb-9f93-9b2df53c2550.png" alt="f2b06778d60907994e463c2649ea753"></p><p>é‡ç‚¹3ï¼šCMDBç›¸å…³</p><p><img src="https://user-images.githubusercontent.com/16149619/118068466-fcc2de00-b3d4-11eb-80f5-4841c766391b.png" alt="43bd7aa2f12acc7183ce089ab7d53f4"><br><img src="https://user-images.githubusercontent.com/16149619/118068476-ffbdce80-b3d4-11eb-9c3d-6226fc2896b7.png" alt="24f3046d112f0daa16561614fad677a"><br><img src="https://user-images.githubusercontent.com/16149619/118068633-47445a80-b3d5-11eb-8892-8ab0c02406f6.png" alt="91c0926c0db9d0e85064f0f944e80e7"></p><p>é‡ç‚¹4ï¼šçŸ¥è¯†å›¾å›¾è°±</p><p><img src="https://user-images.githubusercontent.com/16149619/118069018-fa14b880-b3d5-11eb-87c1-6b59a87d22fc.png" alt="2b060c89fc58c019e5d86588c8b2410"><br><img src="https://user-images.githubusercontent.com/16149619/118069021-fb45e580-b3d5-11eb-9d98-8ffae06434f9.png" alt="cbef862f03ea4a8d74de16e561aa09f"></p><p>é‡ç‚¹5ï¼šäººæœºæ™ºèƒ½é—®ç­”</p><p><img src="https://user-images.githubusercontent.com/16149619/118069086-1b75a480-b3d6-11eb-8f15-74ab2a01f95b.png" alt="f9b1bd79a7e34528ad30ea964f2116b"><br><img src="https://user-images.githubusercontent.com/16149619/118069207-57106e80-b3d6-11eb-9748-0aba1fa8a83a.png" alt="ACL2020_8dbe96fbf771796f37977008d26fc1e"><br><img src="https://user-images.githubusercontent.com/16149619/118069295-82935900-b3d6-11eb-9319-4233679d332c.png" alt="IJCAI2020_668cd8dcc538e660acd07ea56ffa7c2"></p><hr><h1 id="3-è½åœ°ç»éªŒ"><a href="#3-è½åœ°ç»éªŒ" class="headerlink" title="3. è½åœ°ç»éªŒ"></a>3. è½åœ°ç»éªŒ</h1><p><img src="https://user-images.githubusercontent.com/16149619/118069658-1f55f680-b3d7-11eb-9743-cbf86997616e.png" alt="b307dd866403412e2fe07928660b09e"></p>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> AIOps </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in ICML 2021 (2021.07.18 - 2021.07.24)</title>
      <link href="/uncategorized/paperlistfile/ICML2021/"/>
      <url>/uncategorized/paperlistfile/ICML2021/</url>
      
        <content type="html"><![CDATA[<p>Accept papers: <a href="https://icml.cc/Conferences/2021/AcceptedPapersInitial">link</a></p><p><a href="https://mp.weixin.qq.com/s/VRWHgES7NK6j-c1pYjZ3Yg">æ—¶åºè®ºæ–‡ä¸€è§ˆ</a></p><span id="more"></span><h2 id="Anomaly-detection-anomaly-outlier-out-of-distribution-one-class-Malware-detection-â€¦"><a href="#Anomaly-detection-anomaly-outlier-out-of-distribution-one-class-Malware-detection-â€¦" class="headerlink" title="Anomaly detection (anomaly, outlier, out-of-distribution, one-class, Malware detection, â€¦)"></a>Anomaly detection (anomaly, outlier, out-of-distribution, one-class, Malware detection, â€¦)</h2><ul><li><p>Near-Optimal Entrywise Anomaly Detection for Low-Rank Matrices with Sub-Exponential Noise</p><p>Vivek Farias (MIT) Â· Andrew Li (Carnegie Mellon University) Â· Tianyi Peng (MIT)</p></li><li><p>Transfer-Based Semantic Anomaly Detection</p><p>Lucas Deecke (University of Edinburgh) Â· Lukas Ruff (Aignostics) Â· Robert Vandermeulen (TU Berlin) Â· Hakan Bilen (University of Edinburgh)</p></li><li><p>Neural Transformation Learning for Deep Anomaly Detection Beyond Images</p><p>Chen Qiu (TU Kaiserslautern/Bosch Center for Artificial Intelligence) Â· Timo Pfrommer (Bosch Center for Artificial Intelligence) Â· Marius Kloft (TU Kaiserslautern) Â· Stephan Mandt (University of California, Irivine) Â· Maja Rudolph (BCAI)</p></li></ul><ul><li><p>Event Outlier Detection in Continuous Time</p><p>Siqi Liu (University of Pittsburgh) Â· Milos Hauskrecht (University of Pittsburgh)</p></li><li><p>Understanding Failures in Out-of-Distribution Detection with Deep Generative Models</p><p>Lily Zhang (New York University) Â· Mark Goldstein (New York University) Â· Rajesh Ranganath (New York University)</p></li></ul><ul><li><p>Outlier-Robust Optimal Transport</p><p>Debarghya Mukherjee (University of Michigan) Â· Aritra Guha (Duke University) Â· Justin Solomon (MIT) Â· Yuekai Sun (University of Michigan) Â· Mikhail Yurochkin (IBM Research AI)</p></li><li><p>DORO: Distributional and Outlier Robust Optimization</p><p>Runtian Zhai (Carnegie Mellon University) Â· Chen Dan (Carnegie Mellon University) Â· Zico Kolter (Carnegie Mellon University / Bosch Center for AI) Â· Pradeep Ravikumar (Carnegie Mellon University)</p></li><li><p>Consistent regression when oblivious outliers overwhelm</p><p>Tommaso dâ€™Orsi (ETH Zurich) Â· Gleb Novikov (ETH Zurich) Â· David Steurer (ETH Zurich)</p></li><li><p>Fixed-Parameter and Approximation Algorithms for PCA with Outliers</p><p>Yogesh Dahiya (The Institute of Mathematical Sciences (HBNI), Chennai, India) Â· Fedor Fomin (University of Bergen) Â· Fahad Panolan (Indian Institute of Technology Hyderabad) Â· Kirill Simonov (University of Bergen)</p></li><li><p>Generalization Bounds in the Presence of Outliers: a Median-of-Means Study</p><p>Pierre Laforgue (University of Milan) Â· Guillaume Staerman (TÃ©lÃ©com Paris) Â· Stephan ClÃ©menÃ§on (TÃ©lÃ©com Paris)</p></li><li><p>Can Subnetwork Structure Be the Key to Out-of-Distribution Generalization?</p><p>Dinghuai Zhang (Mila) Â· Kartik Ahuja (Mila) Â· Yilun Xu (MIT) Â· Yisen Wang (Peking University) Â· Aaron Courville (UniversitÃ© de MontrÃ©al)</p></li><li><p>Out-of-Distribution Generalization via Risk Extrapolation (REx)</p><p>David Krueger (MILA (University of Montreal)) Â· Ethan Caballero (Mila) Â· Joern-Henrik Jacobsen (Apple Inc.) Â· Amy Zhang (FAIR / UC Berkeley) Â· Jonathan Binas (Mila, Montreal) Â· Dinghuai Zhang (Mila) Â· Remi Le Priol (Mila, UniversitÃ© de MontrÃ©al) Â· Aaron Courville (UniversitÃ© de MontrÃ©al</p></li><li><p>Graph Convolution for Semi-Supervised Classification: Improved Linear Separability and Out-of-Distribution Generalization</p><p>Aseem Baranwal (University of Waterloo) Â· Kimon Fountoulakis (University of Waterloo) Â· Aukosh Jagannath (University of Waterloo)</p></li><li><p>Accuracy on the Line: on the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization</p><p>John Miller (University of California, Berkeley) Â· Rohan Taori (Stanford University) Â· Aditi Raghunathan (Stanford) Â· Shiori Sagawa (Stanford University) Â· Pang Wei Koh (Stanford University) Â· Vaishaal Shankar (UC Berkeley) Â· Percy Liang (Stanford University) Â· Yair Carmon (Tel Aviv University) Â· Ludwig Schmidt (Toyota Research Institute)</p></li></ul><h2 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h2><ul><li><p>Conformal prediction interval for dynamic time-series</p><p>Chen Xu (Georgia Institute of Technology) Â· Yao Xie (Georgia Institute of Technology)</p></li><li><p>Voice2Series: Reprogramming Acoustic Models for Time Series Classification</p><p>Huck Yang (Georgia Tech) Â· Yun-Yun Tsai (Columbia University) Â· Pin-Yu Chen (IBM Research AI)</p></li></ul><ul><li><p>Explaining Time Series Predictions with Dynamic Masks</p><p>Jonathan CrabbÃ© (University of Cambridge) Â· Mihaela van der Schaar (University of Cambridge and UCLA)</p></li><li><p>Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting</p><p>Kashif Rasul (Zalando Research) Â· Calvin Seward (Zalando Research) Â· Ingmar Schuster (Zalando Research) Â· Roland Vollgraf (Zalando Research)</p></li></ul><ul><li><p>Necessary and sufficient conditions for causal feature selection in time series with latent common causes</p><p>Atalanti Mastakouri (Amazon Research Tuebingen) Â· Bernhard SchÃ¶lkopf (MPI for Intelligent Systems TÃ¼bingen, Germany) Â· Dominik Janzing (Amazon)</p></li><li><p>Approximation Theory of Convolutional Architectures for Time Series Modelling</p><p>Haotian Jiang (National University of Singapore) Â· Zhong Li (Peking University) Â· Qianxiao Li (National University of Singapore; IHPC, Singapore)</p></li><li><p>Whittle Networks: A Deep Likelihood Model for Time Series</p><p>Zhongjie Yu (TU Darmstadt) Â· Fabrizio Ventola (TU Darmstadt) Â· Kristian Kersting (TU Darmstadt)</p></li></ul><ul><li><p>Neural Rough Differential Equations for Long Time Series</p><p>James Morrill (University of Oxford) Â· Cristopher Salvi (University of Oxford) Â· Patrick Kidger (University of Oxford) Â· James Foster (University of Oxford)</p></li><li><p>End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series</p><p>Syama Sundar Yadav Rangapuram (Amazon) Â· Lucien D Werner (California Institute of Technology) Â· Konstantinos Benidis (Amazon Research) Â· Pedro Mercado (Amazon Research) Â· Jan Gasthaus (Amazon Research) Â· Tim Januschowski (Amazon Research)</p></li><li><p>Z-GCNETs: Time Zigzags at Graph Convolutional Networks for Time Series Forecasting</p><p>Yuzhou Chen (Southern Methodist University) Â· Ignacio Segovia (University of Texas at Dallas) Â· Yulia R Gel (University of Texas at Dallas)</p></li></ul><h2 id="Heterogeneous-multi-source"><a href="#Heterogeneous-multi-source" class="headerlink" title="Heterogeneous (multi-source)"></a>Heterogeneous (multi-source)</h2><ul><li><p>Quasi-global Momentum: Accelerating Decentralized Deep Learning on Heterogeneous Data</p><p>Tao Lin (EPFL) Â· Sai Praneeth Reddy Karimireddy (EPFL) Â· Sebastian Stich (EPFL) Â· Martin Jaggi (EPFL)</p></li><li><p>Budgeted Heterogeneous Treatment Effect Estimation</p><p>Tian Qin (Nanjing University) Â· Tian-Zuo Wang (Nanjing University) Â· Zhi-Hua Zhou (Nanjing University)</p></li><li><p>Data-Free Knowledge Distillation for Heterogeneous Federated Learning</p><p>Zhuangdi Zhu (Michigan State University) Â· Junyuan Hong (Michigan State University) Â· Jiayu Zhou (Michigan State University)</p></li><li><p>Heterogeneous Risk Minimization</p><p>Jiashuo Liu (Tsinghua University) Â· Zheyuan Hu (Tsinghua University) Â· Peng Cui (Tsinghua University) Â· Bo Li (Tsinghua University) Â· Zheyan Shen (Tsinghua University)</p></li><li><p>Bias-Variance Reduced Local SGD for Less Heterogeneous Federated Learning</p><p>Tomoya Murata (NTT DATA Mathematical Systems Inc.) Â· Taiji Suzuki (The University of Tokyo / RIKEN)</p></li><li><p>Byzantine-Resilient High-Dimensional SGD with Local Iterations on Heterogeneous Data</p><p>Deepesh Data (UCLA) Â· Suhas Diggavi (UCLA)</p></li><li><p>KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation</p><p>Haozhe Feng (State Key Lab of CAD&amp;CG, Zhejiang University) Â· Zhaoyang You (Zhejiang University) Â· Minghao Chen (Zhejiang University) Â· Tianye Zhang (Zhejiang University) Â· Minfeng Zhu (State Key Lab of CAD&amp;CG, Zhejiang University) Â· Fei Wu (Zhejiang University, China) Â· Chao Wu (Zhejiang University) Â· Wei Chen (State Key Lab of CAD&amp;CG, Zhejiang University)</p></li></ul><h2 id="Graph-Representation-Learning"><a href="#Graph-Representation-Learning" class="headerlink" title="Graph Representation Learning"></a>Graph Representation Learning</h2><ul><li><p>Explainable Automated Graph Representation Learning with Hyperparameter Importance</p><p>Xin Wang (Tsinghua University) Â· Shuyi Fan (Tsinghua University) Â· Kun Kuang (Zhejiang University) Â· wenwu zhu (Tsinghua University)</p></li><li><p>Size-Invariant Graph Representations for Graph Classification Extrapolations</p><p>Beatrice Bevilacqua (Purdue University) Â· Yangze Zhou (Purdue University) Â· Bruno Ribeiro (Purdue University)</p></li><li><p>Generative Causal Explanations for Graph Neural Networks</p><p>Wanyu Lin (Department of Computing, The Hong Kong Polytechnic University) Â· Hao Lan (University of Toronto) Â· Baochun Li (University of Toronto)</p></li></ul><h2 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h2><ul><li><p>Near-Optimal Confidence Sequences for Bounded Random Variables</p><p>Arun Kuchibhotla (Carnegie Mellon University) Â· Qinqing Zheng (Facebook AI Research)</p></li><li><p>Off-Policy Confidence Sequences</p><p>Nikos Karampatziakis (Microsoft) Â· Paul Mineiro (Microsoft) Â· Aaditya Ramdas (Carnegie Mellon University)</p></li><li><p>Learning to Rehearse in Long Sequence Memorization</p><p>Zhu Zhang (DAMO Academy, Alibaba Group,) Â· Chang Zhou (Alibaba Group) Â· Jianxin Ma (Alibaba Group) Â· Zhijie Lin (Zhejiang University) Â· Jingren Zhou (Alibaba Group) Â· Hongxia Yang (Alibaba Group) Â· Zhou Zhao (Zhejiang University)</p></li><li><p>Order Matters: Probabilistic Modeling of Node Sequence for Graph Generation</p><p>Xiaohui Chen (Tufts University) Â· Xu Han (Tufts University) Â· Jiajing Hu (Tufts University) Â· Francisco R Ruiz (DeepMind) Â· Liping Liu (Tufts University)</p></li><li><p>A Structured Observation Distribution for Generative Biological Sequence Prediction and Forecasting</p><p>Eli N. Weinstein (Harvard) Â· Debora Marks (Harvard Medical School)</p></li><li><p>Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design</p><p>yue cao (Texas A&amp;M University) Â· Payel Das (IBM Research AI) Â· Vijil Chenthamarakshan (IBM Research) Â· Pin-Yu Chen (IBM Research AI) Â· Igor Melnyk (IBM) Â· Yang Shen (Texas A&amp;M University)</p></li><li><p>Temporally Correlated Task Scheduling for Sequence Learning</p><p>Xueqing Wu (University of Science and Technology of China) Â· Lewen Wang (Microsoft Research Asia) Â· Yingce Xia (Microsoft Research Asia) Â· Weiqing Liu (Microsoft Research) Â· Lijun Wu (Microsoft Research) Â· Shufang Xie (Microsoft Research Asia) Â· Tao Qin (Microsoft Research Asia) Â· Tie-Yan Liu (Microsoft Research Asia)</p></li></ul><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><ul><li><p>Unified Robust Semi-Supervised Variational Autoencoder</p><p>Xu Chen (SAS Inc)</p></li><li><p>MorphVAE: Generating Neural Morphologies from 3D-Walks using a Variational Autoencoder with Spherical Latent Space</p><p>Sophie C Laturnus (University of TÃ¼bingen) Â· Philipp Berens (University of TÃ¼bingen)</p></li><li><p>Spectral Smoothing Unveils Phase Transitions in Hierarchical Variational Autoencoders</p><p>Adeel Pervez (University of Amsterdam) Â· Efstratios Gavves (University of Amsterdam )</p></li><li><p>Autoencoder Image Interpolation by Shaping the Latent Space</p><p>Alon Oring (IDC) Â· Zohar Yakhini (Herzliya Interdisciplinary Center) Â· Yacov Hel-Or (The Interdisciplinary Center, Herzliya)</p></li><li><p>BasisDeVAE: Interpretable Simultaneous Dimensionality Reduction and Feature-Level Clustering with Derivative-Based Variational Autoencoders</p><p>Dominic Danks (Alan Turing Institute) Â· Christopher Yau (University of Manchester)</p></li><li><p>Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization</p><p>Sang Michael Xie (Stanford University) Â· Tengyu Ma (Stanford University) Â· Percy Liang (Stanford University)</p></li><li><p>Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</p><p>Jaehyeon Kim (Kakao Enterprise) Â· Jungil Kong (Kakao Enterprise) Â· Juhee Son (Kakao Enterprise)</p></li></ul><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><ul><li><p>Training Recurrent Neural Networks via Forward Propagation Through Time</p><p>Anil Kag (Boston University) Â· Venkatesh Saligrama (Boston University)</p></li><li><p>Re-understanding Finite-State Representations of Recurrent Policy Networks</p><p>Mohamad H Danesh (Oregon State University) Â· Anurag Koul (Oregon State University) Â· Alan Fern (Oregon State University) Â· Saeed Khorram (Oregon State University) </p></li><li><p>UnICORNN: A recurrent model for learning very long time dependencies</p><p>T. Konstantin Rusch (ETH Zurich) Â· Siddhartha Mishra (ETH Zurich)</p></li></ul><h2 id="Correlation-analysis-association-analysis"><a href="#Correlation-analysis-association-analysis" class="headerlink" title="Correlation analysis (association analysis)"></a>Correlation analysis (association analysis)</h2><ul><li><p>Inferring serial correlation with dynamic backgrounds</p><p>Song Wei (Georgia Tech) Â· Yao Xie (Georgia Institute of Technology) Â· Dobromir Rahnev (Georgia Tech)</p></li><li><p>Connecting Optimal Ex-Ante Collusion in Teams to Extensive-Form Correlation: Faster Algorithms and Positive Complexity Results</p><p>Gabriele Farina (Carnegie Mellon University) Â· Andrea Celli (Facebook CDS) Â· Nicola Gatti (Politecnico di Milano) Â· Tuomas Sandholm (Carnegie Mellon University)</p></li><li><p>Local Correlation Clustering with Asymmetric Classification Errors</p><p>Jafar Jafarov (University of Chicago) Â· Sanchit Kalhan (Northwestern University) Â· Konstantin Makarychev (Northwestern University) Â· Yury Makarychev (Toyota Technological Institute at Chicago)</p></li><li><p>Differentially Private Correlation Clustering</p><p>Mark Bun (Boston University) Â· Marek Elias (CWI) Â· Janardhan Kulkarni (Microsoft Research)</p></li><li><p>A theory of high dimensional regression with arbitrary correlations between input features and target functions: sample complexity, multiple descent curves and a hierarchy of phase transitions</p><p>Gabriel Mel (Stanford University) Â· Surya Ganguli (Stanford)</p></li><li><p>Correlation Clustering in Constant Many Parallel Rounds</p><p>Vincent Cohen-Addad (Google) Â· Silvio Lattanzi (Google) Â· Slobodan MitroviÄ‡ (MIT) Â· Ashkan Norouzi-Fard (Google) Â· Nikos Parotsidis (Google) Â· Jakub Tarnawski (Microsoft Research)</p></li><li><p>Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?</p><p>Ning Liu (Midea Group) Â· Geng Yuan (Northeastern University) Â· Zhengping Che (Didi Chuxing) Â· Xuan Shen (Northeastern University) Â· Xiaolong Ma (Northeastern University) Â· Qing Jin (Northeastern University) Â· Jian Ren (Snap Inc.) Â· Jian Tang (AI Innovation Center, Midea Group) Â· Sijia Liu (Michigan State University) Â· Yanzhi Wang (Northeastern University)</p></li><li><p>Accuracy on the Line: on the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization</p><p>John Miller (University of California, Berkeley) Â· Rohan Taori (Stanford University) Â· Aditi Raghunathan (Stanford) Â· Shiori Sagawa (Stanford University) Â· Pang Wei Koh (Stanford University) Â· Vaishaal Shankar (UC Berkeley) Â· Percy Liang (Stanford University) Â· Yair Carmon (Tel Aviv University) Â· Ludwig Schmidt (Toyota Research Institute)</p></li></ul><h2 id="Causal-analysis"><a href="#Causal-analysis" class="headerlink" title="Causal analysis"></a>Causal analysis</h2><ul><li><p>Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning</p><p>Sumedh Sontakke (University of Southern California) Â· Arash Mehrjou (Max Planck Institute for Intelligent Systems) Â· Laurent Itti (University of Southern California) Â· Bernhard SchÃ¶lkopf (MPI for Intelligent Systems TÃ¼bingen, Germany)</p></li><li><p>Integer Programming for Causal Structure Learning in the Presence of Latent Variables</p><p>Rui Chen (University of Wisconsin-Madison) Â· Sanjeeb Dash (IBM Research) Â· Tian Gao (IBM Research)</p></li><li><p>How and Why to Use Experimental Data to Evaluate Methods for Observational Causal Inference</p><p>Amanda Gentzel (University of Massachusetts Amherst) Â· Purva Pruthi (University of Massachusetts Amherst) Â· David Jensen (University of Massachusetts Amherst)</p></li><li><p>Model-Free and Model-Based Policy Evaluation when Causality is Uncertain</p><p>David Bruns-Smith (UC Berkeley)</p></li><li><p>Domain Generalization using Causal Matching</p><p>Divyat Mahajan (Microsoft Research India) Â· Shruti Tople (Microsoft Research) Â· Amit Sharma (Microsoft Research)</p></li><li><p>Estimating Identifiable Causal Effects on Markov Equivalence Class through Double Machine Learning</p><p>Yonghan Jung (Purdue University) Â· Jin Tian (Iowa State University) Â· Elias Bareinboim (Columbia)</p></li><li><p>Valid Causal Inference with (Some) Invalid Instruments</p><p>Jason Hartford (University of British Columbia) Â· Victor Veitch (Google; University of Chicago) Â· Dhanya Sridhar (Columbia University) Â· Kevin Leyton-Brown (University of British Columbia)</p></li><li><p>Quantifying Ignorance in Individual-Level Causal-Effect Estimates under Hidden Confounding</p><p>Andrew Jesson (University of Oxford) Â· SÃ¶ren Mindermann (University of Oxford) Â· Yarin Gal (University of Oxford) Â· Uri Shalit (Technion)</p></li><li><p>Regularizing towards Causal Invariance: Linear Models with Proxies</p><p>Michael Oberst (MIT) Â· Nikolaj Thams (University of Copenhagen) Â· Jonas Peters (University of Copenhagen) Â· David Sontag (Massachusetts Institute of Technology)</p></li><li><p>Proximal Causal Learning with Kernels: Two-Stage Estimation and Moment Restriction</p><p>Afsaneh Mastouri (University College London) Â· Yuchen Zhu (University College London) Â· Limor Gultchin (University of Oxford) Â· Anna Korba (CREST/ENSAE) Â· Ricardo Silva (University College London) Â· Matt J. Kusner (University College London) Â· Arthur Gretton (Gatsby Computational Neuroscience Unit) Â· Krikamol Muandet (Max Planck Institute for Intelligent Systems)</p></li><li><p>Causality-aware counterfactual confounding adjustment as an alternative to linear residualization in anticausal prediction tasks based on linear learners</p><p>Elias Chaibub Neto (Sage Bionetworks)</p></li></ul><hr><h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><h2 id="About-distribution"><a href="#About-distribution" class="headerlink" title="About distribution"></a>About distribution</h2><h2 id="Interpretable-Understanding-explanation-Attribution-â€¦"><a href="#Interpretable-Understanding-explanation-Attribution-â€¦" class="headerlink" title="Interpretable [Understanding, explanation, Attribution â€¦]"></a>Interpretable [Understanding, explanation, Attribution â€¦]</h2>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in SIRIR 2021 (2021.07.11 - 2021.07.15)</title>
      <link href="/uncategorized/paperlistfile/SIGIR2021/"/>
      <url>/uncategorized/paperlistfile/SIGIR2021/</url>
      
        <content type="html"><![CDATA[<!--tags:æŒ‰ä»»åŠ¡åˆ†ç±»### Anomaly detection / Outlier / Out-of-distribution### Interpretable / Explainable### Causal discovery### Data augmentation æŒ‰æ•°æ®åˆ†ç±»### Time series### Missing value / Irregular sampled / Imputation### Sequence### HeterogeneousæŒ‰æ·±åº¦å­¦ä¹ æ¶æ„åˆ†ç±»### Recurrent neural network / RNN / LSTM / GRU ### Autoencoder--><p><a href="https://sigir.org/sigir2021/accepted-papers/">accept paper list</a></p><span id="more"></span><h2 id="Anomaly-detection-Outlier-Out-of-distribution"><a href="#Anomaly-detection-Outlier-Out-of-distribution" class="headerlink" title="Anomaly detection / Outlier / Out-of-distribution"></a>Anomaly detection / Outlier / Out-of-distribution</h2><ul><li>Decoupling Representation Learning and Classification for GNN-based Anomaly Detection  Yanling Wang, Jing Zhang, Shasha Guo, Hongzhi Yin, Cuiping Li and Hong Chen</li></ul><h2 id="Interpretable-Explainable"><a href="#Interpretable-Explainable" class="headerlink" title="Interpretable / Explainable"></a>Interpretable / Explainable</h2><ul><li><p>FedNLP: An interpretable NLP System to Decode Federal Reserve Communications</p><p>  Jean Lee, Hoyoul Luis Youn, Nicholas Stevens, Josiah Poon and Soyeon Caren Han</p></li><li><p>Interpretable Document Representations for Fast and Accurate Retrieval of Mathematical Information</p><p>  VÃ­t NovotnÃ½</p></li><li><p>Towards Trustworthiness in the context of Explainable Search</p><p>  Sayantan Polley, Rashmi Koparde, Akshaya Bindu Gowri, Maneendra Perera and Andreas Nuernberger</p></li></ul><h2 id="Causal-discovery"><a href="#Causal-discovery" class="headerlink" title="Causal discovery"></a>Causal discovery</h2><ul><li><p>Should Graph Convolution Trust Neighbors? A Simple Causal Inference Method</p><p>  Fuli Feng, Weiran Huang, Xin Xin, Xiangnan He, Tat-Seng Chua and Qifan Wang</p></li><li><p>Deconfounded Video Moment Retrieval with Causal Intervention</p><p>  Xun Yang, Fuli Feng, Wei Ji, Meng Wang and Tat-Seng Chua</p></li></ul><h2 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h2><ul><li>Counterfactual Data-Augmented Sequential Recommendation  Zhenlei Wang, Jingsen Zhang, Hongteng Xu, Xu Chen, Yongfeng Zhang, Wayne Xin Zhao and Ji-Rong Wen</li></ul><ul><li>AdsGNN: Behavior-Graph Augmented Relevance Modeling in Sponsored Search  Xing Xie, Chaozhuo Li, Zheng Liu, Bochen Pang, Tianqi Yang, Yuming Liu, Yanling Cui, Hao Sun, Qi Zhang and Liangjie Zhang</li></ul><ul><li>NIP-GCN: An Augmented Graph Convolutional Network with Node Interaction Patterns  Manish Chandra, Debasis Ganguly, Pabitra Mitra, Bithika Pal and James Thomas</li></ul><ul><li><p>Rumor Detection on Social Media with Event Augmentations</p><p>  Zhenyu He, Ce Li, Fan Zhou and Yi Yang</p></li></ul><ul><li>Unsupervised Extractive Text Summarization with Distance-Augmented Sentence Graphs  Jingzhou Liu, Dominic Hughes and Yiming Yang</li></ul><ul><li>Augmenting Sequential Recommendation with Pseudo-Prior Items via Reversely Pre-training Transformer  Zhiwei Liu, Ziwei Fan, Yu Wang and Philip S. Yu</li></ul><ul><li><p>Cheap and Good? Simple and Effective Data Augmentation for Low Resource Machine Reading</p><p>  Hoang Van, Vikas Yadav and Mihai Surdeanu</p></li></ul><ul><li>ReadsRE: Retrieval-Augmented Distantly Supervised Relation Extraction  Yue Zhang, Hongliang Fei and Ping Li</li></ul><ul><li>Temporal Augmented Graph Neural Networks for Session-Based Recommendations  Huachi Zhou, Qiaoyu Tan, Xiao Huang, Kaixiong Zhou and Xiaoling Wang</li></ul><ul><li>Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering  Mayank Kothyari, Aman Jain, Vishwajeet Kumar, Preethi Jyothi, Soumen Chakrabarti and Ganesh Ramakrishnan</li></ul><h2 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h2><ul><li><p>Temporal Event Profiling based on Multivariate Time Series Analysis over Long-term Document Archives</p><p>  Jiexin Wang, Adam Jatowt and Masatoshi Yoshikawa</p></li><li><p>Time Aware Hyperbolic LSTM for Financial Stream Modeling</p><p>  Ramit Sawhney, Shivam Agarwal, Megh Thakkar, Arnav Wadhwa and Rajiv Shah</p></li></ul><h2 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h2><ul><li>CINES: Explore Citation Network and Event Sequences for Citation Forecasting  Fang He, Wang-Chien Lee, Tao-Yang Fu and Zhen Lei</li></ul><h2 id="Heterogeneous"><a href="#Heterogeneous" class="headerlink" title="Heterogeneous"></a>Heterogeneous</h2><ul><li>Heterogeneous Attention Network for Effective and Efficient Cross-modal Retrieval  Tan Yu, Yi Yang, Yi Li, Lin Liu, Hongliang Fei and Ping Li</li></ul><ul><li>Graph Meta Network for Multi-Behavior Recommendation with Interaction Heterogeneity and Diversity  Lianghao Xia, Chao Huang, Yong Xu, Peng Dai and Liefeng Bo</li></ul><h2 id="Recurrent-neural-network-RNN-LSTM-GRU"><a href="#Recurrent-neural-network-RNN-LSTM-GRU" class="headerlink" title="Recurrent neural network / RNN / LSTM / GRU"></a>Recurrent neural network / RNN / LSTM / GRU</h2><ul><li>Time Aware Hyperbolic LSTM for Financial Stream Modeling  Ramit Sawhney, Shivam Agarwal, Megh Thakkar, Arnav Wadhwa and Rajiv Shah</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Quasi-periodic time series</title>
      <link href="/uncategorized/surveys/QTS/"/>
      <url>/uncategorized/surveys/QTS/</url>
      
        <content type="html"><![CDATA[<h2 id="ç ”ç©¶é—®é¢˜æè¿°"><a href="#ç ”ç©¶é—®é¢˜æè¿°" class="headerlink" title="ç ”ç©¶é—®é¢˜æè¿°"></a>ç ”ç©¶é—®é¢˜æè¿°</h2><ul><li><p>å‡†å‘¨æœŸæ—¶é—´åºåˆ—çš„å¼‚å¸¸æ£€æµ‹é—®é¢˜</p><p>  è¯¥é—®é¢˜å¯ä»¥æ‹†è§£ä¸ºä¸¤éƒ¨åˆ†ï¼šå‡†å‘¨æœŸæ—¶é—´åºåˆ—å»ºæ¨¡ã€å¼‚å¸¸æ£€æµ‹ã€‚æœ¬è°ƒç ”ä¸»è¦é’ˆå¯¹å‡†å‘¨æœŸæ—¶é—´åºåˆ—å»ºæ¨¡è¿›è¡Œè°ƒç ”</p></li><li><p>æ•°å­¦æè¿°</p><p>  ç»™å®šä¸€ä¸ªå‡†å‘¨æœŸæ—¶é—´åºåˆ—çš„ä¸€ç»„å‘¨æœŸæ ·æœ¬${\bf{x}}_1, â€¦, {\bf{x}}_N$ï¼Œå…¶ä¸­${\bf{x}}_i$ä¸ºä¸€ä¸ªå‘¨æœŸå†…çš„æ—¶é—´åºåˆ—ç‰‡æ®µï¼Œå¦‚ä½•å¯¹$\bf{x}_i$è¿›è¡Œå»ºæ¨¡ï¼Œ<br>å³å¦‚ä½•ä»${\bf{x}}_i$ä¸­æå–å‡ºè¡¨å¾å‘é‡${\bf{v}}_i$ç”¨äºåç»­çš„ä»»åŠ¡ï¼ˆå¦‚ï¼šå¼‚å¸¸æ£€æµ‹ã€åˆ†ç±»ã€é¢„æµ‹ï¼Œç­‰ï¼‰</p></li></ul><span id="more"></span><h2 id="é¢†åŸŸç°çŠ¶"><a href="#é¢†åŸŸç°çŠ¶" class="headerlink" title="é¢†åŸŸç°çŠ¶"></a>é¢†åŸŸç°çŠ¶</h2><ol><li>åŸºäºæ‰‹å·¥è®¾è®¡çš„ç‰¹å¾çš„</li></ol><p>è¿™ç±»æ–¹æ³•ä¾èµ–æ‰‹å·¥è®¾å®šçš„ç‰¹å¾ï¼Œæå–æ—¶é—´åºåˆ—çš„ç»Ÿè®¡ç‰¹å¾ä¸å½¢çŠ¶ç‰¹å¾ã€‚è¿™ç±»æ–¹æ³•å¾€å¾€ä»…èƒ½ç”¨äºç‰¹å®šç§ç±»çš„æ•°æ®ï¼Œå¦‚ECGæ•°æ®ï¼Œè€Œç¼ºä¹å¯æ‹“å±•æ€§ã€‚å…·ä½“åŒ…æ‹¬:</p><ol start="2"><li>åŸºäºç»Ÿè®¡ç‰¹å¾çš„</li></ol><p>[1] è®¾è®¡äº†7ç§ç»Ÿè®¡ç‰¹å¾ç”¨äºæè¿°æ—¶é—´åºåˆ—</p><p>[2] å¯¹ECGæ•°æ®è¿›è¡ŒRecurrence Quantiï¬cation Analysisï¼Œä»¥æå–å‡º15ç§ç‰¹å¾ç”¨äºåç»­çš„ä»»åŠ¡</p><ol start="3"><li>æ— éœ€æ‰‹å·¥è®¾è®¡ç‰¹å¾çš„</li></ol><p>æ­¤ç±»æ–¹æ³•æ— éœ€æ‰‹å·¥è®¾è®¡ç‰¹å¾æå–æ–¹æ³•ï¼Œè€Œæ˜¯è‡ªé€‚åº”çš„ä»åŸå§‹æ—¶é—´åºåˆ—ä¸­å­¦ä¹ æ—¶é—´åºåˆ—çš„åŠ¨æ€ã€‚</p><ol start="4"><li>åŸºäºé¢‘ç‡åŸŸç‰¹å¾çš„ï¼š</li></ol><p>[3]: å°†ECGæ•°æ®è¿›è¡Œå°æ³¢å˜æ¢ï¼ˆDWTï¼‰åï¼Œè”åˆä½¿ç”¨ä¸‰ç§é™ç»´æ–¹æ³•ï¼ˆLDAï¼ŒICAï¼ŒPCAï¼‰å¯¹DWTç»“æœè¿›è¡Œé™ç»´ï¼Œæœ€åé€å…¥SVMæˆ–PNNï¼ˆprobabilistic neural networkï¼‰è¿›è¡Œåˆ¤åˆ«</p><ol start="5"><li>åŸºäºå½¢æ€å­¦ç‰¹å¾çš„ï¼š</li></ol><p>[6]: ä½¿ç”¨Triadic Motif Field Imagesæè¿°å‡†å‘¨æœŸåºåˆ—ä¸­æ‰€åŒ…å«çš„motifsï¼Œç„¶åå€ŸåŠ©VGG-16ä½œä¸ºç‰¹å¾æå–å™¨å¯¹TMF imagesè¿›è¡Œç‰¹å¾æå–ä»¥æ”¯æŒå¼‚å¸¸æ£€æµ‹ã€‚</p><ol start="6"><li>åŸºäºæ·±åº¦å­¦ä¹ çš„ï¼š</li></ol><p>[8]: æå‡ºä¸€ç§Hybrid Attentional LSTM-CNN Modelï¼Œå®ƒç»“åˆäº†LSTMä¸CNNï¼Œåˆ†åˆ«ç”¨äºæå–å‡†å‘¨æœŸæ—¶é—´åºåˆ—ä¸­çš„è¶‹åŠ¿å˜åŒ–ä¸å±€éƒ¨ç‰¹å¾å˜åŒ–ã€‚</p><hr><h2 id="ä»£è¡¨æ€§è®ºæ–‡10ç¯‡"><a href="#ä»£è¡¨æ€§è®ºæ–‡10ç¯‡" class="headerlink" title="ä»£è¡¨æ€§è®ºæ–‡10ç¯‡"></a>ä»£è¡¨æ€§è®ºæ–‡10ç¯‡</h2><ol><li>åŸºäºæ‰‹å·¥è®¾è®¡çš„ç‰¹å¾</li></ol><p>[1] Ma, J., Sun, L., Wang, H., Zhang, Y., &amp; Aickelin, U. (2016). Supervised anomaly detection in uncertain pseudoperiodic data streams. ACM Transactions on Internet Technology (TOIT), 16(1), 1-20.</p><p>[2] Desai, U., Martis, R. J., Acharya, U. R., Nayak, C. G., Seshikala, G., &amp; SHETTY K, R. A. N. J. A. N. (2016). Diagnosis of multiclass tachycardia beats using recurrence quantification analysis and ensemble classifiers. Journal of Mechanics in Medicine and Biology, 16(01), 1640005.</p><ol start="2"><li>æ— éœ€æ‰‹å·¥è®¾è®¡ç‰¹å¾çš„</li></ol><p>[3] Martis, R. J., Acharya, U. R., &amp; Min, L. C. (2013). ECG beat classification using PCA, LDA, ICA and discrete wavelet transform. Biomedical Signal Processing and Control, 8(5), 437-448.</p><p><a href="%E6%8F%90%E5%87%BA%E4%BA%86%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AEDFT%E7%9A%84%E5%91%A8%E6%9C%9F%E6%80%A7%E7%82%B9%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95">4</a> ErkuÅŸ, E. C., &amp; PurutÃ§uoÄŸlu, V. (2020). Outlier detection and quasi-periodicity optimization algorithm: Frequency domain based outlier detection (FOD). European Journal of Operational Research.</p><p><a href="%E8%B0%83%E7%A0%94%E4%BA%86%E7%94%A8%E4%BA%8E%E5%87%86%E5%91%A8%E6%9C%9F%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%9A%8410%E7%A7%8D%E9%A2%91%E7%8E%87%E5%9F%9F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E6%96%B9%E6%B3%95%E5%8F%8A%E5%85%B6%E7%89%B9%E6%80%A7">5</a> Iskhakova, A. O., Alekhin, M. D., &amp; Bogomolov, A. V. (2020). Time-frequency transforms in analysis of non-stationary quasi-periodic biomedical signal patterns for acoustic anomaly detection. Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, (1), 15-23.</p><p>[6] Zhang, Y., &amp; Chen, X. (2020). Anomaly Detection in Time Series with Triadic Motif Fields and Application in Atrial Fibrillation ECG Classification. arXiv preprint arXiv:2012.04936.</p><p><a href="%E9%92%88%E5%AF%B9ECG%E4%BF%A1%E5%8F%B7%E9%A6%96%E5%85%88%E5%88%A9%E7%94%A8STFT%E6%8F%90%E5%8F%96%E9%A2%91%E5%9F%9F%E7%89%B9%E5%BE%81%EF%BC%8C%E7%84%B6%E5%90%8E%E8%AE%A1%E7%AE%97%E6%AF%8F%E4%B8%AA%E9%A2%91%E5%B8%A6%E7%9A%84AMDF%EF%BC%8C%E7%94%A8%E4%BB%A5%E8%A7%A3%E6%9E%90ECG%E4%B8%AD%E7%9A%84%E9%95%BF%E7%9F%AD%E6%9C%9F%E4%BE%9D%E8%B5%96%EF%BC%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E5%80%9F%E5%8A%A9LSTM%E5%BB%BA%E6%A8%A1%E9%95%BF%E7%9F%AD%E6%9C%9F%E4%BE%9D%E8%B5%96%E5%B9%B6%E5%81%9A%E6%AD%A3%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E3%80%82">7</a> Ngo, D., &amp; Veeravalli, B. (2015, November). Design of a real-time morphology-based anomaly detection method from ECG streams. In 2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) (pp. 829-836). IEEE.</p><p>[8] Liu, F., Zhou, X., Cao, J., Wang, Z., Wang, T., Wang, H., &amp; Zhang, Y. (2020). Anomaly Detection in Quasi-Periodic Time Series based on Automatic Data Segmentation and Attentional LSTM-CNN. IEEE Transactions on Knowledge and Data Engineering.</p><p><a href="%E5%88%A9%E7%94%A8LSTM%E7%BD%91%E7%BB%9C%E6%9E%84%E5%BB%BA%E4%BA%86%E4%B8%80%E4%B8%AA%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B9%B6%E4%BD%BF%E7%94%A8%E5%A4%9A%E7%BB%B4%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E6%8B%9F%E5%90%88%E8%AF%A5%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B%E8%AF%AF%E5%B7%AE%E3%80%82%E9%80%9A%E8%BF%87%E5%88%A4%E6%96%AD%E9%A2%84%E6%B5%8B%E8%AF%AF%E5%B7%AE%E6%98%AF%E5%90%A6%E5%B1%9E%E4%BA%8E%E6%89%80%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E6%9D%A5%E8%BF%9B%E8%A1%8C%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E3%80%82">9</a> Thill, M., DÃ¤ubener, S., Konen, W., &amp; BÃ¤ck, T. (2019). Anomaly Detection in Electrocardiogram Readings with Stacked LSTM Networks. In ITAT (pp. 17-25).</p><p><a href="%E4%BD%BF%E7%94%A8%E7%94%B1LSTM%E6%9E%84%E5%BB%BA%E7%9A%84seq2seq%E6%A8%A1%E5%9E%8B%E5%BB%BA%E6%A8%A1%E6%AD%A3%E5%B8%B8%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%EF%BC%8C%E5%B9%B6%E4%BD%BF%E7%94%A8%E9%87%8D%E5%BB%BA%E8%AF%AF%E5%B7%AE%E4%BD%9C%E4%B8%BA%E5%BC%82%E5%B8%B8%E5%88%86%E6%95%B0%E3%80%82">10</a> Malhotra, P., Ramakrishnan, A., Anand, G., Vig, L., Agarwal, P., &amp; Shroff, G. (2016). LSTM-based encoder-decoder for multi-sensor anomaly detection. arXiv preprint arXiv:1607.00148.</p><p><a href="%E5%88%A9%E7%94%A8%E5%88%86%E5%88%AB%E5%88%A9%E7%94%A8LSTM%E4%B8%8ECNN%E5%AF%B9ECG%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E8%9E%8D%E5%90%88%EF%BC%8C%E5%B9%B6%E5%88%A9%E7%94%A8MLP%E4%BD%9C%E4%B8%BA%E5%88%86%E7%B1%BB%E5%99%A8%E4%BB%A5%E6%A3%80%E6%B5%8B%E5%BC%82%E5%B8%B8%E5%BF%83%E9%9F%B3">11</a> Dissanayake, T., Fernando, T., Denman, S., Sridharan, S., Ghaemmaghami, H., &amp; Fookes, C. (2020). A robust interpretable deep learning classifier for heart anomaly detection without segmentation. IEEE Journal of Biomedical and Health Informatics.</p><h2 id="ç»å…¸è®ºæ–‡orå¼ºç›¸å…³è®ºæ–‡"><a href="#ç»å…¸è®ºæ–‡orå¼ºç›¸å…³è®ºæ–‡" class="headerlink" title="ç»å…¸è®ºæ–‡orå¼ºç›¸å…³è®ºæ–‡"></a>ç»å…¸è®ºæ–‡orå¼ºç›¸å…³è®ºæ–‡</h2><p>[8] Liu, F., Zhou, X., Cao, J., Wang, Z., Wang, T., Wang, H., &amp; Zhang, Y. (2020). Anomaly Detection in Quasi-Periodic Time Series based on Automatic Data Segmentation and Attentional LSTM-CNN. IEEE Transactions on Knowledge and Data Engineering.</p><p><a href="%E5%88%A9%E7%94%A8LSTM%E7%BD%91%E7%BB%9C%E6%9E%84%E5%BB%BA%E4%BA%86%E4%B8%80%E4%B8%AA%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B9%B6%E4%BD%BF%E7%94%A8%E5%A4%9A%E7%BB%B4%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E6%8B%9F%E5%90%88%E8%AF%A5%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B%E8%AF%AF%E5%B7%AE%E3%80%82%E9%80%9A%E8%BF%87%E5%88%A4%E6%96%AD%E9%A2%84%E6%B5%8B%E8%AF%AF%E5%B7%AE%E6%98%AF%E5%90%A6%E5%B1%9E%E4%BA%8E%E6%89%80%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E6%9D%A5%E8%BF%9B%E8%A1%8C%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E3%80%82">9</a> Thill, M., DÃ¤ubener, S., Konen, W., &amp; BÃ¤ck, T. (2019). Anomaly Detection in Electrocardiogram Readings with Stacked LSTM Networks. In ITAT (pp. 17-25).</p><p><a href="%E4%BD%BF%E7%94%A8%E7%94%B1LSTM%E6%9E%84%E5%BB%BA%E7%9A%84seq2seq%E6%A8%A1%E5%9E%8B%E5%BB%BA%E6%A8%A1%E6%AD%A3%E5%B8%B8%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%EF%BC%8C%E5%B9%B6%E4%BD%BF%E7%94%A8%E9%87%8D%E5%BB%BA%E8%AF%AF%E5%B7%AE%E4%BD%9C%E4%B8%BA%E5%BC%82%E5%B8%B8%E5%88%86%E6%95%B0%E3%80%82">10</a> Malhotra, P., Ramakrishnan, A., Anand, G., Vig, L., Agarwal, P., &amp; Shroff, G. (2016). LSTM-based encoder-decoder for multi-sensor anomaly detection. arXiv preprint arXiv:1607.00148.</p><p>[1] Dissanayake, T., Fernando, T., Denman, S., Sridharan, S., Ghaemmaghami, H., &amp; Fookes, C. (2020). A robust interpretable deep learning classifier for heart anomaly detection without segmentation. IEEE Journal of Biomedical and Health Informatics.</p><h3 id="å¼‚åŒç‚¹"><a href="#å¼‚åŒç‚¹" class="headerlink" title="å¼‚åŒç‚¹"></a>å¼‚åŒç‚¹</h3><p>[8]æå‡ºçš„æ–¹æ³•æ˜¯æœ‰ç›‘ç£çš„ï¼Œæ— æ³•å¾ˆå¥½çš„é€‚ç”¨äºæ ‡ç­¾ä¸è¶³æˆ–æ ‡ç­¾ä¸ç²¾ç¡®çš„ç°å®æƒ…å†µã€‚ä¸”ç¼ºå°‘å¯è§£é‡Šæ€§ï¼Œæ— æ³•å®šä½å¼‚å¸¸æ‰€åœ¨çš„ä½ç½®ã€‚</p><p>[9, 10] æ‰€æå‡ºçš„æ–¹æ³•ç¼ºä¹å¯è§£é‡Šæ€§</p><p><a href="%E5%88%A9%E7%94%A8%E5%88%86%E5%88%AB%E5%88%A9%E7%94%A8LSTM%E4%B8%8ECNN%E5%AF%B9ECG%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E8%9E%8D%E5%90%88%EF%BC%8C%E5%B9%B6%E5%88%A9%E7%94%A8MLP%E4%BD%9C%E4%B8%BA%E5%88%86%E7%B1%BB%E5%99%A8%E4%BB%A5%E6%A3%80%E6%B5%8B%E5%BC%82%E5%B8%B8%E5%BF%83%E9%9F%B3">11</a> æ‰€ä½¿ç”¨çš„è§£é‡Šæ–¹æ³•ä»…èƒ½å¤Ÿæ˜¾ç¤ºå‡ºæ•°æ®å¯¹æ¨¡å‹è¾“å‡ºçš„è´¡çŒ®ï¼Œè€Œä¸èƒ½æŒ‡å‡ºå¼‚å¸¸çš„æ®µã€‚</p>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
            <tag> survey </tag>
            
            <tag> Quasi-periodic </tag>
            
            <tag> time series </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in ICLR 2021 (May 04 2021)</title>
      <link href="/uncategorized/paperlistfile/ICLR2021/"/>
      <url>/uncategorized/paperlistfile/ICLR2021/</url>
      
        <content type="html"><![CDATA[<p><a href="https://openreview.net/group?id=ICLR.cc/2021/Conference">Accept paper list</a></p><span id="more"></span><h2 id="Anomaly-detection-anomaly-outlier-out-of-distribution-one-class"><a href="#Anomaly-detection-anomaly-outlier-out-of-distribution-one-class" class="headerlink" title="Anomaly detection [anomaly, outlier, out-of-distribution, one-class]"></a>Anomaly detection [anomaly, outlier, out-of-distribution, one-class]</h2><ul><li><p><strong>å·²è¯»</strong>SSD: A Unified Framework for Self-Supervised Outlier Detection </p><p>  Vikash Sehwag, Mung Chiang, Prateek Mittal</p><p>  <strong>Reviewers say:</strong><br>  è¿™é¡¹å·¥ä½œè°ƒæŸ¥äº†ä¸€ä¸ªç»å…¸çš„æ— ç›‘ç£ç¦»ç¾¤å€¼æ£€æµ‹é—®é¢˜ï¼Œå…¶ä¸­æˆ‘ä»¬æ²¡æœ‰ä»»ä½•æ ‡ç­¾ä¿¡æ¯ï¼Œéœ€è¦ä»é‚£äº›æœªæ ‡è®°çš„æ•°æ®ä¸­å­¦ä¹ æ£€æµ‹æ¨¡å‹ï¼Œä»¥å°†ä»»ä½•ä¸ä¸€è‡´çš„æ•°æ®ç‚¹è¯†åˆ«ä¸ºç¦»ç¾¤å€¼ã€‚è¿™é‡Œçš„å…³é”®æ–¹æ³•æ˜¯åº”ç”¨ç°æœ‰çš„è‡ªæˆ‘ç›‘ç£å¯¹æ¯”ç‰¹å¾å­¦ä¹ æ–¹æ³•æ¥æå–ç‰¹å¾è¡¨ç¤ºï¼Œç„¶ååº”ç”¨åŸºäºèšç±»çš„æ–¹æ³•æ¥è®¡ç®—ç¦»ç¾¤å€¼ã€‚å®ƒè¿˜æä¾›äº†ä¸¤ç§åˆ©ç”¨æ ‡è®°çš„å¼‚å¸¸æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ”¹è¿›çš„é©¬å“ˆæ‹‰è¯ºæ¯”æ–¯è·ç¦»æ–¹æ³•å’Œæœ€è¿‘æå‡ºçš„ç›‘ç£å¼å¯¹æ¯”å­¦ä¹ æ–¹æ³•çš„åº”ç”¨ã€‚ä½¿ç”¨å››ä¸ªæ•°æ®é›†è¯„ä¼°äº†è¿™äº›æ–¹æ³•ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸€äº›æ ‡è®°çš„å¼‚å¸¸æ•°æ®çš„æ— ç›‘ç£å’ŒåŠç›‘ç£æ–¹æ³•ã€‚</p><p>  <strong>è§£å†³çš„é—®é¢˜</strong>ï¼šåˆ†å¸ƒå¤–æ•°æ®æ£€æµ‹é—®é¢˜ï¼ˆOut-of-distribution detectionï¼‰</p><p>  <strong>éš¾ç‚¹</strong></p><ul><li>ç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•åœ¨å¤æ‚æ•°æ®å½¢æ€ï¼ˆå¦‚å›¾åƒæ•°æ®ï¼‰ä¸Šæ€§èƒ½ä¸å¥½</li><li>ç°æœ‰çš„åœ¨å›¾åƒæ•°æ®ä¸Šæ€§èƒ½è¾ƒå¥½çš„æ–¹æ³•å¤§å¤šå‡è®¾æœ‰å¾ˆå¤šin-distributionæ•°æ®çš„æ ‡å¯ä»¥è·å¾—</li></ul><p>  <strong>åˆ›æ–°ç‚¹</strong></p><ul><li>æœ¬æ–‡æå‡ºä¸€ä¸ªæ— ç›‘ç£çš„OODæ¡†æ¶ï¼Œå¯ä»¥ä¸ä½¿ç”¨ä»»ä½•åˆ†å¸ƒå†…æ•°æ®çš„æ ‡ç­¾ã€‚</li><li>æœ¬æ–‡å°†few-shotè®¾å®šæ‹“å±•åˆ°æ‰€æå‡ºçš„æ¡†æ¶ä¸­ï¼Œå³å·²çŸ¥å°‘é‡çš„OODæ ·æœ¬ï¼Œå°‘é‡æœ‰æ ‡ç­¾çš„OODæ•°æ®æœ‰åŠ©äºæé«˜OODæ€§èƒ½ã€‚</li><li>æœ¬æ–‡å°†æ‰€æå‡ºçš„SSDä¹Ÿæ‹“å±•åˆ°äº†å·²çŸ¥åˆ†éƒ¨å†…æ•°æ®æ ‡ç­¾çš„æƒ…å†µï¼Œä½¿å¾—æ”¹è¿›åçš„æ–¹æ³•æ¯”åŸå§‹æ–¹æ¡ˆæœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li></ul><p>  <strong>ä¸ºä½•é€‰æ‹©/å¦‚ä½•åº”ç”¨</strong></p><ul><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§å®Œå…¨æ— éœ€æ ‡ç­¾çš„æ–¹æ³•ï¼Œè¿™åœ¨å¤§å¤šæ•°å¼‚å¸¸æ£€æµ‹é—®é¢˜ä¸­éƒ½æ˜¯å¯å€Ÿé‰´çš„</li><li>æœ¬æ–‡çš„å…³é”®æ–¹æ³•åœ¨äºåº”ç”¨ç°æœ‰çš„self-supervised contrastive learningæ–¹æ³•å¯¹æ•°æ®è¿›è¡Œäº†æ— ç›‘ç£çš„ç‰¹å¾æå–ï¼Œè¿™ä¸ºåç»­å·¥ä½œä¸­çš„ç‰¹å¾æå–æä¾›äº†æ€è·¯ã€‚ </li><li>æœ¬æ–‡è¿›ä¸€æ­¥è¯„ä¼°äº†æ”¹è¿›åçš„mahalanobis distanceåœ¨èšç±»æ—¶èƒ½å¤Ÿæ˜¾è‘—æé«˜æ€§èƒ½</li></ul></li><li><p>Learning and Evaluating Representations for Deep One-Class Classification </p><p>  Kihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, Tomas Pfister</p><p>  <strong>One-sentence Summary:</strong> We present a two-stage framework for deep one-class classification, composed of state-of-the-art self-supervised representation learning followed by generative or discriminative one-class classifiers.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡ç ”ç©¶äº†ä¸€ç±»åˆ†ç±»é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å­¦ä¹ è‡ªæˆ‘ç›‘ç£è¡¨ç¤ºå’Œåˆ†å¸ƒå¢å¼ºçš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚å…¨é¢çš„ç»“æœå’Œåˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¯æœ‰æ•ˆçš„ï¼Œå¹¶å°±å…¶èµ·ä½œç”¨çš„æ½œåœ¨æœºåˆ¶æ”¯æŒäº†ä»–ä»¬çš„ä¸»å¼ ã€‚æ€»ä½“è€Œè¨€ï¼Œå°½ç®¡å‘ç°æ–°é¢–æ€§ä¸é«˜ï¼Œä½†å®¡ç¨¿äººè®¤ä¸ºè¯¥è®ºæ–‡å†™å¾—å¥½ï¼ŒåŠ¨æœºå¼º/è®ºè¯åŠ›å¼ºï¼Œå¹¶æä¾›äº†è¯¦å°½çš„ç›¸å…³å·¥ä½œæ¯”è¾ƒå’Œå®éªŒã€‚å‡ ä½å®¡ç¨¿äººåœ¨è¯æ˜è¡¨ç¤ºçš„ç»Ÿä¸€æ€§ä»¥åŠå»ºè®®å…¶ä»–æ•°æ®é›†æ–¹é¢æå‡ºäº†ä¸€äº›å¯èƒ½çš„å¼±ç‚¹ã€‚é€šè¿‡æœ‰è¶£çš„è®¨è®ºï¼Œä½œè€…åœ¨Mvtecæ•°æ®é›†ä¸Šæä¾›äº†å…¶ä»–å¯è§†åŒ–æ•ˆæœå’Œç»“æœã€‚è¿™è¿›ä¸€æ­¥æ”¯æŒäº†æœ¬æ–‡çš„è®ºç‚¹ã€‚</p></li></ul><ul><li><p>Explainable Deep One-Class Classification </p><p>  Philipp Liznerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe Franks, Marius Kloft, Klaus Robert Muller</p><p>  <strong>One-sentence Summary:</strong> We introduce an approach to explainable deep anomaly detection based on fully convolutional neural networks. </p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æ¶‰åŠå¯è§£é‡Šçš„å¼‚å¸¸æ£€æµ‹ã€‚ä¸ºæ­¤ï¼Œå®ƒå°†è¶…çƒé¢åˆ†ç±»å™¨ä¿®æ”¹ä¸ºå®Œå…¨å·ç§¯æ•°æ®æè¿°ï¼ˆFCDDï¼‰ã€‚æ­£å¦‚ä¸¤ä¸ªè¯„è®ºè€…æ‰€æŒ‡å‡ºçš„é‚£æ ·ï¼Œè¿™æ˜¯åœ¨è¶…çƒé¢åˆ†ç±»å™¨ä¸­ç›´æ¥åº”ç”¨å…¨å·ç§¯ç½‘ç»œçš„æ–¹æ³•ã€‚ä½†æ˜¯ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨å…·æœ‰å›ºå®šé«˜æ–¯æ ¸çš„è·¨æ­¥è½¬ç½®å·ç§¯å¯¹æ¥æ”¶åœºè¿›è¡Œä¸Šé‡‡æ ·ã€‚ä¸¤è€…ä»¥åŠè§£å†³å¯è§£é‡Šçš„å¼‚å¸¸æ£€æµ‹éƒ½å¾ˆé‡è¦ã€‚æ­¤å¤–ï¼Œå®è¯è¯„ä¼°æ˜¯è¯¦å°½æ— é—çš„ï¼Œå¹¶ä¸”ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”æ˜¾ç¤ºå‡ºä¸€äº›å¥½å¤„ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œå¢é‡å¼çš„ï¼Œä½†æ˜¯å¯¹äºä¸€ä¸ªéå¸¸æœ‰è¶£çš„é‡è¦æ¡ˆä¾‹æ¥è¯´ï¼Œå¢é‡å¼çš„ã€‚</p></li><li><p>Multiscale Score Matching for Out-of-Distribution Detection </p><p>  Ahsan Mahmood, Junier Oliva, Martin Andreas Styner</p><p>  <strong>One-sentence Summary:</strong> Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.</p><p>  <strong>Reviewers say:</strong></p><ol><li>ä½œè€…åˆ©ç”¨å¹¶é‡æ–°è°ƒæ•´äº†å™ªå£°æ¡ä»¶è¯„åˆ†ç½‘ç»œï¼ˆNCSNï¼‰ï¼Œè¯¥æ¡ä»¶æœ€åˆç”±Songï¼†Ermonï¼ˆ2019ï¼‰å¼•å…¥ç”¨äºç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºæ£€æµ‹å¤±é…ï¼ˆOODï¼‰å›¾åƒã€‚ä½œè€…å±•ç¤ºäº†åˆ†æ•°åŒ¹é…èƒŒåçš„ç›´è§‰å’ŒåŸºæœ¬åŸç†ï¼Œç„¶åè¿›è¡Œäº†ç­‰ä»·çš„å»å™ªè‡ªåŠ¨ç¼–ç å™¨ï¼ˆDAEï¼‰æ¥å¾—å‡ºNCSNä½œä¸ºåˆ†æ•°ä¼°ç®—å™¨ï¼Œå¹¶æä¾›äº†åˆ†æä»¥è¯æ˜å¤šå°ºåº¦åˆ†æ•°åˆ†æçš„ä»·å€¼ã€‚åœ¨å¯¹SVHNå’ŒCIFARæ•°æ®é›†è¿›è¡Œçš„å®éªŒåˆ†æä¸­ï¼Œä»–ä»¬è¯æ˜äº†ä»–ä»¬çš„æ–¹æ³•ï¼ˆMSMAï¼‰ä¼˜äºä»¥å‰ä½¿ç”¨OODä»»åŠ¡æ¨¡å‹ï¼ˆODINï¼ŒJEMï¼Œä¼¼ç„¶æ¯”ï¼‰åœ¨æ–‡çŒ®ä¸­æŠ¥é“çš„å‘ç°ã€‚</li><li>æ‘˜è¦ï¼šä»–ä»¬æå‡ºäº†ä¸€ç§æ–°çš„OODæ£€æµ‹æ–¹æ³•ï¼Œå³MSMAï¼Œå®ƒä½¿ç”¨äº†ä¸€ç§æ–°çš„ç”Ÿæˆæ¨¡å‹[NCSN]ï¼Œå¹¶ä¸”åœ¨ä¸åŒè§„æ¨¡çš„ä¼¼ç„¶çŸ¢é‡ä¸Šæ‹Ÿåˆäº†ç®€å•å¯†åº¦æ¨¡å‹çš„ç¬¬äºŒé˜¶æ®µã€‚ä»–ä»¬åœ¨æ ‡å‡†OODå›¾åƒæ•°æ®é›†ï¼ˆCIFAR10ä¸OODï¼ŒSVHNä¸OODç­‰ï¼‰ä¸Šæ˜¾ç¤ºäº†ç»éªŒè‰¯å¥½çš„ç»“æœã€‚ä»–ä»¬èƒ½å¤Ÿåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å®ç°å®Œç¾çš„åˆ†ç¦»ï¼Œå¹¶ä¸”ä¸ä»¥å‰çš„æ— ç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼ŒCIFAR10ä¸SVHNçš„ç»“æœå¤§å¤§æé«˜ã€‚ä»–ä»¬æ˜¾ç¤ºäº†åœ¨åŒ»å­¦å›¾åƒä¸­æ£€æµ‹OODçš„æœ‰è¶£åº”ç”¨ï¼Œè¿™äº›å›¾åƒçš„æ‰«æèŒƒå›´ä¸º9-11å²ï¼Œå¹¶ä¸”OODä¸º&lt;9å²ã€‚</li><li>æœ¬æ–‡å°†å¤šå°ºåº¦å¾—åˆ†ä¼°è®¡åº”ç”¨äºåˆ†å¸ƒå¤–æ£€æµ‹ã€‚ä»–ä»¬è¯æ˜äº†å¤šå°ºåº¦ä¼°è®¡çš„æœ‰ç”¨æ€§ï¼Œå¹¶é‡‡ç”¨äº†è¾…åŠ©æ¨¡å‹æ¥è¯†åˆ«å¼‚å¸¸æ•°æ®ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸¤ä¸ªä¸åŒçš„è®¾ç½®ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¯¹äºåˆ†å¸ƒä¸å‡çš„æ£€æµ‹éå¸¸æœ‰æ•ˆã€‚</li></ol></li></ul><ul><li><p>In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness </p><p>  Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, Percy Liang</p><p>  <strong>One-sentence Summary:</strong> Using auxiliary information as inputs hurts OOD, but using auxiliary information by pretraining and self-training improves in-distribution and OOD accuracies on real-world datasets, with theoretical guarantees in a linear multi-task setting.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡é€šè¿‡åˆ©ç”¨å¯ç”¨çš„è¾…åŠ©ä¿¡æ¯è§£å†³äº†åœ¨æ³¨é‡Šæ•°æ®å¾ˆå°‘çš„æƒ…å†µä¸‹æ”¹å–„æ³›åŒ–çš„é—®é¢˜ã€‚ä½œè€…è€ƒè™‘äº†ä¸¤ç§é€‰æ‹©çš„å„è‡ªä¼˜ç‚¹ï¼šåœ¨å¤šä»»åŠ¡æˆ–ä¼ è¾“è®¾ç½®ä¸­ï¼Œä½¿ç”¨è¾…åŠ©ä¿¡æ¯ä½œä¸ºè¡¥å……è¾“å…¥æˆ–ä½œä¸ºé™„åŠ è¾“å‡ºã€‚å¯¹äºçº¿æ€§å›å½’ï¼Œä»–ä»¬ä»ç†è®ºä¸Šè¡¨æ˜å‰è€…å¯ä»¥å¸®åŠ©æ”¹å–„åˆ†å¸ƒè¯¯å·®ï¼Œä½†å¯èƒ½ä¼šæŸå®³OODè¯¯å·®ï¼Œè€Œåè€…å¯èƒ½æœ‰åŠ©äºæ”¹å–„OODè¯¯å·®ã€‚ä»–ä»¬æå‡ºäº†ä¸€ä¸ªå°†è¿™ä¸¤ç§é€‰æ‹©ç»“åˆèµ·æ¥çš„æ¡†æ¶ï¼Œå¹¶å‡­ç»éªŒè¡¨æ˜å®ƒåœ¨ä¸‰ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šåšåˆ°äº†ã€‚</p></li></ul><h2 id="Heterogeneous"><a href="#Heterogeneous" class="headerlink" title="Heterogeneous"></a>Heterogeneous</h2><ul><li><p>Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks </p><p>  Timothy Castiglia, Anirban Das, Stacy Patterson</p></li><li><p>HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients </p><p>  Enmao Diao, Jie Ding, Vahid Tarokh</p></li></ul><h2 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h2><ul><li>Generative Time-series Modeling with Fourier Flows   Ahmed Alaa, Alex James Chan, Mihaela van der Schaar</li></ul><ul><li><p>Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies </p><p>  T. Konstantin Rusch, Siddhartha Mishra</p><p>  <strong>One-sentence Summary:</strong> A biologically motivated and discretized ODE based RNN for learning long-term dependencies, with rigorous bounds mitigating the exploding and vanishing gradient problem.</p></li></ul><ul><li><p>Multi-Time Attention Networks for Irregularly Sampled Time Series </p><p>  Satya Narayan Shukla, Benjamin Marlin</p><p>  <strong>One-sentence Summary:</strong> This paper presents a new deep learning architecture for learning with sparse and irregularly sampled multivariate time series.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡è®¨è®ºä¸è§„åˆ™æ ·æœ¬æ—¶é—´åºåˆ—çš„åˆ†æã€‚è¯¥æ–¹æ³•ä¸»è¦åŸºäºæ’å€¼ã€‚å› æ­¤ï¼Œä½œè€…å¯ä»¥ç ”ç©¶æœ‰ç›‘ç£å’Œæ— ç›‘ç£çš„é—®é¢˜ã€‚è¯¥æ¶æ„ç”±æ­£å¼¦æ³¢æ³¨æ„å±‚ï¼Œå¯åœ¨æ½œåœ¨ç©ºé—´ä¸­å½¢æˆå›ºå®šå¤§å°çš„ç•Œæ ‡çš„VAEå±‚å’ŒRNNè§£ç å™¨ç»„æˆã€‚å¯¹äºç›‘ç£ä»»åŠ¡ï¼Œä½œè€…æ·»åŠ äº†åˆ†ç±»æŸå¤±ã€‚</p><ul><li>ä»–ä»¬åœ¨æ’å€¼ä»»åŠ¡ä¸­è·å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œè€Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­è·å¾—äº†æœ‰è¶£çš„ç»“æœã€‚</li><li>åœ¨æ’å€¼é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬æƒ³å°†ç¨³å¥çš„åŸºçº¿è§†ä¸ºçº¿æ€§æ’å€¼æˆ–ç±»ä¼¼ARçš„æ¨¡å‹ã€‚å³ä½¿æˆ‘å¿…é¡»æ‰¿è®¤ä½œè€…å·²ç»æå‡ºäº†ä¸æœ€è¿‘æ–‡çŒ®çš„æ¨¡å‹è¿›è¡Œçš„å¤§é‡æ¯”è¾ƒï¼Œè¿™ä¹Ÿå°†ä¸ºæˆ‘ä»¬æä¾›æœ‰æ„ä¹‰çš„MSEç»“æœï¼Œä»¥æ¯”è¾ƒå…¶ä»–æ–¹æ³•ã€‚</li><li>ç»“æœä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†æˆ‘ä¸çŸ¥é“æ¶æ„çš„å“ªä¸ªéƒ¨åˆ†ä¼šå¸¦æ¥å¦‚æ­¤å‡ºè‰²çš„æ€§èƒ½</li></ul></li><li><p>Generative Time-series Modeling with Fourier Flows </p><p>  Ahmed Alaa, Alex James Chan, Mihaela van der Schaar</p><p>  <strong>Reviewers say:</strong></p><ol><li><p>ä½œè€…æå‡ºäº†å‚…é‡Œå¶åŸŸä¸­æ—¶é—´åºåˆ—çš„æµé‡ç”Ÿæˆæ¨¡å‹ã€‚é¦–å…ˆå°†æ—¶é—´åºåˆ—æ•°æ®è½¬æ¢ä¸ºå‚…ç«‹å¶åŸŸã€‚ä»£æ›¿å…ˆå‰æ–‡çŒ®ä¸­æå‡ºçš„ä»¿å°„è€¦åˆå±‚ï¼Œä½œè€…è®¾è®¡äº†ä»¿å°„è€¦åˆå±‚çš„é¢‘åŸŸç‰ˆæœ¬ã€‚</p></li><li><p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å·ç§¯æµä½“ç³»ç»“æ„ï¼Œè¯¥ä½“ç³»ç»“æ„ä½¿ç”¨DFTå°†ç”Ÿæˆçš„æ—¶é—´åºåˆ—è½¬æ¢ä¸ºé¢‘åŸŸã€‚å·ç§¯æ˜¯é€šè¿‡é¢‘è°±ä»¿å°„å±‚åœ¨é¢‘åŸŸä¸­è¿›è¡Œä¹˜æ³•æ¥å®Œæˆçš„ï¼Œè¯¥ä»¿å°„å±‚ä½¿ç”¨ä¾èµ–äºæ•°æ®çš„æ»¤æ³¢å™¨å¯¹ä¿¡å·çš„å¶æ•°æˆ–å¥‡æ•°éƒ¨åˆ†è¿›è¡Œå˜æ¢ã€‚æ‰€å¾—çš„æ—¶åŸŸå·ç§¯å…·æœ‰ä¸è¾“å…¥æœ‰å…³çš„æƒé‡ï¼Œè¿™æ˜¯ä¸€ç§æœ‰è¶£çš„åŸå§‹æ–¹æ³•ï¼Œä¸å…¶ä»–å·ç§¯æµï¼ˆä¾‹å¦‚[1]ï¼‰æ˜æ˜¾ä¸åŒã€‚</p><ul><li>ç›¸å…³æ€§ï¼šæ—¶é—´åºåˆ—ç”Ÿæˆå»ºæ¨¡åœ¨ä»åŒ»å­¦åˆ°é‡‘èçš„å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›çš„å…³é”®åº”ç”¨ã€‚æ–°æ–¹æ³•æ˜¾ç¤ºå‡ºéå¸¸æœ‰å‰é€”çš„æ€§èƒ½ï¼Œå¹¶ä¸”æœ‰å¯èƒ½æˆä¸ºæ—¶é—´åºåˆ—ç”Ÿæˆçš„æœ€æ–°æ–¹æ³•ã€‚</li><li>ç‹¬åˆ›æ€§ï¼šDFTå’Œè°±ä»¿å°„å±‚çš„ä½¿ç”¨åœ¨å½’ä¸€åŒ–æµé‡çš„èƒŒæ™¯ä¸‹æ˜¯ç‹¬åˆ›çš„ã€‚é‡è¦çš„æ˜¯ï¼ŒDFTéå¸¸é€‚åˆæµåŠ¨ï¼Œå› ä¸ºå®ƒæ˜¯ç­‰è½´æµ‹â€‹â€‹å›¾ï¼Œå› æ­¤å…·æœ‰å¾ˆå°çš„é›…å¯æ¯”ã€‚å³ä½¿åœ¨å¸¸è§„ConvNetä½“ç³»ç»“æ„ä¸­ï¼Œä¾èµ–è¾“å…¥çš„å·ç§¯çš„ä½¿ç”¨ä¹Ÿéå¸¸æœ‰è¶£ã€‚</li></ul></li></ol></li><li><p>Clairvoyance: A Pipeline Toolkit for Medical Time Series </p><p>  Daniel Jarrett, Jinsung Yoon, Ioana Bica, Zhaozhi Qian, Ari Ercole, Mihaela van der Schaar</p><p>  <strong>One-sentence Summary:</strong> We develop and present Clairvoyance: a pipeline toolkit for medical time series.</p><p>  <strong>Reviewers say:</strong> è¯¥æ‰‹ç¨¿ä»‹ç»å¹¶è¯´æ˜äº†ç”¨äºæ—¶é—´åºåˆ—æ•°æ®çš„åŒ»å­¦æœºå™¨å­¦ä¹ çš„ç«¯åˆ°ç«¯è½¯ä»¶ç®¡é“ï¼Œç§°ä¸ºClairvoyanceã€‚å¿…é¡»ä¸ºè®¾è®¡å’Œå¼€å‘äº†è¿™ä¸€å‡ºè‰²çš„èµ„æºï¼Œä»¥åŠ é€Ÿè¿™äº›è®¡ç®—æŠ€æœ¯åœ¨ä¸´åºŠå®è·µä¸­çš„é‡‡ç”¨ï¼Œä»¥æ”¯æŒäººä»¬çš„åˆ¤æ–­å’Œå†³ç­–çš„æ–¹å¼ï¼Œå¯¹ä½œè€…è¡¨ç¤ºç¥è´ºã€‚è¯¥æ‰‹ç¨¿æ“…é•¿æè¿°å…¶è´¡çŒ®å¹¶å°†å…¶ä¸ç›¸å…³å·¥ä½œè”ç³»èµ·æ¥ã€‚å®ƒè¿˜åŒ…æ‹¬ä¸€ç»„ä»¤äººä¿¡æœçš„å®éªŒï¼Œè¿™äº›å®éªŒæ˜¯æ¥è‡ªä¸‰ä¸ªç›¸äº’è¡¥å……çš„åŒ»å­¦ç¯å¢ƒçš„æ•°æ®é›†</p></li><li><p>Discrete Graph Structure Learning for Forecasting Multiple Time Series </p><p>  Chao Shang, Jie Chen, Jinbo Bi</p><p>  <strong>One-sentence Summary:</strong> We propose a graph neural network approach that learns a graph structure to enhance the forecasting of multiple multivariate time series.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å°è¯•é€šè¿‡å­¦ä¹ çš„å›¾ç»“æ„ä¼°ç®—å„ä¸ªç»´åº¦ä¹‹é—´çš„ç›¸å…³æ€§æ¥è¿›è¡Œå¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹çš„æ–¹æ³•ã€‚ç»´åº¦è¢«è§†ä¸ºå›¾å½¢ä¸­çš„èŠ‚ç‚¹ï¼Œé—®é¢˜è¢«æ˜ å°„åˆ°å­¦ä¹ ç¦»æ•£å›¾å½¢ç»“æ„ï¼Œè¯¥ç»“æ„å¯ä»¥å¸®åŠ©è¿›è¡Œä¸‹æ¸¸é¢„æµ‹ä»»åŠ¡ã€‚è¯¥è®ºæ–‡è¡¨æ˜ï¼Œå³ä½¿æ˜¾å¼ç»“æ„æœªçŸ¥ï¼Œä¹Ÿå¯ä»¥åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¥æé«˜é¢„æµ‹æ€§èƒ½ã€‚è¿™æ˜¯åœ¨ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼å­¦ä¹ å›¾å½¢ç»“æ„å’Œé¢„æµ‹ä½“ç³»ç»“æ„çš„åŒæ—¶å®ç°çš„ã€‚ä¸åœ¨å…ƒå­¦ä¹ æ¡†æ¶ä¸­å­¦ä¹ ç¦»æ•£å›¾ç»“æ„çš„åŒå±‚ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸Šæ›´é«˜ã€‚è¯¥æ–¹æ³•è¿˜è¢«è¦æ±‚èƒ½å¤Ÿé€šè¿‡æå‡ºç¡®ä¿æ‰€å­¦ä¹ çš„å›¾ç»“æ„ä¿æŒä¸å·²çŸ¥å›¾ç»“æ„æ¥è¿‘çš„æ­£åˆ™åŒ–å™¨æ¥åˆå¹¶å›¾ç»“æ„çš„å…ˆéªŒçŸ¥è¯†ã€‚ä¸åœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šä½¿ç”¨çš„å‡ ç§å¼ºåŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å¯ä»¥æé«˜é¢„æµ‹æ€§èƒ½ã€‚é€šå¸¸ï¼Œè¯¥è®ºæ–‡å†™å¾—å¾ˆå¥½å¹¶ä¸”æ˜“äºé˜…è¯»ã€‚</p></li><li><p>Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding </p><p>  Sana Tonekaboni, Danny Eytan, Anna Goldenberg</p><p>  <strong>One-sentence Summary:</strong> An unsupervised representation learning framework for high-dimensional non-stationary time series </p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ—¶é—´åºåˆ—çš„æ— ç›‘ç£è¡¨ç¤ºï¼ˆåµŒå…¥ï¼‰å­¦ä¹ æ–¹æ³•ã€‚å°½ç®¡æ— ç›‘ç£çš„è¡¨ç¤ºå­¦ä¹ å·²è¢«å¹¿â€‹â€‹æ³›ç ”ç©¶ï¼Œå¹¶åœ¨NLPå’Œè§†è§‰ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰¯å¥½çš„è¡¨ç°ï¼Œä½†å¯¹äºæ—¶é—´åºåˆ—ç¤¾åŒºè€Œè¨€ï¼Œå®ƒç›¸å¯¹è¾ƒæ–°ã€‚ä¸æœ€è¿‘çš„å·¥ä½œï¼ˆCPCå’Œä¸‰é‡æŸå¤±ï¼‰ç›¸æ¯”ï¼Œæœ¬æ–‡å…·æœ‰ä»¥ä¸‹å·®å¼‚ï¼š</p><ol><li>å®ƒä½¿ç”¨å¹³ç¨³æ€§/éå¹³ç¨³æ€§çš„ç»Ÿè®¡æµ‹è¯•æ¥ä¼°è®¡å›ºå®šçš„æ—¶é—´çª—ã€‚</li><li>å®ƒä½¿ç”¨CPCå’ŒTriplet-lossä¸­çš„å¯¹æ¯”å­¦ä¹ æ¥å­¦ä¹ åµŒå…¥ï¼Œä½†æ˜¯è¿˜è€ƒè™‘åˆ°å¤©çœŸçš„è´Ÿé‡‡æ ·å¯èƒ½åŒ…å«è™šå‡è´Ÿæ•°ï¼Œå¹¶ä¸”ä¸åˆ©äºåœ¨å­£èŠ‚æ€§å¼ºçš„æ—¶é—´åºåˆ—ä¸Šè¿›è¡ŒåµŒå…¥å­¦ä¹ çš„äº‹å®ã€‚ç›¸åï¼Œå®ƒé‡‡ç”¨äº†â€œç§¯ææ— æ ‡ç­¾å­¦ä¹ â€æ¡†æ¶æ¥è§£å†³æ­¤é—®é¢˜ã€‚</li></ol></li></ul><h2 id="About-deep-learning"><a href="#About-deep-learning" class="headerlink" title="About deep learning"></a>About deep learning</h2><ul><li><p>Understanding Over-parameterization in Generative Adversarial Networks </p><p>  Yogesh Balaji, Mohammadmahdi Sajedi, Neha Mukund Kalibhat, Mucong Ding, Dominik StÃ¶ger, Mahdi Soltanolkotabi, Soheil Feizi</p><p>  <strong>One-sentence Summary:</strong> We present an analysis of over-paramterization in GANs both theoretically and empirically.</p><p>  Reviewers say : æœ¬æ–‡ä»ç†è®ºå’Œç»éªŒä¸Šç ”ç©¶äº†åœ¨GANè®­ç»ƒä¸­å¢åŠ å‚æ•°æ•°é‡ï¼ˆâ€œè¿‡å‚æ•°åŒ–â€ï¼‰çš„æ•ˆæœã€‚ä¸ç¥ç»ç½‘ç»œåœ¨ç›‘ç£å­¦ä¹ ä¸­å‘ç”Ÿçš„æƒ…å†µç±»ä¼¼ï¼Œè¿‡åº¦å‚æ•°åŒ–ç¡®å®æœ‰åŠ©äºç¨³å®šè®­ç»ƒåŠ¨æ€ï¼ˆå¹¶å‡­ç»éªŒæé«˜æ€§èƒ½ï¼‰ã€‚æœ¬æ–‡ä¸º1å±‚ReLUç½‘ç»œç”Ÿæˆå™¨çš„å®½åº¦æä¾›äº†ä¸€ä¸ªæ˜ç¡®çš„é˜ˆå€¼ï¼Œä»¥ä¾¿ä½¿ç”¨çº¿æ€§é‰´åˆ«å™¨è¿›è¡Œæ¢¯åº¦ä¸Šå‡è®­ç»ƒå¯äº§ç”Ÿä¸å…¨å±€éç‚¹çš„çº¿æ€§æ”¶æ•›é€Ÿåº¦ï¼ˆè¿™å¯¹åº”äºä¸æ•°æ®å‡å€¼ï¼‰ã€‚ä½œè€…è¿˜æä¾›äº†ä¸€ä¸ªæ›´é€šç”¨çš„å®šç†ï¼Œå°†è¿™ä¸ªç»“æœæ¨å¹¿åˆ°æ›´æ·±çš„ç½‘ç»œã€‚</p></li></ul><ul><li><p>Understanding the role of importance weighting for deep learning </p><p>  Da Xu, Yuting Ye, Chuanwei Ruan</p><p>  <strong>One-sentence Summary:</strong> We study the theoretical properties of importance weighting for deep learning.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡ç ”ç©¶äº†æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­é‡è¦æ€§åŠ æƒæ–¹æ¡ˆå¯¹æ¢¯åº¦ä¸‹é™çš„éšå¼åå·®çš„å½±å“ã€‚å®ƒæä¾›äº†ä¸€äº›ç†è®ºç»“æœï¼Œå¯ä¸ºé‡è¦æƒé‡æ–¹æ¡ˆå¯¹æ”¶æ•›æé™ä»¥åŠæ”¶æ•›é€Ÿåº¦çš„å½±å“æä¾›é‡è¦è§è§£ã€‚ç»™å‡ºäº†çº¿æ€§åˆ†éš”ç¬¦å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç»“æœã€‚è¿˜ç ”ç©¶äº†åå˜é‡å¹³ç§»è®¾ç½®ã€‚ç†è®ºç»“æœå¾—åˆ°äº†ç»éªŒè®ºè¯çš„æ”¯æŒï¼Œå¹¶ä¸”ä¹Ÿå¾—å‡ºäº†å…³äºå“ªäº›åŠ æƒæ–¹æ¡ˆå°†æ›´æœ‰ç”¨çš„æœ‰ç”¨è§è§£ã€‚ä»–ä»¬è¿˜è§£é‡Šäº†ä¸€äº›ä»¥å‰è§‚å¯Ÿåˆ°çš„ç»éªŒç°è±¡ã€‚</p></li></ul><h2 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h2><ul><li><p>On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines </p><p>  Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow</p><p>  <strong>One-sentence Summary:</strong> We provide an analysis of the fine-tuning instability of BERT-based models and present a simple method to fix it</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡ç¡®å®šäº†NLPæ·±åº¦å­¦ä¹ ä¸­ä¸€ä¸ªä¸»è¦å·²çŸ¥é—®é¢˜èƒŒåçš„åŸå› ï¼šåœ¨è‡ªç›‘ç£å¼é¢„è®­ç»ƒä¹‹åå¯¹å°å‹æ•°æ®é›†è¿›è¡Œå¾®è°ƒå¯èƒ½ä¼šéå¸¸ä¸ç¨³å®šï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹éœ€è¦æ•°åæ¬¡é‡æ–°å¯åŠ¨æ‰èƒ½è¾¾åˆ°å¯æ¥å—çš„æ€§èƒ½ã€‚ç„¶åï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç®€å•çš„å»ºè®®ä¿®å¤æ–¹æ³•ã€‚</p></li><li><p>Multi-timescale Representation Learning in LSTM Language Models </p><p>  Shivangi Mahto, Vy Ai Vo, Javier S. Turek, Alexander Huth</p><p>  <strong>One-sentence Summary:</strong> This work presents a theoretically-motivated analysis of memory and timescale in LSTM language models.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æŒ‡å‡ºè‡ªç„¶è¯­è¨€ä¸­å•è¯ä¹‹é—´çš„å…³ç³»é€šå¸¸éµå¾ªå¹‚å¾‹ã€‚é—¨æ§é€’å½’ç¥ç»ç½‘ç»œï¼ˆä¾‹å¦‚LSTMï¼‰åœ¨è‡ªç„¶è¯­è¨€å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æ˜¯LSTMçš„é—å¿˜æœºåˆ¶æ˜¯ç”±æŒ‡æ•°è¡°å‡å†³å®šçš„ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†ä¸€ç§å·¥ç¨‹åŒ–LSTMçš„é—å¿˜æœºåˆ¶ï¼Œä»¥æ¨¡ä»¿è‡ªç„¶è¯­è¨€ä¸­è¡¨ç°å‡ºçš„å¹‚å¾‹å…³ç³»çš„æ–¹æ³•ã€‚é€šè¿‡åº”ç”¨ä»–ä»¬çš„æŠ€æœ¯ï¼Œä¿®æ”¹åçš„LSTMæ¨¡å‹å¯ä»¥æ›´å¥½åœ°å»ºæ¨¡ç¨€æœ‰æ ‡è®°ï¼Œè¿™äº›æ ‡è®°é€šå¸¸è·¨è¶Šè¾ƒé•¿çš„æ—¶é—´èŒƒå›´ï¼Œå› æ­¤ï¼Œè¯¥æ¨¡å‹å¯ä»¥åœ¨é¢‘ç‡è¾ƒä½çš„å•è¯ä¸Šè·å¾—è¾ƒä½çš„å›°æƒ‘åº¦ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯æ¨å¯¼ï¼Œè¯¥æ¨å¯¼è¡¨æ˜ï¼Œåœ¨ç»™å‡ºç¬¬ä¸€ä¸ªè¾“å…¥ä»¤ç‰Œåï¼ŒLSTMçš„é—å¿˜é—¨åœ¨é›¶è¾“å…¥çŠ¶æ€ä¸‹ä¼šç»å†æŒ‡æ•°è¡°å‡ã€‚</p><p>  å®éªŒè¡¨æ˜ï¼Œä»åä¼½é©¬åˆ†å¸ƒä¸­ç»˜åˆ¶Tæ˜¯è‡ªç„¶è¯­è¨€çš„è‡ªç„¶æ‹Ÿåˆã€‚ç„¶åï¼Œä½œè€…æå‡ºäº†åˆ©ç”¨æ­¤ç‰¹æ€§çš„å¤šå°ºåº¦LSTMæ¨¡å‹ã€‚æ¯ä¸ªæ—¶é—´æ ‡åº¦Tæ˜¯ä»åä¼½ç›åˆ†å¸ƒä¸­å¾—å‡ºçš„ï¼Œè¯¥åä¼½ç›åˆ†å¸ƒå®é™…ä¸Šæˆä¸ºä¸€ä¸ªé—å¿˜åå·®é¡¹ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å›ºå®šçš„ã€‚ç»˜åˆ¶å¤šä¸ªTæ¥æ¨¡æ‹Ÿå¹‚å¾‹ã€‚å¤šå°ºåº¦LSTMå¯ä»¥æ•è·æ­£ç¡®çš„å½’çº³åç½®ï¼Œä»¥ä¾¿åœ¨å¯¹é¢‘ç‡è¾ƒä½çš„å•è¯è¿›è¡Œå»ºæ¨¡æ—¶è¡¨ç°æ›´å¥½ï¼Œè¿™å¯èƒ½æœ‰åŠ©äºåœ¨å†…å­˜ä¸­ä¿ç•™æ›´é•¿çš„æ—¶é—´ã€‚è¯¥è®ºæ–‡å†™å¾—å¾ˆå¥½ï¼Œå¹¶ä¸”è¯¥æ–¹æ³•çš„åŠ¨æœºå’Œè§£é‡Šéƒ½æ¸…æ¥šã€‚å®éªŒç»è¿‡é€‚å½“è®¾è®¡ï¼Œç»“æœå¾ˆå¥½åœ°æ”¯æŒäº†ä¸»è¦ä¸»å¼ ã€‚</p></li><li><p>Representation Learning for Sequence Data with Deep Autoencoding Predictive Components </p><p>  Junwen Bai, Weiran Wang, Yingbo Zhou, Caiming Xiong</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æå‡ºäº†æ·±åº¦è‡ªåŠ¨ç¼–ç é¢„æµ‹ç»„ä»¶ï¼ˆDAPCï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåºåˆ—æ•°æ®çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæ¨¡å‹å­¦ä¹ æœ€å¤§åŒ–é¢„æµ‹ä¿¡æ¯ï¼Œå³è¿‡å»å’Œæœªæ¥æ—¶é—´çª—å£ä¹‹é—´çš„ç›¸äº’ä¿¡æ¯ã€‚ä¸ºäº†é¿å…é€€åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œæå‡ºçš„æ–¹æ³•ä¾èµ–äºä¼˜åŒ–è’™ç‰ˆé‡å»ºçš„ç¬¬äºŒç§æŸå¤±ã€‚</p></li><li><p>Differentiable Segmentation of Sequences </p><p>  Erik ScharwÃ¤chter, Jonathan Lennartz, Emmanuel MÃ¼ller</p><p>  <strong>One-sentence Summary:</strong> We propose an architecture for effective gradient-based learning of segmented models for sequential data.</p></li><li><p>PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences </p><p>  Fan, Xin Yu, Yuhang Ding, Yi Yang, Mohan Kankanhalli</p><p>  <strong>One-sentence Summary:</strong> This paper proposes a point spatio-temporal (PST) convolution to learn representations of raw point cloud sequences by disentangling space and time.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å·ç§¯æ–¹æ³•æ¥ç›´æ¥å¤„ç†åŸå§‹æ—¶ç©ºï¼ˆSTï¼‰ç‚¹äº‘æ•°æ®ã€‚æå‡ºçš„ç‚¹æ—¶ç©ºï¼ˆPSTï¼‰å·ç§¯åœ¨â€œç‚¹ç®¡â€ä¸Šè¿è¡Œï¼Œå¹¶é€šè¿‡æ¯ä¸ªæ—¶é—´æ­¥çš„å…±äº«ç©ºé—´å·ç§¯è§£è€¦ç©ºé—´å’Œæ—¶é—´ï¼Œç„¶åè¿›è¡Œæ—¶é—´å·ç§¯ã€‚å®ƒè¿˜å¼•å…¥äº†è½¬ç½®çš„PSTï¼Œä»¥åœ¨ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼ˆPSTNetï¼‰ä¸­å¯ç”¨é€ç‚¹é¢„æµ‹ã€‚æå‡ºçš„å®éªŒé€šè¿‡ä½¿ç”¨PSTNetå¯¹ç‚¹äº‘åºåˆ—è¿›è¡ŒåŠ¨ä½œè¯†åˆ«å’Œè¯­ä¹‰åˆ†å‰²ï¼Œè¯æ˜äº†è¿™äº›å·ç§¯çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºäº†å¯¹ç›¸å…³æœ€æ–°å·¥ä½œçš„æ”¹è¿›ã€‚</p></li><li><p>Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning </p><p>  Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, Zhaopeng Tu</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡ä»‹ç»äº†å¯¹ç»†ç²’åº¦å±‚çš„å…³æ³¨ï¼Œä»¥è¯„ä¼°å„ä¸ªç¼–ç å™¨å±‚çš„ä½œç”¨å¹¶ç ”ç©¶ç¼–ç å™¨å±‚èåˆçš„å·¥ä½œåŸç†ï¼Œå…¶ä¸­è§£ç å™¨å±‚å¯ä»¥è®¿é—®å„ç§ç¼–ç å™¨å±‚çš„ä¿¡æ¯ï¼Œè€Œä¸æ ‡å‡†Transformerä¸­çš„æœ€ç»ˆç¼–ç å™¨å±‚ä¸åŒã€‚</p><p>  åŸºäºä»¥ä¸‹è§‚ç‚¹ï¼šç¼–ç å™¨åµŒå…¥å±‚å¯¹äºç¼–ç å™¨å±‚èåˆçš„æˆåŠŸè‡³å…³é‡è¦ï¼Œè€Œæœ€ä¸Šå±‚çš„è§£ç å™¨å±‚åˆ™æ›´åŠ å…³æ³¨ç¼–ç å™¨åµŒå…¥å±‚ï¼Œå› æ­¤æå‡ºäº†SurfaceFusionï¼Œè¯¥æ–¹æ³•ä»…å°†ç¼–ç å™¨åµŒå…¥å±‚è¿æ¥åˆ°è§£ç å™¨çš„softmaxå±‚ã€‚ ï¼Œå¯¼è‡´BLEUç­‰æŒ‡æ ‡è·å¾—äº†å¯è§‚çš„æ”¶ç›Šã€‚</p></li><li><p>Seq2Tens: An Efficient Representation of Sequences by Low-Rank Tensor Projections </p><p>  Csaba Toth, Patric Bonnier, Harald Oberhauser</p><p>  <strong>One-sentence Summary:</strong> An Efficient Representation of Sequences by Low-Rank Tensor Projections</p><p>  <strong>Reviewers say:</strong></p><ol><li>æœ¬æ–‡ä»‹ç»äº†è‡ªç”±ä»£æ•°ï¼Œè¿™æ˜¯ä¸€ç§ç»å…¸çš„æ•°å­¦æ¦‚å¿µï¼Œæ˜¯è¡¨ç¤ºä»»æ„é•¿åº¦çš„é¡ºåºæ•°æ®çš„é€šç”¨å·¥å…·ã€‚æ‰€ææ–¹æ³•å…·æœ‰å¸å¼•äººçš„ç†è®ºç‰¹æ€§ï¼Œä¾‹å¦‚ä¿æŒé™æ€ç‰¹å¾æ˜ å°„çš„é€šç”¨æ€§å’Œè¿ç»­è®¾ç½®çš„æ”¶æ•›æ€§ã€‚ä½œè€…è¿›ä¸€æ­¥å»ºè®®ä½¿ç”¨è‡ªç”±ä»£æ•°çš„å †å ç§©1æŠ•å½±ä½œä¸ºåºåˆ—è¡¨ç¤ºçš„è¿‘ä¼¼å€¼ï¼Œä»¥ä½¿å…¶åœ¨è®¡ç®—ä¸Šå¯è¡Œçš„ç¥ç»ç½‘ç»œå±‚ã€‚ä½œè€…é€šè¿‡å°†NNå®ç°ä¸FCNç»“åˆèµ·æ¥ï¼Œä»¥å¤šå…ƒæ—¶é—´åºåˆ—åˆ†ç±»é—®é¢˜ä¸ºåŸºå‡†ï¼Œå¹¶ä»¥GP-VAEæ¨¡å‹ä¸ºåºåˆ—æ•°æ®å½’çº³é—®é¢˜è¿›è¡Œäº†åŸºå‡†ï¼Œè¯´æ˜äº†è¯¥æ–¹æ³•çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚ä¸ä»¥å‰çš„æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æ˜¾ç¤ºå‡ºæ”¹è¿›çš„ç»“æœã€‚</li></ol><ul><li>å¯ç¤ºï¼šæœ¬æ–‡ä¸ºç¤¾åŒºæä¾›äº†NNåœ¨åºåˆ—æ•°æ®ä¸Šçš„é€šç”¨é€¼è¿‘å®šç†çš„æ‰©å±•ï¼Œä»¥åŠå°†é™æ€ç‰¹å¾å›¾è½¬æ¢ä¸ºåºåˆ—ç‰¹å¾çš„é€šç”¨æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ¤åˆ«å’Œç”Ÿæˆé—®é¢˜ä¸Šå‡å…·æœ‰çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li></ul><ol start="2"><li>æœ¬æ–‡é’ˆå¯¹åºåˆ—æ•°æ®æå‡ºäº†ä¸€ç§æœ‰è¶£çš„ä½ç§©å¼ é‡è¡¨ç¤ºå­¦ä¹ æ¨¡å‹ï¼Œç§°ä¸ºSeq2Tensã€‚å¯ä»¥å°†æ‰€æå‡ºçš„æ¨¡å‹ä½œä¸ºSeq2Tenså±‚æ’å…¥ç°æœ‰çš„æœ€æ–°ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼Œä»¥æé«˜æ€§èƒ½ï¼Œè¿™å·²åœ¨æœ¬æ–‡çš„ä¸€äº›åŸºå‡†æ•°æ®é›†ä¸­å¾—åˆ°äº†è¯æ˜ã€‚</li></ol></li></ul><h2 id="Interpretable"><a href="#Interpretable" class="headerlink" title="Interpretable"></a>Interpretable</h2><ul><li><p>Understanding the failure modes of out-of-distribution generalization </p><p>  Vaishnavh Nagarajan, Anders Andreassen, Behnam Neyshabur</p><p>  <strong>One-sentence Summary:</strong> In this theoretical study, we explain why machine learning models rely on spuriously correlated features in the dataset and fail at out-of-distribution generalization.</p></li><li><p>Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability </p><p>  Suraj Srinivas, Francois Fleuret</p><p>  <strong>One-sentence Summary:</strong> Input-gradients in discriminative neural net models capture information regarding an implicit density model, rather than that of the underlying discriminative model which it is intended to explain.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡ä»åŸºäºèƒ½é‡çš„ç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è§‚å¯Ÿç»“æœå‡ºå‘ï¼Œä»ç†è®ºè§’åº¦ç ”ç©¶äº†å¯è§£é‡Šæ€§æ–‡çŒ®ä¸­æå‡ºçš„åŸºäºæ¢¯åº¦çš„å½’å› æ–¹æ³•ã€‚é¦–å…ˆï¼Œä½œè€…æŒ‡å‡ºäº†åŸºäºæ¢¯åº¦çš„å½’å› çš„æ™®éå¼±ç‚¹ï¼Œè¯¥å¼±ç‚¹æºäºä»¥ä¸‹äº‹å®ï¼šè¾“å…¥æ¢¯åº¦æ²¡æœ‰æä¾›æ˜ç¡®çš„è§£é‡Šï¼Œå› ä¸ºsoftmaxè¾“å‡ºçš„å¹³ç§»ä¸å˜æ€§ä½¿å…¶å…·æœ‰ä»»æ„æ€§ã€‚ç„¶åä½œè€…æå‡ºï¼Œå¯ä»¥é€šè¿‡åˆ¤åˆ«æ¨¡å‹â€œåŒ…å«éšå¼â€ç±»æ¡ä»¶å¯†åº¦æ¨¡å‹ï¼ˆæåˆ°çš„æœ‰å…³åŸºäºèƒ½é‡çš„ç”Ÿæˆæ¨¡å‹çš„æœ€æ–°å‘ç°ï¼‰è¿™ä¸€äº‹å®æ¥è§£é‡ŠåŸºäºæ¢¯åº¦çš„å½’å› æ¨¡å‹æˆåŠŸçš„åŸå› ã€‚ç„¶åï¼Œä»–ä»¬ç»§ç»­é˜è¿°è¿™ä¸ªæƒ³æ³•ï¼Œè¯¥æƒ³æ³•è¡¨æ˜å°†éšå¼ç±»æ¡ä»¶ç”Ÿæˆæ¨¡å‹ä¸æ•°æ®çš„â€œçœŸå®â€ç”Ÿæˆæ¨¡å‹å¯¹é½å°†å¦‚ä½•å¸®åŠ©æä¾›ä¸åŸºäºæ¢¯åº¦çš„å½’å› ç›¸å…³çš„ä¿¡æ¯ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡ä¸€ç§æ–°é¢–çš„å®ç°æ–¹å¼æœ‰æ•ˆåœ°ä¿ƒè¿›å¯¹é½å¾—åˆ†åŒ¹é…ï¼Œä»¥åŠå¦‚ä½•å°†è¯¥æœºåˆ¶å®é™…å®ç°ä¸ºæ­£åˆ™åŒ–æˆæœ¬ã€‚ä½œè€…éšåè¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œä»¤äººä¿¡æœåœ°è¯å®äº†å…¶ç†è®ºè§‚ç‚¹çš„é¢„æµ‹ã€‚é¦–å…ˆï¼Œä»–ä»¬è¡¨æ˜ï¼Œé€šè¿‡â€œ GAN-testæ–¹æ³•â€æå‡ºçš„ç»è¿‡è®­ç»ƒçš„åˆ¤åˆ«æ¨¡å‹ï¼Œåœ¨å™ªå£°è¾ƒå°‘çš„æ„ä¹‰ä¸Šä»¥åŠé€šè¿‡åˆ¤åˆ«å‡†ç¡®æ€§è€Œè¨€ï¼Œç”¨åˆ†æ•°åŒ¹é…å’Œå»ºè®®çš„æ¢¯åº¦èŒƒæ•°æ­£åˆ™åŒ–ç”Ÿæˆçš„æ ·æœ¬æ›´å¥½ã€‚æœ€åï¼Œä»–ä»¬è¡¨æ˜ï¼Œæ ¹æ®åƒç´ æ‰°åŠ¨æµ‹è¯•çš„åˆ¤åˆ«ç‰ˆæœ¬ï¼ŒåŸºäºæ¢¯åº¦çš„è§£é‡Šçš„è´¨é‡æ›´å¥½ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ‰°åŠ¨ä»¥ç›¸å…³æ€§é€’å¢é¡ºåºæ’åˆ—çš„åƒç´ æ¥è¯„ä¼°æ¢¯åº¦è¯´æ˜çš„æ–¹æ³•ã€‚æ€»ä¹‹ï¼Œæœ¬æ–‡åœ¨åˆ¤åˆ«æ¨¡å‹ï¼ŒåŸºäºèƒ½é‡çš„ç”Ÿæˆæ¨¡å‹å’ŒåŸºäºæ¢¯åº¦çš„è§£é‡Šä¹‹é—´å»ºç«‹äº†éå¸¸æœ‰è¶£çš„åŸºæœ¬ç†è®ºè”ç³»ï¼Œä½¿ç”¨æ­¤ç†è®ºæ¡†æ¶æ¥è§£é‡ŠåŸºäºæ¢¯åº¦çš„è§£é‡Šå¦‚ä½•å…‹æœsoftmaxä½ç§»ä¸å˜æ€§é—®é¢˜ï¼ˆå¹¶æŒ‡å‡ºåœ¨æœ¬æ–‡ä¸­ï¼‰ï¼Œå¹¶ä»‹ç»äº†å®ç”¨çš„åŸ¹è®­ç¨‹åºï¼Œä»¥åˆ©ç”¨æ‰€è·å¾—çš„ç†è®ºè§è§£æ¥äº§ç”Ÿæ›´å¥½çš„è§£é‡Šï¼Œå¹¶ä¸”åœ¨ä»¿çœŸä¸­ä¹Ÿè¿›è¡Œäº†å®è¯éªŒè¯ã€‚ä¸€ç§é€šè¿‡æ‰°ä¹±ä»¥ç›¸å…³æ€§é€’å¢é¡ºåºæ’åˆ—çš„åƒç´ æ¥è¯„ä¼°æ¢¯åº¦è¯´æ˜çš„æ–¹æ³•ã€‚</p></li></ul><ul><li><p>Getting a CLUE: A Method for Explaining Uncertainty Estimates </p><p>  Javier Antoran, Umang Bhatt, Tameem Adel, Adrian Weller, JosÃ© Miguel HernÃ¡ndez-Lobato</p><p>  <strong>One-sentence Summary:</strong> We introduce a method to help explain uncertainties of any differentiable probabilistic model by perturbing input features.</p></li><li><p>Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking </p><p>  Michael Sejr Schlichtkrull, Nicola De Cao, Ivan Titov</p><p>  <strong>One-sentence Summary:</strong> We present a novel post-hoc interpretation method for graph neural networks, and apply it to analyse two models from the NLP literature.</p></li><li><p>Interpretable Models for Granger Causality Using Self-explaining Neural Networks </p><p>  RiÄards MarcinkeviÄs, Julia E Vogt</p><p>  <strong>One-sentence Summary:</strong> We propose an interpretable framework for inferring Granger causality based on self-explaining neural networks.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡ä¸»è¦æ¶‰åŠåœ¨éçº¿æ€§åŠ¨åŠ›å­¦è®¾ç½®ä¸­å­¦ä¹ å¤šå…ƒæ—¶é—´åºåˆ—ä¸­çš„æ ¼å…°æ°å› æœå…³ç³»ã€‚æ ¸å¿ƒæ–¹æ³•ä½¿ç”¨å¸¦æœ‰ç¨€ç–è¯±å¯¼æ­£åˆ™åŒ–å™¨ï¼ˆåŸºäºå¼¹æ€§ç½‘å’Œå¹³æ»‘åº¦çš„èåˆå¥—ç´¢ï¼‰çš„çŸ¢é‡è‡ªå›å½’å»ºæ¨¡ï¼Œä»¥åŠæœ€è¿‘æå‡ºçš„å…·æœ‰è‡ªè§£é‡Šç¥ç»ç½‘ç»œï¼ˆç”¨äºè§£é‡Šæ€§ï¼‰ã€‚ä½œè€…è¿˜é€šè¿‡å­¦ä¹ å¯¹åŸå§‹æ•°æ®å’Œæ—¶é—´åè½¬æ•°æ®ç¨³å®šçš„æ ¼å…°æ°å› æœç»“æ„æ¥æ‰©å……æ¡†æ¶ã€‚è¯¦å°½çš„ç»éªŒåˆ†ææ˜¯æ ¹æ®æœ€è¿‘çš„GCåŸºå‡†è¿›è¡Œçš„ã€‚æˆ‘å¯¹æœ¬æ–‡çš„ä¸€äº›å…³æ³¨å¦‚ä¸‹</p></li><li><p>Interpreting Knowledge Graph Relation Representation from Word Embeddings </p><p>  Carl Allen, Ivana Balazevic, Timothy Hospedales</p><p>  <strong>One-sentence Summary:</strong> Interpreting the structure of knowledge graph relation representation using insight from word embeddings.</p><p>  <strong>Reviewers say:</strong> ä½œè€…é€šè¿‡å¯¹å®ä½“ä¹‹é—´çš„å…³ç³»è¿›è¡Œåˆ†ç±»æ¥ç ”ç©¶å•è¯è¡¨ç¤ºæ¨¡å‹çš„æ½œåœ¨è¯­ä¹‰å±æ€§ã€‚ç›®çš„æ˜¯è¡¨æ˜å³ä½¿ä¸¤ç§ç±»å‹çš„æ¨¡å‹éƒ½å…·æœ‰ä¸åŒçš„å­¦ä¹ ç›®æ ‡ï¼Œè¯åµŒå…¥å’ŒçŸ¥è¯†å›¾è¡¨ç¤ºä¹Ÿå¯ä»¥å­¦ä¹ å…±åŒçš„æ½œåœ¨ç»“æ„ã€‚ä¸»è¦çš„è´¡çŒ®æ˜¯å¯¹è±¡ä¹‹é—´çš„å…³ç³»åˆ°ç›®æ ‡è¯åµŒå…¥çš„æ˜ å°„ï¼Œè¿™ç§å…³ç³»çš„åˆ†ç±»ä»¥åŠå¯¹æœ€æ–°çŸ¥è¯†å›¾è¡¨ç¤ºçš„è¯„ä¼°ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒçŸ¥è¯†è¡¨ç¤ºæ¨¡å‹éµå¾ªå®šä¹‰çš„å…³ç³»æ¡ä»¶ã€‚</p></li><li><p>Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning </p><p>  Alihan HÃ¼yÃ¼k, Daniel Jarrett, Cem Tekin, Mihaela van der Schaar</p><p>  <strong>One-sentence Summary:</strong> We present a method for learning interpretable representations of behavior to enable auditing, quantifying, and understanding human decision-making processes.</p><p>  <strong>Reviewers say:</strong> è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§ç†è§£å’Œè§£é‡Šå†³ç­–è¡Œä¸ºçš„æ–¹æ³•ã€‚ä½œè€…æ—¨åœ¨ä½¿æ–¹æ³•1ï¼‰é€æ˜ï¼Œ2ï¼‰èƒ½å¤Ÿå¤„ç†éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ï¼Œä»¥åŠ3ï¼‰å¤„ç†è„±æœºæ•°æ®ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬å¼€å‘äº†INTERPOLEï¼Œå®ƒä½¿ç”¨è´å¶æ–¯æŠ€æœ¯æ¥ä¼°è®¡å†³ç­–åŠ¨æ€ä»¥åŠå†³ç­–è¾¹ç•Œã€‚åœ¨æ¨¡æ‹ŸåŸŸå’ŒçœŸå®åŸŸä¸Šçš„ç»“æœè¡¨æ˜ï¼Œä»–ä»¬çš„æ–¹æ³•åœ¨ä¿æŒè¡Œä¸ºå‡†ç¡®æ€§çš„åŒæ—¶è§£é‡Šäº†è¡Œä¸ºæ•°æ®ä¸­çš„å†³ç­–ï¼Œå¹¶ç€é‡äºè§£é‡Šå†³ç­–åŠ¨æ€ï¼Œè€Œä¸æ˜¯ä¸–ç•Œçš„â€œçœŸå®â€åŠ¨æ€ã€‚</p></li><li><p>BERTology Meets Biology: Interpreting Attention in Protein Language Models </p><p>  Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, richard socher, Nazneen Rajani</p><p>  <strong>One-sentence Summary:</strong> We analyze the internal representations of protein language models, and show that attention targets structural and functional properties of protein sequences.</p></li><li><p>Representation learning for improved interpretability and classification accuracy of clinical factors from EEG </p><p>  Garrett Honke, Irina Higgins, Nina Thigpen, Vladimir Miskovic, Katie Link, Sunny Duan, Pramod Gupta, Julia Klawohn, Greg Hajcak</p><p>  <strong>One-sentence Summary:</strong> We use disentangled representations of EEG signals to improve performance on clinical classification tasks, provide interpretable recommendations for post-hoc analysis and allow for extraction of ERPs from novel single EEG trajectories.</p><p>  <strong>Reviewers say:</strong> ä½œè€…å…³æ³¨EEGä¿¡å·çš„åˆ†ç±»ï¼Œä»¥ä¾¿æ ¹æ®EEGä¿¡å·é¢„æµ‹å¹´é¾„ï¼Œæ€§åˆ«ï¼ŒæŠ‘éƒå’Œ1è½´å¤±è°ƒç—‡ã€‚ç»è¿‡æ ‡å‡†çš„é¢„å¤„ç†å’Œå¯é€‰çš„å¹³å‡å€¼ä»¥è·å¾—è¯±å‘çš„ååº”åï¼Œä½œè€…å°†æ ·å“å–‚å…¥å®¹å™¨ä¸­ã€‚-VAEï¼Œç„¶åä½¿ç”¨æ ‡å‡†åˆ†ç±»ç®—æ³•æˆ–SCANæ–¹æ³•æ¥é¢„æµ‹æ ‡ç­¾ã€‚ä½œè€…æŠ¥å‘Šäº†æ¯”åŸºäºæ™šæœŸé˜³æ€§æ½œåŠ›çš„å¸¸è§„æ–¹æ³•æ›´å¥½çš„ç»“æœã€‚ä»–ä»¬è¿˜è¡¨æ˜ï¼Œä»–ä»¬çš„æ–¹æ³•å¯ä»¥ä½¿ç”¨éå¹³å‡EEGæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”åœ¨ERPä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œåä¹‹äº¦ç„¶ã€‚æœ€åï¼Œä½œè€…æ£€æŸ¥å­¦ä¹ åˆ°çš„è¡¨ç¤ºã€‚</p></li><li><p>Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels </p><p>  Xingchen Wan, Binxin Ru, Xiaowen Dong, Michael Osborne</p><p>  <strong>One-sentence Summary:</strong> We propose a NAS method that is sample-efficient, highly performant and interpretable.</p><p>  <strong>Reviewers say:</strong> ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ä½“ç³»ç»“æ„æœç´¢ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†è´å¶æ–¯ä¼˜åŒ–ä¸å¯Œæœ‰è¡¨ç°åŠ›ä¸”å¹¿å—æ¬¢è¿çš„Weisfeiler-Lehmanï¼ˆWLï¼‰å›¾æ ¸ç›¸ç»“åˆã€‚ä½¿ç”¨WLçš„ä¸€ä¸ªä¼˜ç‚¹æ˜¯æºäºå†…æ ¸è®¡ç®—æ–¹å¼çš„æœ¬è´¨çš„å¯è§£é‡Šç»“æœï¼Œå³é€šè¿‡å›¾å½¢çš„ä¼ æ’­æ–¹æ¡ˆã€‚åˆå¹¶æ–¹ç¨‹å¼çš„å¯¼æ•° 3.2å¯ä»¥æå–ç›´æ¥è´Ÿè´£æé«˜æ€§èƒ½çš„å­å›¾ã€‚åœ¨å„ç§å®éªŒä¸­ï¼Œä½œè€…ä¸ä»…æ˜¾ç¤ºå‡ºå·²æ£€æµ‹æ¶æ„çš„æ€§èƒ½æé«˜ï¼Œè€Œä¸”è¿˜å‘ç°äº†å…¶ä»–ç®—æ³•ä¹Ÿå¯ä»¥æ‰¾åˆ°çš„å­å›¾ã€‚<br>  å³ä½¿æˆ‘çš„ä¸“ä¸šçŸ¥è¯†ä¸å±äºNASé¢†åŸŸï¼Œæˆ‘ä»ç„¶è®¤ä¸ºè¿™é¡¹å·¥ä½œå¾ˆæœ‰å¸å¼•åŠ›ã€‚å®ƒæ˜¯å›¾å½¢å†…æ ¸çš„åˆ›æ–°åº”ç”¨ç¨‹åºï¼Œå®ƒå…·æœ‰å¯ä¼¸ç¼©æ€§ï¼Œè€Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯ä¼¸ç¼©æ€§å‡ ä¹æ²¡æœ‰é—®é¢˜ã€‚æˆ‘å‘ç°æ–°é¢–æ€§ï¼Œå¯è§£é‡Šæ€§å’Œå®šé‡ç»“æœæ–¹é¢ä»¤äººä¿¡æœï¼Œè¶³ä»¥å»ºè®®æ‚¨æ¥å—ã€‚æ­¤å¤–ï¼Œä½œå“çš„ç»“æ„å’Œä¹¦é¢ç»“æ„éƒ½å¾ˆå¥½ï¼Œæ•°å­—æ¸…æ™°æ˜“è¯»ã€‚ä¸å…¶ä»–SOTA NASç®—æ³•çš„æ¯”è¾ƒæ˜¯å¦è´¨é‡å¥½ï¼Œæ˜¯å¦å…¬å¹³ï¼Œæˆ‘è®¤ä¸ºå…·æœ‰NASèƒŒæ™¯çš„å®¡é˜…è€…çš„æ„è§å¾ˆæœ‰ä»·å€¼ã€‚</p></li><li><p>Explainable Deep One-Class Classification </p><p>  Philipp Liznerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe Franks, Marius Kloft, Klaus Robert Muller</p><p>  <strong>One-sentence Summary:</strong> We introduce an approach to explainable deep anomaly detection based on fully convolutional neural networks. </p></li><li><p>A Learning Theoretic Perspective on Local Explainability </p><p>  Jeffrey Li, Vaishnavh Nagarajan, Gregory Plumb, Ameet Talwalkar</p><p>  åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å±€éƒ¨é€¼è¿‘è§£é‡Šçš„è§’åº¦æ¢ç´¢äº†å¯è§£é‡Šæœºå™¨å­¦ä¹ ä¸å­¦ä¹ ç†è®ºä¹‹é—´çš„è”ç³»ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è§£å†³äº†æ€§èƒ½æ³›åŒ–çš„ä¼ ç»Ÿé—®é¢˜ï¼Œå¹¶ä½¿ç”¨å±€éƒ¨è§£é‡Šæ€§çš„æ¦‚å¿µæ¥é™åˆ¶æ¨¡å‹çš„æµ‹è¯•æ—¶é—´å‡†ç¡®æ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ¢è®¨äº†è§£é‡Šæ³›åŒ–çš„æ–°é—®é¢˜ï¼Œè¿™æ˜¯è¶Šæ¥è¶Šå¤šçš„åŸºäºæœ‰é™æ ·æœ¬çš„å±€éƒ¨é€¼è¿‘è§£é‡Šçš„é‡è¦é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ç»éªŒéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºç»“æœï¼Œå¹¶è¡¨æ˜å®ƒä»¬åæ˜ äº†åœ¨å®è·µä¸­å¯ä»¥çœ‹åˆ°çš„ç»“æœã€‚</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡è¯•å›¾åœ¨å±€éƒ¨å¯è§£é‡Šæ€§å’Œå­¦ä¹ ç†è®ºä¹‹é—´å»ºç«‹ä¸€ç§æ–°é¢–çš„è”ç³»ï¼Œå¹¶é’ˆå¯¹ä¸è¡¨ç°æ¦‚æ‹¬å’Œè§£é‡Šæ¦‚æ‹¬ç›¸å…³çš„ç•Œçº¿æå‡ºäº†ä¸¤ä¸ªå®šç†ã€‚æœ¬æ–‡æä¾›äº†ä¸¤ç»„å®è¯ç»“æœï¼Œä»¥è¯´æ˜è¾¹ç•Œçš„æœ‰ç”¨æ€§ã€‚</p><p>  æ€»ä½“è€Œè¨€ï¼Œé€šè¿‡ä»å­¦ä¹ ç†è®ºçš„è§’åº¦æ¢ç´¢é»‘åŒ£å­æœºå™¨å­¦ä¹ æ¨¡å‹çš„æœ¬åœ°è§£é‡Šï¼Œæœ¬æ–‡çš„æƒ³æ³•å¾ˆæœ‰è¶£ã€‚æœ¬æ–‡æå‡ºé•œåƒé‚»å±…ä¿çœŸåº¦ï¼ˆMNFï¼‰ï¼Œä½œä¸ºè¡¡é‡æœ¬åœ°å¯è§£é‡Šæ€§çš„æ–°æ–¹æ³•ä»¥åŠè®ºç‚¹å’Œç»“è®ºçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚</p><p>  è¯¥è®ºæ–‡å£°ç§°ï¼ŒMNFè‡ªç„¶è¡¥å……äº†å¸¸ç”¨çš„é‚»åŸŸä¿çœŸåº¦ï¼ˆNFï¼‰ï¼Œå¹¶ä¸”åœ¨è¯„ä¼°å¯¹â€œç°å®çš„â€é«˜ç»´åˆ†å¸ƒæ•°æ®çš„æœ¬åœ°è§£é‡Šæ—¶ï¼ŒNFå…·æœ‰ç›¸å¯¹äºNFçš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œè€Œè¿™äº›è§£é‡Šé€šå¸¸è¡¨ç°å‡ºæ˜¾ç€çš„ç‰¹å¾ä¾èµ–æ€§ã€‚ä½†æ˜¯ï¼Œé™¤äº†é™„å½•ä¸­çš„ä¸€ä¸ªç©å…·ç¤ºä¾‹å¤–ï¼Œæ²¡æœ‰ä»»ä½•å¯é æˆ–ä»¤äººä¿¡æœçš„è¯æ®å’Œç»éªŒå®éªŒå¯æ”¯æŒä¸Šè¿°æƒåˆ©è¦æ±‚ã€‚</p></li><li><p>Shapley explainability on the data manifold </p><p>  Christopher Frye, Damien de Mijolla, Tom Begley, Laurence Cowton, Megan Stanley, Ilya Feige</p><p>  <strong>One-sentence Summary:</strong> We present drawbacks of model explanations that do not respect the data manifold, and introduce two methods for on-manifold explainability.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡é‡ç‚¹è®¨è®ºShapleyå€¼çš„ç¦»æ•°æ®æµå½¢é—®é¢˜ï¼Œè¯¥é—®é¢˜æ˜¯é€šè¿‡å¯¹åˆ†å¸ƒä¸å…¨çš„æ•°æ®è¿›è¡Œé‡‡æ ·è€Œåˆ›å»ºçš„ã€‚ç›®æ ‡æ˜¯å¼€å‘æœ‰æ•ˆçš„æ–¹æ³•ã€‚æå‡ºäº†ä¸¤ç§ä¸»è¦ç®—æ³•ï¼šç”¨äºè¿‘ä¼¼æ¡ä»¶åˆ†å¸ƒçš„ç”Ÿæˆæ¨¡å‹å’Œç”¨äºç›´æ¥è¿‘ä¼¼çš„è®­ç»ƒç›‘ç£æ¨¡å‹ã€‚å®ƒä»¬åœ¨å®éªŒä¸­æ˜¾ç¤ºå‡ºä¼˜äºåŸå§‹éæµå½¢Shapleyå€¼çš„ä¼˜åŠ¿ã€‚å°è±¡å®éªŒç‰¹åˆ«æœ‰è¶£ã€‚ä½¿ç”¨ç”Ÿæˆæ¨¡å‹æ¥è§£å†³å¯è§£é‡Šæ€§æ–¹æ³•ï¼ˆä¸ä»…ä»…æ˜¯SHAPï¼‰ä¸­çš„æµå½¢æ•°æ®é—®é¢˜çš„æ€»ä½“æ€è·¯é€šå¸¸æ˜¯ä¸€ä¸ªä¸é”™çš„æ–¹å‘ã€‚è¯·æ³¨æ„ï¼Œå¯¹äºSHAPè€Œè¨€ï¼Œè¯¥é—®é¢˜æ›´ä¸ºçªå‡ºï¼Œå› ä¸ºè¯¥æ–¹æ³•åŸºäºè¾“å…¥ç‰¹å¾çš„æ‰€æœ‰å­é›†çš„æ€§èƒ½ï¼Œå¹¶ä¸”æœ¬æ–‡å¾ˆå¥½åœ°è¯´æ˜äº†è§£å†³åŸºäºShapleyçš„æ–¹æ³•çš„æµå½¢å¤–æ•°æ®é—®é¢˜çš„å¿…è¦æ€§ã€‚å®éªŒç»“æœä¹Ÿå¾ˆå…¨é¢ï¼Œå¹¶ä¸ºå®ƒä»¬çš„æœ‰ç”¨æ€§æä¾›äº†è¶³å¤Ÿçš„è¯æ®ã€‚ä¼¼ä¹å­˜åœ¨ä¸€äº›æ–°å¥‡çš„æ‹…å¿§ï¼šâ€œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œå°šç¼ºä¹ä¸€ç§åœ¨ä¸€èˆ¬æ•°æ®ä¸Šä¼°è®¡æµå½¢Shapleyå€¼çš„é«˜æ•ˆæ–¹æ³•ï¼Œå¹¶ä¸”è¯¥å·¥ä½œçš„é‡ç‚¹ã€‚â€ æ­§ç®¡å¤–æ•°æ®åœ¨å¯è§£é‡Šæ€§æ–¹é¢çš„é—®é¢˜å·²å¾—åˆ°å¾ˆå¥½çš„ç ”ç©¶ï¼Œå…¶SHAPæ–¹æ³•çš„ç»†èŠ‚å·²åœ¨ä¹‹å‰è¿›è¡Œäº†è®¨è®ºã€‚<a href="https://proceedings.icml.cc/static/paper_files/icml/2020/334-Paper.pdf">this paprt</a>ä½œè€…å¯¹æœ¬æ–‡è¿›è¡Œäº†éå¸¸ç®€çŸ­çš„å¼•ç”¨ï¼Œä½†å®é™…ä¸Šå¹¶æœªæåŠå®ƒä»¬çš„ä¸åŒä¹‹å¤„ã€‚é™¤éè¯¥ä½œå“å¯¹ç°æœ‰æ–‡çŒ®çš„ä¸»è¦è´¡çŒ®æ˜¯æ˜¾è€Œæ˜“è§çš„ï¼Œå¦åˆ™æˆ‘æ— æ³•æ”¹å˜è‡ªå·±çš„åˆ†æ•°ã€‚å¦‚æœè´¡çŒ®æ˜¯â€œâ€¦â€¦æ”¯æŒæµå½¢æ–¹æ³•çš„å®éªŒè¯æ®â€ï¼Œåˆ™è¯¥è´¡çŒ®ä¸è¶³ä»¥ç”¨äºè¯¥åœºæ‰€ã€‚</p></li><li><p>Information-theoretic Probing Explains Reliance on Spurious Features </p><p>  Charles Lovering, Rohan Jha, Tal Linzen, Ellie Pavlick</p><p>  <strong>One-sentence Summary:</strong> We find that feature extractability, measured by probing classifiers, can be viewed as an inductive bias: the more extractable a feature is after pre-training, the less statistical evidence needed during fine-tuning for the model to use the feature.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡ç ”ç©¶äº†æ¥è‡ªé¢„è®­ç»ƒè¡¨ç¤ºçš„ç‰¹å¾çš„å¯æŠ½å–æ€§ä¸ç»è¿‡å¾®è°ƒçš„æ¨¡å‹ä½¿ç”¨è¯¥ç‰¹å¾çš„ç¨‹åº¦ä¹‹é—´çš„å…³ç³»ã€‚ç‰¹å¾çš„å¯æå–æ€§é€šè¿‡è®­ç»ƒä¸ºä»é¢„è®­ç»ƒè¡¨ç¤ºä¸­æ£€æµ‹ç‰¹å¾çš„æ¢æµ‹åˆ†ç±»å™¨çš„æœ€å°æè¿°é•¿åº¦æ¥è¡¡é‡ï¼ˆä½¿ç”¨Voitaå’ŒTitovçš„åœ¨çº¿ä»£ç ç‰ˆæœ¬ï¼‰ã€‚ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹ä½¿ç”¨ç‰¹å¾çš„ç¨‹åº¦é€šè¿‡æ¨¡å‹å°†è™šå‡ç‰¹å¾ä¸éè™šå‡ç‰¹å¾ï¼ˆç§°ä¸ºâ€œç›®æ ‡â€ç‰¹å¾ï¼‰åˆ†å¼€æ‰€éœ€çš„è¯æ®é‡æ¥è¡¡é‡ã€‚è¿™é‡Œçš„è¯æ®æ˜¯æŒ‡å‡ºç°è™šå‡ç‰¹å¾ä½†æœªå‘ç”Ÿéè™šå‡ç‰¹å¾çš„ç¤ºä¾‹ã€‚å½“å­˜åœ¨è®¸å¤šæ­¤ç±»ç¤ºä¾‹æ—¶ï¼ˆä»…é«˜ä¼ªé€ ç‡ï¼‰ï¼Œæ¨¡å‹æ›´å®¹æ˜“æ‹’ç»è™šå‡ç‰¹å¾å¹¶å­¦ä¼šä¾èµ–ç›®æ ‡ç‰¹å¾ã€‚â€œ</p><p>  æœ¬æ–‡é’ˆå¯¹åˆæˆæ•°æ®å’Œæ›´è‡ªç„¶çš„æ•°æ®è¿›è¡Œäº†ä¸¤ç§å®éªŒã€‚åˆæˆæ•°æ®æ˜¯ç¬¦å·åºåˆ—ï¼Œå…¶ä¸­çš„ä»»åŠ¡æ˜¯è¯†åˆ«ç®€å•å±æ€§ï¼Œä¾‹å¦‚ç¬¦å·çš„å‡ºç°æˆ–é‡å¤ã€‚è¿›è¡Œå®éªŒæ—¶ï¼Œåº”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æä¾›ä¸åŒæ¯”ç‡çš„ä»…è™šå‡ç¤ºä¾‹ï¼Œä»¥æä¾›è¶Šæ¥è¶Šå¤šçš„è¯æ®æ¥è¯æ˜è™šå‡ç‰¹å¾ï¼ˆç¬¦å·2çš„å­˜åœ¨ï¼‰å¹¶æ”¯æŒç›®æ ‡ç‰¹å¾ã€‚ç›®æ ‡ç‰¹å¾ä¸æ ‡ç­¾ç›¸åŒï¼Œå³ç¤ºä¾‹å¯¹åº”äºæ ‡ç­¾æ—¶ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚è¯¥è®ºæ–‡é€šè¿‡æ¢æµ‹åˆ†ç±»å™¨çš„MDLæŠ¥å‘Šäº†è™šå‡ç‰¹å¾å’Œç›®æ ‡ç‰¹å¾çš„å¯æå–æ€§ã€‚æ„Ÿå…´è¶£çš„åº¦é‡æ ‡å‡†æ˜¯ç›¸å¯¹MDLï¼Œå…¶ä¸­è¶Šé«˜è¡¨ç¤ºåŠŸèƒ½è¶Šå®¹æ˜“æå–ã€‚å½“åŠŸèƒ½æ›´æ˜“äºæå–æ—¶ï¼Œæ¨¡å‹æ‹’ç»è™šå‡ç‰¹å¾æ‰€éœ€çš„è¯æ®è¾ƒå°‘ã€‚ç”±äºæå–ç‰¹å¾è¾ƒå°‘ï¼Œå› æ­¤éœ€è¦æ›´å¤šè¯æ®ã€‚</p><p>  è‡ªç„¶è¯­è¨€ç¤ºä¾‹æ˜¯é€šè¿‡å¯¹ä¸‰ç§è¯­è¨€ç°è±¡ï¼ˆä¸»è¯­-åŠ¨è¯ä¸€è‡´ï¼Œè´Ÿææ€§é¡¹ç›®å’Œå¡«å……é¡¹ç›¸å…³æ€§ï¼‰è¯­æ³•ç”Ÿæˆçš„ç¤ºä¾‹çš„å¯æ¥å—æ€§åˆ¤æ–­è€Œåˆ¶æˆçš„ã€‚æ­¤å¤„çš„è®¾ç½®å†æ¬¡ç›¸ä¼¼ï¼Œåªæ˜¯å¯¹å¦‚ä½•è®¡ç®—å¯æå–æ€§è¿›è¡Œäº†è°ƒæ•´ã€‚æ­¤å¤„çš„ä¸»è¦ç»“æœæ˜¯ï¼Œå¯æå–æ€§ä¸æ‹’ç»è™šå‡ç‰¹å¾æ‰€éœ€çš„è¯æ®ä¹‹é—´å…·æœ‰é«˜åº¦ï¼ˆè´Ÿï¼‰ç›¸å…³æ€§ã€‚</p></li></ul><ul><li><p>Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization </p><p>  Judy Borowski, Roland Simon Zimmermann, Judith Schepers, Robert Geirhos, Thomas S. A. Wallis, Matthias Bethge, Wieland Brendel</p><p>  <strong>One-sentence Summary:</strong> Using human psychophysical experiments, we show that natural images can be significantly more informative for interpreting neural network activations than synthetic feature visualizations.</p><p>  <strong>Reviewers say:</strong> </p><ol><li>æœ¬æ–‡ç€é‡äºç‰¹å¾å¯è§†åŒ–ï¼Œè¯¥å¯è§†åŒ–ä¸ºç»™å®šçš„éšè—èŠ‚ç‚¹ç”Ÿæˆæœ€å¤§ç¨‹åº¦çš„æ¿€æ´»å›¾åƒï¼Œä»¥äº†è§£CNNçš„å†…éƒ¨å·¥ä½œåŸç†ã€‚ä»–ä»¬å°†è¿™äº›å›¾åƒçš„ä¿¡æ¯é‡ä¸è‡ªç„¶å›¾åƒç›¸æ¯”è¾ƒï¼Œåè€…ä¹Ÿå¼ºçƒˆæ¿€æ´»äº†æŒ‡å®šçš„éšè—èŠ‚ç‚¹ï¼Œå¹¶å‘ç°è‡ªç„¶å›¾åƒå¯ä»¥å¸®åŠ©äººç±»æ›´å¥½åœ°å›ç­”å“ªäº›å…¶ä»–æµ‹è¯•è‡ªç„¶å›¾åƒä¹Ÿè¢«æœ€å¤§ç¨‹åº¦åœ°æ¿€æ´»äº†ã€‚</li><li>ä¸»è¦æ€æƒ³æ˜¯ç ”ç©¶æç«¯æ¿€æ´»å›¾åƒå¦‚ä½•å¸®åŠ©äººç±»é¢„æµ‹CNNæ¿€æ´»ã€‚ä½œè€…é€šè¿‡å°†æåº¦æ¿€æ´»çš„å›¾åƒä¸ç¤ºä¾‹æ€§çš„è‡ªç„¶å›¾åƒè¿›è¡Œæ¯”è¾ƒæ¥å®ç°ï¼Œè¿™äº›è‡ªç„¶å›¾åƒä¹Ÿå¼ºçƒˆåœ°æ¿€æ´»äº†ç‰¹å®šçš„ç‰¹å¾å›¾ï¼ˆå¹¶ä½¿ç”¨å¿ƒç†ç‰©ç†å­¦æµ‹è¯•æ¥æŸ¥çœ‹å“ªç§å›¾åƒå¯ä»¥æ›´å¥½åœ°å¸®åŠ©äººç±»ï¼‰ã€‚</li><li>ä½œè€…æŒ‡å‡ºï¼Œè®¸å¤šå¯è§†åŒ–æ–¹æ³•éƒ½å°†å“åº”æœ€å¤§åŒ–ä¸äººä¸ºå®šä¹‰çš„æ­£åˆ™åŒ–æ–¹æ³•ç›¸ç»“åˆï¼Œè¿™äº›æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯è‰ºæœ¯ä¸Šçš„é€‰æ‹©ï¼Œæ—¨åœ¨é™ä½å›¾åƒçš„å™ªç‚¹ï¼Œè¿™æ˜¯æ­£ç¡®ä¸”æ­£ç¡®çš„ã€‚è¿™äº›æ­£åˆ™åŒ–å½’å› äºå®ƒä»¬è‡ªèº«å¯¹ç»“æœå›¾åƒçš„åè§ï¼Œè¿™å¯èƒ½ä¼šä½¿å®ƒä»¬çš„ä¿¡æ¯é‡æ›´å°‘ã€‚è¿˜å¯ä»¥å¾ˆå¥½åœ°è®¤è¯†åˆ°ï¼Œå•å…ƒå¯èƒ½è¢«ä¸€ä¸ªä»¥ä¸Šçš„è¯­ä¹‰æ¦‚å¿µé«˜åº¦æ¿€æ´»ï¼Œæˆ–è€…ä¸å…¶ä»–å•å…ƒï¼ˆå¯èƒ½ä¼ é€’æ›´å¤šçš„ä¿¡æ¯è€Œä¸æ˜¯é€‰æ‹©æ€§åœ°æœ€å¤§åŒ–å•ä¸ªç¥ç»å…ƒçš„æ¿€æ´»ï¼‰ç›¸ç»“åˆè€Œæ´»è·ƒã€‚</li></ol></li></ul><ul><li><p>Scaling Symbolic Methods using Gradients for Neural Model Explanation </p><p>  Subham Sekhar Sahoo, Subhashini Venugopalan, Li Li, Rishabh Singh, Patrick Riley</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¼–ç æœ€å°è¾“å…¥ç‰¹å¾å‘ç°é—®é¢˜çš„æ–¹æ³•-å°†ä¸€ç§æœ€å°çš„ç‰¹å¾é›†è¾“å…¥åˆ°é¢„æµ‹æ‰€éœ€çš„ç‰¹å¾ä¸­-å°†å…¶ç¼–ç ä¸ºä¸€ç§å¯æ»¡è¶³æ»¡æ„åº¦æ¨¡ç†è®ºï¼ˆSMTï¼‰æ±‚è§£å™¨çš„å½¢å¼ã€‚ç‰¹åˆ«æ˜¯ï¼Œä»–ä»¬é¦–å…ˆä½¿ç”¨é›†æˆæ¢¯åº¦æ–¹æ³•å¯¹ç¬¬ä¸€å±‚ç¥ç»å…ƒçš„å½±å“ç¨‹åº¦è¿›è¡Œè¯„åˆ†ã€‚ç„¶åï¼Œä»–ä»¬äº§ç”Ÿå¹¶è§£å†³ä¸€ä¸ªSMTé—®é¢˜ï¼Œè¯¥é—®é¢˜æ‰¾åˆ°äº†æ”¹å˜è¿™äº›æœ‰å½±å“çš„ç¥ç»å…ƒçš„æœ€å°é¢ç½©ã€‚ä»–ä»¬å±•ç¤ºäº†ä»–ä»¬åœ¨ä¸€äº›é—®é¢˜ä¸Šçš„æ–¹æ³•ã€‚</p></li><li><p><strong>ã€é‡ç‚¹é˜…è¯»ã€‘</strong>Evaluation of Similarity-based Explanations </p><p>  Kazuaki Hanawa, Sho Yokoi, Satoshi Hara, Kentaro Inui</p><p>  <strong>One-sentence Summary:</strong> We investigated empirically which of the relevance metrics (e.g. similarity of hidden layer, influence function, etc.) are appropriate for similarity-based explanation.</p><p>  <strong>Reviewers say:</strong> è¿™é¡¹å·¥ä½œæä¾›äº†åŸºäºç¤ºä¾‹çš„è§£é‡Šæ–¹æ³•ä¸­ä½¿ç”¨çš„ç›¸ä¼¼æ€§åº¦é‡çš„ç»éªŒè¯„ä¼°ï¼Œå…¶ç›®çš„æ˜¯åœ¨è®­ç»ƒé›†ä¸­ä¸ºé»‘åŒ£å­æ¨¡å‹çš„é¢„æµ‹æä¾›å†³ç­–æ”¯æŒç¤ºä¾‹ã€‚æœ¬æ–‡è¯„ä¼°äº†æ–‡çŒ®ä¸­æµè¡Œçš„åŸºäºæ¢¯åº¦çš„åº¦é‡ï¼Œä¾‹å¦‚å½±å“å‡½æ•°ï¼Œè´¹èˆå°”æ ¸ï¼Œä»¥åŠåŸºäºl2ï¼Œä½™å¼¦è·ç¦»å’Œä¸åŒåµŒå…¥ç©ºé—´ä¸Šçš„ç‚¹ç§¯çš„ç®€å•æ–¹æ³•ã€‚ä½œè€…ä»‹ç»äº†ä¸¤ä¸ªè¯„ä¼°ä¸åŒæ–¹æ³•å¯é æ€§çš„æ–°ä»»åŠ¡ï¼šç›¸åŒçš„ç±»æµ‹è¯•å’Œç›¸åŒçš„å­ç±»æµ‹è¯•ã€‚</p></li><li><p>Learning explanations that are hard to vary </p><p>  Giambattista Parascandolo, Alexander Neitz, ANTONIO ORVIETO, Luigi Gresele, Bernhard SchÃ¶lkopf</p><p>  <strong>Reviewers say:</strong> è¿™é¡¹å·¥ä½œå‡å®šæ•°æ®é›†ä¸­å­˜åœ¨ä¸å˜æœºåˆ¶ã€‚ä½¿ç”¨æ¢¯åº¦ä¸‹é™è®­ç»ƒçš„æœºå™¨å­¦ä¹ ç®—æ³•é€šå¸¸ä¼šåœ¨ç¤ºä¾‹ä¸­å¹³å‡æ¢¯åº¦ã€‚æœ¬æ–‡çš„è§‚ç‚¹æ˜¯ï¼Œé€šè¿‡å¹³å‡æ¢¯åº¦ï¼Œä¿¡æ¯ä¼šä¸¢å¤±ã€‚è¯¥æ–¹æ³•å‡å®šï¼Œåœ¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­ï¼Œå¯ä»¥ä½¿ç”¨å‡ ä½•ï¼ˆæˆ–karcherï¼‰å¹³å‡å€¼ä»£æ›¿ç®—æœ¯å¹³å‡å€¼æ¥ä¿å­˜æœ‰å…³ä¸å˜æœºåˆ¶çš„ä¿¡æ¯-è€Œå¿½ç•¥æ··æ‚å› ç´ ã€‚åœ¨ç›´æ¥åº”ç”¨å‡ ä½•å¹³å‡å€¼æ—¶ä¼šé‡åˆ°å›°éš¾ï¼Œå› æ­¤å¼€å‘äº†ä¸€ç§ç®€å•çš„å¯å‘å¼ç®—æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬æ ¹æ®æ¢¯åº¦çš„ç¬¦å·æ˜¯å¦åœ¨ä¸€æ‰¹ç¤ºä¾‹ä¸­ä¸€è‡´ï¼ˆæˆ–æ˜¯å¦è¾¾åˆ°æŸä¸ªä¸€è‡´é˜ˆå€¼ï¼‰æ¥æ©ç›–æ¢¯åº¦ã€‚è¯¥ç®—æ³•å·²åœ¨åˆæˆæ•°æ®é›†ï¼ŒCIFAR-10ä¸Šçš„åŠåˆæˆä»»åŠ¡ä»¥åŠRLç®—æ³•coinbaseä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚</p></li><li><p>Debiasing Concept-based Explanations with Causal Analysis </p><p>  Mohammad Taha Bahadori, David Heckerman</p><p>  <strong>One-sentence Summary:</strong> We use a technique from instrumental variables literature and remove the impact of noise and latent confounding from concept-based explanations.</p><p>  <strong>Reviewers say:</strong> è¿™é¡¹å·¥ä½œçš„é‡ç‚¹æ˜¯ä½¿ç”¨åŸºäºæ¦‚å¿µçš„è§£é‡Šè¿›è¡Œæ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚ä½œè€…è®¤ä¸ºï¼Œæ¦‚å¿µé—®é¢˜ä¸åŠŸèƒ½ä¸­çš„æ··æ·†ä¿¡æ¯ç›¸å…³ã€‚ä»–ä»¬æå‡ºäº†è¡¨ç¤ºç³»ç»Ÿçš„å› æœå›¾ï¼Œå¹¶ä½¿ç”¨å·¥å…·å˜é‡æ–¹æ³•æ¥æ¶ˆé™¤æœªè§‚å¯Ÿåˆ°çš„æ··æ‚å› ç´ çš„å½±å“ã€‚è¯¥æ–¹æ³•åœ¨ç»¼åˆå’ŒçœŸå®æ•°æ®ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</p></li><li><p>Evaluations and Methods for Explanation through Robustness Analysis </p><p>  Cheng-Yu Hsieh, Chih-Kuan Yeh, Xuanqing Liu, Pradeep Kumar Ravikumar, Seungyeon Kim, Sanjiv Kumar, Cho-Jui Hsieh</p><p>  <strong>One-sentence Summary:</strong> We propose a suite of objective measurements for evaluating feature based explanations by the notion of robustness analysis; we further derive new explanation that captures different characteristics of explanation comparing to existing methods.</p><p>  <strong>Reviewers say:</strong> è®¸å¤šå¯è§£é‡Šæ€§æŠ€æœ¯éƒ½é›†ä¸­åœ¨è¯†åˆ«â€œæœ€ç›¸å…³ç‰¹å¾â€çš„å­é›†ä¸Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä½œè€…å»ºè®®å°†è¯¥é›†åˆå®šä¹‰ä¸ºæœ€å®¹æ˜“é­åˆ°æ”»å‡»çš„ç‰¹å¾é›†ï¼ˆåœ¨<br>æ„Ÿï¼‰ã€‚é¦–å…ˆï¼Œç”±äºå‚ç›´ç©ºé—´LaTeXéª‡å®¢æ”»å‡»çš„æ•°é‡ä»¤äººæ²®ä¸§ï¼Œè®ºæ–‡æœ‰ç‚¹éš¾ä»¥é˜…è¯»ï¼Œä»¥è‡³äºèŠ‚å’Œæ®µè½ä¹‹é—´çš„é—´è·ç”šè‡³å°äºå¥å­ä¹‹é—´çš„æ­£å¸¸é—´è·ã€‚è¿™ä¸æ˜¯å°†æ‰€æœ‰å†…å®¹å‹ç¼©åˆ°8é¡µçš„å¥½æ–¹æ³•ã€‚</p><p>  é™¤æ­¤ä¹‹å¤–ï¼Œè¯¥è®ºæ–‡åœ¨å®éªŒè¯„ä¼°æ–¹é¢éå¸¸å…¨é¢ï¼Œå¹¶æä¾›äº†ä¸€ç³»åˆ—é€‚å½“çš„åŸºå‡†ï¼Œå¥å…¨æ€§æ£€æŸ¥å’Œäººä½“ç ”ç©¶ã€‚æˆ‘è®¤ä¸ºè¿™æ˜¯å¯¹å½“å‰åŠŸèƒ½å½’å› æŠ€æœ¯å¥—ä»¶çš„æœ‰è¶£è¡¥å……ã€‚ä»æ¦‚å¿µä¸Šè®²ï¼Œå®ƒéå¸¸ç›¸ä¼¼ã€‚æ­£å¦‚ä½œè€…æ‰€æŒ‡å‡ºçš„é‚£æ ·ï¼Œæœ‰è®¸å¤šç›¸å…³æŠ€æœ¯è¯•å›¾é€šè¿‡å‘å®ƒä»¬æ·»åŠ å™ªå£°ï¼Œå°†å…¶è®¾ç½®ä¸ºåŸºå‡†å€¼æˆ–æ¨¡ç³Šå®ƒä»¬æ¥â€œå»é™¤ç‰¹å¾â€ã€‚åœ¨è¿™é‡Œï¼Œä½œè€…æ”¹ä¸ºè€ƒè™‘å¯¹æŠ—æ€§åœ°å¹²æ‰°ä»–ä»¬ï¼Œä»–ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„è´ªå©ªç­–ç•¥ï¼Œè¯¥æ–¹æ³•ä¼¼ä¹æ•ˆæœå¾ˆå¥½ã€‚å¯¹äºæˆ‘æ¥è¯´ï¼Œè¿˜æ˜¯æœ‰ç‚¹ä¸æ¸…æ¥šï¼Œä¸ºä»€ä¹ˆè€ƒè™‘å¯¹æŠ—æ€§æ‰°åŠ¨æ¯”è¯´è€ƒè™‘ï¼ˆä¾‹å¦‚è€ƒè™‘æ¨¡ç³Šæˆ–å¢åŠ é€‰å®šç‰¹å¾çš„å™ªå£°ï¼‰æ›´å…·è¯´æœåŠ›ï¼Œä½†æ˜¯å®ƒä»¬çš„ä½œç”¨ç•¥æœ‰ä¸åŒï¼Œè€Œä¸”ä»ç»éªŒä¸Šè®²ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œæ­¤æ–¹æ³•å¯åœ¨æ’å…¥å’Œåˆ é™¤æŒ‡æ ‡ä¸‹è·å¾—æ”¶ç›Šã€‚</p></li><li><p>Shapley Explanation Networks </p><p>  Rui Wang, Xiaoqian Wang, David I. Inouye</p><p>  <strong>One-sentence Summary:</strong> To enable new capabilities, we propose to use Shapley values as inter-layer representations in deep neural networks rather than as post-hoc explanations.</p></li></ul><ul><li><p>Shape or Texture: Understanding Discriminative Features in CNNs </p><p>  Md Amirul Islam, Matthew Kowal, Patrick Esser, Sen Jia, BjÃ¶rn Ommer, Konstantinos G. Derpanis, Neil Bruce</p><p>  <strong>One-sentence Summary:</strong> Exploring and quantifying shape information encoded in CNNs.</p><p>  <strong>Reviewers say:</strong></p><ol><li>ä½œè€…è¯•å›¾ç†è§£çš„é—®é¢˜åœ¨å¯¹è±¡è¯†åˆ«ï¼Œçº¹ç†/å½¢çŠ¶åå·®å’Œæ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„å­¦ä¹ è¡¨ç¤ºç­‰é¢†åŸŸéƒ½å¾ˆæœ‰è¶£ä¸”ç›¸å…³ã€‚</li><li>ä½œè€…æä¾›äº†ä¸€ç»„ä¸é”™çš„å—æ§å®éªŒï¼Œè¿™äº›å®éªŒè¡¨æ˜äº†å…¶ä¸­çš„æŸäº›æ•ˆæœï¼Œå¹¶ä¸”ä½œè€…åœ¨å…¶æ–¹æ³•ä¸­æå‡ºäº†ç§‘å­¦ä¾æ®ï¼ˆå°½ç®¡å¹¶ä¸å®Œç¾ï¼‰ï¼Œä½†è¿™ä¸è¯¥é¢†åŸŸéå¸¸ç›¸å…³ã€‚ç›¸åï¼Œè¿™ä¸æ˜¯â€œå¦ä¸€ç¯‡è®ºæ–‡ï¼Œè¯•å›¾å…‹æœçº¹ç†åå·®è€Œæ²¡æœ‰ä»»ä½•ç›´è§‰ï¼Œè¯•å›¾è·å¾—æ›´å¥½çš„æ•°å­—ï¼ˆä¸å¹¸çš„æ˜¯ï¼Œè¿™äº›å¤©åœ¨è®¡ç®—æœºè§†è§‰ä¸­å·²ç»å®Œæˆï¼‰â€ï¼Œç›¸åï¼Œè¿™ç¯‡è®ºæ–‡æ˜¯å…³äºç†è§£çº¹ç†/å½¢çŠ¶åå·®çš„åŸºæœ¬æœºåˆ¶çš„çŸ¥è¯†ï¼Œè¿™äº›æœºåˆ¶æ‰©å±•äº†è§†è§‰å±‚æ¬¡ç»“æ„ä¸­çš„è®¡ç®—çš„æœ€ç»ˆé˜¶æ®µï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘å€¾å‘äºæ¥å—ã€‚è®ºæ–‡ä¸­å‡ ä¹æ‰€æœ‰æ•°å­—éƒ½å¾ˆæ¸…æ¥šï¼Œå¹¶æœ‰åŠ©äºä¼ è¾¾ä½œè€…è¯•å›¾è¡¨è¾¾çš„å†…å®¹ï¼ˆå°½ç®¡æœ‰ä¸€äº›æ¾„æ¸…ç‚¹ï¼‰</li></ol></li></ul><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><ul><li><p>VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models<br>  Zhisheng Xiao, Karsten Kreis, Jan Kautz, Arash Vahdat</p></li><li><p>Disentangled Recurrent Wasserstein Autoencoder<br>  Jun Han, Martin Renqiang Min, Ligong Han, Xuan Zhang, Li Erran Li</p></li><li><p>Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders<br>  Mangal Prakash, Alexander Krull, Florian Jug</p></li><li><p>Tomographic Auto-Encoder: Unsupervised Bayesian Recovery of Corrupted Data<br>  Francesco Tonolini, Andreas Damianou, Pablo Garcia Moreno, Roderick Murray-Smith</p></li><li><p>Unsupervised Audiovisual Synthesis via Exemplar Autoencoders<br>  Kangle Deng, Aayush Bansal, Deva Ramanan</p></li><li><p>Property Controllable Variational Autoencoder via Invertible Mutual Dependence<br>  Xiaojie Guo, Yuanqi Du, Liang Zhao</p></li><li><p>Improving relational regularized autoencoders with spherical sliced fused Gromov Wasserstein<br>  Khai Nguyen, Son Nguyen, Nhat Ho, Tung Pham, Hung Bui</p></li><li><p>Learning a Latent Search Space for Routing Problems using Variational Autoencoders<br>  AndrÃ© Hottung, Bhanu Bhandari, Kevin Tierney</p></li><li><p>Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation<br>  Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, Noah Smith</p></li><li><p>Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning </p><p>  Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, Zhaopeng Tu</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡ä»‹ç»äº†å¯¹ç»†ç²’åº¦å±‚çš„å…³æ³¨ï¼Œä»¥è¯„ä¼°å„ä¸ªç¼–ç å™¨å±‚çš„ä½œç”¨å¹¶ç ”ç©¶ç¼–ç å™¨å±‚èåˆçš„å·¥ä½œåŸç†ï¼Œå…¶ä¸­è§£ç å™¨å±‚å¯ä»¥è®¿é—®å„ç§ç¼–ç å™¨å±‚çš„ä¿¡æ¯ï¼Œè€Œä¸æ ‡å‡†Transformerä¸­çš„æœ€ç»ˆç¼–ç å™¨å±‚ä¸åŒã€‚</p><p>  åŸºäºä»¥ä¸‹è§‚ç‚¹ï¼šç¼–ç å™¨åµŒå…¥å±‚å¯¹äºç¼–ç å™¨å±‚èåˆçš„æˆåŠŸè‡³å…³é‡è¦ï¼Œè€Œæœ€ä¸Šå±‚çš„è§£ç å™¨å±‚åˆ™æ›´åŠ å…³æ³¨ç¼–ç å™¨åµŒå…¥å±‚ï¼Œå› æ­¤æå‡ºäº†SurfaceFusionï¼Œè¯¥æ–¹æ³•ä»…å°†ç¼–ç å™¨åµŒå…¥å±‚è¿æ¥åˆ°è§£ç å™¨çš„softmaxå±‚ã€‚ ï¼Œå¯¼è‡´BLEUç­‰æŒ‡æ ‡è·å¾—äº†å¯è§‚çš„æ”¶ç›Šã€‚</p></li></ul><h2 id="Missing-value-amp-irregularly-sampled-time-series"><a href="#Missing-value-amp-irregularly-sampled-time-series" class="headerlink" title="Missing value &amp; irregularly sampled time series"></a>Missing value &amp; irregularly sampled time series</h2><ul><li><p>Multi-Time Attention Networks for Irregularly Sampled Time Series </p><p>  Satya Narayan Shukla, Benjamin Marlin</p><p>  <strong>One-sentence Summary:</strong> This paper presents a new deep learning architecture for learning with sparse and irregularly sampled multivariate time series.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡è®¨è®ºä¸è§„åˆ™æ ·æœ¬æ—¶é—´åºåˆ—çš„åˆ†æã€‚è¯¥æ–¹æ³•ä¸»è¦åŸºäºæ’å€¼ã€‚å› æ­¤ï¼Œä½œè€…å¯ä»¥ç ”ç©¶æœ‰ç›‘ç£å’Œæ— ç›‘ç£çš„é—®é¢˜ã€‚è¯¥æ¶æ„ç”±æ­£å¼¦æ³¢æ³¨æ„å±‚ï¼Œå¯åœ¨æ½œåœ¨ç©ºé—´ä¸­å½¢æˆå›ºå®šå¤§å°çš„ç•Œæ ‡çš„VAEå±‚å’ŒRNNè§£ç å™¨ç»„æˆã€‚å¯¹äºç›‘ç£ä»»åŠ¡ï¼Œä½œè€…æ·»åŠ äº†åˆ†ç±»æŸå¤±ã€‚</p><ul><li>ä»–ä»¬åœ¨æ’å€¼ä»»åŠ¡ä¸­è·å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œè€Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­è·å¾—äº†æœ‰è¶£çš„ç»“æœã€‚</li><li>åœ¨æ’å€¼é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬æƒ³å°†ç¨³å¥çš„åŸºçº¿è§†ä¸ºçº¿æ€§æ’å€¼æˆ–ç±»ä¼¼ARçš„æ¨¡å‹ã€‚å³ä½¿æˆ‘å¿…é¡»æ‰¿è®¤ä½œè€…å·²ç»æå‡ºäº†ä¸æœ€è¿‘æ–‡çŒ®çš„æ¨¡å‹è¿›è¡Œçš„å¤§é‡æ¯”è¾ƒï¼Œè¿™ä¹Ÿå°†ä¸ºæˆ‘ä»¬æä¾›æœ‰æ„ä¹‰çš„MSEç»“æœï¼Œä»¥æ¯”è¾ƒå…¶ä»–æ–¹æ³•ã€‚</li><li>ç»“æœä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†æˆ‘ä¸çŸ¥é“æ¶æ„çš„å“ªä¸ªéƒ¨åˆ†ä¼šå¸¦æ¥å¦‚æ­¤å‡ºè‰²çš„æ€§èƒ½</li></ul></li><li><p>not-MIWAE: Deep Generative Modelling with Missing not at Random Data </p><p>  Niels Bruun Ipsen, Pierre-Alexandre Mattei, Jes Frellsen</p><p>  <strong>One-sentence Summary:</strong> We present an approach for building and fitting deep latent variable models (DLVMs) in cases where the missing process is dependent on the missing data.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯¹æ•°æ®è¿›è¡Œæ·±åº¦æ½œå˜é‡æ¨¡å‹è®­ç»ƒçš„æ–¹æ³•ï¼Œè¿™äº›æ•°æ®ä¸æ˜¯éšæœºä¸¢å¤±çš„ã€‚ä¸ºäº†å­¦ä¹ æ·±æ½œå˜é‡æ¨¡å‹çš„å‚æ•°ï¼Œæœ¬æ–‡é‡‡ç”¨é‡è¦æ€§åŠ æƒå˜åˆ†æ¨ç†æŠ€æœ¯ã€‚åœ¨å„ç§æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ˜¾å¼åœ°å»ºæ¨¡éšæœºæ•°æ®ä¸­ç¼ºå¤±çš„æ¨¡å‹è€Œæœ‰æ•ˆã€‚</p></li></ul><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><ul><li><p>Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies </p><p>  T. Konstantin Rusch, Siddhartha Mishra</p><p>  <strong>One-sentence Summary:</strong> A biologically motivated and discretized ODE based RNN for learning long-term dependencies, with rigorous bounds mitigating the exploding and vanishing gradient problem.</p></li></ul><ul><li><p><strong>ã€é‡ç‚¹é˜…è¯»ã€‘</strong>Recurrent Independent Mechanisms (Spotlight)</p><p>  Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, Bernhard SchÃ¶lkopf</p><p>  <strong>One-sentence Summary:</strong> Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é€’å½’ç½‘ç»œï¼Œç§°ä¸ºRIMï¼Œä»¥æé«˜å¯¹å±€éƒ¨å˜åŒ–çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚è¯¥ç½‘ç»œç”±å¾ˆå¤§ç¨‹åº¦ä¸Šç‹¬ç«‹çš„ç»å¸¸æ€§æ¨¡å—ç»„æˆï¼Œè¿™äº›æ¨¡å—å¾ˆå°‘è¢«æ¿€æ´»ï¼Œå¹¶é€šè¿‡æŸ”å’Œçš„æ³¨æ„åŠ›è¿›è¡Œäº¤äº’ã€‚åœ¨ä¸€ç³»åˆ—ä¸åŒä»»åŠ¡ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒRIMçš„æ¨å¹¿æ€§ä¼˜äºLSTMã€‚</p></li><li><p>Disentangled Recurrent Wasserstein Autoencoder (Spotlight)</p><p>  Jun Han, Martin Renqiang Min, Ligong Han, Xuan Zhang, Li Erran Li</p><p>  <strong>One-sentence Summary:</strong> We propose the first recurrent Wasserstein Autoencoder for learning disentangled representations of sequential data with theoretical analysis.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§å­¦ä¹ åºåˆ—æ•°æ®çš„é™æ€å’ŒåŠ¨æ€æ½œåœ¨å˜é‡çš„è§£ç¼ æ–¹æ³•ã€‚åœ¨å­¦ä¹ ç›®æ ‡æ–¹é¢ï¼Œæœ¬æ–‡å°†Wassersteinè‡ªåŠ¨ç¼–ç å™¨æ‰©å±•åˆ°é¡ºåºæ•°æ®ï¼Œè¿™ç§æ–¹æ³•æ–°é¢–ä¸”åŠ¨æœºè‰¯å¥½ã€‚é™æ€å˜é‡çš„èšåˆåéªŒè‡ªç„¶è€Œç„¶åœ°å‡ºç°ï¼Œå¹¶ä¸”å¯¹æ­£åˆ™åŒ–èµ·ç€é‡è¦ä½œç”¨ï¼ˆè¿™å¯¹äºåºåˆ—æ•°æ®æ¥è¯´ä¼¼ä¹æ˜¯æ–°çš„ï¼‰ã€‚ä½œè€…è¿˜ç ”ç©¶äº†å¦‚ä½•ä¸ºçœŸå®æƒ…æ™¯ä¸­çš„å¼±ç›‘ç£å­¦ä¹ å»ºæ¨¡å…¶ä»–åˆ†ç±»å˜é‡ã€‚å›¾å½¢åŒ–çš„æ¨¡å‹æ¸…æ¥šåœ°è¯´æ˜äº†ä¸»è¦æ­¥éª¤ï¼ˆç”Ÿæˆå’Œæ¨æ–­ï¼‰ï¼Œå¹¶æä¾›äº†ä¸¥æ ¼çš„è¯´æ˜æ¥æ”¯æŒå®ƒä»¬ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨çº ç¼ æ€§èƒ½å’Œç”Ÿæˆè´¨é‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p></li><li><p>The geometry of integration in text classification RNNs </p><p>  Kyle Aitken, Vinay Venkatesh Ramasesh, Ankush Garg, Yuan Cao, David Sussillo, Niru Maheswaranathan</p><p>  <strong>One-sentence Summary:</strong> We study text classification RNNs using tools from dynamical systems analysis, finding and explaining the geometry of low-dimensional attractor manifolds.</p><p>  <strong>Reviewers say:</strong> ç»§Maheswaranathanç­‰äººï¼ˆ2019ï¼‰å’ŒMaheswaranathanï¼†Sussilloï¼ˆ2020ï¼‰ç­‰æœ€è¿‘çš„ç ”ç©¶ä¹‹åï¼Œæœ¬æ–‡åŠ å…¥äº†ç ”ç©¶å¾ªç¯ç½‘ç»œè§£å†³ç›‘ç£åºåˆ—åˆ†ç±»é—®é¢˜çš„æœºåˆ¶çš„ç ”ç©¶é¢†åŸŸã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæœ¬æ–‡å‡è®¾å¹¶ç¡®è®¤äº†é€’å½’ç½‘ç»œï¼ˆæ— è®ºæ˜¯GRUè¿˜æ˜¯LSTMï¼‰çš„å†…éƒ¨éšè—çŠ¶æ€åœ¨è¯»å–è¾“å…¥æ—¶åœ¨å¹³é¢ï¼ˆè¿‘ä¼¼ï¼‰å¸å¼•å­ä¸Šæ¼”åŒ–ï¼Œç›¸å½“äºåœ¨å¤„ç†è¾“å…¥åºåˆ—æ—¶æ•´åˆäº†è¯æ®ã€‚ ï¼Œå¹¶é’ˆå¯¹ä¸‰ç§ç±»å‹çš„é—®é¢˜ï¼ˆåˆ†ç±»ï¼Œæœ‰åºåˆ†ç±»å’Œå¤šæ ‡ç­¾åˆ†ç±»ï¼‰å±•ç¤ºäº†è¿™äº›å¸å¼•å­çš„å­˜åœ¨å’Œé›†æˆåŠ¨æ€ã€‚</p></li><li><p>Uncertainty Estimation and Calibration with Finite-State Probabilistic RNNs </p><p>  Cheng Wang, Carolin Lawrence, Mathias Niepert</p><p>  <strong>One-sentence Summary:</strong> A method to estimate and calibrate uncertainty in recurrent state transitions.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§é‡åŒ–RNNä¸ç¡®å®šæ€§çš„æ–¹æ³•ï¼Œè¿™æ˜¯å„ç§åº”ç”¨ä¸­çš„é‡è¦é—®é¢˜ã€‚å®ƒæä¾›äº†å„ç§é¢†åŸŸçš„ç»“æœï¼Œè¡¨æ˜æ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºåŸºçº¿ã€‚ä½†æ˜¯ï¼Œé™¤äº†è€ƒè™‘çš„åŸºçº¿ï¼ˆä¾‹å¦‚åæ–¹å·®ä¼ æ’­ï¼Œå…ˆéªŒç½‘ç»œå’Œæ­£äº¤è¯ä¹¦ï¼‰ä»¥å¤–ï¼Œé€šè¿‡é’ˆå¯¹ç‰¹å®šä»»åŠ¡ä¸SOTAæ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œè¿™äº›å®éªŒå°†å¤§å¤§å—ç›Šã€‚è¿˜å¯ä»¥é€šè¿‡æ·»åŠ ç†è®ºä¸Šçš„è§£é‡Šæ¥è§£é‡ŠGumbel softmaxå‡½æ•°å¦‚ä½•æ•è·åŸºæœ¬æ•°æ®å’Œæ¨¡å‹ä¸ç¡®å®šæ€§ï¼Œä»è€Œå¯¹æœ¬æ–‡è¿›è¡Œæ”¹è¿›ã€‚</p></li><li><p>RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs </p><p>  Meng Qu, Junkun Chen, Louis-Pascal Xhonneux, Yoshua Bengio, Jian Tang</p><p>  <strong>One-sentence Summary:</strong> Learn Logic Rules for Reasoning on Knowledge Graphs.</p><p>  <strong>Reviewers say:</strong> åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä½œè€…ä¸¾ä¾‹è¯´æ˜äº†ä¸€ç§ä»çŸ¥è¯†å›¾å¼€å§‹å­¦ä¹ é€»è¾‘è§„åˆ™çš„æ–¹æ³•ã€‚å­¦ä¹ é€»è¾‘è§„åˆ™æ¯”ç®€å•åœ°æ‰§è¡Œé“¾æ¥é¢„æµ‹æ›´æœ‰è¶£ï¼Œå› ä¸ºè§„åˆ™æ˜¯äººç±»å¯è¯»çš„ï¼Œå› æ­¤å…·æœ‰å¯è§£é‡Šæ€§ã€‚è¯¥æ–¹æ³•ä¼¼ä¹å¾ˆæœ‰è¶£ï¼Œå¹¶ä¸”æ‰€è§£å†³çš„é—®é¢˜å¯èƒ½ä¼šå¼•èµ·å¹¿æ³›çš„å…³æ³¨ã€‚å®ƒä¼¼ä¹ä¸æ˜¯å¾ˆæ–°é¢–ï¼Œä½†æ˜¯å¯¹æˆ‘æ¥è¯´ä¼¼ä¹æ˜¯æœ‰æ•ˆçš„ã€‚è¿™ç¯‡è®ºæ–‡å†™å¾—å¾ˆå¥½å¹¶ä¸”è‡ªæˆä½“ç³»ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–ç³»ç»Ÿç›¸æ¯”ï¼ˆç”šè‡³ä¸ä¸å­¦ä¹ è§„åˆ™è€Œä»…æ‰§è¡Œé“¾æ¥é¢„æµ‹çš„ç³»ç»Ÿç›¸æ¯”ï¼‰ï¼Œè¯¥æ–¹æ³•å…·æœ‰ç«äº‰ä¼˜åŠ¿ã€‚</p></li><li><p>SkipW: Resource adaptable RNN with strict upper computational limit </p><p>  Tsiry Mayet, Anne Lambert, Pascal Leguyadec, Francoise Le Bolzer, FranÃ§ois Schnitzler</p><p>  <strong>One-sentence Summary:</strong> Skip-Window is a method to allow recurrent neural networks (RNNs) to trade off accuracy for computational cost during the analysis of a sequence while keeping a strict upper computational limit.</p><p>  <strong>Reviewers say:</strong> æ­¤æäº¤å†…å®¹æä¾›äº†SkipRNNï¼ˆè·³è¿‡çª—å£ï¼‰çš„æ‰©å±•ï¼Œå®ƒå°†è¾“å…¥åºåˆ—åˆ†å‰²ä¸ºé•¿åº¦ä¸ºLçš„çª—å£ï¼Œä»ä¸­åªèƒ½ä½¿ç”¨Kä¸ªæ ·æœ¬ã€‚è¿™ä¿è¯äº†ä¸ä¼šè¶…è¿‡è®¡ç®—é¢„ç®—ã€‚è·³è¿‡çª—å£é€šè¿‡åœ¨æ¯ä¸ªçª—å£çš„å¼€å¤´å¹¶è¡Œé¢„æµ‹Lä¸ªæ›´æ–°æ¦‚ç‡æ¥å®ç°è¿™ç§å½’çº³åå·®ã€‚éœ€è¦åœ¨è®­ç»ƒä¹‹å‰è®¾ç½®Lï¼Œè€Œå¯ä»¥åœ¨æµ‹è¯•æ—¶ä¿®æ”¹Kã€‚è¯¥æ¨¡å‹åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸­è¯„ä¼°ï¼Œå³åˆæˆæ·»åŠ ä»»åŠ¡å’Œäººç±»æ´»åŠ¨è¯†åˆ«ã€‚ä½œè€…æŠ¥å‘Šäº†å°å‹å¹³å°ä¸­çš„å»¶è¿Ÿå’Œèƒ½è€—ï¼Œæ˜¾ç¤ºäº†è¯¥ç ”ç©¶æ–¹å‘å¯¹å®é™…åº”ç”¨çš„å½±å“ã€‚</p></li><li><p>Multi-timescale Representation Learning in LSTM Language Models </p><p>  Shivangi Mahto, Vy Ai Vo, Javier S. Turek, Alexander Huth</p><p>  <strong>One-sentence Summary:</strong> This work presents a theoretically-motivated analysis of memory and timescale in LSTM language models.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æŒ‡å‡ºè‡ªç„¶è¯­è¨€ä¸­å•è¯ä¹‹é—´çš„å…³ç³»é€šå¸¸éµå¾ªå¹‚å¾‹ã€‚é—¨æ§é€’å½’ç¥ç»ç½‘ç»œï¼ˆä¾‹å¦‚LSTMï¼‰åœ¨è‡ªç„¶è¯­è¨€å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æ˜¯LSTMçš„é—å¿˜æœºåˆ¶æ˜¯ç”±æŒ‡æ•°è¡°å‡å†³å®šçš„ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†ä¸€ç§å·¥ç¨‹åŒ–LSTMçš„é—å¿˜æœºåˆ¶ï¼Œä»¥æ¨¡ä»¿è‡ªç„¶è¯­è¨€ä¸­è¡¨ç°å‡ºçš„å¹‚å¾‹å…³ç³»çš„æ–¹æ³•ã€‚é€šè¿‡åº”ç”¨ä»–ä»¬çš„æŠ€æœ¯ï¼Œä¿®æ”¹åçš„LSTMæ¨¡å‹å¯ä»¥æ›´å¥½åœ°å»ºæ¨¡ç¨€æœ‰æ ‡è®°ï¼Œè¿™äº›æ ‡è®°é€šå¸¸è·¨è¶Šè¾ƒé•¿çš„æ—¶é—´èŒƒå›´ï¼Œå› æ­¤ï¼Œè¯¥æ¨¡å‹å¯ä»¥åœ¨é¢‘ç‡è¾ƒä½çš„å•è¯ä¸Šè·å¾—è¾ƒä½çš„å›°æƒ‘åº¦ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯æ¨å¯¼ï¼Œè¯¥æ¨å¯¼è¡¨æ˜ï¼Œåœ¨ç»™å‡ºç¬¬ä¸€ä¸ªè¾“å…¥ä»¤ç‰Œåï¼ŒLSTMçš„é—å¿˜é—¨åœ¨é›¶è¾“å…¥çŠ¶æ€ä¸‹ä¼šç»å†æŒ‡æ•°è¡°å‡ã€‚</p><p>  å®éªŒè¡¨æ˜ï¼Œä»åä¼½é©¬åˆ†å¸ƒä¸­ç»˜åˆ¶Tæ˜¯è‡ªç„¶è¯­è¨€çš„è‡ªç„¶æ‹Ÿåˆã€‚ç„¶åï¼Œä½œè€…æå‡ºäº†åˆ©ç”¨æ­¤ç‰¹æ€§çš„å¤šå°ºåº¦LSTMæ¨¡å‹ã€‚æ¯ä¸ªæ—¶é—´æ ‡åº¦Tæ˜¯ä»åä¼½ç›åˆ†å¸ƒä¸­å¾—å‡ºçš„ï¼Œè¯¥åä¼½ç›åˆ†å¸ƒå®é™…ä¸Šæˆä¸ºä¸€ä¸ªé—å¿˜åå·®é¡¹ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å›ºå®šçš„ã€‚ç»˜åˆ¶å¤šä¸ªTæ¥æ¨¡æ‹Ÿå¹‚å¾‹ã€‚å¤šå°ºåº¦LSTMå¯ä»¥æ•è·æ­£ç¡®çš„å½’çº³åç½®ï¼Œä»¥ä¾¿åœ¨å¯¹é¢‘ç‡è¾ƒä½çš„å•è¯è¿›è¡Œå»ºæ¨¡æ—¶è¡¨ç°æ›´å¥½ï¼Œè¿™å¯èƒ½æœ‰åŠ©äºåœ¨å†…å­˜ä¸­ä¿ç•™æ›´é•¿çš„æ—¶é—´ã€‚è¯¥è®ºæ–‡å†™å¾—å¾ˆå¥½ï¼Œå¹¶ä¸”è¯¥æ–¹æ³•çš„åŠ¨æœºå’Œè§£é‡Šéƒ½æ¸…æ¥šã€‚å®éªŒç»è¿‡é€‚å½“è®¾è®¡ï¼Œç»“æœå¾ˆå¥½åœ°æ”¯æŒäº†ä¸»è¦ä¸»å¼ ã€‚</p></li></ul><h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><ul><li><p>Sparse Quantized Spectral Clustering (Spotlight)</p><p>  Zhenyu Liao, Romain Couillet, Michael W. Mahoney</p><p>  <strong>Reviewers say:</strong> </p><ol><li>æœ¬æ–‡å¯¹é€šè¿‡å°†éçº¿æ€§å‡½æ•°åº”ç”¨äºéšæœºçŸ©é˜µè€Œè·å¾—çš„çŸ©é˜µå…‰è°±è¿›è¡Œäº†å¾ˆå¥½çš„åˆ†æã€‚</li><li>è¿™æ˜¯ä¸€ç¯‡å¾ˆå¥½çš„è®ºæ–‡ï¼Œè¡¨æ˜äººä»¬å¯ä»¥æ‰°åŠ¨å†…æ ¸çŸ©é˜µï¼ˆæˆ–ä½¿å…¶é€šè¿‡éçº¿æ€§å˜æ¢ï¼‰ï¼Œè€Œä¸å¿…æ˜¾ç€ä¿®æ”¹åŸºç¡€æœ¬å¾è°±ï¼Œå› æ­¤ä¸ä¼šæŸå®³åº”ç”¨äºçŸ©é˜µçš„å…‰è°±èšç±»çš„æ€§èƒ½ã€‚æˆ‘ç«‹å³å¯ä»¥çœ‹åˆ°çš„æœ€é‡è¦çš„åº”ç”¨æ˜¯ç¨€ç–åŒ–å†…æ ¸çŸ©é˜µï¼Œä»¥ä¾¿å¯ä»¥æœ‰æ•ˆåœ°è¿›è¡Œè®¡ç®—ä½¿ç”¨ã€‚æˆ–ç±»ä¼¼åœ°ï¼Œå¦‚ä½œè€…æ‰€è¿°ï¼Œåº”ç”¨é‡åŒ–å’ŒäºŒå€¼åŒ–ã€‚</li></ol></li></ul><ul><li><p>A Critique of Self-Expressive Deep Subspace Clustering </p><p>  Benjamin David Haeffele, Chong You, Rene Vidal</p><p>  <strong>One-sentence Summary:</strong> Here we show theoretically and experimentally that there are a number of flaws with many existing self-expressive deep subspace clustering models.</p><p>  <strong>Reviewers say:</strong> æ‘˜è¦ï¼šæœ¬æ–‡å¯¹è‡ªè¡¨è¾¾æ·±åº¦å­ç©ºé—´èšç±»ï¼ˆSEDSCï¼‰æ¨¡å‹çš„å…ˆå‰ç»“æœçš„é‡è¦æ€§æå‡ºäº†ç–‘é—®ï¼Œè¿™äº›æ¨¡å‹è¢«å¹æ§ä¸ºçº¿æ€§å­ç©ºé—´èšç±»ï¼ˆä½¿ç”¨è‡ªè¡¨è¾¾å±æ€§ï¼‰æˆåŠŸæ‰©å±•åˆ°éçº¿æ€§æ•°æ®ç»“æ„ã€‚ä½œè€…æå‡ºäº†ä¸€ç»„ç†è®ºç»“æœï¼Œè¿™äº›ç»“æœè¡¨æ˜SEDSCçš„æ ‡å‡†é…æ–¹é€šå¸¸æ˜¯ä¸é€‚çš„ã€‚å³ä½¿å¢åŠ äº†æ­£åˆ™åŒ–ï¼Œä¹Ÿæ˜¾ç¤ºå‡ºè¿™ç§å…¬å¼å¯ä»¥å¾ˆå¥½åœ°äº§ç”Ÿçç¢çš„å‡ ä½•å½¢çŠ¶ï¼Œä¸åˆ©äºæˆåŠŸçš„å­ç©ºé—´èšç±»ã€‚</p></li></ul><ul><li><p>Intraclass clustering: an implicit learning ability that regularizes DNNs </p><p>  Simon Carbonnelle, Christophe De Vleeschouwer</p><p>  <strong>One-sentence Summary:</strong> This paper provides empirical evidence that deep neural networks are implicitly regularized through their ability to extract meaningful clusters among the samples of a class.</p><p>  <strong>Reviewers say:</strong></p><ol><li>æœ¬æ–‡ç ”ç©¶äº†åœ¨ç›‘ç£å­¦ä¹ ä¸­è®­ç»ƒçš„ç¥ç»ç½‘ç»œçš„ç±»å†…èšç±»èƒ½åŠ›ï¼Œå‘ç°å°½ç®¡æ ‡ç­¾æ²¡æœ‰æ˜ç¡®å¼ºåˆ¶è¿™æ ·åšï¼Œä½†ç½‘ç»œä»æ˜¾ç¤ºå‡ºç±»å†…èšç±»èƒ½åŠ›ã€‚å¹¶ä¸”åŸºäºè¿™äº›æ ‡å‡†çš„å‡†åˆ™ä¸æ¨¡å‹æ³›åŒ–æ€§èƒ½å¾ˆå¥½åœ°ç›¸å…³ã€‚</li><li>ä½œè€…æ³¨æ„åˆ°ï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œé€šå¸¸å­˜åœ¨æœªåœ¨ç²—ç³™ç±»æ ‡ç­¾ä¸­æ˜ç¡®ç¼–ç çš„ç›¸ä¼¼å›¾åƒçš„ç±»å†…ç»„ï¼Œä»–ä»¬ç§°ä¹‹ä¸ºç±»å†…èšç±»ã€‚ä»–ä»¬å‡è®¾ï¼ŒDNNåœ¨æ²¡æœ‰æ˜ç¡®å‘ŠçŸ¥çš„æƒ…å†µä¸‹è¯†åˆ«è¿™äº›ç±»å†…é›†ç¾¤çš„èƒ½åŠ›å¯èƒ½ä¸æ³›åŒ–ç›¸å…³ã€‚ç„¶åï¼Œä»–ä»¬ç»§ç»­åœ¨ä¸€ç³»åˆ—ç½‘ç»œï¼Œä½“ç³»ç»“æ„å’Œå¤§é‡è¶…å‚æ•°é…ç½®ä¸Šå¯¹æ­¤è¿›è¡ŒéªŒè¯ã€‚ä»–ä»¬ä¼šåœ¨å¯èƒ½çš„æƒ…å†µä¸‹å»ºç«‹å› æœå…³ç³»ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¡¨æ˜ï¼Œå¯ä»¥ä½¿ç”¨ç®€å•çš„åŸºäºæ–¹å·®çš„æ–¹æ³•æ£€æµ‹ç±»å†…èšç±»ï¼Œå¹¶ä¸”å®ƒåœ¨è®­ç»ƒçš„æ—©æœŸå°±å‡ºç°äº†ã€‚</li></ol></li><li><p>Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation </p><p>  Yaling Tao, Kentaro Takagi, Kouta Nakata</p><p>  <strong>One-sentence Summary:</strong> We present a clustering-friendly representation learning method using instance discrimination and feature decorrelation, which achieves accuracy of 81.5% and 95.4% on CIFAR-10 and ImageNet-10, respectively, far above state-of-the-art values.</p><p>  <strong>Reviewers say:</strong> ä½œè€…æå‡ºäº†ä¸€ç§æ”¹è¿›çš„åŸºäºæ·±åº¦å­¦ä¹ çš„è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸ºèšç±»åˆ†ææä¾›äº†æ›´æœ‰æ•ˆçš„åŠŸèƒ½ã€‚ï¼ˆ1ï¼‰æ ¹æ®åœ¨å‡ ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„æ¯”è¾ƒå®éªŒï¼Œå°†softmaxå…¬å¼åŒ–çš„æ­£äº¤çº¦æŸçš„é›†æˆèƒ½å¤Ÿæä¾›æ›´ç¨³å®šçš„æ½œåœ¨ç‰¹å¾è¡¨ç¤ºã€‚ï¼ˆ2ï¼‰æ®äº†è§£ï¼Œå¹¿æ³›ä½¿ç”¨çš„æ·±åº¦èšç±»æ–¹æ³•ç”¨äºæ›¿ä»£ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºæ¨¡å‹å‚æ•°å¹¶æ›´æ–°ç”±èšç±»æ–¹æ³•ï¼ˆä¾‹å¦‚k-meansï¼‰æä¾›çš„é”šç‚¹ï¼Œæˆ‘æƒ³çŸ¥é“æœ¬ç ”ç©¶ä¸­æå‡ºçš„æ–¹æ³•æ˜¯å¦å¯ä»¥ä»¥çœŸæ­£çš„ç«¯åˆ°ç«¯æ–¹å¼é›†æˆè¿™ä¸¤ä¸ªæ­¥éª¤ã€‚ï¼ˆ3ï¼‰æˆ‘å¯¹è¿™ç§æ‹Ÿè®®çš„è¡¨å¾å­¦ä¹ æ–¹æ³•çš„è¯„ä¼°æŒ‡æ ‡è¿œè¿œè¶…è¿‡äº†æœ€æ–°çš„æ°´å¹³ï¼Œå°è±¡æ·±åˆ»ã€‚å°½ç®¡ä½œè€…æä¾›äº†CIFAR-10æ•°æ®é›†ä¸Šæ½œåœ¨ç‰¹å¾çš„ä¸€äº›åˆ†å¸ƒå›¾ï¼Œä½†æ˜¯ImageNet-10ä¸Šçš„å¯è§†åŒ–åˆå¦‚ä½•å‘¢ï¼Ÿæ­¤å¤–ï¼Œæ·»åŠ ä¸€äº›å­˜åœ¨äºåŸå§‹å›¾åƒç©ºé—´è€Œéæ½œåœ¨ç©ºé—´ä¸­çš„â€œçœŸå®â€å¯è§†åŒ–ç»“æœå¯ä»¥å¸®åŠ©è¯´æ˜æ‰€æå‡ºçš„æ–¹æ³•æ˜¯å¦å¯ä»¥ä»è§†è§‰å†…å®¹çš„è§’åº¦æŒ–æ˜è§†è§‰ä¸Šæœ‰æ„ä¹‰çš„æ¦‚å¿µã€‚</p></li><li><p>MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering </p><p>  Tsung Wei Tsai, Chongxuan Li, Jun Zhu</p><p>  <strong>One-sentence Summary:</strong> A principled probabilistic clustering method that exploits the discriminative representations learned by contrastive learning and the semantic structures captured by a latent mixture model in a unified framework.</p><p>  <strong>Reviewers say:</strong> ç®€ä»‹ï¼šä½œè€…æå‡ºäº†â€œä¸“å®¶æ··åˆâ€ç±»å‹çš„æ–¹æ³•æ¥è§£å†³æ— ç›‘ç£å­¦ä¹ é—®é¢˜çš„èšç±»ã€‚è¯¥æ–¹æ³•ç§°ä¸ºå¯¹æ¯”ä¸“å®¶æ··åˆï¼ˆMiCEï¼‰ï¼Œå®ƒä½¿ç”¨å¯¹æ¯”å­¦ä¹ ä½œä¸ºåŸºæœ¬æ¨¡å—ï¼Œå¹¶å°†å…¶ä¸æ½œåœ¨çš„æ··åˆæ¨¡å‹ç›¸ç»“åˆã€‚ä½œè€…ä¸ºMiCEå¼€å‘äº†ä¸€ç§å¯æ‰©å±•ç®—æ³•ï¼Œå¹¶æ ¹æ®ç»éªŒè¯„ä¼°äº†æå‡ºçš„å›¾åƒèšç±»æ–¹æ³•ã€‚</p></li><li><p>Deep Learning meets Projective Clustering </p><p>  Alaa Maalouf, Harry Lang, Daniela Rus, Dan Feldman</p><p>  <strong>One-sentence Summary:</strong> We suggest a novel technique for compressing a fully connected layer (or an embedding layer).</p><p>  <strong>Reviewers say:</strong>     è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºæŠ•å½±èšç±»çš„æ–°æ–¹æ³•ï¼Œç”¨äºå‹ç¼©DNNçš„åµŒå…¥å±‚ä»¥å®ç°è‡ªç„¶è¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚ä½œè€…è¡¨æ˜ï¼Œé€šè¿‡è€ƒè™‘ä¸€ç»„kä¸ªå­ç©ºé—´è€Œä¸æ˜¯å•ä¸ªå­ç©ºé—´ï¼Œå¯ä»¥æ”¹å–„å‹ç¼©å’Œæ¨¡å‹ç²¾åº¦ä¹‹é—´çš„æŠ˜è¡·ã€‚å‹ç¼©DNNçš„æ–¹æ³•æ˜¯ç ”ç©¶çš„æ´»è·ƒé¢†åŸŸï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ä»¥åŠæœ‰è¶£çš„ç»“æœã€‚</p><p>  è¯„çº§ï¼šæœ¬æ–‡æå‡ºäº†ä¸€äº›æœ‰è¶£çš„æƒ³æ³•æ¥å‹ç¼©åµŒå…¥å±‚ã€‚ä½†æ˜¯ï¼Œç”±äºè¿™æ˜¯ä¸€ç¯‡ç»éªŒè®ºæ–‡ï¼Œå› æ­¤æˆ‘å¸Œæœ›èƒ½å¾—åˆ°ä¸€ç»„æ›´å…¨é¢çš„ç»éªŒç»“æœï¼Œå¹¶èƒ½æ›´å¥½åœ°ä¸å…¶ä»–ç›¸å…³æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚</p></li><li><p>Isotropy in the Contextual Embedding Space: Clusters and Manifolds </p><p>  Xingyu Cai, Jiaji Huang, Yuchen Bian, Kenneth Church</p><p>  <strong>One-sentence Summary:</strong> This paper reveals isotropy in the clustered contextual embedding space, and found low-dimensional manifolds in there.</p><p>  <strong>Reviewers say:</strong> ä½œè€…ç ”ç©¶äº†è‡ªç„¶è¯­è¨€çš„å„ç§ä¸Šä¸‹æ–‡åµŒå…¥æ¨¡å‹çš„ä»¤ç‰ŒåµŒå…¥ç©ºé—´ã€‚ä»–ä»¬ä½¿ç”¨åŸºäºæœ€è¿‘é‚»å±…ï¼Œèšç±»å’ŒPCAçš„æŠ€æœ¯ï¼Œåœ¨è¿™äº›åµŒå…¥æ¨¡å‹ä¸­æŠ¥å‘Šäº†å…³äºå±€éƒ¨ç»´æ•°/å„å‘å¼‚æ€§/èšç±»/æµå½¢ç»“æ„çš„å„ç§ç»“æœï¼Œè¿™æ˜¯å¸Œæœ›äº†è§£è¿™äº›æ¨¡å‹çš„ç§‘å­¦å®¶å’Œä»ä¸šäººå‘˜æ™®éæ„Ÿå…´è¶£çš„ç»“æœã€‚è¿™äº›åŒ…æ‹¬åœ¨é€‚å½“èšç±»å’Œç§»åŠ¨æ—¶å‘ç°åµŒå…¥ä¸­çš„ï¼ˆå±€éƒ¨ï¼‰å„å‘åŒæ€§ï¼Œä»¥åŠåœ¨GPTæ¨¡å‹ä¸­å‡ºç°æ˜æ˜¾çš„æµå½¢ç»“æ„ã€‚</p></li><li><p>Deep Repulsive Clustering of Ordered Data Based on Order-Identity Decomposition </p><p>  Seon-Ho Lee, Chang-Su Kim</p><p>  <strong>One-sentence Summary:</strong> A deep clustering algorithm for ordered data is proposed based on the order-identity decomposition.</p><p>  <strong>Reviewers say:</strong> ä½œè€…æè¿°äº†ä¸€ç§å¯¹æœ‰åºæ•°æ®è¿›è¡Œé¢„æµ‹çš„ç›´è§‚æœ‰æ•ˆçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨äº†ä¸€ç§åŸºäºèšç±»çš„ç›´è§‚æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ•°æ®åˆ†ç»„ä¸ºå­é›†ï¼Œå­é›†ä¸­çš„é¡¹æ˜“äºè®¢è´­ã€‚è¯¥æ–‡ä»¶å†™å¾—å¾ˆæ¸…æ¥šï¼Œå¹¶æ¸…æ¥šåœ°è¯´æ˜äº†è¯¥æ–¹æ³•ã€‚æœ¬æ–‡æ˜¾ç¤ºäº†è¯¥æ–¹æ³•çš„é¢„æµ‹è¾“å‡ºçš„å‡ ä¸ªç¤ºä¾‹ï¼Œå¹¶æ˜¾ç¤ºäº†ä¸¤ä¸ªä»»åŠ¡çš„ç»“æœï¼ˆä¼°è®¡å¹´é¾„ï¼Œç¾å­¦è¯„åˆ†å›å½’ï¼‰ã€‚è¯¥æ–¹æ³•åœ¨ä¼°è®¡å¹´é¾„çš„ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶ä¸”åœ¨å…¶ä»–ä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ã€‚ä½œè€…æ˜¾ç¤ºäº†å…³äºå¹´é¾„è½¬å˜çš„è¿›ä¸€æ­¥ç»“æœã€‚</p></li></ul><h2 id="data-augmentation"><a href="#data-augmentation" class="headerlink" title="data augmentation"></a>data augmentation</h2><ul><li><p>Removing Undesirable Feature Contributions Using Out-of-Distribution Data </p><p>  Saehyung Lee, Changhwa Park, Hyungyu Lee, Jihun Yi, Jonghyun Lee, Sungroh Yoon</p><p>  <strong>One-sentence Summary:</strong> We propose a simple method, Out-of-distribution data Augmented Training (OAT), to leverage OOD data for adversarial and standard learning.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡ç ”ç©¶äº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨æœªæ ‡è®°çš„åˆ†å¸ƒå¤±è°ƒï¼ˆOODï¼‰æ•°æ®æ¥æé«˜é²æ£’æ€§ï¼ˆå’Œæ ‡å‡†ï¼‰å‡†ç¡®æ€§çš„æ•ˆæœã€‚ä¸»è¦ç®—æ³•è´¡çŒ®æ˜¯åŸºäºæ•°æ®å¢å¼ºçš„é²æ£’è®­ç»ƒç®—æ³•æ¥è®­ç»ƒæŸå¤±ï¼Œè¯¥ç®—æ³•ç»è¿‡ç²¾å¿ƒè®¾è®¡ä»¥ä»å…¶ä»–OODæ•°æ®ä¸­å—ç›Šã€‚æœ‰è¶£çš„æ˜¯ï¼ŒOODæ•°æ®å¸¦æœ‰éšæœºæ ‡ç­¾ï¼Œç”¨äºè®­ç»ƒè¿‡ç¨‹ã€‚å¦‚ç†è®ºç»“æœæ‰€ç¤ºï¼Œè¿™ç§è¾“å…¥OODæ•°æ®çš„æ–¹å¼æœ‰åŠ©äºæ¶ˆé™¤å¯¹éé²æ£’ç‰¹å¾çš„ä¾èµ–æ€§ï¼Œä»è€Œæé«˜é²æ£’æ€§ã€‚</p><p>  æ­£å¦‚æ‰€æœ‰è¯„è®ºè€…ï¼ˆæˆ‘éƒ½åŒæ„ï¼‰æ‰€æŒ‡å‡ºçš„é‚£æ ·ï¼Œåœ¨è®­ç»ƒä¸­ä½¿ç”¨æœªæ ‡è®°çš„OODæ•°æ®çš„æƒ³æ³•æ˜¯æ–°é¢–çš„/æœ‰è¶£çš„ï¼Œå¹¶ä¸”è¯¥è®ºæ–‡è¿˜å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç®—æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æ•°å€¼ç»“æœä¹Ÿè¯å®äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p><strong>ã€å€¼å¾—é˜…è¯»ã€‘</strong> Negative Data Augmentation </p><p>  Abhishek Sinha, Kumar Ayush, Jiaming Song, Burak Uzkent, Hongxia Jin, Stefano Ermon</p><p>  <strong>One-sentence Summary:</strong> We propose a framework to do Negative Data Augmentation for generative models and self-supervised learning</p><p>  **Reviewers say:**æœ¬æ–‡ç ”ç©¶äº†æ‰©å¤§è´Ÿé¢å®ä¾‹ï¼ˆä¸ä»…ä»…æ˜¯æ­£é¢å®ä¾‹ï¼‰å¦‚ä½•æ”¹å–„å„ç§è¡¨å¾å­¦ä¹ ä»»åŠ¡ã€‚æœ¬æ–‡ç ”ç©¶äº†è®¸å¤šä¸åŒçš„å¢å¼ºï¼Œå¹¶å°†å®ƒä»¬åº”ç”¨äºGANå’Œå¸¦æœ‰å›¾åƒå’Œè§†é¢‘çš„å¯¹æ¯”å­¦ä¹ ã€‚</p><ul><li>ä¼˜ç‚¹ï¼šæœ¬æ–‡çš„ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯å®ƒçš„ç®€å•æ€§ã€‚è¯¥æ–¹æ³•å¾ˆå®¹æ˜“å®ç°ä¸ºå‡ ç§æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æœ¬æ–‡è¯„ä¼°çš„æ¯ç§æ–¹æ³•ä¸Šéƒ½å¯è·å¾—å¾ˆå¼ºçš„ç»“æœã€‚è¿™äº›æ–¹æ³•åŸºäºGANå’Œå›¾åƒå’Œè§†é¢‘çš„å¯¹æ¯”å­¦ä¹ è¿›è¡Œäº†è¯„ä¼°ã€‚</li><li><pre><code>å°½ç®¡è¯¥æ–¹æ³•çš„æ–°é¢–æ€§æœ‰é™ï¼Œä½†åœ¨å»ºç«‹ä¸€äº›ç†è®ºç»“æœä»¥ç›´è§‚è¯´æ˜è¯¥æ–¹æ³•ä¸ºä½•èµ·ä½œç”¨æ–¹é¢ï¼Œæœ¬æ–‡åšå¾—å¾ˆå¥½ã€‚ä¸ç¼ºä¹ç›´è§‰äº†è§£å…¶å·¥ä½œåŸç†çš„æœºå™¨å­¦ä¹ è¿›æ­¥ç›¸æ¯”ï¼Œæœ¬æ–‡åœ¨ä¸ºè¿™ç§æ–¹æ³•æä¾›ä¸€äº›è§£é‡Šå’ŒåŠ¨æœºæ–¹é¢åšå¾—å¾ˆå¥½ã€‚</code></pre></li><li>å°½ç®¡æœ¬æ–‡ç€é‡äºå›¾åƒå’Œè§†é¢‘ï¼Œä½†ç›¸åŒçš„æ€æƒ³ä¹Ÿå¯ä»¥æ‰©å±•åˆ°å…¶ä»–å½¢å¼ï¼Œä¾‹å¦‚æ–‡æœ¬æˆ–éŸ³é¢‘ã€‚</li><li>å®éªŒä»¤äººä¿¡æœï¼Œè¡¨æ˜äº†è¿™ç§æƒ³æ³•çš„æ™®éæ€§ã€‚å®éªŒæ˜¯åœ¨å‡ ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„ã€‚å®éªŒå¾—åˆ°ç†è®ºç»“æœçš„æ”¯æŒï¼Œä»è€Œç›´è§‚åœ°è¯´æ˜äº†è¯¥æ–¹æ³•ä¸ºä½•æœ‰æ•ˆã€‚å¼•è¨€åœ¨ç¡®å®šä¸å…¶ä»–æ•°æ®å¢å¼ºæ–¹æ³•çš„å·®å¼‚æ–¹é¢åšå¾—å¾ˆå¥½ï¼Œå°¤å…¶æ˜¯é€šè¿‡ä½¿ç”¨è´Ÿé¢ç¤ºä¾‹ã€‚</li></ul></li><li><p>CoDA: Contrast-enhanced and Diversity-promoting Data Augmentation for Natural Language Understanding </p><p>  Yanru Qu, Dinghan Shen, Yelong Shen, Sandra Sajeev, Weizhu Chen, Jiawei Han</p><p>  <strong>Reviewers say:</strong> NLPæ ·æœ¬çš„å¢åŠ æ˜¯ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œæ²¡æœ‰æ˜ç¡®çš„â€œé€‚ç”¨äºæ‰€æœ‰äººâ€æœºåˆ¶ã€‚è¿™ä¸è®¡ç®—æœºè§†è§‰å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œå­˜åœ¨è¯¸å¦‚æ—‹è½¬ï¼Œè‰²è°ƒä¿®æ”¹ï¼Œé¥±å’Œåº¦ä»¥åŠå¤šç§å…¶ä»–æŠ€æœ¯çš„æŠ€æœ¯ã€‚è¿™é¡¹å·¥ä½œè¯•å›¾é€šè¿‡æå‡ºä¸€ç§å°†å¤šç§å…ˆå‰å·²çŸ¥çš„æ–¹æ³•ä»”ç»†åˆå¹¶ä»¥ç”Ÿæˆå„ç§æ ‡ç­¾ä¿å­˜ç¤ºä¾‹çš„æŠ€æœ¯æ¥è§£å†³è¯¥é—®é¢˜ã€‚RoBERTaä¸Šçš„å®éªŒç»“æœå¼ºè°ƒäº†è¿™ç§æ•°æ®å¢å¼ºæ–¹æ³•åœ¨æ–‡æœ¬åˆ†ç±»ï¼ˆGLUEï¼‰ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§å’Œé‡è¦æ€§ã€‚</p></li><li><p>MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space </p><p>  Tsz-Him Cheung, Dit-Yan Yeung</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨æ½œåœ¨åµŒå…¥ç©ºé—´çš„ç»Ÿä¸€æ•°æ®æ‰©å……æ–¹æ³•-å­¦ä¹ è¿ç»­çš„æ½œåœ¨è½¬æ¢ç©ºé—´ï¼Œå¹¶æ‰¾åˆ°åœ¨è¯¥ç©ºé—´ä¸­éå†çš„æœ‰æ•ˆæ–¹å‘ä»¥è¿›è¡Œæ•°æ®å¢å¼ºã€‚æ‰€æå‡ºçš„æ–¹æ³•ç»“åˆäº†ç°æœ‰çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œä¾‹å¦‚å¯¹æŠ—è®­ç»ƒï¼Œä¸‰é‡ä¸¢å¤±å’Œè”åˆè®­ç»ƒã€‚æœ¬æ–‡è¿˜ç¡®å®šäº†æ¨¡å‹æ€§èƒ½ä½ä¸‹çš„è¾“å…¥ç¤ºä¾‹ï¼Œå¹¶åˆ›å»ºäº†æ›´éš¾çš„ç¤ºä¾‹æ¥å¸®åŠ©æ¨¡å‹æ”¹è¿›å…¶æ€§èƒ½ã€‚åœ¨ä¸æ–‡æœ¬ï¼Œè¡¨æ ¼ï¼Œæ—¶é—´åºåˆ—å’Œå›¾åƒæ¨¡æ€ç›¸å¯¹åº”çš„å¤šä¸ªå€¼ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œé™¤å›¾åƒæ•°æ®å¤–ï¼Œå…¶æ€§èƒ½å‡ä¼˜äºSOTAã€‚</p><p>  æœ¬æ–‡å›åº”äº†å®¡é˜…è€…çš„åé¦ˆï¼Œä»¥æä¾›æ›´è¯¦ç»†çš„å®éªŒå’Œæ›´å¼ºçš„åŸºå‡†ï¼Œå¹¶è¿›è¡Œäº†æ¶ˆèç ”ç©¶ä»¥æ˜¾ç¤ºè¯¥æ–¹æ³•ä¸åŒç»„æˆéƒ¨åˆ†çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡ä¸å…¶ä»–SOTAæ–¹æ³•è¿›è¡Œå½»åº•çš„ç»éªŒæ¯”è¾ƒï¼Œä»¥åŠä½¿ç”¨å…¶ä»–æŸå¤±å‡½æ•°ï¼ˆä¾‹å¦‚ä¸­å¿ƒæŸå¤±ï¼Œå¤§è¾¹é™…æŸå¤±å’Œå…¶ä»–å¯¹æ¯”æŸå¤±ï¼‰ä½œä¸ºæœ¬æ–‡ä¸­æå‡ºçš„ä¸‰é‡æŸå¤±çš„æ›¿ä»£æ–¹æ³•ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ”¹å–„ç»“æœã€‚</p></li><li><p>Training GANs with Stronger Augmentations via Contrastive Discriminator </p><p>  Jongheon Jeong, Jinwoo Shin</p><p>  <strong>One-sentence Summary:</strong> We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. </p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æ—¨åœ¨é€šè¿‡å°†å¯¹æ¯”æ€§å­¦ä¹ çš„åŸç†çº³å…¥GANé‰´åˆ«å™¨çš„è®­ç»ƒä¸­æ¥æ”¹è¿›ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„è®­ç»ƒã€‚ä¸è¯•å›¾å°†GANæŸå¤±ç›´æ¥æœ€å°åŒ–çš„æ™®é€šGANä¸åŒï¼Œæ‹Ÿè®®çš„å¸¦æœ‰å¯¹æ¯”é‰´åˆ«å™¨ï¼ˆContraDï¼‰çš„GANå˜ä½“ä½¿ç”¨é‰´åˆ«å™¨ç½‘ç»œé¦–å…ˆä»ç»™å®šçš„æ•°æ®æ‰©å……å’Œå®é™…/ç”Ÿæˆçš„ç¤ºä¾‹é›†ä¸­å­¦ä¹ å¯¹æ¯”è¡¨ç¤ºï¼Œç„¶åè®­ç»ƒåŸºäºå­¦ä¹ åˆ°çš„å¯¹æ¯”è¡¨ç¤ºçš„é‰´åˆ«å™¨ã€‚æ³¨æ„åˆ°è¿™ç§æ··åˆçš„å‰¯ä½œç”¨æ˜¯ç”±äºGANè®­ç»ƒè€Œåœ¨å¯¹æ¯”å­¦ä¹ ä¸­çš„æ”¹è¿›ã€‚ç»“æœè¡¨æ˜ï¼Œå¸¦æœ‰å¯¹æ¯”é‰´åˆ«å™¨çš„GANæ¨¡å‹ä¼˜äºä½¿ç”¨æ•°æ®å¢å¼ºçš„å…¶ä»–æŠ€æœ¯ã€‚</p></li><li><p>Model Patching: Closing the Subgroup Performance Gap with Data Augmentation </p><p>  Karan Goel, Albert Gu, Yixuan Li, Christopher Re</p><p>  <strong>One-sentence Summary:</strong> We describe how to fix classifiers that fail on subgroups of a class using a combination of learned data augmentation &amp; consistency training to achieve subgroup invariance.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨åˆ†ç±»å™¨ä¾èµ–äºç‰¹å®šå­ç»„ç‰¹å¾çš„æƒ…å†µä¸‹å‡è½»å›¾åƒä¸­å­ç»„æ€§èƒ½å·®è·çš„æ–¹æ³•ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå…¶ä¸­ï¼ˆç”±GANsç”Ÿæˆçš„ï¼‰åˆæˆç¤ºä¾‹å……å½“æ‰€æœ‰å¯èƒ½å­ç»„ä¸­çœŸå®æ ·æœ¬çš„å®ä¾‹ã€‚é€šè¿‡åŒ¹é…åŸå§‹ç¤ºä¾‹å’Œæ‰©å±•ç¤ºä¾‹çš„é¢„æµ‹ï¼Œé¢„æµ‹æ¨¡å‹è¢«è¿«å¿½ç•¥é¼“åŠ±ä¸å˜æ€§çš„äºšç»„å·®å¼‚ã€‚æ‰€æå‡ºçš„â€œå—æ§æ•°æ®å¢å¼ºâ€æ–¹æ³•ï¼ˆå¦‚R4æ‰€ç²¾ç¡®è°ƒç”¨çš„ï¼‰æ˜¯ç›¸å…³ä¸”åŠ¨æœºè‰¯å¥½çš„ï¼Œç†è®ºä¾æ®æ”¯æŒä¸»è¦ä¸»å¼ ï¼Œå¹¶ä¸”å®éªŒç»“æœå¤šç§å¤šæ ·ï¼Œå¹¶è¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„ä¼˜ç‚¹ã€‚æ­£å¦‚R3æ­£ç¡®æŒ‡å‡ºçš„é‚£æ ·ï¼Œâ€œé™„å½•ä¹Ÿéå¸¸è¯¦å°½ï¼Œä»£ç ç»„ç»‡å¾—å¾ˆå¥½â€ã€‚</p></li><li><p>SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization </p><p>  A F M Shahab Uddin, Mst. Sirazam Monira, Wheemyung Shin, TaeChoong Chung, Sung-Ho Bae</p><p>  <strong>One-sentence Summary:</strong> The proposed method carefully selects a representative image patch with the help of a saliency map and mixes this indicative patch with the target image that leads the model to learn more appropriate feature representation</p><p>  <strong>Reviewers say:</strong> </p><ol><li>æœ¬æ–‡æå‡ºäº†å¯¹æ•°æ®æ··åˆçš„cutmixç­–ç•¥çš„ä¸€ç§æ”¹è¿›ï¼Œå…¶ä¸­æºè¡¥ä¸ä¸æ˜¯éšæœºé€‰æ‹©è€Œæ˜¯åŸºäºæ˜¾ç€æ€§é€‰æ‹©ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨Imagenetï¼ŒCIFAR 10/100ä¸Šï¼Œæ··åˆå’Œå…¶ä»–ç›¸å…³ç­–ç•¥å¾—åˆ°äº†æ”¹è¿›ï¼Œå¹¶ä¸”è¿˜å¯ä»¥è½¬ç§»åˆ°å¯¹è±¡æ£€æµ‹ä¸­</li><li>æ€»ç»“å’Œè´¡çŒ®ï¼š æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºç­–ç•¥æ¥è®­ç»ƒå›¾åƒåˆ†ç±»å™¨å’Œç›®æ ‡æ£€æµ‹å™¨ã€‚å…³é”®è§è§£æ˜¯ä½¿ç”¨å›¾åƒæ˜¾ç€æ€§ä¿¡å·æ¥æŒ‡å¯¼æ··åˆå›¾åƒæ—¶åœ¨ä½•å¤„è£å‰ªå’Œç²˜è´´å›¾åƒã€‚æœ¬æ–‡åŒ…æ‹¬å¯¹è¿™ç§æ–¹æ³•çš„è®¾è®¡ç©ºé—´çš„æ¢ç´¢ï¼Œä»¥åŠå¤šä¸ªå®éªŒç»“æœï¼Œè¡¨æ˜ä¸ç°æœ‰çš„æ•°æ®å¢å¼ºç­–ç•¥ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„ç»éªŒä¼˜è¶Šæ€§ã€‚</li></ol><p>  å¯ç¤ºï¼š è¿™ç¯‡è®ºæ–‡å¾ˆæœ‰è¶£ï¼Œå› ä¸ºå®ƒæä¾›äº†ä¸€ä¸ªæ–°çš„æŠ€å·§ï¼Œæ—¢æ˜“äºç†è§£ï¼Œå¯ä»¥è¾©é©³ï¼Œåˆï¼ˆç°åœ¨ï¼‰å…·æœ‰è‰¯å¥½çš„ç»éªŒæ”¯æŒï¼ˆç”¨äºåˆ†ç±»ï¼Œæ£€æµ‹å’Œå¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§ï¼‰ã€‚</p><p>  åˆ›æ„ï¼šæœ‰é™ã€‚å°½ç®¡ä»¥å‰çš„å·¥ä½œéƒ½æ²¡æœ‰æä¾›æ­¤å¤„ä»‹ç»çš„å®éªŒç»“æœï¼Œä½†ç»“æœæ˜¯å¯ä»¥é¢„æœŸçš„ã€‚è¿™é¡¹å·¥ä½œæ˜¯è‰¯å¥½çš„A + Bå¢é‡å·¥ä½œã€‚</p></li><li><p>On Graph Neural Networks versus Graph-Augmented MLPs </p><p>  Zhengdao Chen, Lei Chen, Joan Bruna</p><p>  <strong>One-sentence Summary:</strong> We establish a separation in representation power between GNNs and Graph-Augmented MLPs.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡ç ”ç©¶äº†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„ä¸€ç§å˜ä½“ï¼Œå³å›¾å¢å¼ºMLPï¼ˆGA-MLPï¼‰ã€‚ä¸åœ¨GNNä¸­ï¼ŒèŠ‚ç‚¹å°†æ¶ˆæ¯å‘é€åˆ°é‚»å±…å¹¶é€šè¿‡éçº¿æ€§MLPèšåˆæ¥æ”¶åˆ°çš„æ¶ˆæ¯ä¸åŒï¼ŒGA-MLPä¾èµ–äºä¸€æ¬¡è®¡ç®—çš„å•ä¸ªå¢å¼ºåµŒå…¥ï¼Œç„¶åå°†MLPåº”ç”¨äºæ–°çš„åµŒå…¥ã€‚å¯ä»¥é€šè¿‡å¯¹è¾“å…¥è¡¨ç¤ºåº”ç”¨å½¢å¼ä¸ºAï¼ŒA ^ 2ï¼Œâ€¦ï¼ŒA ^ kçš„çº¿æ€§å˜æ¢æ¥è·å¾—å¢å¼ºåµŒå…¥ï¼Œä»è€Œæ•è·æ›´å¤§çš„é‚»åŸŸã€‚æœ¬æ–‡çš„ä¸»è¦ç›®çš„æ˜¯è¯æ˜ä¸GNNç›¸æ¯”ï¼Œä½¿ç”¨GA-MLPè§£å†³å›¾å½¢é—®é¢˜æ—¶çš„æ ¹æœ¬ç¼ºé™·ã€‚æ²¿ç€è¿™äº›æ€è·¯ï¼Œä¸»è¦ç»“æœå¯ä»¥æè¿°å¦‚ä¸‹ï¼š1ï¼‰æœ¬æ–‡ç¡®å®šäº†è¯†åˆ«éåŒæ„å›¾çš„ç‰¹å®šå®ä¾‹ï¼Œå¯ä»¥é€šè¿‡GNNè€Œä¸æ˜¯GA-MLPæ¡†æ¶æ¥è§£å†³ã€‚2ï¼‰æœ¬æ–‡å¯¹GNNä¸å›¾å¢å¼ºMLPçš„è¡¨ç¤ºèƒ½åŠ›è¿›è¡Œäº†å®éªŒå’Œå®éªŒè¯„ä¼°ï¼Œå¹¶æ ¹æ®æ ¹å›¾ä¸Šçš„èŠ‚ç‚¹çº§å‡½æ•°æ˜¾ç¤ºäº†ä¸¤è€…ä¹‹é—´çš„è¡¨è¾¾èƒ½åŠ›åˆ†ç¦»ã€‚å…·ä½“æ¥è¯´ï¼Œä»–ä»¬è¡¨æ˜ï¼Œå¯ä»¥ç”¨ä¸€å®šæ·±åº¦çš„GNNè¡¨ç¤ºçš„ä¸€ç»„å‡½æ•°åœ¨kä¸­å‘ˆæŒ‡æ•°å¢é•¿ï¼Œè€Œåœ¨è€ƒè™‘ç±»ä¼¼çš„GA-MLPä½“ç³»ç»“æ„æ—¶ï¼Œå‡½æ•°ç±»ä»…å‘ˆæŒ‡æ•°å¢é•¿ã€‚ä»–ä»¬è¿˜æ ¹æ®ç»éªŒè¯„ä¼°äº†ä¸¤ç§æ¨¡å‹åœ¨ç¤¾åŒºæ£€æµ‹å’Œæ­¥è¡Œé—®é¢˜è®¡æ•°æ–¹é¢çš„æ€§èƒ½å·®å¼‚ã€‚</p></li><li><p>Explaining the Efficacy of Counterfactually Augmented Data </p><p>  Divyansh Kaushik, Amrith Setlur, Eduard H Hovy, Zachary Chase Lipton</p><p>  <strong>One-sentence Summary:</strong> We present a framework for thinking about counterfactually augmented data and make strides towards understanding its benefits in out-of-domain generalization.</p><p>  Reviewers says: æœ¬æ–‡ç ”ç©¶äº†åäº‹å®æ‰©å……çš„æ•°æ®å¯¹åŸŸå¤–æ³›åŒ–çš„å½±å“ã€‚æœ¬æ–‡ä»å…·æœ‰é«˜æ–¯çº¿æ€§æ¨¡å‹çš„ç»“æ„å› æœæ¨¡å‹çš„ç©å…·ç¤ºä¾‹å¼€å§‹ï¼Œå…¶ä¸­å°†å™ªå£°æ·»åŠ åˆ°å› æœæˆ–éå› æœç‰¹å¾ä¸Šã€‚ä¼‘é—²ç‰¹å¾ä¸Šçš„å™ªå£°å¢åŠ ä¼šå½±å“æœ€å°äºŒä¹˜ä¼°è®¡ï¼Œè€Œéå› æœç‰¹å¾ä¸Šçš„å™ªå£°å´ä¸ä¼šå½±å“æœ€å°äºŒä¹˜ä¼°è®¡ã€‚æœ¬æ–‡åœ¨è¿™ç§æƒ…å†µå’Œåäº‹å®æ–‡æœ¬ç¼–è¾‘ä¹‹é—´ä½œäº†ä¸€ä¸ªç±»æ¯”ï¼Œå…¶ä¸­å‡å®šè·¨åº¦ï¼ˆåˆç†å€¼ï¼‰è¢«è®¤ä¸ºæ˜¯å› æœå…³ç³»ã€‚æå‡ºäº†ä¸€ä¸ªå‡è®¾ï¼Œå³åœ¨åŸºæœ¬åŸç†ï¼ˆå› æœå…³ç³»ç‰¹å¾ï¼‰ä¸Šæ·»åŠ å™ªå£°ä¼šå¯¼è‡´æ¨¡å‹ä¾èµ–éç†æ€§å› ç´ ï¼ˆéå› æœå…³ç³»ç‰¹å¾ï¼‰å¹¶å¯¼è‡´è¾ƒå·®çš„æ ·æœ¬å¤–æ€§èƒ½ï¼Œè€Œåœ¨éç†æ€§å› ç´ ä¸­æ·»åŠ å™ªå£°åˆ™ä¼šå¯¼è‡´æ›´ç³Ÿçš„ç»“æœã€‚æ ·æœ¬å†…æ€§èƒ½ï¼Œä½†æ›´å¥½çš„æ ·æœ¬å¤–æ€§èƒ½ã€‚å¯¹æƒ…ç»ªå’Œè‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰æ•°æ®é›†çš„å®éªŒå¤§å¤šè¯å®äº†è¿™ä¸€å‡è®¾ï¼Œä½†æœ‰ä¸€äº›ä¾‹å¤–éœ€è¦è®¨è®ºã€‚å®éªŒåŒ…æ‹¬ä¸‰ç§è¯†åˆ«åŸç†çš„æ–¹æ³•ï¼ˆäººå·¥ç¼–è¾‘ï¼Œäººå·¥è¯†åˆ«çš„è·¨åº¦å’Œé€šè¿‡è‡ªæˆ‘æ³¨æ„è¯†åˆ«çš„è·¨åº¦ï¼‰ã€‚åœ¨æœ‰ç†æˆ–æ— ç†çš„æƒ…å†µä¸‹å¯¹æ¨¡å‹è¿›è¡Œæœ‰æ— å™ªå£°è®­ç»ƒï¼ˆç”¨éšæœºä»¤ç‰Œä»£æ›¿çœŸå®ä»¤ç‰Œï¼‰ã€‚</p></li><li><p>Reweighting Augmented Samples by Minimizing the Maximal Expected Loss </p><p>  Mingyang Yi, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, Zhi-Ming Ma</p><p>  <strong>One-sentence Summary:</strong> a new reweighting strategy on augmented samples</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ•°æ®æ‰©å……æ–¹æ³•ã€‚ç‰¹åˆ«åœ°ï¼Œå®ƒæå‡ºäº†é‡æ–°åŠ æƒæŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°å…è®¸æ‰¾åˆ°æ‰©å¢æ ·æœ¬çš„æœ€ä½³åŠ æƒã€‚è¯¥æ–¹æ³•åœ¨æ ‡å‡†å›¾åƒå’Œè¯­è¨€ä»»åŠ¡ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå¹¶ä¸å¤šç§æ›¿ä»£æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚</p></li><li><p>Simple Augmentation Goes a Long Way: ADRL for DNN Quantization </p><p>  Lin Ning, Guoyang Chen, Weifeng Zhang, Xipeng Shen</p><p>  <strong>One-sentence Summary:</strong> Augments the neural networks in Deep Reinforcement Learning(DRL) with a complementary scheme to boost the performance of learning and solve the common low convergence problem in the early stage of DRL</p></li><li><p>On Data-Augmentation and Consistency-Based Semi-Supervised Learning </p><p>  Atin Ghosh, Alexandre H. Thiery</p><p>  <strong>One-sentence Summary:</strong> We propose a simple and natural framework leveraging the Hidden Manifold Model to study modern SSL methods.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æä¾›äº†ä¸€äº›æœ‰å…³åœ¨åŸºäºä¸€è‡´æ€§æ­£åˆ™åŒ–çš„åŠç›‘ç£å­¦ä¹ ä¸­ä½¿ç”¨æ•°æ®å¢å¼ºçš„ç†è®ºè§‚ç‚¹ã€‚æœ¬æ–‡ä½¿ç”¨çš„æ¡†æ¶è®¤ä¸ºï¼Œé«˜è´¨é‡çš„æ•°æ®å¢å¼ºåº”è¯¥æ²¿ç€æ•°æ®æµå½¢ç§»åŠ¨ã€‚è¿™ç§é€šç”¨è§†å›¾å…è®¸å°†è®ºæ–‡çš„æ€æƒ³åº”ç”¨äºæ•°æ®é›†ï¼ˆä¸ç°æœ‰æŠ€æœ¯çš„åŠç›‘ç£å­¦ä¹ ç®—æ³•ä¸­ä½¿ç”¨çš„å›¾åƒç‰¹å®šæ•°æ®å¢å¼ºç›¸åï¼‰ã€‚æˆ‘ä¸çŸ¥é“æœ‰ä»»ä½•å…¶ä»–å·¥ä½œå¯ä»¥æå‡ºè¿™äº›è§‚ç‚¹ï¼Œå¹¶ä¸”äº‹å®ä¸Šï¼Œæœ¬æ–‡çš„æ„ä¹‰åœ¨äºï¼Œå®ƒä¸ºæœ€æœ‰æ•ˆçš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•æä¾›äº†æ–°çš„ä¸”å¯èƒ½æœ‰ç”¨çš„è§‚ç‚¹ã€‚å®¡ç¨¿äººè®¤ä¸ºè¯¥æ–‡ä»¶æ¸…æ™°å®ç”¨ã€‚ä¸»è¦å…³æ³¨çš„æ˜¯ï¼Œè¯¥è®ºæ–‡ä»…åŒ…æ‹¬ç©å…·ç¯å¢ƒä¸­çš„å®éªŒã€‚ç¡®å®ï¼Œ</p></li></ul><ul><li><p>Tradeoffs in Data Augmentation: An Empirical Study </p><p>  Raphael Gontijo-Lopes, Sylvia Smullin, Ekin Dogus Cubuk, Ethan Dyer</p><p>  <strong>One-sentence Summary:</strong> We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡ç ”ç©¶äº†æ•°æ®æ‰©å……é—®é¢˜ï¼Œå³é€šè¿‡ä¿®æ”¹ç°æœ‰çš„æ•°æ®æ¥è·å¾—æ–°çš„è®­ç»ƒç¤ºä¾‹ã€‚æ•°æ®æ‰©å……åœ¨æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½ä¸­å¾ˆæµè¡Œï¼Œå› ä¸ºå®ƒå¢åŠ äº†è®­ç»ƒç¤ºä¾‹çš„æ•°é‡ã€‚ä½†æ˜¯ï¼Œå…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“åœ¨å®è·µä¸­ä»ç„¶æœªçŸ¥ã€‚å¢å¼ºè¿ç®—ç¬¦ï¼ˆä¾‹å¦‚å›¾åƒæ—‹è½¬ï¼‰å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„ï¼Œä¹Ÿå¯èƒ½æ˜¯æœ‰å®³çš„ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸¤ä¸ªæ–°çš„æŒ‡æ ‡ï¼Œç§°ä¸ºäº²å’ŒåŠ›å’Œå¤šæ ·æ€§ï¼Œä»¥é‡åŒ–ä»»ä½•ç»™å®šçš„æ‰©å¢ç®—å­çš„æ•ˆæœã€‚ä½œè€…å‘ç°ï¼Œå…·æœ‰é«˜äº²å’ŒåŠ›åˆ†æ•°å’Œé«˜å¤šæ ·æ€§åˆ†æ•°çš„è¿ç®—ç¬¦å¯å¸¦æ¥æœ€ä½³æ€§èƒ½æ”¹è¿›ã€‚</p><p>  é•¿å¤„</p><pre><code>  - å¼•å…¥çš„æªæ–½åŒæ—¶è€ƒè™‘äº†æ•°æ®å’Œæ¨¡å‹ï¼Œè¿™åœ¨ç°ä»£æ·±åº¦å­¦ä¹ ä¸­æ˜¯ä¸å¯åˆ†å‰²çš„ã€‚  - è¿™äº›æªæ–½å¯ä»¥ç›´è§‚åœ°è§£é‡Šä¸ºä»€ä¹ˆæŸäº›æ•°æ®å¢å¼ºæ–¹æ³•æœ‰æ•ˆè€Œå¦ä¸€äº›æ— æ•ˆçš„åŸå› ã€‚  - äº²å’ŒåŠ›å¾ˆå®¹æ˜“è®¡ç®—ã€‚ä¸ºäº†è·å¾—äº²å’ŒåŠ›ï¼Œéœ€è¦ä¸€ä¸ªåœ¨å¹²å‡€æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å…·æœ‰ç»™å®šæ‰©å……çš„éªŒè¯é›†ã€‚å¯ä»¥é‡ç”¨è®­ç»ƒåçš„æ¨¡å‹æ¥è¡¡é‡å…¶ä»–å¢å¼ºã€‚  - å®éªŒæ˜¯å¹¿æ³›çš„ã€‚</code></pre></li><li><p>Combining Ensembles and Data Augmentation Can Harm Your Calibration </p><p>  Yeming Wen, Ghassen Jerfel, Rafael Muller, Michael W Dusenberry, Jasper Snoek, Balaji Lakshminarayanan, Dustin Tran</p><p>  <strong>One-sentence Summary:</strong> We found that combining ensembles and data augmentation worsens calibration than applying them individually, and we proposed a simple fix to it.</p><p>  <strong>Reviewers say:</strong> è¿™é¡¹å·¥ä½œåˆ†æäº†æ•°æ®å¢å¼ºç­–ç•¥ï¼ˆä¾‹å¦‚MixUpï¼‰ä¸æ¨¡å‹é›†æˆä¹‹é—´åœ¨æ ¡å‡†æ€§èƒ½æ–¹é¢çš„äº¤äº’ä½œç”¨ã€‚ä½œè€…æŒ‡å‡ºï¼Œå°†å•ä¸ªæ¨¡å‹ç»„åˆåœ¨ä¸€èµ·æ—¶ï¼Œè¯¸å¦‚æ··åˆå’Œæ ‡ç­¾å¹³æ»‘ä¹‹ç±»çš„ç­–ç•¥å¦‚ä½•å‡å°‘å•ä¸ªæ¨¡å‹çš„è¿‡åˆ†è‡ªä¿¡ï¼Œä»è€Œå¯¼è‡´æ ¡å‡†æ€§èƒ½ä¸‹é™ã€‚å…·ä½“æ¥è¯´ï¼Œæ‰€æœ‰æŠ€æœ¯éƒ½æ˜¯å•ç‹¬é‡‡å–çš„ï¼Œé€šè¿‡å‡å°‘è¿‡åº¦è‡ªä¿¡æ¥æ”¹å–„æ ¡å‡†ã€‚ä½†æ˜¯ï¼Œç»“åˆèµ·æ¥ï¼Œå®ƒä»¬ä¼šå¯¼è‡´æ¨¡å‹ç½®ä¿¡åº¦ä¸è¶³ï¼Œå› æ­¤æ ¡å‡†æ•ˆæœä¼šæ›´å·®ã€‚åŸºäºæ­¤åˆ†æï¼Œä½œè€…æä¾›äº†ä¸€ç§ç®€å•çš„æŠ€æœ¯ï¼Œå¯åœ¨CIFAR-10ï¼ŒCIFAR-10-Cï¼ŒCIFAR-100å’ŒCIFAR-100-Cå’ŒImageNetä¸Šäº§ç”ŸSOTAæ ¡å‡†æ€§èƒ½ã€‚ä½œè€…å»ºè®®æ ¹æ®æ¨¡å‹åœ¨ç‰¹å®šç±»åˆ«ä¸Šæ˜¯å¦è¿‡åº¦è‡ªä¿¡æ¥åŠ¨æ€å¯ç”¨å’Œç¦ç”¨MixUpï¼Œ</p><p>  æˆ‘è®¤ä¸ºè¿™é¡¹å·¥ä½œæä¾›äº†æœ‰ç”¨çš„è§è§£ä»¥åŠç®€å•æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å¦å¤–ï¼Œå®ƒå†™å¾—å¾ˆæ¸…æ¥šï¼Œè¯»èµ·æ¥éå¸¸å®¹æ˜“å’Œæ„‰å¿«ã€‚</p></li><li><p>GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing </p><p>  Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, bailin wang, Yi Chern Tan, Xinyi Yang, Dragomir Radev, richard socher, Caiming Xiong</p><p>  <strong>One-sentence Summary:</strong> Language model pre-training for table semantic parsing.</p><p>  <strong>Reviewers say:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºè¯­ä¹‰è§£æçš„é¢„è®­ç»ƒæŠ€æœ¯ï¼Œé‡ç‚¹æ˜¯è¯­ä¹‰è§£æä»¥åŠä½¿å…¶å®é™…å·¥ä½œæ‰€éœ€çš„æŠ€æœ¯ç»†èŠ‚ã€‚æ€»ä½“è€Œè¨€ï¼Œæ‰€æœ‰å®¡é˜…è€…éƒ½è®¤ä¸ºç»“æœéå¸¸å¥½ï¼Œå¹¶ä¸”æ‚¨çœ‹åˆ°è·¨å¤šä¸ªæ–‡æœ¬åˆ°SQLæ•°æ®é›†çš„è‰¯å¥½æ”¹è¿›ã€‚å¯¹äºï¼ˆaï¼‰åˆ›å»ºç”¨äºç”Ÿæˆç»¼åˆæ•°æ®çš„SCFGçš„éš¾åº¦æå‡ºäº†ä¸€äº›ä¿ç•™ï¼Œä½†æ˜¯ä½œè€…ä¼¼ä¹å·²ç»é€‚å½“åœ°è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œå¹¶ä¸”éœ€è¦ä»˜å‡ºåˆç†çš„åŠªåŠ›ã€‚ï¼ˆbï¼‰é¢„è®­ç»ƒä»»åŠ¡æ˜¯å¦‚ä½•é’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼ˆæ–‡æœ¬åˆ°SQLï¼‰å’Œæ•°æ®é›†ï¼ˆèœ˜è››ï¼‰é‡èº«å®šåˆ¶çš„ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘å€¾å‘äºåŒæ„ä»¥ä¸‹äº‹å®ï¼šç”±äºè¯­æ³•æ˜¯ä»èœ˜è››è¡ç”Ÿè€Œæ¥çš„ï¼Œå› æ­¤å¯¹èœ˜è››çš„æ”¹è¿›å¹¶ä¸é‚£ä¹ˆå¼•äººæ³¨ç›®ï¼Œä½†æ˜¯ä½œè€…æ­£ç¡®åœ°å£°ç§°ï¼Œå³ä½¿æ”¶ç›Šç•¥å¾®å‡å°‘ï¼Œå…¶ä»–æ•°æ®é›†çš„æŒç»­æ”¹è¿›ä¹Ÿæ˜¯æ˜¾è€Œæ˜“è§çš„ã€‚å¯ä»¥å¸Œæœ›è¿™ä¸ªæƒ³æ³•ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–å¯ä»¥ç”Ÿæˆåˆæˆæ•°æ®çš„è®¾ç½®ï¼Œå¹¶ä¸”åº”è¯¥å°†åˆæˆæ•°æ®ä¸å®é™…æ•°æ®ç»“åˆèµ·æ¥çš„è¯¦ç»†ä¿¡æ¯å¾ˆæœ‰ç”¨ã€‚</p></li></ul><h2 id="About-distribution"><a href="#About-distribution" class="headerlink" title="About distribution"></a>About distribution</h2><ul><li><p>Free Lunch for Few-shot Learning: Distribution Calibration<br>  Shuo Yang, Lu Liu, Min Xu</p></li><li><p>Improved Autoregressive Modeling with Distribution Smoothing<br>  Chenlin Meng, Jiaming Song, Yang Song, Shengjia Zhao, Stefano Ermon</p></li><li><p>Long-tailed Recognition by Routing Diverse Distribution-Aware Experts<br>  Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, Stella Yu</p></li></ul><ul><li><p>Meta-Learning of Structured Task Distributions in Humans and Machines<br>  Sreejan Kumar, Ishita Dasgupta, Jonathan Cohen, Nathaniel Daw, Thomas Griffiths</p></li><li><p>Convex Potential Flows: Universal Probability Distributions with Optimal Transport and Convex Optimization<br>  Chin-Wei Huang, Ricky T. Q. Chen, Christos Tsirigotis, Aaron Courville</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in WWW 2021 (2021.04.19)</title>
      <link href="/uncategorized/paperlistfile/WWW2021/"/>
      <url>/uncategorized/paperlistfile/WWW2021/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www2021.thewebconf.org/program/papers/">paper list</a></p><span id="more"></span><h2 id="Anomaly-detection-Failure-detection"><a href="#Anomaly-detection-Failure-detection" class="headerlink" title="Anomaly detection / Failure detection"></a>Anomaly detection / Failure detection</h2><ul><li><p>Few-shot Network Anomaly Detection via Cross-network Meta-learning</p><p><strong>Authors:</strong> Kaize Ding (ASU), Qinghai Zhou (UIUC), Hanghang Tong (UIUC) and Huan Liu (ASU).</p><p><strong>Abstract:</strong> Network anomaly detection aims to find network elements (e.g., nodes, edges, subgraphs) with significantly different behaviors from the vast majority. It has a profound impact in a variety of applications ranging from finance, healthcare to social network analysis. Due to the unbearable labeling cost, existing methods are predominately developed in an unsupervised manner. Nonetheless, the anomalies they identify may turn out to be data noises or uninteresting data instances due to the lack of prior knowledge on the anomalies of interest. Hence, it is critical to investigate and develop few-shot learning for network anomaly detection. In real-world scenarios, few labeled anomalies are also easy to be accessed on similar networks from the same domain as the target network, while most of the existing works omit to leverage them and merely focus on a single network. Taking advantage of this potential, in this work, we tackle the problem of few-shot network anomaly detection by (1) proposing a new family of graph neural networks â€“ Graph Deviation Networks (GDN) that can leverage a small number of labeled anomalies for enforcing statistically significant deviations between abnormal and normal nodes on a network; (2) equipping the proposed GDN with a new cross- network meta-learning algorithm to realize few-shot network anomaly detection by transferring meta-knowledge from multiple auxiliary networks. Extensive experimental evaluations demonstrate the<br>efficacy of the proposed approach on few-shot or even one-shot network anomaly detection.</p></li><li><p>MSTREAM: Fast Anomaly Detection in Multi-Aspect Streams</p><p><strong>Authors:</strong> Siddharth Bhatia (National University of Singapore), Arjit Jain (IIT Bombay), Pan Li (Purdue University), Ritesh Kumar (IIT Kanpur) and Bryan Hooi (National University of Singapore).</p><p><strong>Abstract:</strong> Given a stream of entries in a multi-aspect data setting i.e., entries having multiple dimensions, how can we detect anomalous activities in an unsupervised manner? For example, in the intrusion detection setting, existing work seeks to detect anomalous events or edges in dynamic graph streams, but this does not allow us to take into account additional attributes of each entry. Our work aims to define a streaming multi-aspect data anomaly detection framework, termed MSTREAM which can detect unusual group anomalies as they occur, in a dynamic manner. MSTREAM has the following properties: (a) it detects anomalies in multi-aspect data including both categorical and numeric attributes; (b) it is online, thus processing each record in constant time and constant memory; (c) it can capture the correlation between multiple aspects of the data. MSTREAM is evaluated over the KDDCUP99, CICIDS-DoS, UNSW-NB 15 and CICIDS-DDoS datasets, and outperforms state-of-the-art baselines.</p></li><li><p>SDFVAE: Static and Dynamic Factorized VAE for Anomaly Detection of Multivariate CDN KPIs</p><p><strong>Authors:</strong> Liang Dai (Institute of Information Engineering, Chinese Academy of Sciences), Tao Lin (Communication University of China),<br>Chang Liu (Institute of Information Engineering, Chinese Academy of Science &amp; University of Chinese Academy of Sciences),<br>Bo Jiang (School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University),<br>Yanwei Liu (Institute of Information Engineering, Chinese Academy of Sciences), Zhen Xu (INSTITUTE OF INFORMATION ENGINEERING,CAS) and<br>Zhi-Li Zhang (University of Minnesota).</p><p><strong>Abstract:</strong> Content Delivery Networks (CDNs) are critical for providing good user experience of cloud services. CDN providers typically collect various multivariate Key Performance Indicators (KPIs) time series to monitor and diagnose system performance. State-of-the-art anomaly detection methods mostly use deep learning to extract the normal patterns of data, due to its superior performance. However, KPI data usually exhibit non-additive Gaussian noise, which makes it difficult for deep learning models to learn the normal patterns, resulting in degraded performance in anomaly detection. In this paper, we propose a robust and noise-resilient anomaly detection mechanism using multivariate KPIs. Our key insight is that different KPIs are constrained by certain time-invariant characteristics of the underlying system, and that explicitly modelling such invariance may help resist noise in the data. We thus propose a novel anomaly detection method called SDFVAE, short for Static and Dynamic Factorized VAE, that learns the representations of KPIs by explicitly factorizing the latent variables into dynamic and static parts. Extensive experiments using real-world data show that SDFVAE achieves a F1-score ranging from 0.92 to 0.99 on both regular and noisy dataset, outperforming state-of-the-art methods by a large margin.</p></li><li><p>NTAM: Neighborhood-Temporal Attention Model for Disk Failure Prediction in Cloud Platforms</p><p><strong>Authors:</strong> Chuan Luo (Microsoft Research), Pu Zhao (Microsoft Research), Bo Qiao (Microsoft Research), Youjiang Wu (Microsoft Azure),<br>Hongyu Zhang (The University of Newcastle), Wei Wu (Leibniz University Hannover), Weihai Lu (Microsoft Research), Yingnong Dang (Microsoft Azure),<br>Saravanakumar Rajmohan (Microsoft Office), Qingwei Lin (Microsoft Research) and Dongmei Zhang (Microsoft Research).</p><p><strong>Abstract:</strong> With the rapid deployment of cloud platforms, high service reliability is of critical importance. An industrial cloud platform contains a huge number of disks and disk failure is a common cause of service unreliability. In recent years, many machine learning based disk failure prediction approaches have been proposed, which can predict disk failures based on disk status data before the failures actually happen. In this way, proactive actions can be taken in advance to improve service reliability. However, existing approaches treat each disk individually and do not explore the influence of the neighboring disks. In this paper, we propose Neighborhood-Temporal Attention Model (NTAM), a novel deep learning based approach to disk failure prediction. When predicting whether or not a disk will fail in near future, NTAM is a novel approach that not only utilizes a diskâ€™s own status data, but also considers its neighborsâ€™ status data. Moreover, NTAM includes a novel attention-based temporal component to capture the temporal nature of the disk status data. Besides, we propose a data enhancement method, called Temporal Progressive Sampling (TPS), to handle the extreme data imbalance issue. We evaluate NTAM on a public dataset as well as two industrial datasets collected from around 9 million of disks in an industrial public cloud platform. Our experimental results show that NTAM significantly outperform state- of-the-art competitors. Also, our empirical evaluations indicate the effectiveness of the neighborhood-ware component and the temporal component underlying NTAM and the effectiveness of TPS. More encouragingly, we have successfully applied NTAM and TPS to Company Mâ€™s public cloud platform and obtained practical benefits in industrial practice.</p></li></ul><h2 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h2><ul><li><p>Network of Tensor Time Series</p><p><strong>Authors:</strong> Baoyu Jing (University of Illinois at Urbana-Champaign), Hanghang Tong (University of Illinois at Urbana-Champaign) and Yada Zhu (IBM Thomas J. Watson Research Center).</p><p><strong>Abstract:</strong> Co-evolving time series appears in a multitude of applications such as environmental monitoring, financial analysis, and smart transportation. This paper aims to address the following three challenges, including (C1) how to effectively model its multi-mode tensor structure at each time step; (C2) how to incorporate explicit relationship networks of the time series; (C3) how to model the implicit relationship of the temporal dynamics. We propose a novel model called Network of Tensor Time Series, which is comprised of two modules, including Tensor Graph Convolutional Network (TGCN) and Tensor Recurrent Neural Network (TRNN). TGCN tackles the first two challenges by generalizing Graph Convolutional Network (GCN) for flat graphs to tensor graphs, which captures the synergy between multiple graphs associated with the tensors. TRNN leverages tensor decomposition to balance the trade-off between the commonality and specificity of the co-evolving time series. The experimental results on five real-world datasets demonstrate the efficacy of the proposed method.</p></li><li><p>Radflow: A Recurrent, Aggregated, and Decomposable Model for Networks of Time Series</p><p><strong>Authors:</strong> Alasdair Tran (Australian National University), Alexander Mathews (Australian National University), Cheng Soon Ong (CSIRO) and Lexing Xie (Australian National University).</p><p><strong>Abstract:</strong> We propose a new model for networks of time series that influence each other. Graph structures among time series is found in diverse domains, such as web traffic influenced by hyperlinks, product sales influenced by recommendation, or urban transport volume influenced by the road network and weather. There has been recent progress in modeling graphs and time series, respectively, but an expressive and scalable approach for a network of series does not yet exist. We introduce Radflow, a novel model that embodies three main ideas: the recurrent structure of LSTM to obtain time- dependent node embeddings, aggregation of the flow of influence from other nodes with multi-head attention, and multi-layer decomposition of time series. Radflow naturally takes into account dynamic networks where nodes and edges appear over time, and it can be used for prediction and data imputation tasks. On four real-world datasets ranging from a few hundred to a few hundred thousand nodes, we observe Radflow variants being the best performing model across all tasks. We also report that the recurrent component in Radflow consistently outperforms N-BEATS, the state-of-the-art time series model. We show that Radflow can learn different trends and seasonal patterns, that it is robust to missing nodes and edges, and that correlated temporal patterns among network neighbors reflect influence strength. We curate WikiTraffic, the largest dynamic network of time series with 360K nodes and 22M time-dependent links spanning five yearsâ€”this dataset provides an open benchmark for developing models in this area, and prototyping applications for problems such as estimating web resources and optimizing collaborative infrastructures. More broadly, Radflow can be used to improve the forecasts in correlated time series networks such as the stock market, or impute missing measurements of natural phenomena such as geographically dispersed networks of waterbodies.</p></li></ul><h2 id="Micro-service-cloud-native"><a href="#Micro-service-cloud-native" class="headerlink" title="Micro-service / cloud native"></a>Micro-service / cloud native</h2><ul><li><p>MicroRank: End-to-End Latency Issue Localization with Extended Spectrum Analysis in Microservice Environments</p><p><strong>Authors:</strong> Guangba Yu (Sun Yat-Sen University), Pengfei Chen (Sun Yat-sen University), Hongyang Chen (Sun Yat-sen University), Zijie Guan (Tencent),<br>Zicheng Huang (Sun Yat-sen University), Linxiao Jing (Sun Yat-sen University), Tianjun Weng (Sun Yat-Sen University), Xinmeng Sun (Sun Yat-Sen University)<br>and Xiaoyun Li (Sun Yat-sen University).</p><p><strong>Abstract:</strong> With the advantages of strong scalability and fast delivery, microser-vice has become a popular software architecture in the modern ITindustry. Most of microservice faults manifest themselves in terms of service latency increase and impact user experience. The explosion in the number of service instances and complex dependencies among them make the application diagnosis extremely challenging.To help understand and troubleshoot a microservice system,the end-to-end tracing technology has been widely applied to capturethe execution path of each request. However, the tracing data are not fully leveraged by cloud and application providers when con-ducting latency issue localization in the microservice environment.This paper proposes a novel system ,named MicroRank, which analyzes clues provided by normal and abnormal traces to locateroot causes of latency issues. Once a latency issue is detected by the Anomaly Detector in MicroRank, the cause localization procedure is triggered. MicroRank first distinguishs which traces are abnormal. Then, MicroRankâ€™s PageRank Scorer module uses the abnormal and normal trace information as its input and differentials the importance of different traces to extended spectrum techniques . Finally, the spectrum techniques can calculate the ranking list based on the weighted spectrum information from PageRank Scorer to locate root causes more effectively. The experimental evaluationson a widely-used open-source system and a production system show that MicroRank achieves excellent results not only in one root cause situation but also in two issues that happen at the same time. Moreover,MicroRank makes 6% to 22% improvement in recall in localizing root causes compared to current state-of-the- art methods.</p></li></ul><h2 id="Augmentation"><a href="#Augmentation" class="headerlink" title="Augmentation"></a>Augmentation</h2><ul><li><p>Graph Contrastive Learning with Adaptive Augmentation</p><p><strong>Authors:</strong> Yanqiao Zhu (Institute of Automation, Chinese Academy of Sciences), Yichen Xu (Beijing University of Posts and Telecommunications),<br>Feng Yu (Alibaba), Qiang Liu (RealAI and Tsinghua University), Shu Wu (Institute of Automation, Chinese Academy of Sciences) and<br>Liang Wang (Institute of Automation, Chinese Academy of Sciences).</p><p><strong>Abstract:</strong> Recently, contrastive learning (CL) has emerged as a successful method for unsupervised graph representation learning. Most graph CL methods first perform stochastic augmentation on the input graph to obtain two graph views and maximize the agreement of representations in the two views. Despite the prosperous development of graph CL methods, the design of graph augmentation schemesâ€”a crucial component in CLâ€”remains rarely explored. We argue that the data augmentation schemes should preserve intrinsic structural and attribute information of graphs, which will force the model to learn representations that are insensitive to perturbation on unimportant nodes and edges. However, most existing methods adopt uniform data augmentation schemes, like uniformly dropping edges and uniformly shuffling features, leading to suboptimal performance. In this paper, we propose a novel graph contrastive representation learning method with adaptive augmentation that incorporates various priors for topological and semantic aspects of the graph. Specifically, on the topology level, we design augmentation schemes based on node centrality measures to highlight important connective structures. On the node attribute level, we corrupt node features by adding more noise to unimportant node features, to enforce the model to recognize underlying semantic information. We perform extensive experiments of node classification on a variety of real-world datasets. Experimental results demonstrate that our proposed method consistently outperforms existing state-of-the-art methods and even surpasses some supervised counterparts, which validates the effectiveness of the proposed contrastive framework with adaptive augmentation.</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>å¾®æœåŠ¡æ¡†æ¶ä¸‹çš„å¤šæºå¼‚æ„æ•°æ®å¼‚å¸¸æ£€æµ‹ - è°ƒç ”</title>
      <link href="/uncategorized/surveys/aiops_micro-service/"/>
      <url>/uncategorized/surveys/aiops_micro-service/</url>
      
        <content type="html"><![CDATA[<p>2021-04-13: æ•´ç†äº†å¾®æœåŠ¡æ¡†æ¶ä¸‹ï¼Œåšå¼‚å¸¸æ£€æµ‹çš„å¤§ä½“æ€è·¯ï¼Œåˆ†åˆ«è°ƒç ”â€œå¤šæºå¼‚æ„æ•°æ®èåˆâ€ï¼Œä»¥åŠåˆ†åˆ«åŸºäºâ€œæ—¶é—´åºåˆ—â€â€œæ—¥å¿—â€â€œè°ƒç”¨é“¾â€æ•°æ®çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚</p><p>2022-04-18: è¡¥å……äº†â€œå¼‚å¸¸æ ¹å› åˆ†æâ€ç›¸å…³çš„å·¥ä½œï¼Œè¿™äº›å·¥ä½œå¤§å¤šæ˜¯åŸºäºå›¾çš„ï¼Œå¯ä»¥ä¸º4G LTEç§çš„æ ¹å› åˆ†ææä¾›æ€è·¯ã€‚</p><span id="more"></span><p><a href="https://netman.aiops.org/publications/">è£´ä¸¹å›¢é˜Ÿçš„ä¸»é¡µ</a></p><p><a href="https://aiopsworkshop.github.io/accepted_papers/index.html">2020 AIOPS workshop</a></p><p><a href="https://github.com/FudanSELab/train-ticket/">Train Ticket â€“ Test Bed</a></p><p><a href="https://microservices-demo.github.io/">Sock shop â€“ Test bed</a></p><h2 id="ä¸€äº›ç¬”è®°"><a href="#ä¸€äº›ç¬”è®°" class="headerlink" title="ä¸€äº›ç¬”è®°"></a>ä¸€äº›ç¬”è®°</h2><p>åœ¨ç›¸å…³å·¥ä½œ<a href="#lundetecting2021">[1]</a>ä¸­ï¼š</p><ol><li><p>æ‰€è¦æ£€æµ‹çš„å¼‚å¸¸å¦‚ä½•å®šä¹‰ï¼Œæ˜¯ä¸€ä¸ªå…³é”®çš„é—®é¢˜ã€‚æœ¬æ–‡æåˆ°â€error messagesâ€, â€œperformance degradationsâ€ä¸â€trace structure and response time anomaliesâ€æœ¬èº«å°±æ˜¯å‡ ç§ä¸åŒçš„å¼‚å¸¸ç±»å‹ï¼Œèåˆå¤šç§æ•°æ®å°†æœ‰åŠ©äºæˆ‘ä»¬æ¢æµ‹æ›´å¤šç§ç±»çš„å¼‚å¸¸ã€‚</p></li><li><p>æœ¬æ–‡æ‰€å»ºç«‹çš„è°ƒç”¨æ ‘ä¸­åŒ…å«äº†ä¸¤ç§è°ƒç”¨å…³ç³»â€”â€”æœ¬åœ°è°ƒç”¨ï¼ˆä¸¤ä¸ªå¾®æœåŠ¡ä½äºåŒä¸€ä¸ªä¸»æœºï¼Œå½¼æ­¤ä¹‹é—´è¿›è¡Œè°ƒç”¨ï¼‰ã€è¿œç¨‹è°ƒç”¨ï¼ˆä¸¤ä¸ªå¾®æœåŠ¡ä¸åœ¨åŒä¸€ä¸ªä¸»æœºï¼Œè¿œç¨‹è°ƒç”¨ï¼‰ï¼Œä½†å¥½åƒåœ¨åˆ†æçš„æ—¶å€™æ²¡ä»€ä¹ˆåŒºåˆ«ã€‚</p></li></ol><p>åœ¨ç›¸å…³å·¥ä½œ<a href="#multimodalsasho2019">[2]</a>ä¸­ï¼š</p><ol><li><p>traceæ•°æ®å…·æœ‰å¤šæ¨¡æ€ï¼Œå³meta actionä¹‹é—´çš„ç»“æ„ï¼ˆå› æœå…³ç³»æ¨¡æ€ï¼Œæˆ–sequential natureæ¨¡æ€ï¼‰ã€æœåŠ¡å“åº”æ—¶é—´ï¼ˆå®å€¼åºåˆ—ï¼‰ã€‚</p></li><li><p>traceæ•°æ®çš„ä¸€ä¸ªæœ‰è¶£çš„ç‰¹æ€§åœ¨äºï¼Œå®ƒæ˜¯ä¸€ç§èƒ½å¤Ÿè‰¯å¥½çš„åæ˜ æœåŠ¡å±‚çŠ¶æ€çš„æ•°æ®ï¼ŒåŒæ—¶è¿˜åŒ…å«æœ‰å¤§é‡çš„åº•å±‚ä¿¡æ¯ã€‚</p></li><li><p>treeç»“æ„çš„è°ƒç”¨åœ¨å®é™…è°ƒç”¨çš„æ—¶å€™ï¼Œå­˜åœ¨é«˜å¹¶å‘çš„ç‰¹æ€§ï¼Œå³ï¼šä¸¤ä¸ªè¢«åŒä¸€ç»„ä»¶åŒæ—¶è°ƒç”¨çš„æœåŠ¡ï¼Œåœ¨å“åº”çš„æ—¶å€™å¯èƒ½å‡ºç°å…ˆåçš„å·®å¼‚ï¼Œè¿™åœ¨ä½¿ç”¨åºåˆ—ä½œä¸ºtraceè¡¨å¾æ–¹å¼ä¸­æ˜¯éå¸¸å¸¸è§çš„ï¼Œä½†è¿™ç§å·®å¼‚ä¸ä»£è¡¨å¼‚å¸¸ã€‚</p></li></ol><h2 id="ç›¸å…³è®ºæ–‡"><a href="#ç›¸å…³è®ºæ–‡" class="headerlink" title="ç›¸å…³è®ºæ–‡"></a>ç›¸å…³è®ºæ–‡</h2><h4 id="æ ¹å› åˆ†æ"><a href="#æ ¹å› åˆ†æ" class="headerlink" title="æ ¹å› åˆ†æ"></a>æ ¹å› åˆ†æ</h4><h5 id="åŸºäºå›¾çš„æ–¹æ³•"><a href="#åŸºäºå›¾çš„æ–¹æ³•" class="headerlink" title="åŸºäºå›¾çš„æ–¹æ³•"></a>åŸºäºå›¾çš„æ–¹æ³•</h5><ol><li><p><strong>Graph-based root cause analysis for service-oriented and microservice architectures</strong></p><p> ÃlvaroBrandÃ³, Marc SolÃ©, Alberto HuÃ©lamo, David Solans, MarÃ­a S.PÃ©rez, VictorMuntÃ©s-Mulero</p><p> Journal of Systems and Software, Volume 159, January 2020, 110432</p></li><li><p><strong>GRANO: interactive graph-based root cause analysis for cloud-native distributed data platform</strong></p><p> Hanzhang Wang, Phuong Nguyen, Jun Li, Selcuk Kopru, Gene Zhang, Sanjeev Katariya, Sami Ben-Romdhane</p><p> Proceedings of the VLDB EndowmentVolume 12Issue 12August 2019</p></li></ol><p>æˆ‘ä»¬é€šè¿‡æä¾›ç³»ç»Ÿç»„ä»¶æ‹“æ‰‘ã€è­¦æŠ¥å’Œåº”ç”¨ç¨‹åºäº‹ä»¶çš„æ•´ä½“è§†å›¾ï¼Œå±•ç¤ºäº† Granoï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºäº‘åŸç”Ÿåˆ†å¸ƒå¼æ•°æ®å¹³å°çš„ç«¯åˆ°ç«¯å¼‚å¸¸æ£€æµ‹å’Œæ ¹æœ¬åŸå› åˆ†æï¼ˆæˆ–ç®€ç§° RCAï¼‰ç³»ç»Ÿã€‚Grano æä¾›ï¼š ä¸€ä¸ª<em>æ£€æµ‹å±‚</em>ï¼Œç”¨äºå¤„ç†å¤§é‡æ—¶é—´åºåˆ—ç›‘æ§æ•°æ®ï¼Œä»¥æ£€æµ‹é€»è¾‘å’Œç‰©ç†ç³»ç»Ÿç»„ä»¶çš„å¼‚å¸¸æƒ…å†µï¼›<em>Anomaly Graph Layer</em>å…·æœ‰æ–°é¢–çš„å›¾å½¢å»ºæ¨¡å’Œç®—æ³•ï¼Œç”¨äºåˆ©ç”¨ç³»ç»Ÿæ‹“æ‰‘æ•°æ®å’Œæ£€æµ‹ç»“æœæ¥è¯†åˆ«ç³»ç»Ÿç»„ä»¶çº§åˆ«çš„æ ¹æœ¬åŸå› ç›¸å…³æ€§ï¼›å’Œ<em>åº”ç”¨å±‚</em>è‡ªåŠ¨é€šçŸ¥å¾…å‘½äººå‘˜ï¼Œå¹¶é€šè¿‡äº¤äº’å¼å›¾å½¢ç•Œé¢æä¾›å®æ—¶å’ŒæŒ‰éœ€ RCA æ”¯æŒã€‚è¯¥ç³»ç»Ÿä½¿ç”¨ eBay çš„ç”Ÿäº§æ•°æ®è¿›è¡Œéƒ¨ç½²å’Œè¯„ä¼°ï¼Œä»¥å¸®åŠ©å€¼ç­äººå‘˜å°†æ ¹æœ¬åŸå› çš„è¯†åˆ«æ—¶é—´ä»å‡ å°æ—¶ç¼©çŸ­åˆ°å‡ åˆ†é’Ÿã€‚</p><ol start="3"><li><p><strong>A Causality Mining and Knowledge Graph Based Method of Root Cause Diagnosis for Performance Anomaly in Cloud Application</strong>s</p><p> Juan Qiu, Qingfeng Du, Kanglin Yin, Shuang-Li Zhang, and Chongshu Qian</p><p> Applied Sciences, Volume 10, Issue 6</p></li></ol><p>éšç€äº‘è®¡ç®—æŠ€æœ¯çš„å‘å±•ï¼Œå¾®æœåŠ¡æ¶æ„ï¼ˆMSAï¼‰å·²ç»æˆä¸ºäº‘åŸç”Ÿåº”ç”¨ä¸­æµè¡Œçš„åº”ç”¨æ¶æ„ã€‚å¾ˆå¤šé¢å‘ç”¨æˆ·çš„æœåŠ¡ç”±å¾ˆå¤šå¾®æœåŠ¡æ”¯æŒï¼ŒæœåŠ¡ä¹‹é—´çš„ä¾èµ–æ¯”ä¼ ç»Ÿçš„å•ä½“æ¶æ„åº”ç”¨æ›´åŠ å¤æ‚ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœä¸€ä¸ªå¾®æœåŠ¡çš„æ€§èƒ½æŒ‡æ ‡å‘ç”Ÿå¼‚å¸¸å˜åŒ–ï¼Œå°±ä¼šå¯¼è‡´å…¶ä»–ç›¸å…³æœåŠ¡é™çº§ç”šè‡³å‡ºç°æ•…éšœï¼Œè¿™å¯èƒ½ä¼šç»™ä¾èµ–çš„ä¸šåŠ¡å¸¦æ¥å¾ˆå¤§çš„æŸå¤±ã€‚å› æ­¤ï¼Œåœ¨äº‘åº”ç”¨çš„è¿ç»´å·¥ä½œä¸­ï¼ŒæŒ–æ˜é—®é¢˜çš„å› æœå…³ç³»ï¼Œå°½å¿«æ‰¾åˆ°æ ¹æºè‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æŒ–æ˜å› æœå…³ç³»å’Œè¯Šæ–­æ ¹æœ¬åŸå› çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨çŸ¥è¯†å›¾è°±æŠ€æœ¯å’Œå› æœæœç´¢ç®—æ³•ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªç»å…¸çš„äº‘åŸç”Ÿåº”ç”¨ä¸ŠéªŒè¯äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œå‘ç°è¯¥æ–¹æ³•æ˜¯æœ‰æ•ˆçš„ã€‚å°†æˆ‘ä»¬çš„æ–¹æ³•åº”ç”¨äºäº‘åŸç”Ÿåº”ç”¨ç¨‹åºçš„å¤§éƒ¨åˆ†æœåŠ¡åï¼Œå‡†ç¡®ç‡å’Œå¬å›ç‡å‡è¶…è¿‡ 80%ã€‚</p><ol start="4"><li><p><strong>Groot: An Event-graph-based Approach for Root Cause Analysis in Industrial Settings</strong></p><p> Hanzhang Wang; Zhengkai Wu; Huai Jiang; Yichao Huang; Jiamu Wang; Selcuk Kopru; Tao Xie</p><p> 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)</p></li></ol><p>å¯¹äºå¤§å‹åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œæœ‰æ•ˆè¯Šæ–­äº‹ä»¶çš„æ ¹æœ¬åŸå› ä»¥ä¿æŒç³»ç»Ÿçš„é«˜å¯ç”¨æ€§è‡³å…³é‡è¦ã€‚å¾®æœåŠ¡æ¶æ„çš„æœ€æ–°å‘å±•ä¸ºå·¥ä¸šç¯å¢ƒä¸­çš„æ ¹æœ¬åŸå› åˆ†æ (RCA) å¸¦æ¥äº†ä¸‰å¤§æŒ‘æˆ˜ï¼ˆå³æ“ä½œçš„å¤æ‚æ€§ã€ç³»ç»Ÿè§„æ¨¡å’Œç›‘æ§ï¼‰ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† Grootï¼Œä¸€ç§åŸºäºäº‹ä»¶å›¾çš„ RCA æ–¹æ³•ã€‚ Groot åŸºäºäº‹ä»¶æ„å»ºå®æ—¶å› æœå…³ç³»å›¾ï¼Œè¿™äº›äº‹ä»¶æ±‡æ€»äº†è¢«åˆ†æç³»ç»Ÿä¸­çš„å„ç§ç±»å‹çš„æŒ‡æ ‡ã€æ—¥å¿—å’Œæ´»åŠ¨ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ•´åˆæ¥è‡ªç«™ç‚¹å¯é æ€§å·¥ç¨‹ (SRE) å·¥ç¨‹å¸ˆçš„é¢†åŸŸçŸ¥è¯†ï¼ŒGroot å¯ä»¥ä½¿ç”¨ç”¨æˆ·å®šä¹‰çš„äº‹ä»¶å’Œç‰¹å®šé¢†åŸŸçš„è§„åˆ™è¿›è¡Œå®šåˆ¶ã€‚ç›®å‰ï¼ŒGroot åœ¨ 5,000 é¡¹å®é™…ç”Ÿäº§æœåŠ¡ä¸­æ”¯æŒ RCAï¼Œå¹¶è¢« eBay çš„ SRE å›¢é˜Ÿç§¯æä½¿ç”¨ï¼ŒeBay æ˜¯ä¸€ä¸ªå…¨çƒç”µå­å•†åŠ¡ç³»ç»Ÿï¼Œæ¯å¹´ä¸ºè¶…è¿‡ 1.59 äº¿æ´»è·ƒä¹°å®¶æä¾›æœåŠ¡ã€‚åœ¨ 15 ä¸ªæœˆå†…ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å« 952 ä¸ªå®é™…ç”Ÿäº§äº‹ä»¶çš„æ ‡è®°æ ¹æœ¬åŸå› ä»¥è¿›è¡Œè¯„ä¼°ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒGroot èƒ½å¤Ÿè¾¾åˆ° 95% çš„ top-3 å‡†ç¡®ç‡å’Œ 78% çš„ top-1 å‡†ç¡®ç‡ã€‚ä¸ºäº†åˆ†äº«æˆ‘ä»¬åœ¨å·¥ä¸šç¯å¢ƒä¸­éƒ¨ç½²å’Œé‡‡ç”¨ RCA çš„ç»éªŒï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹è°ƒæŸ¥ä»¥è¡¨æ˜ Groot çš„ç”¨æˆ·å‘ç°å®ƒæœ‰ç”¨ä¸”æ˜“äºä½¿ç”¨ã€‚æˆ‘ä»¬è¿˜åˆ†äº«äº†éƒ¨ç½²å’Œé‡‡ç”¨ Groot è§£å†³ç”Ÿäº§ç¯å¢ƒä¸­çš„ RCA é—®é¢˜çš„ç»éªŒæ•™è®­ã€‚</p><ol start="5"><li><p><strong>Graph Based Root Cause Analysis in Cloud Data Center</strong></p><p> Divyaansh Dandona; Mevlut Demir; John J. Prevost</p><p> 2020 IEEE 15th International Conference of System of Systems Engineering (SoSE)</p></li></ol><p>ä½æˆæœ¬è®¡ç®—çš„å¸å¼•åŠ›å’Œäº‘æŠ€æœ¯çš„æŒ‰éœ€æ‰©å±•å·²å¯¼è‡´è®¸å¤šè½¯ä»¶åº”ç”¨ç¨‹åºè¿ç§»åˆ°äº‘ã€‚è¿™ç§å¯¹äº‘çš„ä¾èµ–å¢åŠ è½¬åŒ–ä¸ºå¯¹äº‘æ•°æ®ä¸­å¿ƒçš„ç›´æ¥ä¾èµ–ï¼Œè¿™äº›äº‘æ•°æ®ä¸­å¿ƒå½¢æˆäº†ç°ä»£äº‘ã€‚è¿™äº›æ•°æ®ä¸­å¿ƒæ˜¯ç”±è®¸å¤šç³»ç»Ÿç»„æˆçš„å¤æ‚å»ºç­‘ç‰©ï¼Œè¿™äº›ç³»ç»Ÿç›¸äº’äº¤äº’ä»¥æ‰˜ç®¡æœ€ç»ˆåº”ç”¨ç¨‹åºã€‚åœ¨è¿™ä¸ªç³»ç»Ÿç³»ç»Ÿä¸­æ£€æµ‹å¼‚å¸¸äº‹ä»¶ï¼Œç„¶ååŠæ—¶ç¡®å®šå…¶æ ¹æœ¬åŸå› æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå›¾å½¢æ¨¡å‹æ¥å°è£…ç³»ç»Ÿçš„äº‘æ•°æ®ä¸­å¿ƒç³»ç»Ÿï¼Œå¹¶åˆ†äº«ä¸€ç§å‡å°‘æ ¹æœ¬åŸå› åˆ†æçš„æœç´¢ç©ºé—´çš„æ–¹æ³•ã€‚</p><h3 id="å…³äºå¤šæºæ•°æ®èåˆçš„æ–‡ç« "><a href="#å…³äºå¤šæºæ•°æ®èåˆçš„æ–‡ç« " class="headerlink" title="å…³äºå¤šæºæ•°æ®èåˆçš„æ–‡ç« "></a>å…³äºå¤šæºæ•°æ®èåˆçš„æ–‡ç« </h3><ol><li><strong>Multi-Source Anomaly Detection in Distributed IT Systems</strong></li></ol><p>Jasmin Bogatinovski, Sasho Nedelkoski</p><p>çŸ¥ä¹è§£æï¼š<a href="https://zhuanlan.zhihu.com/p/347051870">link</a></p><p>2020 AIOPS workshop</p><ul><li>æœ¬å·¥ä½œè”åˆè€ƒè™‘äº†æ—¥å¿—æ•°æ®ä¸traceæ•°æ®ï¼Œå¼€å‘äº†ä¸€ç§span2vecçš„æ–¹æ³•ï¼Œç”¨äºå°†traceæ•°æ®åƒlogæ•°æ®ä¸€æ ·è¡¨ç¤ºä¸ºä¸€ç³»åˆ—çš„æ¨¡æ¿æ•°æ®ï¼Œè¿›ä¸€æ­¥çš„ä¾¿äºä¸æ—¥å¿—æ•°æ®è¿›è¡Œèåˆã€‚</li><li>å¼‚å¸¸æ£€æµ‹ä»»åŠ¡åœ¨æœ¬æ–‡ä¸­è¢«è½¬åŒ–ä¸ºäº†ä¸€ä¸ªâ€œä¸‹ä¸€æ­¥æ¨¡æ¿é¢„æµ‹â€çš„<strong>æœ‰ç›‘ç£ä»»åŠ¡</strong>ï¼Œå¯ä»¥åˆ†åˆ«å¯¹ä¸‹ä¸€æ­¥å¯èƒ½å‡ºç°çš„æ—¥å¿—ä¸æ¨¡æ¿è¿›è¡Œé¢„æµ‹ï¼Œåˆ™åç¦»é¢„æµ‹çš„trace/logæ¨¡æ¿å³ä¸ºå¼‚å¸¸ã€‚</li><li>åœ¨æœ¬å·¥ä½œä¸­ï¼Œlogä¸traceæ•°æ®æä¾›äº†ä¸åŒè§’åº¦çš„ç³»ç»ŸçŠ¶æ€ä¿¡æ¯â€”â€”æ—¥å¿—æ•°æ®å¯ä»¥åœ¨æœåŠ¡çº§åˆ«ä¸Šæœ‰æ›´ä¸°å¯Œçš„æè¿°ï¼Œå¯ä»¥è¢«è§†ä¸ºæœåŠ¡è¿è¡Œçš„çš„æŒ‡çº¹ã€‚Traceæ•°æ®ä¸­åˆ™æ²¡æœ‰å¤ªå¤šä¸Šè¿°ç³»ç»Ÿçº§çš„ä¿¡æ¯ï¼Œä½†åŒ…å«æ‰§è¡Œä¸€æ¬¡ç”¨æˆ·æ¸…è¯·æ±‚çš„æ€»æµç¨‹å›¾ã€‚ä½†æ˜¯ä»ç»“æœä¸Šæ¥çœ‹ï¼Œæ—¥å¿—æ•°æ®çš„åŠ å…¥ï¼Œæå‡äº†é¢„æµ‹traceå¼‚å¸¸çš„recallï¼Œä½†åä¹‹ï¼Œtraceæ•°æ®çš„çš„åŠ å…¥å¹¶æ²¡æœ‰æ˜¾è‘—æå‡logæ•°æ®çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ã€‚å¯èƒ½çš„åŸå› åœ¨äºlogçš„æ•°æ®ç²’åº¦æ¯”è¾ƒå¤§ã€‚</li></ul><blockquote><p>One explanation of this behaviour is that the granularity of the information from the logs is truncated on the level of the data source with a lower frequency of generation â€“ the trace is harder for the information in the trace to be transferred to the logs. The information that the multimodal method is receiving from the logs when it is aiming to predict the next relevant span complements the information as obtained just from the sequence of spans individually.</p></blockquote><p>â€‹<br>2. <strong>Multi-source Distributed System Data for AI-Powered Analytics</strong></p><pre><code>Sasho Nedelkoski, Jasmin Bogatinovski, Ajay Kumar Mandapati, Soeren Becker, Jorge Cardoso, Odej KaoEuropean Conference on Service-Oriented and Cloud Computing, ESOCC 2020: Service-Oriented and Cloud Computingå¼€å‘äº†ä¸€ä¸ªç”¨äºæ•æ‰å¤šæºï¼ˆä¸‰ç§ï¼‰æ•°æ®çš„ä¸€ä¸ªåˆ†æå¹³å°ï¼Œå¯¹è¿™ä¸ªtest bedåšäº†æè¿°</code></pre><ol start="3"><li><p><strong>An Intelligent Anomaly Detection Scheme for Micro-Services Architectures With Temporal and Spatial Data Analysis</strong></p><p> Yuan Zuo; Yulei Wu; Geyong Min; Chengqiang Huang; Ke Pei</p><p> IEEE Transactions on Cognitive Communications and Networking ( Volume: 6, Issue: 2, June 2020)</p><p> è”åˆä½¿ç”¨æ—¥å¿—ï¼ˆæ—¶é—´æ•°æ®ï¼‰å’Œè°ƒç”¨é“¾æ•°æ®query traceï¼ˆç©ºé—´æ•°æ®ï¼‰åšç‰¹å¾æå–ä¸èåˆï¼Œå¹¶æ„å»ºone-class classifier</p></li></ol><h3 id="å¾®æœåŠ¡æ¶æ„ä¸‹çš„å¼‚å¸¸æ£€æµ‹"><a href="#å¾®æœåŠ¡æ¶æ„ä¸‹çš„å¼‚å¸¸æ£€æµ‹" class="headerlink" title="å¾®æœåŠ¡æ¶æ„ä¸‹çš„å¼‚å¸¸æ£€æµ‹"></a>å¾®æœåŠ¡æ¶æ„ä¸‹çš„å¼‚å¸¸æ£€æµ‹</h3><h4 id="åŸºäºè°ƒç”¨é“¾æ•°æ®"><a href="#åŸºäºè°ƒç”¨é“¾æ•°æ®" class="headerlink" title="åŸºäºè°ƒç”¨é“¾æ•°æ®"></a>åŸºäºè°ƒç”¨é“¾æ•°æ®</h4><ol><li><p><strong>Observing and Controlling Performance in Microservices</strong></p><p> å­¦æœ¯æ¯•ä¸šè®ºæ–‡ï¼šAndrÃ© Pascoal Bento</p><p> è®²ä½¿ç”¨traceæ•°æ®ï¼Œå»ºæ¨¡ç³»ç»Ÿä¸­å¾®æœåŠ¡ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶å»ºç«‹ä¸ºå›¾æ¨¡å‹ï¼Œç„¶åè®¡ç®—å‡ºæ¯ä¸ªä¾èµ–å…³ç³»ä¹‹é—´çš„åº”ç­”å“åº”æ—¶é—´</p></li><li><p><strong>Unsupervised Detection of Microservice Trace Anomalies through Service-Level Deep Bayesian Networks</strong></p><p> Ping Liu; Haowen Xu; Qianyu Ouyang; Rui Jiao; Zhekang Chen; Shenglin Zhang; â€¦</p><p> 2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE)</p><p> <strong>åŸºäºå¾®æœåŠ¡traceæ•°æ®ï¼Œæ£€æµ‹æ„å¤–çš„è°ƒç”¨å…³ç³»æˆ–è€…æ„å¤–çš„è°ƒç”¨å“åº”æ—¶é—´</strong></p></li></ol><ul><li><p>æœ¬æ–‡ä¸å…¶ä»–å·¥ä½œæœ‰æ˜¾è‘—çš„ä¸åŒï¼Œåˆ«çš„å·¥ä½œçš„å¼‚å¸¸æ£€æµ‹å¯¹è±¡ä¸€èˆ¬éƒ½æ˜¯é’ˆå¯¹å•ä¸ªçš„è°ƒç”¨meta actionï¼Œè€Œæœ¬å·¥ä½œæ˜¯é’ˆå¯¹ä¸€æ•´æ¡treeçŠ¶çš„traceã€‚</p></li><li><p>æœ¬æ–‡ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„æ‰‹å·¥ç‰¹å¾æ¥è¡¨å¾traceæ•°æ®ï¼Œå‘é‡åŒ–çš„traceæ•°æ®ç”¨ä¸€ä¸ªå®¹é‡å¾ˆå¤§çš„æ¦‚ç‡æ¨¡å‹æ¥å­¦ä¹ ä»–ä»¬çš„æ­£å¸¸æ¨¡å¼ã€‚å…¶ä¸­ä¹‹æ‰€ä»¥è¦ç”¨åˆ°å®¹é‡å¤§çš„æ¨¡å‹ï¼Œæ˜¯å› ä¸ºè¦å­¦ä¹ çš„æ¨¡å¼æ˜¯ä¸€æ•´ä¸ªapplicationçš„traceï¼Œè€Œéä¸€ä¸ªtraceã€‚</p></li><li><p>æ‰‹å·¥è®¾è®¡çš„traceæ•°æ®ä½¿å¾—å¼‚å¸¸æ£€æµ‹åçš„ç»“æœå¯ä»¥å¿«é€Ÿå®šä½å¼‚å¸¸ç±»å‹ä¸å¼‚å¸¸æ ¹å› ï¼Œè‡ªå¸¦å¯è§£é‡Šæ€§ã€‚</p></li></ul><ol start="3"><li> <a name="multimodalsasho2019"><sup>[2]</sup></a> <strong>Anomaly Detection from System Tracing Data Using Multimodal Deep Learning</strong></li></ol><p>Sasho Nedelkoski; Jorge Cardoso; Odej Kao</p><p>2019 IEEE 12th International Conference on Cloud Computing (CLOUD)</p><p>æœ¬æ–‡å°†traceæ•°æ®è§†ä¸ºäº†<strong>å¤šæ¨¡æ€æ•°æ®</strong>ï¼Œç¬¬ä¸€ç§æ¨¡æ€ä¸ºç”±äº‹ä»¶åºåˆ—ç»„æˆçš„ç±»ä¼¼äºNLPçš„åºåˆ—æ•°æ®ï¼ˆå…¶å®ä¸æ—¥å¿—æ¨¡æ¿åºåˆ—æ˜¯ç›¸ä¼¼çš„ï¼‰ï¼Œç¬¬äºŒç§æ¨¡æ€æ˜¯æ¯ä¸ªäº‹ä»¶å¯¹åº”çš„å“åº”æ—¶é—´ã€‚</p><ul><li><p>ç¬¬ä¸€æ­¥å°†traceæ•°æ®å¼‚å¸¸æ£€æµ‹è§†ä¸ºäº†å•æ¨¡æ€æ•°æ®çš„å¼‚å¸¸æ£€æµ‹ã€‚å…¶æœ¬è´¨æ˜¯ï¼Œå°†ä¸Šè¿°ä¸¤ç§â€œæ–‡æœ¬åºåˆ—â€â€œå®å€¼åºåˆ—â€é€è¿›LSTMç½‘ç»œä¸­ï¼Œè¿›è¡Œé¢„æµ‹ï¼Œå½“çœŸå®å€¼ä¸åœ¨TopK labelï¼ˆæ–‡æœ¬åºåˆ—é¢„æµ‹ï¼‰æˆ–ä¸åœ¨95%ç½®ä¿¡åŒºé—´ï¼ˆå®å€¼åºåˆ—é¢„æµ‹ï¼‰æ—¶ï¼Œå³ä¸ºå¼‚å¸¸ã€‚</p></li><li><p>æœ¬æ–‡æåˆ°çš„å¤šæ¨¡æ€èåˆï¼Œå…¶å…·ä½“æ–¹æ¡ˆæ˜¯ï¼Œä¸¤ç§æ•°æ®å‡é€šè¿‡å•å±‚çš„LSTMç½‘ç»œï¼Œç„¶ååœ¨ç¬¬äºŒä¸ªéšå±‚ä¸­è¿›è¡Œèåˆï¼ˆconcatï¼‰ã€‚</p></li></ul><p>æœ¬æ–‡çš„æœ€å¤§ç¼ºé™·åœ¨äºï¼š</p><ul><li>è™½ç„¶ç†è®ºä¸Ševenté—´çš„è°ƒç”¨å› æœå…³ç³»è¢«è§†ä¸ºäº†æ–‡æœ¬åºåˆ—ä¸­çš„è¯­æ³•ï¼Œäº¤ç”±LSTMå­¦ä¹ ï¼Œä½†æ˜¯è¿™ç§æ‹“æ‰‘å…³ç³»æœ¬èº«æ˜¯å·²çŸ¥çš„ï¼Œè€Œæ²¡æœ‰è¢«åˆ©ç”¨ï¼ŒLSTMå­¦ä¹ åˆ°çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Œä¹Ÿæ˜¯ä¸å¯è§£é‡Šçš„</li><li>æœ¬æ–‡å®éªŒä¸­çš„å¼‚å¸¸å…¨éƒ¨éƒ½æ˜¯äººå·¥ç”Ÿæˆçš„ï¼Œè€Œä¸”æœ‰å¯¹ç…§ç½‘ç»œè¾“å‡ºæ„é€ å¼‚å¸¸æ ·æœ¬çš„å«Œç–‘ï¼Œå®éªŒç»“æœå­˜ç–‘ã€‚</li><li>æœ¬æ–‡åœ¨è¿˜æåˆ°ï¼Œæœ¬æ–‡é€šè¿‡æ‰€å­¦ä¹ çš„é¢„æµ‹æ¨¡å‹ï¼Œå¯¹è°ƒç”¨é“¾ä¸­çš„å¹¶å‘ä¸ä¾èµ–äº‹ä»¶è¿›è¡Œäº†é‡å»ºä¸è¯†åˆ«ï¼Œä½†æ˜¯è¯†åˆ«è¿™äº›äº‹ä»¶å¯¹å¼‚å¸¸æ£€æµ‹çš„ä½œç”¨åœ¨å“ªï¼Œå¹¶å‘ä¸å¦åœ¨traceæ•°æ®çš„JSONæ–‡ä»¶ä¸­ä¸åº”è¯¥æ˜¯å¯ä»¥ç›´æ¥è¢«è§£æçš„å— </li></ul><ol start="4"><li><strong>Self-Supervised Anomaly Detection from Distributed Traces</strong></li></ol><p>Jasmin Bogatinovski; Sasho Nedelkoski; Jorge Cardoso; Odej Kao</p><p>2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC)</p><ul><li><p>è¿™ç¯‡è®ºæ–‡ä»ç„¶å°†traceæ•°æ®å»ºæ¨¡ä¸º<strong>æ¨¡æ¿åºåˆ—</strong>ï¼Œå°†å¼‚å¸¸æ£€æµ‹é—®é¢˜è§†ä¸ºä¸‹ä¸€æ¨¡æ¿é¢„æµ‹é—®é¢˜ã€‚</p></li><li><p>ä¸åŒä¹‹å¤„åœ¨äºå¼•å…¥äº†self-supervised learningï¼Œå…·ä½“çš„ï¼Œåœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œé®è”½traceä¸­çš„éšæœºä¸€ä¸ªspanï¼Œè®­ç»ƒç½‘ç»œé¢„æµ‹è¢«é®è”½ä½ç½®çš„spanæ˜¯ä»€ä¹ˆã€‚<br>å…·ä½“çš„ï¼Œç½‘ç»œå­¦ä¹ åˆ°çš„æ˜¯ï¼štraceä¸­æ¯ä¸€ä¸ªä½ç½®çš„é¢„æµ‹å¯èƒ½æ€§åˆ—è¡¨</p></li><li><p>åœ¨å¼‚å¸¸æ£€æµ‹çš„æ—¶å€™ï¼Œç»Ÿè®¡å¾…æµ‹æ ·æœ¬ä¸­ï¼Œspanä¸åœ¨é¢„æµ‹å‡ºé¥¿top-k listä¸­çš„æ¯”ä¾‹ï¼Œè®°ä¸ºanomaly score</p></li></ul><ol start="5"><li><strong>Anomaly Detection and Classification using Distributed Tracing and Deep Learning</strong></li></ol><p>Sasho Nedelkoski; Jorge Cardoso; Odej Kao</p><p>2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)</p><ul><li><p>æœ¬æ–‡æœ¬è´¨ä¸Šæ˜¯å¯¹äºæ—¶é—´åºåˆ—æå‡ºäº†ä¸€ç§å¼‚å¸¸æ£€æµ‹çš„æ–¹æ³•ï¼Œåªæ˜¯æ ¹æ®æœ¬æ–‡çš„æ£€æµ‹ç›®æ ‡â€”â€”å“åº”æ—¶é—´ï¼Œå°†ç‰¹å®šä¸€ç»„å¾®æœåŠ¡çš„å“åº”æ—¶é—´åºåˆ—ä½œä¸ºäº†ç ”ç©¶å¯¹è±¡ã€‚ç„¶è€Œæ‰€æå‡ºçš„æ–¹æ³•å…¶å®å¯ä»¥åœ¨å¹¿ä¹‰çš„æ—¶é—´åºåˆ—ä¸Šè¿›è¡Œè¯„ä¼°ã€‚</p></li><li><p>æœ¬æ–‡ç€é‡è¯´æ˜äº†åœ¨æ­£å¸¸åºåˆ—å…·æœ‰å¤šç§æ¨¡å¼çš„æ—¶å€™ï¼Œæˆ‘ä»¬è¯¥å¦‚ä½•å»å»ºæ¨¡æ­£å¸¸ï¼Œè¿™é‡Œæå‡ºä½¿ç”¨VAEçš„å¤šç»´é«˜æ–¯åˆ†å¸ƒæ¥å»ºæ¨¡è¿™ç§å¤æ‚åˆ†å¸ƒï¼Œå†åˆ©ç”¨é‡å»ºè¯¯å·®ä½œä¸ºå¼‚å¸¸å¾—åˆ†ã€‚</p></li><li><p>åç»­ï¼Œä»–è¿˜é€šè¿‡è®­ç»ƒäº†ä¸€ä¸ªåŸºäº1D-CNNçš„ç½‘è·¯ï¼Œåšäº†å¼‚å¸¸åˆ†ç±»ï¼Œé¦–å…ˆç”±ç”¨æˆ·é¢„å®šä¹‰å‡ ç§å¼‚å¸¸ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªæœ‰ç›‘ç£çš„åˆ†ç±»å™¨ï¼Œä»¥å¸®åŠ©å°†é‚£äº›æ£€æµ‹åˆ°çš„å¼‚å¸¸å½’ç±»åˆ°å·²çŸ¥çš„å¼‚å¸¸ç±»å‹ä¸­å»ã€‚</p></li><li><p>æœ¬æ–‡æ¯”è¾ƒæœ‰ç”¨çš„ç‚¹åœ¨äºï¼šåœ¨å¤„ç†è¿ç»´æ—¶é—´åºåˆ—ä¸Šï¼Œå…¶å®å­˜åœ¨å¾ˆå¤šéš¾ç‚¹ï¼Œéœ€è¦æˆ‘ä»¬ä½¿ç”¨ä¸€äº›é¢„å¤„ç†æ–¹æ³•ï¼Œå¦‚å»å™ªã€å¹³æ»‘ã€å»ç¦»ç¾¤å€¼ç­‰æ–¹æ³•æ¥å¤„ç†ï¼Œä»¥ä¿è¯æ¨¡å‹æ€§èƒ½ï¼Œä¸”å…·å¤‡å¯è§£é‡Šæ€§ã€‚</p></li></ul><blockquote><p>However, the large amount of events in the time series and the fact that proper training of neural networks <strong>requires normalization</strong>, leads to obligation of having an outlier removal technique. <strong>The presence of a strong outlier, will lead to values clamped to zero after the normalization</strong>. Therefore, events having response time greater than three standard deviations from the mean are removed from the training batch.</p><p>Next, we normalize the values by using min-max scaling (0, 1) to ensure that we stay in the positive range of values for the response time. In contrast, <strong>using standardization might produce negative values that do not have natural meaning when we deal with response time (no negative time)</strong>. Normalization is required and makes the optimization function of the neural network well-conditioned, which is key for convergence.</p><p>we apply <strong>smoothing for noise removal and robustness to small deviations</strong>. The time series is convolved with Hamming smoothing filter defined with its optimal parameters [35] and size M asâ€¦</p></blockquote><ol start="6"><li><strong>MicroRAS: Automatic Recovery in the Absence of Historical Failure Data for Microservice Systems</strong></li></ol><p>Li Wu; Johan Tordsson; Alexander Acker; Odej Kao</p><p>2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC)</p><ul><li><p>æœ¬æ–‡è‡´åŠ›äºæå‡ºä¸€ä¸ªæ•…éšœè‡ªåŠ¨æ¢å¤æ–¹æ³•ï¼ŒæŒ‡çš„æ˜¯å½“ç³»ç»Ÿå·²ç»å¼‚å¸¸åï¼Œæˆ‘ä»¬å¦‚ä½•è¯„ä¼°æŸäº›æ“ä½œæ‰€å¸¦æ¥çš„æ­£é¢/è´Ÿé¢å½±å“ï¼Œå¹¶åœ¨æ¢å¤æ•ˆæœä¸æ¢å¤æ—¶é—´ä¹‹é—´åšæƒè¡¡ï¼Œé€‰æ‹©æœ‰è¾ƒé«˜æ”¶ç›Šçš„åŠ¨ä½œã€‚</p></li><li><p>æœ¬æ–‡å°†traceæ•°æ®å»ºæ¨¡ä¸ºä¸€ä¸ªå±æ€§å›¾ï¼Œå±æ€§å›¾ç”¨äºæ„å»ºç³»ç»ŸçŠ¶æ€æ¨¡å‹ï¼Œåˆ†æåŠ¨ä½œçš„ä¼ æ’­å½±å“ç­‰ã€‚</p></li></ul><ol start="7"><li><strong>MicroRCA: Root Cause Localization of Performance Issues in Microservices</strong></li></ol><p>Li Wu; Johan Tordsson; Erik Elmroth; Odej Kao</p><p>NOMS 2020 - 2020 IEEE/IFIP Network Operations and Management Symposium</p><ul><li>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¸åº”ç”¨æ— å…³çš„ç³»ç»ŸMicroRCAï¼Œç”¨äºå®šä½åŸºäºå®¹å™¨çš„å¾®æœåŠ¡ä¸­æ€§èƒ½å¼‚å¸¸çš„æ ¹æœ¬åŸå› ã€‚è¯¥æ–¹æ³•æ„é€ äº†ä¸€ä¸ªå±æ€§å›¾æ¨¡å‹ï¼Œå°†æœåŠ¡å¼‚å¸¸æ€§èƒ½ç—‡çŠ¶ä¸ç›¸åº”çš„èµ„æºåˆ©ç”¨ç‡ç›¸å…³è”ï¼Œä»è€Œæ¨æ–­å‡ºå¼‚å¸¸å¾®æœåŠ¡ã€‚</li></ul><ol start="8"><li><a name="lundetecting2021"><sup>[1]</sup></a> <strong>Detecting anomalies in microservices with execution trace comparison</strong></li></ol><p>Lun Meng, Feng Ji, Yao Sun, TaoWang</p><p>Future Generation Computer Systems, Volume 116, March 2021, Pages 291-301</p><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè°ƒç”¨é“¾æ•°æ®çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œæ‹Ÿæ£€æµ‹çš„å¼‚å¸¸åˆ†ä¸ºä¸¤ç§ï¼š1) è°ƒç”¨å…³ç³»å¼‚å¸¸ï¼šè°ƒç”¨å…³ç³»æœ¬èº«å¹¶ä¸æ˜¯ä¸€æˆä¸å˜çš„ï¼Œè°ƒç”¨å…³ç³»ä¼šå› ä¸ºè°ƒç”¨æ—¶çš„å‚æ•°è€Œå‘ç”ŸåŠ¨æ€å˜åŒ–ï¼Œä½†æ˜¯æŸäº›å¼‚å¸¸ä¼šå¯¼è‡´å¾®æœåŠ¡è°ƒç”¨ä¸å¸¸è§çš„å¼‚å¸¸çš„è°ƒç”¨æ€»æ˜¯ä¸å·²æœ‰çš„è°ƒç”¨å…³ç³»ä¸åŒï¼Œå› æ­¤å¯ä»¥è¢«æ£€æµ‹å‡ºæ¥ï¼›2) è°ƒç”¨å“åº”å¼‚å¸¸ï¼šè¿™ç§å¼‚å¸¸ä¸ä¼šç ´åè°ƒç”¨å…³ç³»ï¼Œä½†æ˜¯ä¼šç›´æ¥å½±å“æœåŠ¡è¿Ÿå»¶ï¼Œå› æ­¤ä¹Ÿæ˜¯å¼‚å¸¸ã€‚</p><p>ä¸ºè§£å†³è°ƒç”¨å…³ç³»å¼‚å¸¸ï¼Œæœ¬æ–‡<strong>é¦–å…ˆ</strong>æ”¶é›†è½¯ä»¶æµ‹è¯•æœŸé—´çš„è°ƒç”¨é“¾æ•°æ®ç”¨äºåˆæˆåº”ç”¨è¿è¡Œæ—¶çš„tarce treeï¼Œæ³¨æ„ç”±äºtrace treeä¼šå› ä¸ºè°ƒç”¨æºå¸¦çš„å‚æ•°ä¸åŒè€Œä¸åŒï¼Œå› æ­¤åœ¨è½¯ä»¶æµ‹è¯•é˜¶æ®µæ”¶é›†ï¼ˆå‡ ä¹ï¼‰æ‰€æœ‰æƒ…å†µçš„è°ƒç”¨å…³ç³»æ•°æ®æ˜¯æœ‰å¿…è¦çš„ã€‚<strong>ç„¶å</strong>å°†å®æ—¶çš„è°ƒç”¨é“¾æ•°æ®ä¸åˆšæ‰çš„baselineä¹‹é—´è®¡ç®—æœ€å°ç¼–è¾‘è·ç¦»ï¼ˆbaselineä¸­æœ‰å¤šç§è°ƒç”¨baselineï¼Œå› æ­¤éœ€è¦ä¸æ¯ä¸€ä¸ªbaselineè®¡ç®—ä»–ä»¬ä¹‹é—´çš„è·ç¦»ï¼Œå–è·ç¦»æœ€å°çš„baselineä½œä¸ºæ­£å¸¸æ¨¡æ¿ï¼Œå¹¶è®¡ç®—anomaly scoreï¼‰ï¼Œä»¥ä½œä¸ºanomaly scoreã€‚</p><blockquote><p> To construct a baseline execution trace set S, for every arrival execution trace T_i, if we<br> cannot match T_i with an execution trace C_i in S, we add T_i in S.</p></blockquote><p>è°ƒç”¨æ—¶é—´åœ¨ç‰©ç†èµ„æºå……åˆ†çš„æƒ…å†µä¸‹ï¼Œä¸€èˆ¬æ˜¯ä¸ä¼šæœ‰å¤§çš„æ³¢åŠ¨çš„ï¼Œå› æ­¤ï¼Œè°ƒç”¨æ—¶é—´çš„æ¿€å¢å°±å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªè°ƒç”¨æ—¶é—´å¼‚å¸¸ã€‚ä¸ºäº†è¯†åˆ«_æ¿€å¢_ï¼Œæœ¬æ–‡ä½¿ç”¨coefficient of variationï¼ˆCVï¼‰æ¥è¡¨ç¤ºä¸€æ¬¡è¯·æ±‚çš„å“åº”æ—¶é—´å¼‚å¸¸ç¨‹åº¦ã€‚æ­¤æ—¶çš„traceæ•°æ®è¢«ç”¨ä¸€ä¸ª$M \times N$çš„metrixè¡¨ç¤ºï¼Œå…¶ä¸­ç¬¬mè¡Œç¬¬nåˆ—è¡¨ç¤ºï¼Œåœ¨ç¬¬iæ¬¡ç”¨æˆ·è¯·æ±‚æ—¶ï¼Œç¬¬jä¸ªç»„ä»¶ï¼ˆå¾®æœåŠ¡ï¼‰çš„å“åº”æƒ…å†µã€‚ç„¶åå€ŸåŠ©PCAå¯¹çŸ©é˜µè¿›è¡Œåˆ†è§£ï¼Œç”¨æ¥è¯†åˆ«å¯¼è‡´å¼‚å¸¸çš„å¾®æœåŠ¡ã€‚è¿™é‡Œæ²¡å¤ªçœ‹æ‡‚ã€‚</p><p><img src="https://user-images.githubusercontent.com/16149619/115550685-b9e17f00-a2dc-11eb-9094-980b220a86f3.png" alt="image"><br><img src="https://user-images.githubusercontent.com/16149619/115550693-bea63300-a2dc-11eb-8f08-d44732ca1b41.png" alt="image"></p><p>â€‹<br>9. <strong>Midiag: A Sequential Trace-Based Fault Diagnosis Framework for Microservices</strong></p><pre><code>Lun Meng, Yao Sun, Shudong ZhangInternational Conference on Services Computing, SCC 2020: Services Computing â€“ SCC 2020 pp 137-144</code></pre><ul><li>æœ¬æ–‡è¿˜æ˜¯å°†å¼‚å¸¸æ£€æµ‹é—®é¢˜è½¬åŒ–ä¸ºä¸‹ä¸€system callé¢„æµ‹é—®é¢˜ï¼Œè¿™é‡Œçš„system callä¹Ÿæ˜¯ç”¨æ¨¡æ¿æ¥ä»£æ›¿ï¼Œä¸ä¹‹å‰çš„ä¸åŒçš„æ˜¯ï¼Œè¿™é‡Œçš„æ¨¡æ¿æ˜¯é€šè¿‡K-means+æœ€é•¿å…¬å…±å­åºåˆ—æœç´¢æ¥å¾—åˆ°çš„ï¼Œè¿™ä¸æ—¥å¿—ç›¸å…³çš„å·¥ä½œååˆ†ç›¸ä¼¼ã€‚</li></ul><hr><h4 id="åŸºäºæ—¥å¿—æ•°æ®"><a href="#åŸºäºæ—¥å¿—æ•°æ®" class="headerlink" title="åŸºäºæ—¥å¿—æ•°æ®"></a>åŸºäºæ—¥å¿—æ•°æ®</h4><ul><li><p>åŸºäºæ—¥å¿—è§£æçš„å¤§è§„æ¨¡å¾®æœåŠ¡æ¶æ„è½¯ä»¶ç³»ç»Ÿå¼‚å¸¸æ£€æµ‹</p><p>  Anomaly Detection of Large Scale Microservice Architecture Software System Based on Log Parsing</p><p>  é‚°ä¸½åª›, ç”°æ˜¥å² ï¼šåŒæµå¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ç³»ï¼Œä¸Šæµ·;<br>  ç‹ ä¼Ÿ ï¼šåä¸œå¸ˆèŒƒå¤§å­¦æ•°æ®ç§‘å­¦å­¦é™¢ï¼Œä¸Šæµ·;</p></li><li><p>Root-Cause Metric Location for Microservice Systems via Log Anomaly Detection</p><p>  Lingzhi Wang; Nengwen Zhao; Junjie Chen; Pinnong Li; Wenchi Zhang; Kaixin Sui</p><p>  2020 IEEE International Conference on Web Services (ICWS)    </p></li><li><p>Anomaly Detection of Large Scale Microservice Architecture Software System Based on Log Parsing</p><p>  Liyuan Tai, Chun-qi Tian, W. Wang</p></li></ul><hr><h4 id="åŸºäºæ—¶é—´åºåˆ—æ•°æ®"><a href="#åŸºäºæ—¶é—´åºåˆ—æ•°æ®" class="headerlink" title="åŸºäºæ—¶é—´åºåˆ—æ•°æ®"></a>åŸºäºæ—¶é—´åºåˆ—æ•°æ®</h4><ul><li>Localizing Failure Root Causes in a Microservice through Causality Inference   Yuan Meng; Shenglin Zhang; Yongqian Sun; Ruru Zhang; Zhilong Hu; Yiyin Zhang, â€¦  2020 IEEE/ACM 28th International Symposium on Quality of Service (IWQoS)  <strong>åŸºäºå¾®æœåŠ¡KPIæ•°æ®çš„å…³è”æ¨æ–­æ–¹æ³•</strong>  æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„PCTS(è·¯å¾„æ¡ä»¶æ—¶é—´åºåˆ—)ç®—æ³•ï¼Œåœ¨å……åˆ†åˆ©ç”¨ä¼ æ’­å»¶è¿Ÿçš„æƒ…å†µä¸‹å­¦ä¹ ç›‘æ§æŒ‡æ ‡çš„ä¾èµ–å›¾ã€‚åœ¨PCTSä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨æ”¹è¿›çš„PC[10]å­¦ä¹ æ—¶é—´åºåˆ—ä¸­æ¯ä¸ªç‚¹çš„å› æœå›¾ã€‚ç„¶åç”Ÿæˆä¸¤ä¸ªæ—¶é—´åºåˆ—ä¹‹é—´çš„è¾¹ï¼Œç”Ÿæˆå¤±æ•ˆå› æœå›¾ã€‚  æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ—¶é—´åŸå› çš„éšæœºæ¼«æ­¥(TCORW)æ–¹æ³•ã€‚åœ¨TCORWä¸­ï¼Œæˆ‘ä»¬æˆåŠŸåœ°æ•´åˆäº†ä¸‰ç§ç±»å‹çš„ä¿¡æ¯:(1)ç›‘æµ‹æŒ‡æ ‡çš„å› æœå…³ç³»;(2)åº¦é‡çš„å¼‚å¸¸ä¿¡æ¯ï¼ŒåŒ…æ‹¬å‘ç”Ÿæ—¶é—´å’Œå¼‚å¸¸ç¨‹åº¦;(3)åŸºäºé¢†åŸŸçŸ¥è¯†è·å¾—çš„åº¦é‡ä¼˜å…ˆçº§  ç»“åˆPCTSå’ŒTCORWï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶â€”â€”å¾®åŸå› ï¼Œæ¥æ¨æ–­å¾®æœåŠ¡å¤±è´¥çš„å‰Nä¸ªæ ¹æœ¬åŸå› ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯åœ¨å¾®æœåŠ¡ä¸­å®šä½æ•…éšœæ ¹æºçš„ç¬¬ä¸€ä¸ªå·¥ä½œã€‚</li></ul><ul><li>Performance Diagnosis in Cloud Microservices using Deep Learning  Li Wu, Jasmin Bogatinovski, Sasho Nedelkoski, Johan Tordsson and Odej Kao  2020 AIOPS workshop  <strong>å¤šæºæ—¶é—´åºåˆ—çš„å¼‚å¸¸æ£€æµ‹ä¸æ ¹å› å®šä½</strong>â€”â€”æˆ‘ä»¬ä»å¤šä¸ªæ•°æ®æºæ”¶é›†æ•°æ®ï¼ŒåŒ…æ‹¬åº”ç”¨ç¨‹åºã€æ“ä½œç³»ç»Ÿå’Œç½‘ç»œï¼Œä»¥æä¾›ç”±ä¸åŒæ ¹æº(å¦‚è½¯ä»¶bugã€ç¡¬ä»¶é—®é¢˜ã€èµ„æºäº‰ç”¨ç­‰)å¼•èµ·çš„æ€§èƒ½é—®é¢˜çš„ç½ªé­ç¥¸é¦–ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿè¢«è®¾è®¡æˆ<strong>ä¸åº”ç”¨ç¨‹åºæ— å…³çš„</strong>ï¼Œä¸éœ€è¦åº”ç”¨ç¨‹åºä½¿ç”¨ä»ªå™¨æ¥è·å–æ•°æ®ã€‚ç›¸åï¼Œæˆ‘ä»¬æ”¶é›†åº”ç”¨ç¨‹åºå’Œè¿è¡Œæ—¶ç³»ç»Ÿæœ¬èº«æŠ¥å‘Šçš„æŒ‡æ ‡ã€‚  æœ¬æ–‡æå‡ºäº†ä¸€ç§åº”ç”¨ä¸å¯çŸ¥ç³»ç»Ÿï¼Œä»¥ç»†ç²’åº¦å®šä½å¾®æœåŠ¡æ€§èƒ½ä¸‹é™çš„ç½ªé­ç¥¸é¦–ï¼Œä¸ä»…åŒ…æ‹¬äº§ç”Ÿæ€§èƒ½é—®é¢˜çš„å¼‚å¸¸æœåŠ¡ï¼Œè¿˜åŒ…æ‹¬ä¸æœåŠ¡å¼‚å¸¸ç›¸å…³çš„ç½ªé­ç¥¸é¦–æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡æ„å»ºæœåŠ¡ä¾èµ–å›¾æ¥å‘ç°æ½œåœ¨çš„ç½ªé­ç¥¸é¦–æœåŠ¡ï¼Œç„¶ååº”ç”¨è‡ªåŠ¨ç¼–ç å™¨æ ¹æ®é‡æ„é”™è¯¯çš„æ’åºåˆ—è¡¨æ¥è¯†åˆ«å¼‚å¸¸æœåŠ¡æŒ‡æ ‡ã€‚  æˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•è¿›è¡Œå¼‚å¸¸æ£€æµ‹å’Œæ ¹æœ¬åŸå› åˆ†æ(ç³»ç»Ÿæ¦‚è¿°åœ¨ç¬¬3èŠ‚ä¸­æè¿°)ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬æ ¹æ®åŸºäºå›¾çš„æ–¹æ³•[16]å¯¹å¯¼è‡´æ•…éšœçš„æœåŠ¡è¿›è¡Œå»ºæ¨¡ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡è¯†åˆ«å¯¼è‡´é”™è¯¯æœåŠ¡æ€§èƒ½ä¸‹é™çš„æ ¹æœ¬åŸå› (å¼‚å¸¸åº¦é‡)æ¥æŸ¥æ˜å¼•å‘æ€§èƒ½ä¸‹é™çš„æ½œåœ¨é”™è¯¯æœåŠ¡ã€‚ç¬¬äºŒé˜¶æ®µï¼Œå¯¹æ½œåœ¨æ•…éšœçš„æ¨æ–­ï¼Œæ˜¯åŸºäºä»¥ä¸‹å‡è®¾:æ•…éšœè¡Œä¸ºçš„æœ€é‡è¦ç—‡çŠ¶ä¸æ­£å¸¸è¿è¡Œæ—¶çš„å€¼å­˜åœ¨æ˜¾è‘—åå·®ã€‚åœ¨ä»»ä½•æ—¶é—´ç‚¹æµ‹é‡æ¯ä¸ªç—‡çŠ¶çš„ä¸ªä½“è´¡çŒ®ï¼Œä»è€Œå¯¼è‡´è§‚å¯Ÿåˆ°çš„è¡Œä¸ºä¸æ­£å¸¸è¡Œä¸ºä¹‹é—´çš„å·®å¼‚ï¼Œä»è€Œå¯ä»¥å®šä½æœ€å¯èƒ½åæ˜ æ•…éšœçš„ç—‡çŠ¶ã€‚æœ‰äº†è¿™ä¸ªå‡è®¾ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨æ­£å¸¸ç³»ç»Ÿè¡Œä¸ºä¸‹æ¨¡æ‹Ÿç—‡çŠ¶å€¼</li></ul><ul><li><p>TELESTO: A Graph Neural Network Model for Anomaly Classification in Cloud Services</p><p>  Dominik Scheinert and Alexander Acker</p><p>  2020 AIOPS workshop</p><p>  <strong>å¤šç»´æ—¶é—´åºåˆ—çš„å¼‚å¸¸åˆ†ç±»ä»»åŠ¡</strong>ï¼Œå³ä¸ä»…åªè¯†åˆ«æ­£å¼‚å¸¸ï¼Œè¿˜æ˜¯è¢«å¼‚å¸¸çš„ç§ç±»</p><p>  æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡è®­ç»ƒåˆ†ç±»æ¨¡å‹æ¥è¯†åˆ«å†æ¬¡å‡ºç°çš„å¼‚å¸¸çš„æ–¹æ³•ã€‚åˆ©ç”¨ç³»ç»Ÿåº¦é‡æ•°æ®ï¼Œå¦‚CPUåˆ©ç”¨ç‡ã€å·²åˆ†é…å†…å­˜æˆ–ç£ç›˜I/Oç»Ÿè®¡æ•°æ®ï¼Œå¹¶å°†è¿™äº›æ•°æ®å»ºæ¨¡ä¸ºå¤šå…ƒæ—¶é—´åºåˆ—ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å¼‚å¸¸ç±»å‹ç‰¹å®šçš„æ¨¡å¼ï¼Œå¹¶ä¸ºå®ƒä»¬åˆ†é…å„è‡ªçš„å¼‚å¸¸æ ‡ç­¾ã€‚</p><p>  æˆ‘ä»¬æå‡ºçš„æ¨¡å‹æ¶æ„TELESTOåˆ©ç”¨ä¸€ç§æ–°é¢–çš„å›¾ç¥ç»ç½‘ç»œæ¶æ„ï¼Œåœ¨ç©ºé—´å’Œæ—¶é—´ç»´åº¦ä¸Šåˆ©ç”¨å¤šå˜é‡æ—¶é—´åºåˆ—å»ºæ¨¡ä¸ºå›¾ã€‚å®ƒä¸å—ç»´åº¦å˜åŒ–çš„å½±å“ï¼Œä¼˜äºå…¶ä»–ä¸¤ç§å¸¸ç”¨çš„å›¾ç¥ç»ç½‘ç»œæ–¹æ³•ã€‚</p></li></ul><hr><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><ol><li>ToN IoT-The role of heterogeneity and the need for standardization of features and attack types in IoT network intrusion datasets</li></ol><ol start="2"><li><p><a href="https://zenodo.org/record/3549604#.YEGfWI5LiUk">https://zenodo.org/record/3549604#.YEGfWI5LiUk</a></p><p> ä¸€ä¸ªå…¬å¼€æ•°æ®é›†ï¼Œé‡Œé¢åŒ…æ‹¬AIOPSå¸¸è§çš„ä¸‰ç§æ•°æ®</p></li><li><p>test bedæ­å»ºï¼š</p><p> frameworkï¼šOpenStack, Kolla-Ansible(dockerized environment)/k8s<br> For the <strong>metrics</strong> collection across the physical nodes in the infrastructure, we utilize <a href="20">Glances</a>,</p><p> OpenStack introduces a small but powerful library called <a href="21"><em>osprofiler</em></a> that is used by all OpenStack projects and their Python clients to generate <strong>traces</strong>.</p><p> The <strong>log</strong> files are distributed over the infrastructure and they are grouped in directories by the OpenStack projects (e.g., nova, neutron, glance, etc.) at the wally nodes.</p><p> anomaly injection: (ref) Multi-source Distributed System Data for AI-Powered Analytics</p><p> To generate workloads and inject faults into the infrastructure we used <a href="25">Rally</a>    </p></li></ol><h3 id="Trace-data"><a href="#Trace-data" class="headerlink" title="Trace data"></a>Trace data</h3><ol><li><p>Azure Public dataset: composes of two datasets representing two representative traces of the virtual machine of Microsoft Azure</p><p> <a href="5">link</a></p></li><li><p>Alibabaâ€™s cluster data is a collection of two datasets from real-world production</p><p> [link](2, 14, 28)</p></li><li><p>Googleâ€™s collection of two tracing datasets originates from parts of Google cluster management software and systems</p><p> <a href="10">link</a></p></li></ol><h3 id="metric-data"><a href="#metric-data" class="headerlink" title="metric data"></a>metric data</h3><ol><li><p>A plethora of available collections of datasets containing metric data can be found in Stonybrook</p><p> <a href="31">link</a></p></li><li><p><a href="1">Numenta</a> predominantly contains datasets<br>from streaming and real-time applications, while <a href="9">Harvard</a>, <a href="8">ELKI</a>, <a href="15">LMU</a> store network intrusion data.</p></li></ol><h3 id="log-data"><a href="#log-data" class="headerlink" title="log data"></a>log data</h3><ol><li><p>The <a href="3">CFDR resource</a> stores links or 19 log datasets grouped in 11 data collections.</p></li><li><p>The second resource is the <a href="35">loghub data resource</a>.</p></li></ol><hr><h2 id="æ²¡å•¥ç”¨çš„æ–‡ç« "><a href="#æ²¡å•¥ç”¨çš„æ–‡ç« " class="headerlink" title="æ²¡å•¥ç”¨çš„æ–‡ç« "></a>æ²¡å•¥ç”¨çš„æ–‡ç« </h2><ul><li><p><strong>MicroMon: A Monitoring Framework for Tackling Distributed Heterogeneity</strong></p><p>  Babar Khalid, Nolan Rudolph, Ramakrishnan Durairajan, Sudarsun Kannan</p><p>  12th {USENIX} Workshop on Hot Topics in Storage and File Systems (HotStorage 20).</p><p>  æ²¡å•¥ç”¨ï¼Œæè¿°äº†ä¸€ç§åœ¨å¾®æœåŠ¡ä¸Šè¿è¡Œçš„ç›‘è§†ç³»ç»Ÿï¼Œä¸»è¦åº”å¯¹çš„æ˜¯å¾®æœåŠ¡ç³»ç»Ÿçš„è½¯ä»¶/ç¡¬ä»¶å¼‚è´¨æ€§é—®é¢˜ï¼Œæé«˜ç›‘è§†ç³»ç»Ÿçš„ååé‡</p></li><li><p><strong>Advancing Monitoring in Microservices Systems</strong></p><p>  Marcello Cinque; Raffaele Della Corte; Antonio Pecchia</p><p>  2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)</p><p>  æ²¡å•¥ç”¨ï¼Œæè¿°äº†ä¸€ç§åœ¨å¾®æœåŠ¡ä¸Šè¿è¡Œçš„ç›‘è§†ç³»ç»Ÿ</p></li><li><p><strong>å­¦ä½è®ºæ–‡ï¼šANOMALY DETECTION IN CLOUD-NATIVE SYSTEMS</strong></p><p>  Surace, Pinoâ€™ (2019)</p><p>  æ²¡å•¥ç”¨ï¼Œè®²äº†ä¸€äº›äº‘åŸç”Ÿåº”ç”¨ä¸­çš„ç»„ä»¶ç»¼è¿°ï¼Œä»¥åŠå¦‚ä½•ç”¨ç®€å•çš„MLæ–¹æ³•åˆ©ç”¨å¾®æœåŠ¡ä¸­çš„æ—¶é—´åºåˆ—åšå¼‚å¸¸æ£€æµ‹</p></li><li><p>Artificial Intelligence for IT Operations (AIOPS) Workshop White Paper</p><p>  Jasmin Bogatinovski, Sasho Nedelkoski, Alexander Acker, Florian Schmidt, Thorsten Wittkopp, Soeren Becker, Jorge Cardoso, Odej Kao</p><p>  white paper for the AIOPS 2020 workshop at ICSOC 2020</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> AIOps </tag>
            
            <tag> survey </tag>
            
            <tag> micro-service </tag>
            
            <tag> cloud native </tag>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Some Notes</title>
      <link href="/uncategorized/notes/notes/"/>
      <url>/uncategorized/notes/notes/</url>
      
        <content type="html"><![CDATA[<ol><li><strong>posterior collapse</strong>: where the latents are ignored when they are paired with a powerful autoregressive decoder â€” typically observed in the VAE framework, i.e., the latents are ignored as the decoder is powerful enough to model x perfectly.ä¸ªäººç†è§£æ˜¯æŸäº›VAEçš„decoderçš„é‡å»ºèƒ½åŠ›è¿‡äºå¥½å¯¼è‡´é‡å»ºè¯¯å·®å¾ˆå°ï¼Œæœ€åæ¨¡å‹ä¸èƒ½å¾ˆå¥½çš„æœ€å°åŒ–priorç›¸å…³çš„loss itemã€‚</li></ol><span id="more"></span><ol start="2"><li>å…³äºself-supervised learningä¸­çš„é™„åŠ ä»»åŠ¡ï¼š<ol><li>ä¸€ç§å¸¸ç”¨ä¸”ç®€å•çš„ä»»åŠ¡ï¼šå¯¹åŸå§‹æ•°æ®$x$åšä¸€å®šçš„transformation($t$)ï¼Œå³$t(x)$ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªé™„åŠ ä»»åŠ¡ï¼Œå³åŒºåˆ†è¿™äº›transformationçš„ç§ç±»ã€‚<br> <strong>ç›®çš„</strong>ï¼šè®­ç»ƒç¥ç»ç½‘ç»œè¾¨åˆ«å›¾ç‰‡æ˜¯å¦ä¸ºè‡ªç„¶çš„ã€‚(To predict such transformations, a model should distinguish between what is semantically natural or not, and consequently, it<br> learns high-level semantic representations of inputs.)</li><li>å½“ä»…ä½¿ç”¨äº†transformationåçš„æ•°æ®$t(x)$è€Œæ²¡æœ‰å°†å¯¹åº”çš„self-supervised learning taskåŠ å…¥åˆ°lossä¸­ï¼Œåˆ™è¯¥æ–¹æ³•é€€åŒ–ä¸º<strong>æ•°æ®å¢å¼º</strong>ã€‚ <strong>ç›®çš„</strong>ï¼šæé«˜ç½‘ç»œçš„é€šç”¨æ€§ï¼Œå³å¯¹å¤šç§å˜åŒ–åçš„æ ·æœ¬ï¼Œéƒ½èƒ½å¤Ÿæå–å‡ºå·®ä¸å¤šçš„è¯­ä¹‰ä¿¡æ¯<blockquote><p>This conventional data augmentation aims to improve upon<br>the generalization ability of the target neural network f by<br>leveraging certain transformations that can preserve their semantics,<br>e.g., cropping, contrast enhancement, and flipping.</p></blockquote> <strong>ç¼ºç‚¹</strong>ï¼šå¦ä¸€æ–¹é¢ï¼Œå¦‚æœtransformationä¿®æ”¹äº†æ•°æ®ä¸­çš„è¯­ä¹‰ï¼Œåˆ™è½¬æ¢çš„ä¸å˜å±æ€§å¯èƒ½ä¼šå¹²æ‰°è¯­ä¹‰è¡¨ç¤ºå­¦ä¹ ï¼ˆè¯·å‚é˜…ç¬¬3.2èŠ‚ä¸­çš„è¡¨1ï¼‰ã€‚<blockquote><p>On the other hands, if a transformation modifies the semantics,<br>the invariant property with respect to the transformation<br>could interfere with semantic representation learning</p></blockquote></li></ol></li><li><strong>mode collapse</strong>: meaning that GANs have the tendency to only generate a subset of the original dataset.</li></ol><ol start="4"><li><p>Tensorflow1ä¸­å…³äºTensorArrayçš„ç”¨æ³•</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># æ”¹åŠ¨TensorFlowæºç çš„æ—¶å€™ï¼Œä½¿ç”¨ä»¥ä¸‹è¯­å¥</span>i_list <span class="token operator">=</span> tensor_array_ops<span class="token punctuation">.</span>TensorArray<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> dynamic_size<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'i_list'</span><span class="token punctuation">,</span> clear_after_read<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment"># å½“åœ¨å…¶ä»–ç¯å¢ƒä¸­æ—¶ï¼Œtensor_array_ops -> tf</span><span class="token comment"># ä½¿ç”¨</span>self<span class="token punctuation">.</span>j_list<span class="token punctuation">.</span>write<span class="token punctuation">(</span>_step<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">.</span>mark_used<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''def write(self, index, value, name=None):  """Write `value` into index `index` of the TensorArray.  Args:  index: 0-D.  int32 scalar with the index to write to.  value: N-D.  Tensor of type `dtype`.  The Tensor to write to this index.  name: A name for the operation (optional).  Returns:  A new TensorArray object with flow that ensures the write occurs.  Use this object all for subsequent operations.'''</span><span class="token comment"># ä¸Šè¿°è¯­å¥çš„è¿”å›éœ€è¦è¢«ä½¿ç”¨ï¼Œå½“æ²¡æœ‰è¢«ä½¿ç”¨çš„æ—¶å€™ä¼šåœ¨logä¸­ç”Ÿæˆä¸€ä¸ªwarningï¼Œè§£å†³æ–¹æ¡ˆæ˜¯åœ¨write()åä½¿ç”¨æ–¹æ³•ï¼šmark_used()</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>åœ¨å¯¹LSTM/GRUçš„æ›´æ–°è¿‡ç¨‹åšæ•°å­¦ä¸Šçš„æ¨å¯¼çš„æ—¶å€™ï¼Œé‡åˆ°é—®é¢˜ï¼Œ$W * [h_{t-1}, x_t] + b$ä¸­çš„$concat$æ“ä½œå¦‚ä½•åœ¨æ•°å­¦ä¸Šåˆ†æï¼Ÿå…¶ä¸­ï¼š</p><ul><li>shape of W: [hidden_dim, hidden_dim + data_dim](10, 11)</li></ul></li></ol><ul><li>shape of h: [hidden_dim, 1](10, 1)<ul><li>shape of x: [data_dim, 1](1,1)</li><li>shape of b: [hidden_dim, 1](10, 1)</li></ul>åœ¨ä»£ç ä¸­ï¼Œå‘é‡ä¸ºè¡Œå‘é‡ï¼Œå› æ­¤ä¸Šè¿°å…¬å¼åœ¨ä»£ç ä¸­çš„å®ç°ä¸ºï¼š$[h_{t-1}, x_t] * W + b$, å…¶ä¸­ï¼š<ul><li>shape of W: [hidden_dim + data_dim, hidden_dim](11, 10)</li><li>shape of h: [1, hidden_dim](1, 10)</li><li>shape of x: [1, data_dim](1,1)</li><li>shape of b: [1, hidden_dim](10, 1)</li></ul>ä¸Šè¿°çš„å¼å­$W * [h_{t-1}, x_t] + b$ç”±äºå­˜åœ¨$concat$æ“ä½œï¼Œè€Œéš¾ä»¥ä½¿ç”¨æ•°å­¦åˆ†æï¼Œæ­¤æ—¶å¯ä»¥ä½¿ç”¨$Wâ€™ * h_{t-1} + u * x_t + b$ä»£æ›¿ï¼Œå…¶ä¸­Wâ€™: [hidden_dim, hidden_dim], u: [hidden_dim, 1], W = [Wâ€™, u]</li></ul>  <pre class="line-numbers language-python" data-language="python"><code class="language-python">h <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token comment"># [1, 10]</span><span class="token builtin">input</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># [1, 1]</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'h shape &#123;&#125;, input shape: &#123;&#125;, concat shape: &#123;&#125;'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>h<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">,</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span>h<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>res <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span>h<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> w_i<span class="token punctuation">)</span> <span class="token operator">+</span> b_ires1 <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>h<span class="token punctuation">,</span> w_i<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> w_i<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> b_ipprint<span class="token punctuation">(</span>res<span class="token punctuation">)</span>pprint<span class="token punctuation">(</span>res1<span class="token punctuation">)</span>pprint<span class="token punctuation">(</span>res <span class="token operator">==</span> res1<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  output<br>  <pre class="line-numbers language-console" data-language="console"><code class="language-console">h shape (1, 10), input shape: (1, 1), concat shape: (1, 11)array([[ 1.61824865, -4.07500279,  1.56455836,  1.2925876 , -2.16892738,        -2.38041244, -2.28631814,  2.84973208, -4.34229152, -1.44608608]])array([[ 1.61824865, -4.07500279,  1.56455836,  1.2925876 , -2.16892738,        -2.38041244, -2.28631814,  2.84973208, -4.34229152, -1.44608608]])array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,         True]])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in AAAI 2021 (Feb 02-09 2021)</title>
      <link href="/uncategorized/paperlistfile/AAAI2021/"/>
      <url>/uncategorized/paperlistfile/AAAI2021/</url>
      
        <content type="html"><![CDATA[<p><a href="https://aaai.org/Conferences/AAAI-21/wp-content/uploads/2020/12/AAAI-21_Accepted-Paper-List.Main_.Technical.Track_.pdf">Accept paper list</a></p><span id="more"></span><h2 id="anomaly-detection-anomaly-outlier-out-of-distribution-one-class-Malware-detection-â€¦"><a href="#anomaly-detection-anomaly-outlier-out-of-distribution-one-class-Malware-detection-â€¦" class="headerlink" title="anomaly detection [anomaly, outlier, out-of-distribution, one-class, Malware detection, â€¦]"></a>anomaly detection [anomaly, outlier, out-of-distribution, one-class, Malware detection, â€¦]</h2><ul><li><p>LREN: Low-Rank Embedded Network for Sample-Free Hyperspectral Anomaly Detection</p><p>  Kai Jiang, Weiying Xie, Jie Lei, Tao Jiang, Yunsong Li</p></li><li><p>GAN Ensemble for Anomaly Detection</p><p>  Xiaohui Chen, Xu Han, Liping Liu</p></li><li><p>Anomaly Attribution with Likelihood Compensation</p><p>  Tsuyoshi Ide, Amit Dhurandhar, Jiri Navratil, Moninder Singh, Naoki Abe</p></li><li><p>Regularizing Attention Networks for Anomaly Detection in Visual Question Answering</p><p>  Doyup Lee, Yeongjae Cheon, Wook-Shin Han</p></li><li><p>Appearance-Motion Memory Consistency Network for Video Anomaly Detection</p><p>  Ruichu Cai, Hao Zhang, Wen Liu, Shenghua Gao, Zhifeng Hao</p></li><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> Learning Semantic Context from Normal Samples for Unsupervised Anomaly Detection</p><p>  Xudong Yan, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, Pheng-Ann Heng</p></li><li><p>Graph Neural Network-Based Anomaly Detection in Multivariate Time Series</p><p>  Ailin Deng, Bryan Hooi</p></li><li><p><strong>ã€é‡ç‚¹é˜…è¯»ã€‘</strong> Time Series Anomaly Detection with Multiresolution Ensemble Decoding</p><p>  Lifeng Shen, Zhongzhong Yu, Qianli Ma, James Tin-Yau Kwok</p></li><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> Outlier Impact Characterization for Time Series Data</p><p>  Jianbo Li, Lecheng Zheng, Yada Zhu, Jingrui He</p></li><li><p>Graph Neural Network to Dilute Outliers for Refactoring Monolith Application</p><p>  Utkarsh Desai, Sambaran Bandyopadhyay, Srikanth Tamilselvam</p></li><li><p>Accelerated Combinatorial Search for Outlier Detection with Provable Bound on Sub-<br>Optimality</p><p>  Guihong Wan, Haim Schweitzer</p></li><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> Neighborhood Consensus Networks for Unsupervised Multi-View Outlier Detection</p><p>  Li Cheng, Yijie Wang, Xinwang Liu</p></li><li><p>DecAug: Out-of-Distribution Generalization via Decomposed Feature Representation and<br>Semantic Augmentation</p><p>  Haoyue Bai, Rui Sun, Lanqing Hong, Fengwei Zhou, Nanyang Ye, Han-Jia Ye, Gary Chan, Zhenguo Li</p></li><li><p>Few-Shot One-Class Classification via Meta-Learning</p><p>  Ahmed Frikha, Denis Krompass, Hans-Georg Koepken, Volker Tresp</p></li><li><p>Classifying Sequences of Extreme Length with Constant Memory Applied to Malware<br>Detection</p><p>  Edward Raff, William Fleshman, Richard J Zak, Hyrum Anderson, Bobby Filar, Mark McLean</p></li><li><p>Disentangled Representation Learning in Heterogeneous Information Network for Large-<br>Scale Android Malware Detection in the COVID-19 Era and Beyond</p><p>  Shifu Hou, Yujie Fan, Mingxuan Ju, Yanfang Ye, Wenqiang Wan, Kui Wang, Yinming Mei, Qi Xiong,<br>Fudong Shao</p></li></ul><h2 id="heterogeneous"><a href="#heterogeneous" class="headerlink" title="heterogeneous"></a>heterogeneous</h2><ul><li><p>Embedding Heterogeneous Networks into Hyperbolic Space without Meta-Â­â€Path</p><p>  Lili Wang, Chongyang Gao, Chenghan Huang, Ruibo Liu, Weicheng Ma, Soroush Vosoughi</p></li><li><p>Synergetic Learning of Heterogeneous Temporal Sequences for Multi-Â­â€Horizon Probabilistic Forecasting</p><p>  Longyuan Li, Jihai Zhang, Junchi Yan, Yaohui Jin, Yunhao Zhang, Yanjie Duan, Guangjian Tian</p></li><li><p>Multi-Â­â€Modal Multi-Â­â€Label Emotion Recognition with Heterogeneous Hierarchical Message Passing</p><p>  Dong Zhang, Xincheng Ju, Wei Zhang, Junhui Li, Shoushan Li, Zhu Qiaoming, Zhou Guodong</p></li><li><p>Heterogeneous Graph Structure Learning for Graph Neural Networks</p><p>  Jianan Zhao, Xiao Wang, Chuan Shi, Binbin Hu, Guojie Song, Yanfang Ye</p></li><li><p>Disentangled Representation Learning in Heterogeneous Information Network for Large-Â­â€<br>Scale Android Malware Detection in the COVID-Â­â€19 Era and Beyond</p><p>  Shifu Hou, Yujie Fan, Mingxuan Ju, Yanfang Ye, Wenqiang Wan, Kui Wang, Yinming Mei, Qi Xiong, Fudong Shao </p></li><li><p>MERL: Multimodal Event Representation Learning in Heterogeneous Embedding Spaces</p><p>  Linhai Zhang, Deyu Zhou, Yulan He, Zeng Yang</p></li><li><p>Modeling Heterogeneous Relations across Multiple Modes for Potential Crowd Flow Prediction</p><p>  Qiang Zhou, Jingjing Gu, Xinjiang Lu, Fuzhen Zhuang, Yanchao Zhao, Qiuhong Wang, Xiao Zhang</p></li><li><p><strong>ã€é‡è¦ã€‘</strong> Infusing Multi-Â­â€Source Knowledge with Heterogeneous Graph Neural Network for Emotional Conversation Generation</p><p>  Yunlong Liang, Fandong Meng, Ying Zhang, Yufeng Chen, Jinan Xu, Jie Zhou</p></li><li><p>HARGAN: Heterogeneous Argument Attention Network for Persuasiveness Prediction</p><p>  Kuo-Â­â€Yu Huang, Hen-Â­â€Hsen Huang, Hsin-Â­â€Hsi Chen</p></li><li><p>Deep Innovation Protection: Confronting the Credit Assignment Problem in Training Heterogeneous Neural Architectures</p><p>  Sebastian Risi, Kenneth O Stanley</p></li><li><p>Real-Â­â€Time Tropical Cyclone Intensity Estimation by Handling Temporally Heterogeneous Satellite Data</p><p>  Boyo Chen, Buo-Â­â€Fu Chen, Yun-Â­â€Nung Chen</p></li></ul><h2 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h2><ul><li><p>Deep Switching Auto-Regressive Factorization: Application to Time Series Forecasting</p><p>  Amirreza Farnoosh, Bahar Azari, Sarah Ostadabbas</p></li><li><p><strong>ã€é‡ç‚¹é˜…è¯»ã€‘</strong> Dynamic Gaussian Mixture Based Deep Generative Model for Robust Forecasting on Sparse<br>Multivariate Time Series</p><p>  Yinjun Wu, Jingchao Ni, Wei Cheng, Bo Zong, Dongjin Song, Zhengzhang Chen, Yanchi Liu, Xuchao<br>Zhang, Haifeng Chen, Susan B Davidson</p></li><li><p>Second Order Techniques for Learning Time-Series with Structural Breaks</p><p>  Takayuki Osogami</p></li><li><p>Correlative Channel-Aware Fusion for Multi-View Time Series Classification</p><p>  Yue Bai, Lichen Wang, Zhiqiang Tao, Sheng Li, Yun Fu</p></li></ul><ul><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> Learnable Dynamic Temporal Pooling for Time Series Classification</p><p>  Dongha Lee, Seonghyeon Lee, Hwanjo Yu</p></li><li><p>Time Series Domain Adaptation via Sparse Associative Structure Alignment</p><p>  Ruichu Cai, Jiawei Chen, Zijian Li, Wei Chen, Keli Zhang, Junjian Ye, Zhuozhang Li, Xiaoyan Yang,<br>Zhenjie Zhang</p></li><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> Learning Representations for Incomplete Time Series Clustering</p><p>  Qianli Ma, Chuxin Chen, Sen Li, Garrison Cottrell</p></li><li><p>Temporal Latent Autoencoder: A Method for Probabilistic Multivariate Time Series<br>Forecasting</p><p>  Nam Nguyen, Brian Quanz</p></li></ul><ul><li><p>ShapeNet: A Shapelet-Neural Network Approach for Multivariate Time Series Classification</p><p>  Guozhong Li, Byron Choi, Jianliang Xu, Sourav S Bhowmick, Kwok-Pan Chun, Grace Lai-Hung Wong</p></li><li><p>Joint-Label Learning by Dual Augmentation for Time Series Classification</p><p>  Qianli Ma, Zhenjing Zheng, Jiawei Zheng, Sen Li, Wanqing Zhuang, Garrison Cottrell</p></li><li><p><strong>ã€Best paper awardã€‘</strong> Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</p><p>  Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang</p></li><li><p>Meta-Learning Framework with Applications to Zero-Shot Time-Series Forecasting</p><p>  Boris N. Oreshkin, Dmitri Carpov, Chapados Nicolas, Yoshua Bengio</p></li></ul><h2 id="about-deep-learning"><a href="#about-deep-learning" class="headerlink" title="about deep learning"></a>about deep learning</h2><ul><li><p>Deep Frequency Principle Towards Understanding Why Deeper Learning Is Faster</p><p>  Zhiqin John Xu, Hanxu Zhou</p></li><li><p>Understanding Decoupled and Early Weight Decay</p><p>  Johan BjÃ¶rck, Kilian Weinberger, Carla P Gomes</p></li></ul><h2 id="sequence"><a href="#sequence" class="headerlink" title="sequence"></a>sequence</h2><ul><li><p>Copy That! Editing Sequences by Copying Spans</p><p>  Sheena L Panthaplackel, Miltiadis Allamanis, Marc Brockschmidt</p></li><li><p>Semi-Supervised Knowledge Amalgamation for Sequence Classification</p><p>  Jidapa Thadajarassiri, Thomas Hartvigsen, Xiangnan Kong, Elke Rundensteiner</p></li><li><p>Neural Sequence-to-Grid Module for Learning Symbolic Rules</p><p>  Segwang Kim, Hyoungwook Nam, Joonyoung Kim, Kyomin Jung</p></li></ul><ul><li><p>Synergetic Learning of Heterogeneous Temporal Sequences for Multi-Horizon Probabilistic<br>Forecasting</p><p>  Longyuan Li, Jihai Zhang, Junchi Yan, Yaohui Jin, Yunhao Zhang, Yanjie Duan, Guangjian Tian</p></li></ul><ul><li><p>Semi-Supervised Sequence Classification through Change Point Detection</p><p>  Nauman Ahad, Mark Davenport</p></li><li><p>Bridging Towers of Multi-Task Learning with a Gating Mechanism for Aspect-Based<br>Sentiment Analysis and Sequential Metaphor Identification</p><p>  Rui Mao, Xiao Li</p></li><li><p>Deterministic Mini-Batch Sequencing for Training Deep Neural Networks</p><p>  Subhankar Banerjee, Shayok Chakraborty</p></li><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> SeCo: Exploring Sequence Supervision for Unsupervised Representation Learning</p><p>  Ting Yao, Yiheng Zhang, Zhaofan Qiu, Yingwei Pan, Tao Mei</p></li><li><p>Answering Complex Queries in Knowledge Graphs with Bidirectional Sequence Encoders</p><p>  Bhushan Kotnis, Carolin Lawrence, Mathias Niepert</p></li><li><p>Residual Shuffle-Exchange Networks for Fast Processing of Long Sequences</p><p>  Andis Draguns, EmÄ«ls OzoliÅ†Å¡, Agris Å ostaks, MatÄ«ss Apinis, Karlis Freivalds</p></li><li><p>Entity Guided Question Generation with Contextual Structure and Sequence Information<br>Capturing</p><p>  Qingbao Huang, Mingyi Fu, Linzhang Mo, Yi Cai, Jingyun Xu, Pijian Li, Qing Li, Ho-fung Leung</p></li><li><p>Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-<br>Generation Networks</p><p>  Cunchao Zhu, Muhao Chen, Changjun Fan, Guangquan Cheng, Yan Zhang</p></li></ul><ul><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> Continuous-Time Attention for Sequential Learning</p><p>  Yi-Hsiang Chen, Jen-Tzung Chien</p></li><li><p>Interpretable Sequence Classification via Discrete Optimization</p><p>  Maayan Shvo, Andrew C Li, Rodrigo A Toro Icarte, Sheila A. McIlraith</p></li></ul><h2 id="interpretable-Understanding-explanation-Attribution-â€¦"><a href="#interpretable-Understanding-explanation-Attribution-â€¦" class="headerlink" title="interpretable [Understanding, explanation, Attribution â€¦]"></a>interpretable [Understanding, explanation, Attribution â€¦]</h2><ul><li><p>Building Interpretable Interaction Trees for Deep NLP Models</p><p>  Die Zhang, HuiLin Zhou, Xiaoyi Bao, Da Huo, Ruizhao Chen, Hao Zhang, Xu Cheng, Mengyue Wu,<br>Quanshi Zhang</p></li><li><p>Interpretable Embedding Procedure Knowledge Transfer via Stacked Principal Component<br>Analysis and Graph Neural Network</p><p>  Seunghyun Lee, Byung Cheol Song</p></li><li><p>Interpreting Neural Networks as Quantitative Argumentation Frameworks</p><p>  Nico Potyka</p></li><li><p>Interpretable Clustering on Dynamic Graphs with Recurrent Graph Neural Networks</p><p>  Yuhang Yao, Carlee Joe-Wong</p></li><li><p>Interpreting Deep Neural Networks with Relative Sectional Propagation by Analyzing<br>Comparative Gradients and Hostile Activations</p><p>  Woo Jeoung Nam, Jaesik Choi, Seong-Whan Lee</p></li><li><p>Human-Level Interpretable Learning for Aspect-Based Sentiment Analysis</p><p>  Rohan K Yadav, Lei Jiao, Ole-Christoffer Granmo, Morten Goodwin</p></li><li><p>Learning Accurate and Interpretable Decision Rule Sets from Neural Networks</p><p>  Litao Qiao, Weijia Wang, Bill Lin</p></li><li><p>Learning Interpretable Models for Couple Networks under Domain Constraints</p><p>  Hongyuan You, Sikun Lin, Ambuj Singh</p></li><li><p>Explanation Consistency Training: Facilitating Consistency-Based Semi-Supervised Learning<br>with Interpretability</p><p>  Tao Han, Wei-Wei Tu, Yu-Feng Li</p></li><li><p>i-Algebra: Towards Interactive Interpretability of Deep Neural Networks</p><p>  Xinyang Zhang, Pang Ren, Shouling Ji, Fenglong Ma, Ting Wang</p></li><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> Explainable Models with Consistent Interpretations</p><p>  Vipin Pillai, Hamed Pirsiavash</p></li><li><p>Iterative Bounding MDPs: Learning Interpretable Policies via Non-Interpretable Methods</p><p>  Nicholay Topin, Stephanie Milani, Fei Fang, Manuela Veloso</p></li><li><p>HyDRA: Hypergradient Data Relevance Analysis for Interpreting Deep Neural Networks</p><p>  Yuanyuan Chen, Boyang Li, Han Yu, Pengcheng Wu, Chunyan Miao</p></li><li><p>Interpreting Multivariate Shapley Interactions in DNNs</p><p>  Hao Zhang, Yichen Xie, Longjie Zheng, Die Zhang, Quanshi Zhang</p></li><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> Self-Attention Attribution: Interpreting Information Interactions Inside Transformer</p><p>  Yaru Hao, Li Dong, Furu Wei, Ke Xu</p></li><li><p>Interpretable Sequence Classification via Discrete Optimization</p><p>  Maayan Shvo, Andrew C Li, Rodrigo A Toro Icarte, Sheila A. McIlraith</p></li><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> The Heads Hypothesis: A Unifying Statistical Approach towards Understanding Multi-Headed<br>Attention in BERT</p><p>  Madhura Pande, Aakriti Budhraja, Preksha Nema, Pratyush Kumar, Mitesh M. Khapra</p></li><li><p>Ordered Counterfactual Explanation by Mixed-Integer Linear Optimization</p><p>  Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, Yuichi Ike, Kento Uemura, Hiroki Arimura</p></li><li><p>Strong Explanations in Abstract Argumentation</p><p>  Markus Ulbricht, Johannes Peter Wallner</p></li><li><p>On Generating Plausible Counterfactual and Semi-Factual Explanations for Deep Learning</p><p>  Eoin Kenny, Mark Keane</p></li><li><p>The Tractability of SHAP-Score-Based Explanations for Classification over Deterministic and<br>Decomposable Boolean Circuits</p><p>  Marcelo Arenas, Pablo BarcelÃ³, Leopoldo Bertossi, MikaÃ«l Monet</p></li><li><p>On the Tractability of SHAP Explanations</p><p>  Guy Van den Broeck, Anton Lykov, Maximilian Schleich, Dan Suciu</p></li><li><p>Responsibility Attribution in Parameterized Markovian Models</p><p>  Christel Baier, Florian Funke, Rupak Majumdar</p></li><li><p>A Unified Taylor Framework for Revisiting Attribution Methods</p><p>  Huiqi Deng, Na Zou, Mengnan Du, Weifu Chen, Guocan Feng, Xia Hu</p></li><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> Explaining Convolutional Neural Networks through Attribution-Based Input Sampling and<br>Block-Wise Feature Aggregation</p><p>  Sam Sattarzadeh, Mahesh Sudhakar, Anthony Lem, Shervin Mehryar, Konstantinos N Plataniotis,<br>Jongseong Jang, Hyunwoo Kim, Yeonjeong Jeong, SangMin Lee, Kyunghoon Bae</p></li><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> Visualization of Supervised and Self-Supervised Neural Networks via Attribution Guided<br>Factorization</p><p>  Shir Gur, Ameen Ali, Lior Wolf</p></li><li><p>Enhanced Regularizers for Attributional Robustness</p><p>  Anindya Sarkar, Anirban Sarkar, Vineeth N Balasubramanian</p></li><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> Explaining a Black-Box by Using a Deep Variational information Bottleneck Approach</p><p>  Seojin Bang, Pengtao Xie, Heewook Lee, Wei Wu, Eric Xing</p></li></ul><ul><li><p>Explaining Neural Matrix Factorization with Gradient Rollback</p><p>  Carolin Lawrence, Timo Sztyler, Mathias Niepert</p></li></ul><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><ul><li><p>Content Learning with Structure-Aware Writing: A Graph-Infused Dual Conditional<br>Variational Autoencoder for Automatic Storytelling</p><p>  Meng Hsuan Yu, Juntao Li , Zhangming Chan, Dongyan Zhao, Rui Yan</p></li><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> HOT-VAE: Learning High-Order Label Correlation for Multi-LabelClassification via Attention-<br>Based Variational Autoencoders</p><p>  Wenting Zhao, Shufeng Kong, Junwen Bai, Daniel Fink, Carla P Gomes</p></li><li><p>Fractal Autoencoders for Feature Selection</p><p>  Xinxing Wu, Qiang Cheng</p></li><li><p>Temporal Latent Autoencoder: A Method for Probabilistic Multivariate Time Series<br>Forecasting</p><p>  Nam Nguyen, Brian Quanz</p></li><li><p>Open-Set Recognition with Gaussian Mixture Variational Autoencoders</p><p>  Alexander Cao, Yuan Luo, Diego Klabjan</p></li><li><p>Unsupervised Learning of Discourse Structures Using a Tree Autoencoder</p><p>  Patrick Huber, Giuseppe Carenini</p></li></ul><h2 id="missing-value-amp-irregularly-sampled-time-series-Incomplete-imputation-â€¦"><a href="#missing-value-amp-irregularly-sampled-time-series-Incomplete-imputation-â€¦" class="headerlink" title="missing value &amp; irregularly sampled time series [Incomplete, imputation, â€¦]"></a>missing value &amp; irregularly sampled time series [Incomplete, imputation, â€¦]</h2><ul><li><p>Generative Semi-Supervised Learning for Multivariate Time Series Imputation</p><p>  Xiaoye Miao, Yangyang Wu, Jun Wang, Yunjun Gao, Xudong Mao, Jianwei Yin</p></li><li><p>Tripartite Collaborative Filtering with Observability and Selection for Debiasing Rating<br>Estimation on Missing-Not-at-Random Data</p><p>  Qi Zhang, Longbing Cao, Chongyang Shi, Liang Hu</p></li><li><p>Unified Tensor Framework for Incomplete Multi-View Clustering and Missing-View Inferring</p><p>  Jie Wen, Zheng Zhang, Zhao Zhang, Lei Zhu, Lunke Fei, Bob Zhang, Yong Xu</p></li><li><p>Quantification of Resource Production Incompleteness</p><p>  Yakoub Salhi</p></li><li><p><strong>ã€çœ‹ä¸€ä¸‹ã€‘</strong> Learning Representations for Incomplete Time Series Clustering</p><p>  Qianli Ma, Chuxin Chen, Sen Li, Garrison Cottrell</p></li><li><p>The Parameterized Complexity of Clustering Incomplete Data</p><p>  Eduard Eiben, Robert Ganian, Iyad Kanj, Sebastian Ordyniak, Stefan Szeider</p></li><li><p>Restricted Domains of Dichotomous Preferences with Possibly Incomplete Information</p><p>  Zoi Terzopoulou, Alexander Karpov, Svetlana Obraztsova</p></li><li><p>Estimating the Number of Induced Subgraphs from Incomplete Data and Neighborhood<br>Queries</p><p>  Dimitris Fotakis, Thanasis Pittas, Stratis Skoulakis</p></li></ul><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><p>è¿™éƒ¨åˆ†éƒ½å¯ä»¥çœ‹ä¸€ä¸‹</p><ul><li><p>Shuffling Recurrent Neural Networks</p><p>  Michael Rotman, Lior Wolf</p></li><li><p>Memory-Gated Recurrent Networks</p><p>  Yaquan Zhang, Qi Wu, Nanbo Peng, Min Dai, Jing Zhang, Hu Wang</p></li><li><p>On the Softmax Bottleneck of Recurrent Language Models</p><p>  Dwarak Govind Parthiban, Yongyi Mao, Diana Inkpen</p></li><li><p>Forecasting Reservoir Inflow via Recurrent Neural ODEs</p><p>  Fan Zhou, Liang Li</p></li></ul><h2 id="clustering"><a href="#clustering" class="headerlink" title="clustering"></a>clustering</h2><ul><li><p>Hierarchical Multiple Kernel Clustering</p><p>  Jiyuan Liu, Xinwang Liu, Siwei Wang, Sihang Zhou, Yuexiang Yang</p></li><li><p>Interpretable Clustering on Dynamic Graphs with Recurrent Graph Neural Networks</p><p>  Yuhang Yao, Carlee Joe-Wong</p></li><li><p>Clustering Ensemble Meets Low-Rank Tensor Approximation</p><p>  Yuheng Jia, Hui Liu, Junhui Hou, Qingfu Zhang</p></li><li><p>Contrastive Clustering</p><p>  Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, Xi Peng</p></li><li><p>GoT: a Growing Tree Model for Clustering Ensemble</p><p>  Feijiang Li, Yuhua Qian, Jieting Wang</p></li><li><p>Unified Tensor Framework for Incomplete Multi-View Clustering and Missing-View Inferring</p><p>  Jie Wen, Zheng Zhang, Zhao Zhang, Lei Zhu, Lunke Fei, Bob Zhang, Yong Xu</p></li><li><p>LRSC: Learning Representations for Subspace Clustering</p><p>  Changsheng Li, Chen Yang, Bo Liu, Ye Yuan, Guoren Wang</p></li><li><p>Automated Clustering of High-Dimensional Data with a Feature Weighted Mean-Shift<br>Algorithm</p><p>  Saptarshi Chakraborty, Debolina Paul, Swagatam Das</p></li><li><p>Learning Representations for Incomplete Time Series Clustering</p><p>  Qianli Ma, Chuxin Chen, Sen Li, Garrison Cottrell</p></li><li><p>Multiple Kernel Clustering with Kernel k-Means Coupled Graph Tensor Learning</p><p>  Zhenwen Ren, Quansen Sun, Dong Wei</p></li><li><p>Tri-Level Robust Clustering Ensemble with Multiple Graph Learning</p><p>  Peng Zhou, Liang Du, Yi-Dong Shen, Xuejun Li</p></li><li><p>Deep Mutual Information Maximin for Cross-Modal Clustering</p><p>  Yiqiao Mao, Xiaoqiang Yan, Qiang Guo, Yangdong Ye</p></li><li><p>Fairness, Semi-Supervised Learning, and More: A General Framework for Clustering with<br>Stochastic Pairwise Constraints</p><p>  Brian Brubach, Darshan Chakrabarti, John P Dickerson, Aravind Srinivasan, Leonidas Tsepenekas</p></li><li><p>Deep Fusion Clustering Network</p><p>  Wenxuan Tu, Sihang Zhou, Xinwang Liu, Xifeng Guo, Zhiping Cai, En Zhu, Jieren Cheng</p></li><li><p>The Parameterized Complexity of Clustering Incomplete Data</p><p>  Eduard Eiben, Robert Ganian, Iyad Kanj, Sebastian Ordyniak, Stefan Szeider</p></li><li><p>Objective-Based Hierarchical Clustering of Deep Embedding Vectors</p><p>  Dmitrii Avdiukhin, Stanislav Naumov, Grigory Yaroslavtsev</p></li><li><p>Variational Fair Clustering</p><p>  Imtiaz Masud Ziko, Jing Yuan, Eric Granger, Ismail Ben Ayed</p></li><li><p>Extreme k-Center Clustering</p><p>  MohammadHossein Bateni, Hossein Esfandiari, Manuela Fischer, Vahab Mirrokni</p></li><li><p>Differentially Private Clustering via Maximum Coverage</p><p>  Matthew Jones, Huy Nguyen, Thy D Nguyen</p></li></ul><h2 id="data-augmentation"><a href="#data-augmentation" class="headerlink" title="data augmentation"></a>data augmentation</h2><ul><li><p>AttaNet: Attention-Augmented Network for Fast and Accurate Scene Parsing</p><p>  Qi Song, Kangfu Mei, Rui Huang</p></li><li><p>How Does Data Augmentation Affect Privacy in Machine Learning?</p><p>  Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, Tie-Yan Liu</p></li><li><p>SnapMix: Semantically Proportional Mixing for Augmenting Fine-Grained Data</p><p>  Shaoli Huang, Xinchao Wang, Dacheng Tao</p></li><li><p>Inferring Emotion from Large-Scale Internet Voice Data: A Semi-Supervised Curriculum<br>Augmentation Based Deep Learning Approach</p><p>  Suping Zhou, Jia Jia, Zhiyong Wu, Zhihan Yang, Yanfeng Wang, Wei Chen, Fanbo Meng, Shuo<br>Huang, Jialie Shen, Xiaochuan Wang</p></li><li><p>Kernel-Convoluted Deep Neural Networks with Data Augmentation</p><p>  Minjin Kim, Young-geun Kim, Dongha Kim, Yongdai Kim, Myunghee Cho Paik</p></li><li><p>Improving Commonsense Causal Reasoning by Adversarial Training and Data Augmentation</p><p>  Ignacio Iacobacci, Ieva StaliÅ«naitÄ—, Philip John Gorinski</p></li><li><p>Self-Supervised Multi-View Stereo via Effective Co-Segmentation and Data-Augmentation</p><p>  Hongbin Xu, Zhipeng Zhou, Yu Qiao, Wenxiong Kang, Qiuxia Wu</p></li><li><p>Joint-Label Learning by Dual Augmentation for Time Series Classification</p><p>  Qianli Ma, Zhenjing Zheng, Jiawei Zheng, Sen Li, Wanqing Zhuang, Garrison Cottrell</p></li><li><p>Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-<br>Training</p><p>  Peng Shi, Patrick Ng, Zhiguo Wang, Henghui Zhu, Alexander Hanbo Li, Jun Wang, Cicero Nogueira<br>dos Santos, Bing Xiang</p></li><li><p>Two-Stream Convolution Augmented Transformer for Human Activity Recognition</p><p>  Bing Li, Wei Cui, Wei Wang, Le Zhang, Zhenghua Chen, Min Wu</p></li><li><p>Data Augmentation for Graph Neural Networks</p><p>  Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J Woodford, Meng Jiang, Neil Shah</p></li></ul><h2 id="About-distribution"><a href="#About-distribution" class="headerlink" title="About distribution"></a>About distribution</h2><ul><li><p>Many-to-One Distribution Learning and K-Nearest Neighbor Smoothing for Thoracic Disease<br>Identification</p><p>  Yi Zhou, Lei Huang, Tianfei Zhou, Ling Shao</p></li><li><p>Robust Lightweight Facial Expression Recognition Network with Label Distribution Training</p><p>  Zengqun Zhao, Qingshan Liu, Feng Zhou</p></li><li><p>Wasserstein Distributionally Robust Inverse Multiobjective Optimization</p><p>  Chaosheng Dong, Bo Zeng</p></li><li><p>The Gap on Gap: Tackling the Problem of Differing Data Distributions in Bias-Measuring<br>Datasets</p><p>  Vid Kocijan, Oana-Maria Camburu, Thomas Lukasiewicz</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in Other Top Conferences (2021)</title>
      <link href="/uncategorized/paperlistfile/otherconf_2021/"/>
      <url>/uncategorized/paperlistfile/otherconf_2021/</url>
      
        <content type="html"><![CDATA[<h2 id="SIGMOD-2021"><a href="#SIGMOD-2021" class="headerlink" title="SIGMOD 2021"></a>SIGMOD 2021</h2><p><a href="http://www.2021.sigmod.org/sigmod_research_list.shtml">link</a></p><span id="more"></span><h3 id="anomaly-detection"><a href="#anomaly-detection" class="headerlink" title="anomaly detection"></a>anomaly detection</h3><ul><li><p>Multiple Dynamic Outlier-Detection from a Data Stream by Exploiting Duality of Data and Queries</p><p>Susik Yoon (KAIST); Yooju Shin (KAIST); Jae-Gil Lee (KAIST)$^{\star}$; Byung Suk Lee (University of Vermont)</p></li><li><p>GPU-Accelerated Graph Label Propagation for Real-Time Fraud Detection</p><p>chang ye (Singapore Management University)$^{\star}$; Yuchen Li (Singapore Management University); Bingsheng He (National University of Singapore); Zhao Li (Alibaba Group); Jianling Sun (Zhejiang University)</p></li><li><p>Fast and Exact Outlier Detection in Metric Spaces: A Proximity Graph-based Approach</p><p>Daichi Amagata (Osaka University)$^{\star}$; Makoto Onizuka (Osaka University); Takahiro Hara (Osaka University, Japan)</p></li><li><p>RobustPeriod: Robust Time-Frequency Mining for Multiple Periodicity Detection</p><p>Qingsong Wen (Alibaba DAMO Academy)$^{\star}$; Kai He (Alibaba DAMO Academy); Liang Sun (Alibaba Group); Yingying Zhang (Alibaba Group); Min Ke (Alibaba Group); Huan Xu (Alibaba Group)</p></li></ul><ul><li>On Saving Outliers for Better Clustering over Noisy DataShaoxu Song (Tsinghua University)$^{\star}$; Fei Gao (Tsinghua University); Ruihong Huang (Tsinghua University); Yihan Wang (Tsinghua University)</li></ul><h3 id="causal-analysis"><a href="#causal-analysis" class="headerlink" title="causal analysis"></a>causal analysis</h3><ul><li>Clonos: Consistent Causal Recovery for Highly-Available Streaming Dataflows<br>Pedro Silvestre (TU Delft); Marios Fragkoulis (TU Delft)$^{\star}$; Diomidis Spinellis (TU Delft); Asterios Katsifodimos (TU Delft)</li></ul><h3 id="heterogeneous"><a href="#heterogeneous" class="headerlink" title="heterogeneous"></a>heterogeneous</h3><ul><li><p>EquiTensors: Learning Fair Integrations of Heterogeneous Urban Data</p><p>An Yan (University of Washington)$^{\star}$; Bill G Howe (University of Washington)</p></li><li><p>Heterogeneity-Aware Distributed Machine Learning Training via Partial Reduce</p><p>Xupeng Miao (Peking University)$^{\star}$; Xiaonan Nie (Peking University); Yingxia Shao (BUPT); Zhi Yang (Peking University); Jiawei Jiang (ETH Zurich); Lingxiao Ma (Peking University); Bin Cui (Peking University)</p></li></ul><h2 id="NDSS-2021"><a href="#NDSS-2021" class="headerlink" title="NDSS 2021"></a>NDSS 2021</h2><p>accept papers: <a href="https://www.ndss-symposium.org/ndss2021/">link</a></p><ul><li><p>Evading Voltage-Based Intrusion Detection on Automotive CAN</p><p>Rohit Bhatia (Purdue University); Vireshwar Kumar (Indian Institute of Technology Delhi); Khaled Serag and Z. Berkay Celik (Purdue University); Mathias Payer (EPFL); Dongyan Xu (Purdue University)</p></li><li><p>Differential Training: A Generic Framework to Reduce Label Noises for Android Malware Detection</p><p>Jiayun Xu (School of Information Systems, Singapore Management University, Singapore); Yingjiu Li (University of Oregon); Robert H. Deng (School of Information Systems, Singapore Management University, Singapore)</p></li></ul><h2 id="ESEC-FSE-2021-CCF-A"><a href="#ESEC-FSE-2021-CCF-A" class="headerlink" title="ESEC/FSE 2021 (CCF-A)"></a>ESEC/FSE 2021 (CCF-A)</h2><ul><li><p>Detecting and Localizing Keyboard Accessibility Failures in Web Applications</p><p>Paul T. Chiou, Ali S. Alotaibi, William G.J. Halfond</p></li></ul><ul><li><p>Explaining Mispredictions of ML Models</p><p>JÃ¼rgen Cito, Isil Dillig, Vijayaraghavan Murali, Seohyun Kim, Satish Chandra</p></li><li><p>Feature Trace Recording</p><p>Paul Maximilian Bittner, Alexander SchultheiÃŸ, Thomas ThÃ¼m, Timo Kehrer, Jeffrey M. Young, Lukas Linsbauer</p></li><li><p>Identifying Bad Software Changes via Multimodal Anomaly Detection for Online Service Systems</p><p>Nengwen Zhao, Junjie Chen, Zhaoyang Yu, Honglin Wang, Jiesong Li, Bin Qiu, Hongyu Xu, Wenchi Zhang, Kaixin Sui, Dan Pei</p></li></ul><h2 id="ICSE-2021-CCF-A"><a href="#ICSE-2021-CCF-A" class="headerlink" title="ICSE 2021 (CCF-A)"></a>ICSE 2021 (CCF-A)</h2><ul><li><p>AUTOTRAINER: An Automatic DNN Training Problem Detection and Repair SystemTechnical Track</p><p>Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Chao Shen</p></li><li><p>Interpretation-enabled Software Reuse Detection Based on a Multi-Level Birthmark ModelTechnical Track</p><p>Xi Xu, Qinghua Zheng, Zheng Yan, Ming Fan, Ang Jia, Ting Liu</p></li><li><p>Semi-supervised Log-based Anomaly Detection via Probabilistic Label EstimationArtifact ReusableTechnical TrackArtifact Available</p><p>Lin Yang, Junjie Chen, Zan Wang, Weijing Wang, Jiajun Jiang, Xuyuan Dong, Wenbin Zhang</p></li></ul><hr><h2 id="ASE-2021-CCF-A"><a href="#ASE-2021-CCF-A" class="headerlink" title="ASE 2021 (CCF-A)"></a>ASE 2021 (CCF-A)</h2><h2 id="ISSRE-2021"><a href="#ISSRE-2021" class="headerlink" title="ISSRE 2021"></a>ISSRE 2021</h2>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in NDSS 2020ï¼ˆå®‰å…¨é¢†åŸŸçš„é¡¶ä¼šï¼‰</title>
      <link href="/uncategorized/paperlistfile/NDSS2020/"/>
      <url>/uncategorized/paperlistfile/NDSS2020/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.ndss-symposium.org/ndss2020/accepted-papers/">Link</a></p><span id="more"></span><h2 id="Attack"><a href="#Attack" class="headerlink" title="Attack"></a>Attack</h2><ul><li><p>Practical Traffic Analysis Attacks on Secure Messaging Applications</p><p>Alireza Bahramali, Amir Houmansadr, Ramin Soltani, Dennis Goeckel, and Don Towsley (University of Massachusetts Amherst)</p></li></ul><h2 id="Traffic"><a href="#Traffic" class="headerlink" title="Traffic"></a>Traffic</h2><ul><li><p>Encrypted DNS â€“ Privacy? A Traffic Analysis Perspective</p><p>Sandra Siby (EPFL); Marc Juarez (University of Southern California); Claudia Diaz (imec-COSIC KU Leuven); Narseo Vallina-Rodriguez (IMDEA Networks Institute); Carmela Troncoso (EPFL)</p></li><li><p>FlowPrint: Semi-Supervised Mobile-App Fingerprinting on Encrypted Network Traffic</p><p>Thijs van Ede (University of Twente); Riccardo Bortolameotti (Bitdefender); Andrea Continella (UC Santa Barbara); Jingjing Ren and Daniel J. Dubois (Northeastern University); Martina Lindorfer (TU Wien); David Choffnes (Northeastern University); Maarten van Steen and Andreas Peter (University of Twente)</p></li><li><p>Practical Traffic Analysis Attacks on Secure Messaging Applications</p><p>Alireza Bahramali, Amir Houmansadr, Ramin Soltani, Dennis Goeckel, and Don Towsley (University of Massachusetts Amherst)</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Post-hoc interpretability</title>
      <link href="/uncategorized/surveys/interpretability/"/>
      <url>/uncategorized/surveys/interpretability/</url>
      
        <content type="html"><![CDATA[<h2 id="ç ”ç©¶é—®é¢˜æè¿°"><a href="#ç ”ç©¶é—®é¢˜æè¿°" class="headerlink" title="ç ”ç©¶é—®é¢˜æè¿°"></a>ç ”ç©¶é—®é¢˜æè¿°</h2><p>æ·±åº¦å­¦ä¹ æ¨¡å‹çš„äº‹åï¼ˆpost-hocï¼‰å¯è§£é‡Šæ–¹æ³•ï¼šç»™å®šä¸€ä¸ªå·²è®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå¦‚ä½•å¯¹å…¶è¾“å‡ºè¿›è¡Œè§£é‡Šã€‚</p><span id="more"></span><h2 id="é¢†åŸŸç°çŠ¶"><a href="#é¢†åŸŸç°çŠ¶" class="headerlink" title="é¢†åŸŸç°çŠ¶"></a>é¢†åŸŸç°çŠ¶</h2><h3 id="åŸºäºå åŠ çš„æ–¹æ³•-Superimposition-based-explanation"><a href="#åŸºäºå åŠ çš„æ–¹æ³•-Superimposition-based-explanation" class="headerlink" title="åŸºäºå åŠ çš„æ–¹æ³• (Superimposition-based explanation)"></a>åŸºäºå åŠ çš„æ–¹æ³• (Superimposition-based explanation)</h3><p>  è¿™ç±»æ–¹æ³•å°†ç½‘ç»œçš„è¾“å‡ºå½’å› åˆ°ç½‘ç»œçš„è¾“å…¥ä¸Šï¼Œæ˜¾å¼çš„æŒ‡å‡ºç½‘ç»œè¾“å…¥ä¸­çš„æ¯ä¸ªç»´åº¦å¯¹ç½‘ç»œè¾“å‡ºçš„è´¡çŒ®ç¨‹åº¦ï¼Œ</p><ul><li><p>ä¼˜ç‚¹ï¼šç›´è§‚</p></li><li><p>ç¼ºç‚¹ï¼šæœ‰æ—¶ä¼šç”Ÿæˆä»¤äººè¯¯è§£çš„è§£é‡Šï¼Œå¦‚å½“ç½‘ç»œå°†å·²çŸ¥çš„æ— å…³è¾“å…¥è§†ä¸ºé‡è¦åˆ¤æ–­ä¾æ®æ—¶</p></li></ul><p>  [1]LIMEï¼šé€šè¿‡å¯¹è¾“å…¥æ–½åŠ è½»å¾®çš„æ‰°åŠ¨ï¼Œä»¥æ¢æµ‹é»‘ç›’æ¨¡å‹çš„è¾“å‡ºå˜åŒ–ï¼Œä¼˜åŒ–ä¸€ä¸ªå¯è§£é‡Šæ¨¡å‹ï¼ˆçº¿æ€§æ¨¡å‹æˆ–tree-based modelï¼‰å±€éƒ¨è¿‘ä¼¼é»‘ç›’æ¨¡å‹çš„é¢„æµ‹ã€‚</p><p>  [2]SHAPï¼šåŸºäºåšå¼ˆç†è®ºï¼ˆshapley value, a game-theory based methodï¼‰ï¼Œè®¡ç®—ç½‘ç»œè¾“å…¥çš„æ¯ä¸€ç»´å¯¹ç½‘ç»œè¾“å‡ºçš„è´¡çŒ®ï¼ˆä¹Ÿæ˜¯å°†é»‘ç›’æ¨¡å‹åšå±€éƒ¨è¿‘æ‹Ÿï¼Œä½¿å…¶å…·æœ‰å¯è§£é‡Šæ€§ï¼‰</p><p>  [3]saliency mapï¼šè®­ç»ƒä¸€ä¸ªé®ç½©æ¨¡å‹ï¼ˆmasking modelï¼‰ä»¥è¯†åˆ«æœ€å½±å“åˆ†ç±»å™¨å†³æ–­çš„è¾“å…¥ç‰¹å¾ã€‚</p><p>  [4]Integrated Gradientsï¼šä»¥è¾“å…¥ç‰¹å¾åœ¨æŸä¸ªè·¯å¾„ä¸Šçš„æ¢¯åº¦ç§¯åˆ†ï¼ˆ Integrated Gradients ï¼‰ä½œä¸ºè¯¥ç‰¹å¾çš„é‡è¦æ€§å¾—åˆ†ã€‚</p><h3 id="åŸºäºä¾‹å­çš„æ–¹æ³•-example-based-explanation"><a href="#åŸºäºä¾‹å­çš„æ–¹æ³•-example-based-explanation" class="headerlink" title="åŸºäºä¾‹å­çš„æ–¹æ³• (example-based explanation)"></a>åŸºäºä¾‹å­çš„æ–¹æ³• (example-based explanation)</h3><p>  è¿™ç±»æ–¹æ³•é’ˆå¯¹æŸä¸ªå¾…è§£é‡Šçš„æ¡ˆä¾‹ï¼Œä¾ç…§æŸç§ç­–ç•¥ç”Ÿæˆä¸€ç»„æ¡ˆä¾‹ï¼ˆa set of exampleï¼‰ï¼Œè¿™ç»„æ¡ˆä¾‹ä¸€èˆ¬æ˜¯æ”¯æŒç½‘ç»œå¯¹å¾…è§£é‡Šæ¡ˆä¾‹åšå‡ºåˆ¤æ–­çš„ä¸»è¦ä¾æ®ã€‚é€šè¿‡äººç±»ç›´è§‚çš„å¯¹æ¯”è¿™ç»„æ¡ˆä¾‹ä¸å¾…è§£é‡Šçš„æ¡ˆä¾‹ï¼Œæ€»ç»“å‡ºçš„å·®å¼‚ä¸å…±åŒç‚¹å°†è¢«è§†ä¸ºä¸€ç§ç›´æ¥çš„è§£é‡Šã€‚</p><ul><li><p>ä¼˜ç‚¹ï¼šæ˜“äºç†è§£</p></li><li><p>ç¼ºç‚¹ï¼šä¾èµ–äºè®­ç»ƒé›†çš„è´¨é‡ä¸æ•°é‡</p><p>[10]: é€šè¿‡åœ¨è®­ç»ƒæ ·æœ¬ä¸Šæ–½åŠ ä¸€ä¸ªå¾®å°çš„æ‰°åŠ¨ï¼Œè§‚å¯Ÿè¯¥æ‰°åŠ¨å¯¹æ‰€è®­ç»ƒå‡ºçš„æ¨¡å‹æƒé‡çš„å½±å“ï¼Œä»¥æ­¤æ¥ç¡®å®šç½‘ç»œåœ¨å¯¹ç»™å®šæ ·æœ¬è¿›è¡Œé¢„æµ‹æ—¶ï¼Œæ˜¯å“ªäº›è®­ç»ƒæ ·æœ¬èµ·åˆ°äº†å†³å®šæ€§ä½œç”¨ã€‚</p><p>[11]: æœç´¢ç»™å®šæ ·æœ¬åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¯ä¸€å±‚ç‰¹å¾ç©ºé—´ä¸­çš„æœ€è¿‘é‚»ï¼Œå…¶é‚»å±…æ„æˆçš„åˆé›†å³ä¸ºç”¨äºè§£é‡Šç»™å®šæ ·æœ¬çš„è®­ç»ƒé›†ï¼ˆç»™å®šæ ·æœ¬ä¸è¯¥é›†åˆä¸­çš„æ ·æœ¬å…±æ€§å³ä¸ºä»–ä»¬è·å¾—ç›¸åŒæ ‡ç­¾çš„åŸå› ï¼‰ã€‚</p></li></ul><hr><h2 id="ä»£è¡¨æ€§è®ºæ–‡10ç¯‡"><a href="#ä»£è¡¨æ€§è®ºæ–‡10ç¯‡" class="headerlink" title="ä»£è¡¨æ€§è®ºæ–‡10ç¯‡"></a>ä»£è¡¨æ€§è®ºæ–‡10ç¯‡</h2><h3 id="åŸºäºå åŠ çš„æ–¹æ³•"><a href="#åŸºäºå åŠ çš„æ–¹æ³•" class="headerlink" title="åŸºäºå åŠ çš„æ–¹æ³•"></a>åŸºäºå åŠ çš„æ–¹æ³•</h3><p>  <strong>ç»å…¸æ–¹æ³•</strong></p><p>  [1]: Marco TÃºlio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. â€œWhy Should I Trust You?â€: Explaining the Predictions of Any Classifier. In SIGKDD. ACM, 1135â€“1144. (LIME)</p><p>  [2]: Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In NeurIPS. 4765â€“4774. (SHAP)</p><p>  [3]: Piotr Dabkowski and Yarin Gal. 2017. Real Time Image Saliency for Black Box Classifiers. In NeurIPS. 6967â€“6976. (saliency map)</p><p>  [4]: Sundararajan, M., Taly, A., &amp; Yan, Q. (2017, August). Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 3319-3328).</p><p>  <strong>å…¶ä»–å·¥ä½œ</strong></p><p>  [5]: Ismail, A., Gunady, M., Bravo, H., &amp; Feizi, S. (2020). Benchmarking Deep Learning Interpretability in Time Series Predictions. Advances in Neural Information Processing Systems Foundation (NeurIPS).</p><p>  [6]: Giurgiu, I., &amp; Schumann, A. (2019). Explainable failure predictions with rnn classifiers based on time series data. arXiv preprint arXiv:1901.08554.</p><p>  [7]: Mujkanovic, F., DoskoÄ, V., Schirneck, M., SchÃ¤fer, P., &amp; Friedrich, T. (2020). timeXplainâ€“A Framework for Explaining the Predictions of Time Series Classifiers. arXiv preprint arXiv:2007.07606.</p><p>  [8]: Nguyen, T. T., Le Nguyen, T., &amp; Ifrim, G. (2020, September). A Model-Agnostic Approach to Quantifying the Informativeness of Explanation Methods for Time Series Classification. In International Workshop on Advanced Analytics and Learning on Temporal Data (pp. 77-94). Springer, Cham.</p><p>  [9]: Shankaranarayana, S. M., &amp; Runje, D. (2019, November). ALIME: Autoencoder based approach for local interpretability. In International Conference on Intelligent Data Engineering and Automated Learning (pp. 454-463). Springer, Cham.</p><h3 id="åŸºäºä¾‹å­çš„æ–¹æ³•"><a href="#åŸºäºä¾‹å­çš„æ–¹æ³•" class="headerlink" title="åŸºäºä¾‹å­çš„æ–¹æ³•"></a>åŸºäºä¾‹å­çš„æ–¹æ³•</h3><p>  <strong>ç»å…¸æ–¹æ³•</strong></p><p>  [10]: Koh, P. W., &amp; Liang, P. (2017, July). Understanding Black-box Predictions via Influence Functions. In International Conference on Machine Learning (pp. 1885-1894).</p><p>  [11]: Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765, 2018.</p><p>  <strong>å…¶ä»–å·¥ä½œ</strong></p><p>  [12]: Kim, B., Rudin, C., &amp; Shah, J. A. (2014). The bayesian case model: A generative approach for case-based reasoning and prototype classification. Advances in neural information processing systems, 27, 1952-1960.</p><p>  [13]: Jeyakumar, J. V., Noor, J., Cheng, Y. H., Garcia, L., &amp; Srivastava, M. (2020). How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods. Advances in Neural Information Processing Systems, 33.</p><p>  [14]: Papernot, N., &amp; McDaniel, P. (2018). Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765.</p><p>  [15]: Ming, Y., Xu, P., Qu, H., &amp; Ren, L. (2019, July). Interpretable and steerable sequence learning via prototypes. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (pp. 903-913).</p><p>  [16]: Ma, D., Wang, Z., Xie, J., Guo, B., &amp; Yu, Z. (2020, November). Interpretable Multivariate Time Series Classification Based on Prototype Learning. In International Conference on Green, Pervasive, and Cloud Computing (pp. 205-216). Springer, Cham.</p><p>  [17]: Keane, M. T., &amp; Kenny, E. M. (2019, September). How case-based reasoning explains neural networks: A theoretical analysis of XAI using post-hoc explanation-by-example from a survey of ANN-CBR twin-systems. In International Conference on Case-Based Reasoning (pp. 155-171). Springer, Cham.</p><h2 id="ç»å…¸è®ºæ–‡orå¼ºç›¸å…³è®ºæ–‡"><a href="#ç»å…¸è®ºæ–‡orå¼ºç›¸å…³è®ºæ–‡" class="headerlink" title="ç»å…¸è®ºæ–‡orå¼ºç›¸å…³è®ºæ–‡"></a>ç»å…¸è®ºæ–‡orå¼ºç›¸å…³è®ºæ–‡</h2><p>  [11]: Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765, 2018.</p><p>  [13]: Jeyakumar, J. V., Noor, J., Cheng, Y. H., Garcia, L., &amp; Srivastava, M. (2020). How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods. Advances in Neural Information Processing Systems, 33.</p><p>  <strong>å¼‚åŒç‚¹</strong></p><p>  [11, 13]: åˆ†æçš„æ˜¯æœ‰ç›‘ç£åˆ†ç±»æ¨¡å‹ä¸­çš„åŸºäºæ¡ˆä¾‹çš„å¯è§£é‡Šé—®é¢˜ï¼Œä½†ä»…èƒ½è¯´æ˜æŸæ ·æœ¬ä¸ºä½•è¢«åˆ¤å®šä¸ºæŸç±»ï¼Œè€Œä¸èƒ½åšå‡ºåäº‹å®ï¼ˆcounterfactual:ï¼‰è§£é‡Šï¼Œå³â€œæŸæ ·æœ¬ä¸ºä½•ä¸æ˜¯æŸç±»â€ã€‚æ— æ³•ä»ä¸­æå–å‡ºå¯ä»¥ç”¨äºåˆ†ç±»çš„è¯­ä¹‰ä¿¡æ¯</p><p>  [11, 13]: å’Œæˆ‘ä»¬çš„å·¥ä½œå‡ä»éšç©ºé—´å…¥æ‰‹ï¼Œåˆ†æå¾…æµ‹æ ·æœ¬çš„é‚»å±…ï¼Œè€Œæˆ‘ä»¬è¿›ä¸€æ­¥çš„åˆ†æäº†å¾…æµ‹æ ·æœ¬ä¸å…¶é‚»å±…ã€èšç±»ä¸­å¿ƒæ ·æœ¬ ä¹‹é—´çš„å·®å¼‚ï¼Œä»¥å¸®åŠ©æˆ‘ä»¬æ€»ç»“æœ‰åŠ©äºåŒºåˆ†æ­£å¼‚å¸¸çš„è¯­ä¹‰ä¿¡æ¯ã€‚</p>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
            <tag> survey </tag>
            
            <tag> interpretability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Self-explaining DNN</title>
      <link href="/uncategorized/surveys/interpretable_DNN/"/>
      <url>/uncategorized/surveys/interpretable_DNN/</url>
      
        <content type="html"><![CDATA[<p>key words: Self-explaining models</p><span id="more"></span><h2 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h2><ol><li><blockquote><p><a href="#refer-anchor-1"><sup>1</sup></a> the recently-proposed layer-wise relevance propagation (LRP) algorithm from Wojciech Samekâ€™s group [28], [29] uses the fact that the individual neural network units are differentiable to decompose the network output in terms of its input variables. It is a principled method that has a close relationship to Taylor decomposition and is applicable to arbitrary deep neural network architectures [30]</p><blockquote><ul><li><p>[28] A. Binder, G. Montavon, S. Bach, K. MÃ¼ller and W. Samek, â€œLayer-wise relevance propagation for neural networks with local renormalization layersâ€, CoRR, vol. abs/1604. 00825, 2016</p></li><li><p>[29] A. Binder, S. Bach, G. Montavon, K.-R. MÃ¼ller and W. Samek, Layer-Wise Relevance Propagation for Deep Neural Network Architectures, Springer, 2016.</p></li><li><p>[30] G. Montavon, S. Lapuschkin, A. Binder, W. Samek and K.-R. MÃ¼ller, â€œExplaining nonlinear classification decisions with deep taylor decompositionâ€, Pattern Recognition, vol. 65, pp. 211-222, 2017.</p></li></ul></blockquote></blockquote></li><li><p>Alvarez-Melis, D., &amp; Jaakkola, T. S. (2018, December). Towards robust interpretability with self-explaining neural networks. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (pp. 7786-7795).</p></li></ol><blockquote><p>We achieve this with a regularization scheme that ensures our model not only looks like a linear model, but (locally) behaves like one.<br>we start with a simple linear regression model and successively generalize<br>it towards the class of self-explaining models.<br>We progressively generalize the model in the following subsections and<br>discuss how this mechanism of interpretation is preserved.</p></blockquote><ol start="3"><li><blockquote><p><a href="#refer-anchor-2"><sup>2</sup></a> Examples of self-explainable models include simple linear<br>models, or specific nonlinear models, e.g. neural networks with<br>an explicit top-level sum-pooling structure [143], [111], [28],<br>[206], [25]. In all of these models, each summand is linked<br>only to one of a few input variables, which makes attribution<br>of their prediction on the input variables straightforward.</p><blockquote><ul><li>[25] W. Brendel and M. Bethge, â€œApproximating CNNs with bag-of-local-features models works surprisingly well on imagenetâ€, Proc. 7th Int. Conf. Learn. Represent., 2019.</li><li>[28] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm and N. Elhadad, â€œIntelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmissionâ€, Proc. 21th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, pp. 1721-1730, 2015.</li><li>[111] M. Lin, Q. Chen and S. Yan, â€œNetwork in networkâ€, Proc. Int. Conf. Learn. Represent. (ICLR), 2014.</li><li>[143] B. Poulin et al., â€œVisual explanation of evidence with additive classifiersâ€, Proc. 21st Nat. Conf. Artif. Intell. 18th Innov. Appl. Artif. Intell. Conf., pp. 1822-1829, 2006.</li><li>[206] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva and A. Torralba, â€œLearning deep features for discriminative localizationâ€, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2921-2929, Jun. 2016.</li></ul></blockquote></blockquote></li><li><p>Agarwal, R., Frosst, N., Zhang, X., Caruana, R., &amp; Hinton, G. E. (2020). Neural additive models: Interpretable machine learning with neural nets. arXiv preprint arXiv:2004.13912.</p></li></ol><blockquote><p>In this paper, we make restrictions on the structure of neural networks, which yields a family of models called Neural<br>Additive Models (NAMs), that are inherently interpretable while suffering little loss in prediction accuracy when applied to tabular data.</p><p>â€¦</p><p>NAMs belong to a larger model family called Generalized Additive Models<br>(GAMs)</p></blockquote><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><ol start="5"><li><blockquote><p><a href="#refer-anchor-2"><sup>2</sup></a> extensions of LRP have been proposed<br>to deal with the special LSTM blocks in recurrent neural<br>networks [11], [9]</p><blockquote><ul><li>[9] L. Arras et al., â€œExplaining and interpreting LSTMsâ€ in Explainable AI: Interpreting Explaining and Visualizing Deep Learning, Cham, Switzerland:Springer, vol. 11700, pp. 211-238, 2019.</li><li>[11] L. Arras, G. Montavon, K.-R. MÃ¼ller and W. Samek, â€œExplaining recurrent neural network predictions in sentiment analysisâ€, Proc. 8th Workshop Comput. Approaches to Subjectivity Sentiment Social Media Anal., pp. 159-168, 2017.</li></ul></blockquote></blockquote></li></ol><ol start="6"><li>MarcinkeviÄs, R., &amp; Vogt, J. E. (2021). Interpretable Models for Granger Causality Using Self-explaining Neural Networks. ICLR 2021.</li></ol><blockquote><p>We extend self-explaining neural network models (Alvarez-Melis &amp; Jaakkola, 2018) to<br>time series analysis. The resulting autoregressive model, named generalised vector autore-gression (GVAR), is interpretable and allows exploring GC relations between variables, signs of Granger-causal effects, and their variability through time.</p></blockquote><h1 id="Reference-List-of-Surveys"><a href="#Reference-List-of-Surveys" class="headerlink" title="Reference: List of Surveys"></a>Reference: List of Surveys</h1><div id="refer-anchor-1"></div>[1] Chakraborty, S., Tomsett, R., Raghavendra, R., Harborne, D., Alzantot, M., Cerutti, F., ... & Gurram, P. (2017, August). Interpretability of deep learning models: a survey of results. In 2017 IEEE smartworld, ubiquitous intelligence & computing, advanced & trusted computed, scalable computing & communications, cloud & big data computing, Internet of people and smart city innovation (smartworld/SCALCOM/UIC/ATC/CBDcom/IOP/SCI) (pp. 1-6). IEEE.<div id="refer-anchor-2"></div>[2] Samek, W., Montavon, G., Lapuschkin, S., Anders, C. J., & MÃ¼ller, K. R. (2021). Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications. Proceedings of the IEEE, 109(3), 247-278.]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
            <tag> survey </tag>
            
            <tag> interpretability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in NeuralIPS 2020 (2020.12.08)</title>
      <link href="/uncategorized/paperlistfile/NIPS2020/"/>
      <url>/uncategorized/paperlistfile/NIPS2020/</url>
      
        <content type="html"><![CDATA[<p>Accept paper list: <a href="https://neurips.cc/Conferences/2020/AcceptedPapersInitial">link</a></p><span id="more"></span><h2 id="anomaly-detection"><a href="#anomaly-detection" class="headerlink" title="anomaly detection"></a>anomaly detection</h2><ul><li><p>Energy-based Out-of-distribution Detection</p><p>  Weitang Liu (UC San Diego) Â· Xiaoyun Wang (University of California, Davis) Â· John Owens (University of California, Davis) Â· Sharon Yixuan Li (Stanford University)</p></li></ul><ul><li><p>Provable Worst Case Guarantees for the Detection of Out-of-distribution Data</p><p>  Julian Bitterwolf (University of TÃ¼bingen) Â· Alexander Meinke (University of TÃ¼bingen) Â· Matthias Hein (University of TÃ¼bingen)</p></li></ul><ul><li><p>Likelihood Regret: An Out-of-Distribution Detection Score For Variational Auto-encoder</p><p>  Zhisheng Xiao (The University of Chicago) Â· Qing Yan (University of Chicago) Â· Yali Amit (University of Chicago)</p></li></ul><ul><li><p>Why Normalizing Flows Fail to Detect Out-of-Distribution Data</p><p>  Polina Kirichenko (New York University) Â· Pavel Izmailov (New York University) Â· Andrew Gordon Wilson (New York University)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹5ã€‘</strong> Towards Maximizing the Representation Gap between In-Domain &amp; Out-of-Distribution Examples</p><p>  Jay Nandy (National University of Singapore) Â· Wynne Hsu (National University of Singapore) Â· Mong Li Lee (National University of Singapore)</p></li></ul><ul><li><p>On the Value of Out-of-Distribution Testing: An Example of Goodhartâ€™s Law</p><p>  Damien Teney (University of Adelaide) Â· Ehsan Abbasnejad (University of Adelaide) Â· Kushal Kafle (Adobe Research) Â· Robik Shrestha (Rochester Institute of Technology) Â· Christopher Kanan (PAIGE.AI / RIT / CornellTech) Â· Anton van den Hengel (University of Adelaide)</p></li></ul><ul><li><p>Understanding Anomaly Detection with Deep Invertible Networks through Hierarchies of Distributions and Features</p><p>  Robin T Schirrmeister (University Medical Center Freiburg) Â· Yuxuan Zhou (Stuttgart University) Â· Tonio Ball (Albert-Ludwigs-University) Â· Dan Zhang (Bosch Center for Artificial Intelligence)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹1ã€‘</strong> Timeseries Anomaly Detection using Temporal Hierarchical One-Class Network</p><p>  Lifeng Shen (The Hong Kong University of Science and Technology) Â· Zhuocong Li (Tencent) Â· James Kwok (Hong Kong University of Science and Technology)</p></li></ul><ul><li><p>CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances</p><p>  Jihoon Tack (KAIST) Â· Sangwoo Mo (KAIST) Â· Jongheon Jeong (KAIST) Â· Jinwoo Shin (KAIST)</p></li><li><p><strong>ã€çœ‹çœ‹4ã€‘</strong> One Ring to Rule Them All: Certifiably Robust Geometric Perception with Outliers</p><p>  Heng Yang Â· Luca Carlone</p></li><li><p><strong>ã€çœ‹çœ‹3ã€‘</strong> Outlier Robust Mean Estimation with Subgaussian Rates via Stability</p><p>  Ilias Diakonikolas Â· Daniel M. Kane Â· Ankit Pensia</p></li><li><p><strong>ã€çœ‹çœ‹2ã€‘</strong> Further Analysis of Outlier Detection with Deep Generative Models</p><p>  Ziyu Wang Â· Bin Dai Â· David P Wipf Â· Jun Zhu</p></li></ul><h2 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h2><ul><li><p>Probabilistic Time Series Forecasting with Shape and Temporal Diversity</p><p>  Vincent LE GUEN (CNAM, Paris, France) Â· Nicolas THOME (Cnam (Conservatoire national des arts et mÃ©tiers))</p></li><li><p><strong>ã€çœ‹çœ‹ts-5ã€‘</strong> Deep reconstruction of strange attractors from time series</p><p>  William Gilpin (Harvard University)</p></li></ul><ul><li><p>Deep Energy-based Modeling of Discrete-Time Physics</p><p>  Takashi Matsubara (Osaka University) Â· Ai Ishikawa (Kobe University) Â· Takaharu Yaguchi (Kobe University)</p></li><li><p><strong>ã€çœ‹çœ‹ts-3ã€‘</strong> Neural Controlled Differential Equations for Irregular Time Series</p><p>  Patrick Kidger (University of Oxford) Â· James Morrill (University of Oxford) Â· James Foster (University of Oxford) Â· Terry Lyons (University of Oxford)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹ts-4ã€‘</strong> Adversarial Sparse Transformer for Time Series Forecasting</p><p>  Sifan Wu (Tsinghua University) Â· Xi Xiao (Tsinghua University) Â· Qianggang Ding (Tsinghua University) Â· Peilin Zhao (Tencent AI Lab) Â· Ying Wei (Tencent AI Lab) Â· Junzhou Huang (University of Texas at Arlington / Tencent AI Lab)</p></li><li><p><strong>ã€çœ‹çœ‹ts-2ã€‘</strong> Learning Long-Term Dependencies in Irregularly-Sampled Time Series</p><p>  Mathias Lechner (IST Austria) Â· Ramin Hasani (MIT)</p></li><li><p><strong>ã€çœ‹çœ‹ts-1ã€‘</strong> Benchmarking Deep Learning Interpretability in Time Series Predictions</p><p>  Aya Abdelsalam Ismail (University of Maryland) Â· Mohamed Gunady (University of Maryland) Â· Hector Corrada Bravo (University of Maryland) Â· Soheil Feizi (University of Maryland)</p></li><li><p>High-recall causal discovery for autocorrelated time series with latent confounders</p><p>  Andreas Gerhardus (German Aerospace Center (DLR)) Â· Jakob Runge (Institute of Data Science, German Aerospace Center (DLR))</p></li></ul><ul><li><p>Deep Rao-Blackwellised Particle Filters for Time Series Forecasting</p><p>  Richard Kurle (Volkswagen Group) Â· Syama Sundar Rangapuram (Amazon Research) Â· Emmanuel de BÃ©zenac (Sorbonne UniversitÃ©) Â· Stephan GÃ¼nnemann (Technical University of Munich) Â· Jan Gasthaus (Amazon.com)</p></li></ul><ul><li><p>Normalizing Kalman Filters for Multivariate Time Series Analysis</p><p>  Emmanuel de BÃ©zenac (Sorbonne UniversitÃ©) Â· Syama Sundar Rangapuram (Amazon Research) Â· Konstantinos Benidis (Amazon Research) Â· Michael Bohlke-Schneider (Amazon) Â· Lorenzo Stella (Amazon Research) Â· Hilaf Hasson (Amazon Research) Â· Richard Kurle (Volkswagen Group) Â· Tim Januschowski (Amazon Research) Â· Patrick Gallinari (Sorbonne University &amp; Criteo AI Lab, Paris)</p></li></ul><ul><li><p>User-Dependent Neural Sequence Models for Continuous-Time Event Data</p><p>  Alex Boyd (UC Irvine) Â· Robert Bamler (University of California at Irvine) Â· Stephan Mandt (University of California, Irivine) Â· Padhraic Smyth (University of California, Irvine)</p></li></ul><h2 id="About-distribution"><a href="#About-distribution" class="headerlink" title="About distribution"></a>About distribution</h2><ul><li><p>Distribution-free binary classification: prediction sets, confidence intervals and calibration</p><p>  Chirag Gupta (Carnegie Mellon University) Â· Aleksandr Podkopaev (Carnegie Mellon University) Â· Aaditya Ramdas (CMU)</p></li></ul><ul><li><p>Deep Diffusion-Invariant Wasserstein Distributional Classification</p><p>  Sung Woo Park+ (Chung-Ang University) Â· Dong Wook Shu (Chung-Ang Univ., Korea) Â· Junseok Kwon (Chung-Ang Univ., Korea)</p></li></ul><ul><li><p>OOD-MAML: Meta-Learning for Few-Shot Out-of-Distribution Detection and Classification</p><p>  Taewon Jeong (KAIST) Â· Heeyoung Kim (KAIST)</p></li></ul><ul><li><p>Understanding Anomaly Detection with Deep Invertible Networks through Hierarchies of Distributions and Features</p><p>  Robin T Schirrmeister (University Medical Center Freiburg) Â· Yuxuan Zhou (Stuttgart University) Â· Tonio Ball (Albert-Ludwigs-University) Â· Dan Zhang (Bosch Center for Artificial Intelligence)</p></li></ul><ul><li><p>Measuring Robustness to Natural Distribution Shifts in Image Classification</p><p>  Rohan Taori (University of California, Berkeley) Â· Achal Dave (Carnegie Mellon University) Â· Vaishaal Shankar (UC Berkeley) Â· Nicholas Carlini (Google) Â· Benjamin Recht (UC Berkeley) Â· Ludwig Schmidt (UC Berkeley)</p></li></ul><ul><li><p>Fast Epigraphical Projection-based Incremental Algorithms for Wasserstein Distributionally Robust Support Vector Machine</p><p>  Jiajin Li (The Chinese University of Hong Kong) Â· Caihua Chen (Nanjing University) Â· Anthony Man-Cho So (CUHK)</p></li></ul><ul><li><p>Adversarial Distributional Training for Robust Deep Learning</p><p>  Yinpeng Dong (Tsinghua University) Â· Zhijie Deng (Tsinghua University) Â· Tianyu Pang (Tsinghua University) Â· Hang Su (Tsinghua Univiersity) Â· Jun Zhu (Tsinghua University)</p></li></ul><ul><li><p>Mix and Match: An Optimistic Tree-Search Approach for Learning Models from Mixture Distributions</p><p>  Matthew Faw (University of Texas at Austin) Â· Rajat Sen (Amazon) Â· Karthikeyan Shanmugam (IBM Research, NY) Â· Constantine Caramanis (UT Austin) Â· Sanjay Shakkottai (University of Texas at Austin)</p></li></ul><ul><li><p>Distributionally Robust Parametric Maximum Likelihood Estimation</p><p>  Viet Anh Nguyen (Stanford University) Â· Xuhui Zhang (Stanford University) Â· Jose Blanchet (Stanford University) Â· Angelos Georghiou (University of Cyprus)</p></li></ul><ul><li><p>Distributionally Robust Local Non-parametric Conditional Estimation</p><p>  Viet Anh Nguyen (Stanford University) Â· Fan Zhang (Stanford University) Â· Jose Blanchet (Stanford University) Â· Erick Delage (HEC MontrÃ©al) Â· Yinyu Ye (Standord)</p></li></ul><ul><li><p>Large-Scale Methods for Distributionally Robust Optimization</p><p>  Daniel Levy (Stanford University) Â· Yair Carmon (Stanford University) Â· John Duchi (Stanford) Â· Aaron Sidford (Stanford)</p></li></ul><ul><li><p>Efficient Distance Approximation for Structured High-Dimensional Distributions via Learning</p><p>  Arnab Bhattacharyya (National University of Singapore) Â· Sutanu Gayen (National University of SIngapore) Â· Kuldeep S Meel (National University of Singapore) Â· N. V.     Vinodchandran (University of Nebraska)</p></li></ul><ul><li><p>Analytical Probability Distributions and EM-Learning for Deep Generative Networks</p><p>  Randall Balestriero (Rice University) Â· Sebastien PARIS (University of Toulon) Â· Richard Baraniuk (Rice University)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹dis-1ã€‘</strong> Learning Structured Distributions From Untrusted Batches: Faster and Simpler</p><p>  Sitan Chen (MIT) Â· Jerry Li (Microsoft) Â· Ankur Moitra (MIT)</p></li></ul><ul><li><p>Linear-Sample Learning of Low-Rank Distributions</p><p>  Ayush Jain (UC San Diego) Â· Alon Orlitsky (University of California, San Diego)</p></li></ul><ul><li><p>Profile Entropy: A Fundamental Measure for the Learnability and Compressibility of Distributions</p><p>  Yi Hao (University of California, San Diego) Â· Alon Orlitsky (University of California, San Diego)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹dis-2ã€‘</strong> SURF: A Simple, Universal, Robust, Fast Distribution Learning Algorithm</p><p>  Yi Hao (University of California, San Diego) Â· Ayush Jain (UC San Diego) Â· Alon Orlitsky (University of California, San Diego) Â· Vaishakh Ravindrakumar (UC San Diego)</p></li></ul><ul><li><p>Learning discrete distributions with infinite support</p><p>  Doron Cohen (Ben-Gurion University of the Negev) Â· Aryeh Kontorovich (Ben Gurion University) Â· Geoï¬€rey Wolfer (Ben-Gurion University of the Negev)</p></li></ul><ul><li><p>Optimal Private Median Estimation under Minimal Distributional Assumptions</p><p>  Christos Tzamos (UW-Madison) Â· Emmanouil-Vasileios Vlatakis-Gkaragkounis (Columbia University) Â· Ilias Zadik (NYU)</p></li></ul><h2 id="missing-value-amp-irregularly-sampled-time-series"><a href="#missing-value-amp-irregularly-sampled-time-series" class="headerlink" title="missing value &amp; irregularly sampled time series"></a>missing value &amp; irregularly sampled time series</h2><ul><li><p>Estimation and Imputation in Probabilistic Principal Component Analysis with Missing Not At Random Data</p><p>  Aude Sportisse (Sorbonne University, Ecole Polytechnique) Â· Claire Boyer (LPSM, Sorbonne UniversitÃ©) Â· Julie Josses (CMAP / CNRS)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹missing-2ã€‘</strong> Learning Disentangled Representations of Videos with Missing Data</p><p>  Armand Comas (Northeastern University) Â· Chi Zhang (Northeastern University) Â· Zlatan Feric (Northeastern University) Â· Octavia Camps (Northeastern University) Â· Rose Yu (University of California, San Diego)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹missing-3ã€‘</strong> Debiasing Averaged Stochastic Gradient Descent to handle missing values</p><p>  Aude Sportisse (Sorbonne University, Ecole Polytechnique) Â· Claire Boyer (LPSM, Sorbonne UniversitÃ©) Â· Aymeric Dieuleveut (Ecole Polytechnique, IPParis) Â· Julie Josses (CMAP / CNRS)</p></li></ul><ul><li><p>Handling Missing Data with Graph Representation Learning</p><p>  Jiaxuan You (Stanford University) Â· Xiaobai Ma (Stanford University) Â· Yi Ding (Stanford University) Â· Mykel J Kochenderfer (Stanford University) Â· Jure Leskovec (Stanford University and Pinterest)</p></li></ul><ul><li><p>A Functional EM Algorithm for Panel Count Data with Missing Counts</p><p>  Alexander Moreno (Georgia Institute of Technology) Â· Zhenke Wu (University of Michigan) Â· Jamie Roslyn Yap (University of Michigan) Â· Cho Lam (University of Utah) Â· David Wetter (University of Utah) Â· Inbal Nahum-Shani (University of Michigan) Â· Walter Dempsey (University of Michigan) Â· James M Rehg (Georgia Tech)</p></li></ul><ul><li><p>NeuMiss networks: differentiable programming for supervised learning with missing values.</p><p>  Marine Le Morvan (INRIA) Â· Julie Josses (CMAP / CNRS) Â· Thomas Moreau (Inria) Â· Erwan Scornet (Ecole Polytechnique) Â· Gael Varoquaux (Parietal Team, INRIA)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹missing-1ã€‘</strong> Learning Continuous System Dynamics from Irregularly-Sampled Partial Observations</p><p>  Zijie Huang (University of California, Los Angeles) Â· Yizhou Sun (UCLA) Â· Wei Wang (UCLA)</p></li></ul><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><ul><li><p>Convolutional Tensor-Train LSTM for Spatio-Temporal Learning</p><p>  Jiahao Su (University of Maryland) Â· Wonmin Byeon (NVIDIA Research) Â· Jean Kossaifi (NVIDIA) Â· Furong Huang (University of Maryland) Â· Jan Kautz (NVIDIA) Â· Anima Anandkumar (NVIDIA / Caltech)</p></li></ul><ul><li><p>RNNPool: Efficient Non-linear Pooling for RAM Constrained Inference</p><p>  Oindrila Saha (Microsoft Research) Â· Aditya Kusupati (University of Washington) Â· Harsha Vardhan Simhadri (Microsoft Research) Â· Manik Varma (Microsoft Research India) Â· Prateek Jain (Microsoft Research)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹other-3ã€‘</strong> The interplay between randomness and structure during learning in RNNs</p><p>  Friedrich Schuessler (Technion) Â· Francesca Mastrogiuseppe (UCL) Â· Alexis Dubreuil (ENS) Â· Srdjan Ostojic (Ecole Normale Superieure) Â· Omri Barak (Technion - Israeli institute of technology)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹other-4ã€‘</strong> HiPPO: Recurrent Memory with Optimal Polynomial Projections</p><p>  Albert Gu (Stanford) Â· Tri Dao (Stanford University) Â· Stefano Ermon (Stanford) Â· Atri Rudra (University at Buffalo, SUNY) Â· Christopher RÃ© (Stanford)</p></li></ul><ul><li><p>RATT: Recurrent Attention to Transient Tasks for Continual Image Captioning</p><p>  Riccardo Del Chiaro (University of Florence) Â· BartÅ‚omiej Twardowski (Computer Vision Center, UAB) Â· Andrew D Bagdanov (University of Florence) Â· Joost van de Weijer (Computer Vision Center Barcelona)</p></li></ul><ul><li><p>MomentumRNN: Integrating Momentum into Recurrent Neural Networks</p><p>  Tan Nguyen (Rice University/UCLA) Â· Richard Baraniuk (Rice University) Â· Andrea Bertozzi (UCLA) Â· Stanley Osher (UCLA) Â· Bao Wang (UCLA)</p></li></ul><ul><li><p>Recurrent Random Networks as Optimized Kernel Machines</p><p>  Sandra Nestler (Juelich Research Centre) Â· Christian Keup (Juelich Research Centre) Â· David Dahmen (JÃ¼lich Research Centre) Â· Matthieu Gilson (Juelich Forschungszentrum) Â· Holger Rauhut (RWTH Aachen University) Â· Moritz Helias (Juelich Research Centre)</p></li></ul><ul><li><p>Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting</p><p>  LEI BAI (UNSW, Sydney) Â· Lina Yao (University of New South Wales) Â· Can Li (University of New South Wales) Â· Xianzhi Wang (University of Technology Sydney) Â· Can Wang (Griffith University)</p></li></ul><ul><li><p>Using noise to probe recurrent neural network structure and prune synapses</p><p>  Eli Moore (University of California, Davis) Â· Rishidev Chaudhuri (University of California, Davis)</p></li></ul><ul><li><p>Regularizing Towards Permutation Invariance In Recurrent Models</p><p>  Edo Cohen-Karlik (Tel Aviv University) Â· Avichai Ben David (Tel Aviv University) Â· Amir Globerson (Tel Aviv University, Google)</p></li></ul><ul><li><p>STLnet: Signal Temporal Logic Enforced Multivariate Recurrent Neural Networks</p><p>  Meiyi Ma (University of Virginia) Â· Ji Gao (University of Virginia) Â· Lu Feng (University of Virginia) Â· John A Stankovic (University of Virginia)</p></li></ul><ul><li><p>Organizing recurrent network dynamics by task-computation to enable continual learning</p><p>  Lea Duncker (Gatsby Unit, UCL) Â· Laura N Driscoll (Stanford) Â· Krishna V Shenoy (Stanford University) Â· Maneesh Sahani (Gatsby Unit, UCL) Â· David Sussillo (Stanford University)</p></li></ul><ul><li><p>Recurrent Quantum Neural Networks</p><p>  Johannes Bausch (University of Cambridge)</p></li></ul><ul><li><p>Reverse-engineering recurrent neural network solutions to a hierarchical inference task for mice</p><p>  Rylan Schaeffer (Harvard University) Â· Mikail C Khona (MIT) Â· Leenoy Meshulam (Massachusetts Institute of Technology MIT) Â· Brain Laboratory International (International Brain Laboratory) Â· Ila Fiete (Massachusetts Institute of Technology)</p></li></ul><ul><li><p>Recurrent Switching Dynamical Systems Models for Multiple Interacting Neural Populations</p><p>  Joshua Glaser (Columbia) Â· Matthew Whiteway (Columbia University) Â· John Cunningham (University of Columbia) Â· Liam Paninski (Columbia University) Â· Scott Linderman (Stanford University)</p></li></ul><h2 id="sequence"><a href="#sequence" class="headerlink" title="sequence"></a>sequence</h2><ul><li><p><strong>ã€çœ‹çœ‹other-1ã€‘</strong> Big Bird: Bert for Longer Sequences</p><p>  Manzil Zaheer (Google Research) Â· Guru Guruganesh (Google Research) Â· Kumar Avinava Dubey (Carnegie Mellon University) Â· Joshua Ainslie (Google) Â· Chris Alberti (Google) Â· Santiago Ontanon (Google LLC) Â· Philip Pham (Google) Â· Anirudh Ravula (Google) Â· Qifan Wang (Google Research) Â· Li Yang (Google) Â· Amr Ahmed (Google Research)</p></li></ul><h2 id="interpretable"><a href="#interpretable" class="headerlink" title="interpretable"></a>interpretable</h2><ul><li><p>Explaining Naive Bayes and Other Linear Classifiers with Polynomial Time and Delay</p><p>  Joao Marques-Silva (ANITI, Federal University of Toulouse Midi-PyrÃ©nÃ©es) Â· Thomas Gerspacher (ANITI) Â· Martin Cooper (University of Toulouse 3) Â· Alexey Ignatiev (Monash University) Â· Nina Narodytska (VMmare Research)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹interpre-3ã€‘</strong> Interpretable Sequence Learning for Covid-19 Forecasting</p><p>  Sercan Arik (Google) Â· Chun-Liang Li (Google) Â· Martin Nikoltchev (Google) Â· Rajarishi Sinha (Google) Â· Arkady Epshteyn (Google) Â· Jinsung Yoon (Google) Â· Long Le (Google) Â· Vikas Menon (Google) Â· Shashank Singh (Google) Â· Yash Sonthalia (Google) Â· Hootan Nakhost (Google) Â· Leyou Zhang (Google) Â· Elli Kanal (Google) Â· Tomas Pfister (Google)</p></li></ul><ul><li><p>ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping</p><p>  Cher Bass (Kingâ€™s College London) Â· Mariana da Silva (Kingâ€™s College London) Â· Carole Sudre (Kingâ€™s College London) Â· Petru-Daniel Tudosiu (Kingâ€™s College London) Â· Stephen Smith (FMRIB Centre - University of Oxford) Â· Emma Robinson (Kingâ€™s College)</p></li></ul><ul><li><p>How does this interaction affect me? Interpretable attribution for feature interactions</p><p>  Michael Tsang (University of Southern California) Â· Sirisha Rambhatla (University of Southern California) Â· Yan Liu (University of Southern California)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹interpre-1ã€‘</strong> Learning outside the Black-Box: The pursuit of interpretable models</p><p>  Jonathan Crabbe (University of Cambridge) Â· Yao Zhang (University of Cambridge) Â· William Zame (UCLA) Â· Mihaela van der Schaar (University of Cambridge)</p></li></ul><ul><li><p>GANSpace: Discovering Interpretable GAN Controls</p><p>  Erik HÃ¤rkÃ¶nen (Aalto University) Â· Aaron Hertzmann (Adobe) Â· Jaakko Lehtinen (Aalto University &amp; NVIDIA) Â· Sylvain Paris (Adobe)</p></li></ul><ul><li><p>Interpretable multi-timescale models for predicting fMRI responses to continuous natural speech</p><p>  Shailee Jain (The University of Texas at Austin) Â· Vy Vo (Intel Corporation) Â· Shivangi Mahto (The University of Texas at Austin) Â· Amanda LeBel (The University of Texas at Austin) Â· Javier Turek (Intel Labs) Â· Alexander Huth (The University of Texas at Austin)</p></li></ul><ul><li><p>Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE</p><p>  Ding Zhou (Columbia University) Â· Xue-Xin Wei (University of Pennsylvania)</p></li></ul><ul><li><p>Towards Interpretable Natural Language Understanding with Explanations as Latent Variables</p><p>  Wangchunshu Zhou (Beihang University) Â· Jinyi Hu (Tsinghua University) Â· Hanlin Zhang (South China University of Technology) Â· Xiaodan Liang (Sun Yat-sen University) Â· Maosong Sun (Tsinghua University) Â· Chenyan Xiong (Microsoft Research AI) Â· Jian Tang (Mila)</p></li></ul><ul><li><p>Interpretable and Personalized Apprenticeship Scheduling: Learning Interpretable Scheduling Policies from Heterogeneous User Demonstrations</p><p>  Rohan Paleja (Georgia Institute of Technology) Â· Andrew Silva (Georgia Institute of Technology) Â· Letian Chen (Georgia Institute of Technology) Â· Matthew Gombolay (Georgia Institute of Technology)</p></li></ul><ul><li><p>Incorporating Interpretable Output Constraints in Bayesian Neural Networks</p><p>  Wanqian Yang (Harvard University) Â· Lars Lorch (Harvard) Â· Moritz Graule (Harvard University) Â· Himabindu Lakkaraju (Harvard) Â· Finale Doshi-Velez (Harvard)</p></li></ul><ul><li><p>Implicit Regularization in Deep Learning May Not Be Explainable by Norms</p><p>  Noam Razin (Tel Aviv University) Â· Nadav Cohen (Tel Aviv University)</p></li></ul><ul><li><p>Parameterized Explainer for Graph Neural Network</p><p>  Dongsheng Luo (The Pennsylvania State University) Â· Wei Cheng (NEC Labs America) Â· Dongkuan Xu (The Pennsylvania State University) Â· Wenchao Yu (UCLA) Â· Bo Zong (NEC Labs) Â· Haifeng Chen (NEC Labs America) Â· Xiang Zhang (The Pennsylvania State University)</p></li></ul><ul><li><p>PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks</p><p>  Minh N Vu (University of Florida) Â· My T. Thai (University of Florida)</p></li></ul><ul><li><p>Can Implicit Bias Explain Generalization? Stochastic Convex Optimization as a Case Study</p><p>  Assaf Dauber (Tel-Aviv University) Â· Meir Feder (Tel-Aviv University) Â· Tomer Koren (Tel Aviv University &amp; Google) Â· Roi Livni (Tel Aviv University)</p></li></ul><ul><li><p>Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability</p><p>  Christopher Frye (Faculty) Â· Colin Rowat (University of Birmingham) Â· Ilya Feige (Faculty)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹interpre-2ã€‘</strong> How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods</p><p>  Jeya Vikranth Jeyakumar (University of California, Los Angeles) Â· Joseph Noor (University of California, Los Angeles) Â· Yu-Hsi Cheng (UCLA) Â· Luis Garcia (University of California, Los Angeles) Â· Mani Srivastava (UCLA)</p></li></ul><ul><li><p>Explainable Voting</p><p>  Dominik Peters (Carnegie Mellon University) Â· Ariel Procaccia (Harvard University) Â· Alexandros Psomas (Purdue University) Â· Zixin Zhou (Peking University)</p></li></ul><ul><li><p>What Did You Think Would Happen? Explaining Agent Behaviour through Intended Outcomes</p><p>  Herman Ho-Man Yau (University of Surrey) Â· Chris Russell (The Alan Turing Institute/ The University of Surrey) Â· Simon Hadfield (University of Surrey)</p></li></ul><ul><li><p>Margins are Insufficient for Explaining Gradient Boosting</p><p>  Allan GrÃ¸nlund (Aarhus University, MADALGO) Â· Lior Kamma (Aarhus University) Â· Kasper Green Larsen (Aarhus University)</p></li></ul><ul><li><p>Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models</p><p>  Tom Heskes (Radboud University Nijmegen) Â· Evi Sijben (Radboud University) Â· Ioan Gabriel Bucur (Radboud University Nijmegen) Â· Tom Claassen (Radboud University Nijmegen)</p></li></ul><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><ul><li><p>Implicit Rank-Minimizing Autoencoder</p><p>  Li Jing (Facebook AI Research) Â· Jure Zbontar (Facebook) Â· yann lecun (Facebook)</p></li></ul><ul><li><p>Swapping Autoencoder for Deep Image Manipulation</p><p>  Taesung Park (UC Berkeley) Â· Jun-Yan Zhu (Adobe, CMU) Â· Oliver Wang (Adobe Research) Â· Jingwan Lu (Adobe Research) Â· Eli Shechtman (Adobe Research, US) Â· Alexei Efros (UC Berkeley) Â· Richard Zhang (Adobe)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹other-5ã€‘</strong> Hierarchical Quantized Autoencoders</p><p>  Will Williams (Speechmatics) Â· Sam Ringer (Speechmatics) Â· Tom Ash (Speechmatics) Â· David MacLeod (Speechmatics) Â· Jamie Dougherty (Speechmatics) Â· John Hughes (Speechmatics)</p></li></ul><ul><li><p>Regularized linear autoencoders recover the principal components, eventually</p><p>  Xuchan Bao (University of Toronto) Â· James Lucas (University of Toronto) Â· Sushant Sachdeva (University of Toronto) Â· Roger Grosse (University of Toronto)</p></li></ul><ul><li><p>Dirichlet Graph Variational Autoencoder</p><p>  Jia Li (The Chinese University of Hong Kong) Â· Jianwei Yu (CUHK) Â· Jiajin Li (The Chinese University of Hong Kong) Â· Honglei Zhang (Georgia Institute of Technology) Â· Kangfei Zhao (The Chinese University of Hong Kong) Â· Yu Rong (Tencent AI Lab) Â· Hong Cheng (The Chinese University of Hong Kong) Â· Junzhou Huang (University of Texas at Arlington / Tencent AI Lab)</p></li></ul><ul><li><p>NVAE: A Deep Hierarchical Variational Autoencoder</p><p>  Arash Vahdat (NVIDIA) Â· Jan Kautz (NVIDIA)</p></li></ul><ul><li><p>Evidential Sparsification of Multimodal Latent Spaces in Conditional Variational Autoencoders</p><p>  Masha Itkina (Stanford University) Â· Boris Ivanovic (Stanford University) Â· Ransalu Senanayake (Stanford University) Â· Mykel J Kochenderfer (Stanford University) Â· Marco Pavone (Stanford University)</p></li></ul><ul><li><p>Fully Convolutional Mesh Autoencoder using Efficient Spatially Varying Kernels</p><p>  Yi Zhou (University of Southern California) Â· Chenglei Wu (Facebook) Â· Zimo Li (University of Southern California) Â· Chen Cao (Snap Inc.) Â· Yuting Ye (Facebook Reality Labs) Â· Jason Saragih (Facebook) Â· Hao Li (Pinscreen/University of Southern California/USC ICT) Â· Yaser Sheikh (Facebook Reality Labs)</p></li></ul><ul><li><p>Recursive Inference for Variational Autoencoders</p><p>  Minyoung Kim (Samsung AI Center Cambridge) Â· Vladimir Pavlovic (Rutgers University)</p></li></ul><ul><li><p><strong>ã€çœ‹çœ‹other-2ã€‘</strong> The Autoencoding Variational Autoencoder</p><p>  Taylan Cemgil (DeepMind) Â· Sumedh Ghaisas (DeepMind) Â· Krishnamurthy Dvijotham (DeepMind) Â· Sven Gowal (DeepMind) Â· Pushmeet Kohli (DeepMind)</p></li></ul><ul><li><p>Autoencoders that donâ€™t overfit towards the Identity</p><p>  Harald Steck (Netflix)</p></li></ul><h2 id="clustering"><a href="#clustering" class="headerlink" title="clustering"></a>clustering</h2><ul><li><p>Deep Subspace Clustering with Data Augmentation</p><p>  Mahdi Abavisani (Rutgers, The State University of New Jersey) Â· Alireza Naghizadeh (Rutgers University) Â· Dimitris Metaxas (Rutgers University) Â· Vishal Patel (Johns Hopkins University)</p></li></ul><ul><li><p>Bandit-PAM: Almost Linear Time k-Medoids Clustering via Multi-Armed Bandits</p><p>  Mo Tiwari (Stanford University) Â· Martin Zhang (Harvard University) Â· James J Mayclin (Stanford University) Â· Sebastian Thrun (Stanford University) Â· Chris Piech (Stanford) Â· Ilan Shomorony (University of Illinois at Urbana Champaign)</p></li></ul><ul><li><p>Self-Supervised Learning by Cross-Modal Audio-Video Clustering</p><p>  Humam Alwassel (KAUST) Â· Dhruv Mahajan (Facebook) Â· Bruno Korbar (Facebook) Â· Lorenzo Torresani (Facebook AI) Â· Bernard Ghanem (KAUST) Â· Du Tran (Facebook AI)</p></li></ul><ul><li><p>Near-Optimal Comparison Based Clustering</p><p>  MichaÃ«l Perrot (Max Planck Institute for Intelligent Systems) Â· Pascal Esser (Technical University of Munich) Â· Debarghya Ghoshdastidar (Technical University Munich)</p></li></ul><ul><li><p>Graduated Assignment for Joint Multi-Graph Matching and Clustering with Application to Unsupervised Graph Matching Network Learning</p><p>  Runzhong Wang (Shanghai Jiao Tong University) Â· Junchi Yan (Shanghai Jiao Tong University) Â· Xiaokang Yang (Shanghai Jiao Tong University)</p></li></ul><ul><li><p>Scalable Approximation Algorithm for Fair kâˆ’center Clustering</p><p>  Elfarouk Harb (Hong Kong University of Science and Technology) Â· Ho Shan Lam (The Hong Kong University of Science and Technology)</p></li></ul><ul><li><p>Deep Transformation-Invariant Clustering</p><p>  Tom Monnier (Ã‰cole des ponts Paristech) Â· Thibault Groueix (Ã‰cole des ponts ParisTech) Â· Mathieu Aubry (Ã‰cole des ponts ParisTech)</p></li></ul><ul><li><p>Efficient Clustering for Stretched Mixtures: Landscape and Optimality</p><p>  Kaizheng Wang (Columbia University) Â· Yuling Yan (Princeton University) Â· Mateo Diaz (Cornell University)</p></li></ul><ul><li><p>Efficient Clustering Based On A Unified View Of K-means And Ratio-cut</p><p>  Shenfei Pei (Northwestern Polytechnical University) Â· Feiping Nie (University of Texas Arlington) Â· Rong Wang (Northwestern Polytechnical University) Â· Xuelong Li (Northwestern Polytechnical University)</p></li></ul><ul><li><p>Adversarial Learning for Robust Deep Clustering</p><p>  Xu Yang (Xidian University) Â· Cheng Deng (Xidian University) Â· Kun Wei (Xidian University) Â· Junchi Yan (Shanghai Jiao Tong University) Â· Wei Liu (Tencent AI Lab)</p></li></ul><ul><li><p>Sliding Window Algorithms for k-Clustering Problems</p><p>  Michele Borassi (Google Switzerland GmbH) Â· Alessandro Epasto (Google) Â· Silvio Lattanzi (Google Research) Â· Sergei Vassilvitskii (Google) Â· Morteza Zadimoghaddam (Google Research)</p></li></ul><ul><li><p>From Trees to Continuous Embeddings and Back: Hyperbolic Hierarchical Clustering</p><p>  Ines Chami (Stanford University) Â· Albert Gu (Stanford) Â· Vaggos Chatziafratis (Stanford University, California) Â· Christopher RÃ© (Stanford)</p></li></ul><ul><li><p>Probabilistic Fair Clustering</p><p>  Seyed Esmaeili (University of Maryland, College Park) Â· Brian Brubach (University of Maryland) Â· Leonidas Tsepenekas (University of Maryland) Â· John Dickerson (University of Maryland)</p></li></ul><ul><li><p>Strongly local p-norm-cut algorithms for semi-supervised learning and local graph clustering</p><p>  Meng Liu (Purdue University) Â· David Gleich (Purdue University)</p></li></ul><ul><li><p>Fair Hierarchical Clustering</p><p>  Sara Ahmadian (Google Research) Â· Alessandro Epasto (Google) Â· Marina Knittel (University of Maryland, College Park) Â· Ravi Kumar (Google) Â· Mohammad Mahdian (Google Research) Â· Benjamin Moseley (Carnegie Mellon University) Â· Philip Pham (Google) Â· Sergei Vassilvitskii (Google) Â· Yuyan Wang (Carnegie Mellon University)</p></li></ul><ul><li><p>Partially View-aligned Clustering</p><p>  Zhenyu Huang (Sichuan University) Â· Peng Hu (Institute for Infocomm Research, A<em>STAR) Â· Joey Tianyi Zhou (IHPC, A</em>STAR) Â· Jiancheng Lv (Machine Intelligence Laboratory College of Computer Science, Sichuan University) Â· Xi Peng (Institute for Infocomm, Research Agency for Science, Technology and Research (A*STAR) Singapore)</p></li></ul><ul><li><p>Differentially Private Clustering: Tight Approximation Ratios</p><p>  Badih Ghazi (Google) Â· Ravi Kumar (Google) Â· Pasin Manurangsi (Google)</p></li></ul><ul><li><p>On the Power of Louvain for Graph Clustering</p><p>  Vincent Cohen-Addad (CNRS &amp; Sorbonne UniversitÃ©) Â· Adrian Kosowski (NavAlgo) Â· Frederik Mallmann-Trenn (Kingâ€™s College London) Â· David Saulpic (Ecole normale supÃ©rieure)</p></li></ul><ul><li><p>SMYRF - Efficient attention using asymmetric clustering</p><p>  Giannis Daras (National Technical University of Athens) Â· Nikita Kitaev (University of California, Berkeley) Â· Augustus Odena (Google Brain) Â· Alexandros Dimakis (University of Texas, Austin)</p></li></ul><ul><li><p>Higher-Order Spectral Clustering of Directed Graphs</p><p>  Valdimar Steinar Ericsson Laenen (FiveAI) Â· He Sun (School of Informatics, The University of Edinburgh)</p></li></ul><h1 id="data-augmentation"><a href="#data-augmentation" class="headerlink" title="data augmentation"></a>data augmentation</h1><ul><li><p>Maximum-Entropy Adversarial Data Augmentation for Improved Generalization and Robustness</p><p>  Long Zhao (Rutgers University) Â· Ting Liu (Google) Â· Xi Peng (University of Delaware) Â· Dimitris Metaxas (Rutgers University)</p></li></ul><ul><li><p>A Group-Theoretic Framework for Data Augmentation</p><p>  Shuxiao Chen (University of Pennsylvania) Â· Edgar Dobriban (University of Pennsylvania) Â· Jane Lee (University of Pennsylvania)</p></li></ul><ul><li><p>Post-training Iterative Hierarchical Data Augmentation for Deep Networks</p><p>  Adil Khan (Innopolis University) Â· Khadija Fraz (Hazara University)</p></li></ul><ul><li><p>Heavy-tailed Representations, Text Polarity Classification &amp; Data Augmentation</p><p>  Hamid JALALZAI (TÃ©lÃ©com ParisTech) Â· Pierre Colombo (Telecom ParisTech) Â· ChloÃ© Clavel (Telecom-ParisTech, Paris, France) Â· Eric Gaussier (UniversitÃ© Joseph Fourier, Grenoble) Â· Giovanna Varni (Telecom ParisTec) Â· Emmanuel Vignon (IBM) Â· Anne Sabourin (LTCI, Telecom ParisTech, UniversitÃ© Paris-Saclay)</p></li></ul><ul><li><p>Unsupervised Data Augmentation for Consistency Training</p><p>  Qizhe Xie (CMU, Google Brain) Â· Zihang Dai (Carnegie Mellon University) Â· Eduard Hovy (CMU) Â· Thang Luong (Google Brain) Â· Quoc V Le (Google)</p></li></ul><ul><li><p>Exemplar VAEs for Exemplar based Generation and Data Augmentation</p><p>  Sajad Norouzi (University of Toronto / Vector Institute) Â· David J Fleet (University of Toronto) Â· Mohammad Norouzi (Google Brain)</p></li></ul><ul><li><p>Practical automated data augmentation with a reduced search space</p><p>  Ekin Dogus Cubuk (Google Brain) Â· Barret Zoph (Google Brain) Â· Jon Shlens (Google Research) Â· Quoc V Le (Google)</p></li></ul><ul><li><p>Counterfactual Data Augmentation using Locally Factored Dynamics</p><p>  Silviu Pitis (University of Toronto) Â· Elliot Creager (University of Toronto) Â· Animesh Garg (Univ. of Toronto, Vector Institute, Nvidia)</p></li></ul><ul><li><p>Deep Subspace Clustering with Data Augmentation</p><p>  Mahdi Abavisani (Rutgers, The State University of New Jersey) Â· Alireza Naghizadeh (Rutgers University) Â· Dimitris Metaxas (Rutgers University) Â· Vishal Patel (Johns Hopkins University)</p></li></ul><h2 id="æœ‰ç‚¹æ„æ€"><a href="#æœ‰ç‚¹æ„æ€" class="headerlink" title="æœ‰ç‚¹æ„æ€"></a>æœ‰ç‚¹æ„æ€</h2><ul><li><p>Learning Loss for Test-Time Augmentation</p><p>  Ildoo Kim (Kakao Brain) Â· Younghoon Kim (Sungshin Womenâ€™s University) Â· Sungwoong Kim (Kakao Brain)</p></li><li><p>Predicting Training Time Without Training</p><p>  Luca Zancato (University of Padova) Â· Alessandro Achille (Amazon Web Services) Â· Avinash Ravichandran (AWS) Â· Rahul Bhotika (Amazon) Â· Stefano Soatto (UCLA)</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ç›‘è§†æ—¶é—´åºåˆ—ï¼ˆç‰¹åˆ«æ˜¯é’ˆå¯¹è¿ç»´çš„ç›‘æ§æ—¶é—´åºåˆ—æ•°æ®ï¼‰</title>
      <link href="/uncategorized/surveys/monitoring_ts/"/>
      <url>/uncategorized/surveys/monitoring_ts/</url>
      
        <content type="html"><![CDATA[<p>æ€»ç»“å‡ ä¸ªç‚¹ï¼š</p><ol><li>æ•°æ®é›†åç§°ä¸é“¾æ¥</li><li>æ•°æ®é›†èƒŒæ™¯ï¼šæ•°æ®é›†çš„æè¿°å¯¹è±¡æ˜¯ä»€ä¹ˆ</li><li>æ•°æ®é›†ä»»åŠ¡ï¼šè¯¥æ•°æ®é›†é‡‡é›†çš„æ—¶å€™çš„åŸå§‹ä»»åŠ¡æ˜¯ä»€ä¹ˆ</li><li>æ•°æ®é›†æ ¼å¼ï¼ˆå¯é€‰çš„ï¼‰</li></ol><span id="more"></span><h2 id="è¿ç»´æ—¶é—´åºåˆ—"><a href="#è¿ç»´æ—¶é—´åºåˆ—" class="headerlink" title="è¿ç»´æ—¶é—´åºåˆ—"></a>è¿ç»´æ—¶é—´åºåˆ—</h2><ol><li><p><a href="https://catalog.data.gov/dataset/health-monitoring-and-prognostics-for-computer-servers">è®¡ç®—æœºæœåŠ¡å™¨çš„è¿è¡ŒçŠ¶å†µç›‘è§†å’Œé¢„æµ‹</a></p><ul><li><p>æè¿°ï¼šå…³é”®ä»»åŠ¡ç³»ç»Ÿçš„è¯Šæ–­è§£å†³æ–¹æ¡ˆéœ€è¦ä¸€ç§å…¨é¢çš„æ–¹æ³•æ¥ä¸»åŠ¨æ£€æµ‹å’Œéš”ç¦»æ•…éšœï¼Œæ¨èå’ŒæŒ‡å¯¼åŸºäºçŠ¶å†µçš„ç»´æŠ¤æªæ–½ï¼Œ<br>å¹¶å®æ—¶ä¼°ç®—å…³é”®ç»„ä»¶å’Œç›¸å…³å­ç³»ç»Ÿçš„å‰©ä½™ä½¿ç”¨å¯¿å‘½ã€‚ä¸€ä¸ªä¸»è¦çš„æŒ‘æˆ˜æ˜¯å°†é¢„æµ‹çš„ä¼˜åŠ¿æ‰©å±•åˆ°åŒ…æ‹¬è®¡ç®—æœºæœåŠ¡å™¨å’Œå…¶ä»–ç”µå­ç»„ä»¶ã€‚<br>é¢„æµ‹èƒ½åŠ›çš„å…³é”®æ¨åŠ¨å› ç´ æ˜¯ç›‘è§†ä¸æ‰§è¡Œç»„ä»¶å’Œå­ç³»ç»Ÿçš„è¿è¡ŒçŠ¶å†µæœ‰å…³çš„æ—¶é—´åºåˆ—ä¿¡å·ã€‚æ—¶é—´åºåˆ—ä¿¡å·ä½¿ç”¨æ¨¡å¼è¯†åˆ«è¿›è¡Œå®æ—¶å¤„ç†ï¼Œ<br>ä»¥è¿›è¡Œä¸»åŠ¨å¼‚å¸¸æ£€æµ‹å’Œå‰©ä½™ä½¿ç”¨å¯¿å‘½ä¼°ç®—ã€‚å°†æä¾›ä½¿ç”¨æ¨¡å¼è¯†åˆ«æŠ€æœ¯æ¥æ—©æœŸæ£€æµ‹å·²çŸ¥ä¼šå¯¼è‡´ç”µå­ç³»ç»Ÿæ•…éšœçš„å¤šç§æœºåˆ¶çš„ç¤ºä¾‹ï¼Œ<br>åŒ…æ‹¬ï¼šç¯å¢ƒé—®é¢˜ï¼›è½¯ä»¶è€åŒ–ï¼›ä¼ æ„Ÿå™¨é™çº§æˆ–æ•…éšœï¼›ç¡¬ä»¶ç»„ä»¶çš„é€€åŒ–ï¼›æœºæ¢°ï¼Œç”µå­å’Œå…‰å­¦äº’è¿çš„æ€§èƒ½ä¸‹é™ã€‚<br>é¢„åæ¨¡å¼åˆ†ç±»æœ‰åŠ©äºå¤§å¹…æé«˜ç»„ä»¶çš„å¯é æ€§è£•åº¦å’Œç³»ç»Ÿå¯ç”¨æ€§ç›®æ ‡ï¼ŒåŒæ—¶å‡å°‘æ˜‚è´µçš„â€œæ— æ•…éšœâ€äº‹ä»¶çš„æ¥æºï¼Œ<br>è¿™äº›äº‹ä»¶å·²æˆä¸ºé‡è¦çš„ä¿ä¿®æˆæœ¬é—®é¢˜ã€‚</p></li><li><p>ä»»åŠ¡ï¼šanomaly detection, time series prediction</p></li><li><p><a href="https://datasetsearch.research.google.com/search?query=monitoring,%20time%20series&docid=cp3L+CfAjd47GckZAAAAAA==">google link</a></p></li></ul></li><li><p><a href="https://www.kaggle.com/boltzmannbrain/nab">Numentaå¼‚å¸¸åŸºå‡†ï¼ˆNABï¼‰</a></p><ul><li>æè¿°ï¼š58ä¸ªæ—¶é—´åºåˆ—æ•°æ®æ–‡ä»¶çš„NABè¯­æ–™åº“æ—¨åœ¨ä¸ºæµå¼‚å¸¸æ£€æµ‹ä¸­çš„ç ”ç©¶æä¾›æ•°æ®ã€‚å®ƒç”±çœŸå®ä¸–ç•Œå’Œäººä¸ºçš„æ—¶é—´åºåˆ—æ•°æ®ç»„æˆï¼Œ<br>å…¶ä¸­åŒ…å«æ ‡è®°çš„å¼‚å¸¸è¡Œä¸ºæ—¶æœŸã€‚æ•°æ®æ˜¯æœ‰åºçš„ï¼Œå¸¦æœ‰æ—¶é—´æˆ³çš„å•å€¼æŒ‡æ ‡ã€‚é™¤éå¦æœ‰è¯´æ˜ï¼Œå¦åˆ™æ‰€æœ‰æ•°æ®æ–‡ä»¶å‡åŒ…å«å¼‚å¸¸ã€‚<br>å¤§å¤šæ•°æ•°æ®æ¥è‡ªå„ç§æ¥æºï¼Œä¾‹å¦‚AWSæœåŠ¡å™¨æŒ‡æ ‡ï¼ŒTwitteré‡ï¼Œå¹¿å‘Šç‚¹å‡»æŒ‡æ ‡ï¼Œæµé‡æ•°æ®ç­‰ã€‚</li><li>ä»»åŠ¡ï¼šanomaly detection, prediction</li><li><a href="https://datasetsearch.research.google.com/search?query=cpu,%20time%20series&docid=iXJXcx2EBRKHZSc8AAAAAA==">google link</a></li></ul></li><li><p><strong>æŒºå¥½çš„æ•°æ®é›†</strong>ä¼ä¸šçº§åº”ç”¨ç¨‹åºè¿ç»´ </p><p> <a href="https://www.kaggle.com/anomalydetectionml/rawdata">åŸå§‹æ•°æ®</a>,<br> <a href="https://www.kaggle.com/anomalydetectionml/features">feature data</a></p><ul><li>æè¿°ï¼šä¸€ä¸ªä¼ä¸šçº§è½¯ä»¶çš„è¿ç»´æ—¶é—´åºåˆ—ï¼ŒåŸå§‹æ•°æ®é“¾æ¥ä¸­æ˜¯æ²¡æœ‰æ­£å¸¸ã€å¼‚å¸¸æ ‡ç­¾çš„åŸå§‹æ•°æ®ï¼Œæ•°æ®æ›´åŠ è¯¦ç»†ã€‚featuresä¸­æ˜¯ç»è¿‡å¤„ç†å’Œæ ‡ç­¾åçš„æ•°æ®ï¼Œ<br>å°†åŸå§‹æ•°æ®ä¸­çš„åŒä¸€æ—¶åˆ»çš„æ•°æ®å…¨éƒ½æ‹¼æ¥åœ¨äº†ä¸€èµ·ï¼Œå½¢æˆäº†éå¸¸é«˜ç»´åº¦çš„æ•°æ®ã€‚</li><li>ä»»åŠ¡ï¼šå¼‚å¸¸æ£€æµ‹</li></ul></li><li><p>Azure æœåŠ¡å™¨CPUæ—¶é—´åºåˆ— <a href="https://www.kaggle.com/amcs1729/azure-data">link</a></p><ul><li>æè¿°ï¼šæŸä¸ªAzureæœåŠ¡å™¨çš„CPUè¿è½¬æ—¶é—´åºåˆ—ï¼Œå•ç»´æ—¶é—´åºåˆ—ã€‚</li><li>ä»»åŠ¡ï¼šæ—¶é—´åºåˆ—é¢„æµ‹</li></ul></li></ol><ol start="5"><li><p>åº”ç”¨ç¨‹åºè¿ç»´ <a href="https://www.kaggle.com/wolfgangb33r/usercount">link</a></p><ul><li>æè¿°ï¼šæŸä¸ªåº”ç”¨ç¨‹åºåœ¨ä¸€æ®µæ—¶é—´å†…çš„ä¸€äº›ç»Ÿè®¡ä¿¡æ¯ï¼Œ</li></ul><p> <strong>æ•°æ®åŒ…æ‹¬ï¼š</strong> æ—¶é—´æˆ³ã€è·ç¦»ä¸Šæ¬¡è®°å½•æ—¶é—´å†…çš„ç”¨æˆ·è®¿é—®æ¬¡æ•°ã€è·ç¦»ä¸Šæ¬¡è®°å½•æ—¶é—´å†…äº§ç”Ÿçš„ä¼šè¯æ¬¡æ•°ã€è·ç¦»ä¸Šæ¬¡è®°å½•æ—¶é—´æ–°å¢çš„ç”¨æˆ·æ•°ã€<br> è·ç¦»ä¸Šæ¬¡è®°å½•æ—¶é—´å†…çš„åº”ç”¨å´©æºƒæ¬¡æ•°ã€‚</p><ul><li>ä»»åŠ¡ï¼šå›å½’ã€é¢„æµ‹</li></ul></li></ol><ol start="6"><li>æœåŠ¡å™¨è¿ç»´æ—¥å¿—ï¼Œè¿ç»´æ—¥å¿— <a href="https://www.kaggle.com/kartikjaspal/server-logs-suspicious">link</a><ul><li>æè¿°ï¼šæœåŠ¡å™¨æ”»å‡»æ•°æ®é›†ï¼Œè®°å½•äº†1. æ”»å‡»æ—¶é—´å’ŒæŒç»­æ—¶é—´ï¼Œ2. æºIPå’Œç›®çš„IPï¼Œ3. æ•°æ®åŒ…ï¼Œå­—èŠ‚ï¼Œæµå’Œæ ‡å¿—ï¼Œ4.ç±»å‹ï¼ŒIDå’Œæ ‡ç­¾/ç±»</li><li>ä»»åŠ¡ï¼šåˆ†ç±»ã€å¼‚å¸¸æ£€æµ‹</li></ul></li></ol><ol start="7"><li><p><a href="https://zenodo.org/record/3653909#.X0hlBMhLiUk">ç”µæ¢¯è®¾å¤‡çš„è¿ç»´æ•°æ®é›†</a></p><ul><li><p>æè¿°ï¼šåä¸ºæ…•å°¼é»‘ç ”ç©¶ä¸­å¿ƒçš„å…¬å¼€ï¼ˆåŒ¿åï¼‰é¢„æµ‹æ€§ç»´æŠ¤æ•°æ®é›†ã€‚æ¥è‡ªå„ç§IoTä¼ æ„Ÿå™¨çš„æ•°æ®é›†ï¼Œç”¨äºç”µæ¢¯è¡Œä¸šçš„é¢„æµ‹æ€§ç»´æŠ¤ã€‚<br>è¯¥æ•°æ®å¯ç”¨äºç”µæ¢¯é—¨çš„é¢„æµ‹ç»´æŠ¤ï¼Œä»¥å‡å°‘è®¡åˆ’å¤–çš„åœæœºå¹¶æœ€å¤§ç¨‹åº¦åœ°å»¶é•¿è®¾å¤‡ä½¿ç”¨å¯¿å‘½ã€‚æ•°æ®é›†åŒ…å«æ“ä½œæ•°æ®ï¼Œ<br>è¯¥æ•°æ®ä»¥æ—¶é—´åºåˆ—çš„å½¢å¼åœ¨å»ºç­‘ç‰©çš„é«˜å³°å’Œå¤œé—´ç”µæ¢¯ä½¿ç”¨ä¸­ä»¥4Hzé‡‡æ ·ï¼ˆ16:30åˆ°23:30ä¹‹é—´ï¼‰ã€‚å¯¹äºç”µæ¢¯è½¿å¢é—¨ï¼Œ<br>æˆ‘ä»¬è€ƒè™‘çš„ç³»ç»Ÿæ˜¯ï¼šæœºç”µä¼ æ„Ÿå™¨ï¼ˆé—¨çƒè½´æ‰¿ä¼ æ„Ÿå™¨ï¼‰ï¼Œç¯å¢ƒï¼ˆæ¹¿åº¦ï¼‰å’Œç‰©ç†ï¼ˆæŒ¯åŠ¨ï¼‰ã€‚</p></li><li><p>ä»»åŠ¡ï¼šé¢„æµ‹ï¼ˆæ— æ ‡ç­¾ï¼‰</p></li><li><p>å½¢å¼ï¼štime seriesï¼Œæ— æ ‡ç­¾ï¼Œç»´åº¦ä¸º3ï¼Œæœ‰ç›¸å…³è®ºæ–‡å¼•ç”¨</p></li><li><p><a href="https://datasetsearch.research.google.com/save?query=coronavirus%20covid-19&docid=lvVz38vzKrmGqZ48AAAAAA==">google link</a></p></li></ul></li></ol><ol start="8"><li><p><strong>æŒºå¥½çš„æ•°æ®é›†</strong>  å¤§æ•°æ®å¹³å°è¿ç»´æ•°æ®é›† <a href="https://zenodo.org/record/3549604#.X0hpFMhLiUk">link</a></p><ul><li>æè¿°ï¼š<a href="https://link.springer.com/chapter/10.1007/978-3-030-44769-4_13">ç›¸å…³è®ºæ–‡</a>, OpenStackæ˜¯ä¸€ä¸ªäº‘æ“ä½œç³»ç»Ÿï¼Œ<br>å®ƒæ§åˆ¶æ•´ä¸ªæ•°æ®ä¸­å¿ƒå†…çš„å¤§å‹è®¡ç®—ï¼Œå­˜å‚¨å’Œç½‘ç»œèµ„æºæ± ï¼Œæ‰€æœ‰è¿™äº›èµ„æºå‡é€šè¿‡å…·æœ‰é€šç”¨èº«ä»½éªŒè¯æœºåˆ¶çš„APIè¿›è¡Œç®¡ç†å’Œé…ç½®ã€‚<br>å‡ºäºç”Ÿæˆæ•°æ®çš„ç›®çš„ï¼Œå®ƒåŒ…å«ä¸€ä¸ªåä¸ºwally-113çš„æ§åˆ¶èŠ‚ç‚¹å’Œå››ä¸ªè®¡ç®—èŠ‚ç‚¹ï¼šwally-122ï¼Œwally-123ï¼Œwally-124<br>å’Œwally- 117ã€‚å®ƒå·²éƒ¨ç½²åœ¨ç¾¤é›†çš„è£¸æœºèŠ‚ç‚¹ä¸Šï¼Œæ¯ä¸ªèŠ‚ç‚¹å…·æœ‰16 GBçš„RAMï¼Œ3ä¸ª1TBç£ç›˜å’Œ2ä¸ª1Gbitä»¥å¤ªç½‘NICã€‚<br>å°†ä¸‰ä¸ªç¡¬ç›˜ç»„åˆåˆ°è½¯ä»¶RAID 5ä¸­ä»¥å®ç°æ•°æ®å†—ä½™ã€‚</li></ul><p> <strong>å·¥ä½œè´Ÿè½½ä¸æ•…éšœç±»å‹ï¼š</strong>  1. Create and delete server; 2. Create and delete image; 3. Create and delete network;<br> <strong>æ•°æ®ï¼š</strong> 1. Metrics:  CPU, MEM and load of the machine (either controller or the compute nodes). 2. Logs: æ¯ä¸ªç‰©ç†ä¸»æœºä¸ä¸Šé¢çš„é¡¹ç›®æ‰€ç”Ÿæˆçš„æ—¥å¿— 3. Traces:  It generates one trace per request, that goes through all involved services, and builds a tree of calls which captures a workflow of service invocations, request types: HTTP, RPC, DB API, Driver.</p><ul><li>ä»»åŠ¡ï¼šå¼‚å¸¸æ£€æµ‹</li><li><a href="https://datasetsearch.research.google.com/save?query=coronavirus%20covid-19&docid=Vac33QBm10kwUw0FAAAAAA==">google link</a></li></ul></li><li><p>æ™ºèƒ½æ‰‹æœºä¸Šçš„æ¶æ„è½¯ä»¶ä¸æ¶æ„æ´»åŠ¨è®°å½• <a href="http://bigdata.ise.bgu.ac.il/sherlock/">link</a></p><ul><li>æè¿°ï¼šè¯¥æ•°æ®é›†æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåºå¤§çš„æ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œå‡ ä¹æ¶µç›–äº†å¯ä»¥ä»Samsung Galaxy S5æ™ºèƒ½æ‰‹æœºè¿›è¡Œé‡‡æ ·çš„æ¯ç§è½¯ä»¶<br>å’Œç¡¬ä»¶ä¼ æ„Ÿå™¨çš„ç±»å‹ï¼Œè€Œæ— éœ€rootç‰¹æƒã€‚æ•°æ®é›†åŒ…å«è¶…è¿‡100äº¿æ¡æ•°æ®è®°å½•ä¸­çš„6,000äº¿ä¸ªæ•°æ®ç‚¹ã€‚<br>æˆ‘ä»¬æä¾›äº†æ˜ç¡®çš„æ ‡ç­¾ï¼ˆæ—¶é—´æˆ³å’Œæè¿°ï¼‰ï¼Œå¯ä»¥å‡†ç¡®æ•è·è®¾å¤‡ä¸Šçš„æ¶æ„è½¯ä»¶ä½•æ—¶æ‰§è¡Œå…¶æ¶æ„æ´»åŠ¨ã€‚é€šè¿‡è¿™äº›æ ‡ç­¾ï¼Œæ‚¨å¯ä»¥å°†æ•°æ®é›†ç”¨ä½œæœºå™¨å­¦ä¹ ç®—æ³•çš„åŸºå‡†ã€‚</li></ul><p> <strong>æ•°æ®åŒ…æ‹¬ï¼š</strong>å‘¼å«/ SMSæ—¥å¿—ã€ä½ç½®ã€WiFiä¿¡å·å¼ºåº¦ã€ç½‘ç»œç»Ÿè®¡ä¿¡æ¯ã€å…¶ä»–â€¦ï¼ˆå‚è§æ•°æ®é›†æè¿°é¡µé¢ï¼‰</p><ul><li>ä»»åŠ¡ï¼šæ¶æ„æ´»åŠ¨æ£€æµ‹ã€æ¶æ„è½¯ä»¶æ£€æµ‹</li><li><a href="https://datasetsearch.research.google.com/save?query=coronavirus%20covid-19&docid=5CKm58KTxK6n3a22AAAAAA==">google link</a>, <a href="https://www.impactcybertrust.org/dataset_view?idDataset=1258">other link</a></li></ul></li><li><p>è§†é¢‘åº”ç”¨çš„æœåŠ¡è´¨é‡æ£€æµ‹æ•°æ®é›† <a href="https://zenodo.org/record/3459164#.X0h2bMhLiUk">link</a></p><ul><li>æè¿°ï¼šåœºæ™¯ä¸ºåœ¨ç”¨æˆ·è®¾å¤‡ä¸Šä½¿ç”¨LTEç½‘ç»œè®¿é—®ä¸€ä¸ªåŸºäºGStreamerçš„MPEG-DASHè§†é¢‘è½¯ä»¶æ—¶çš„åº”ç”¨æœåŠ¡è´¨é‡ã€‚<br>ç”¨æˆ·é¦–å…ˆè¦ä»åˆ†å¸ƒå¼DASHæœåŠ¡å™¨ä¸Šä¸‹è½½MPEG-DASHè§†é¢‘æ–‡ä»¶ï¼Œè¿‡ç¨‹ä¸­æ”¶é›†ä¸‹è¿°çš„ä¿¡æ¯ï¼š</li></ul><ol><li>Date: date when the data is collected;</li><li>Player: type of the player (in this case it is always â€œGStreamerâ€);</li><li>Num: identifier of the player;</li><li>URLVid: URL of the MPD file;</li><li>Latency: latency experienced by the player;</li><li>BW: bandwidth experienced by the player;</li><li>Quality: chosen DASH video representation;<br>åœ¨å®éªŒè¿‡ç¨‹ä¸­ï¼Œå…¶ä»–æ’­æ”¾å™¨è¿è¡Œä»¥åœ¨ç±»ä¼¼CDNçš„æœåŠ¡å™¨ä¸Šç”Ÿæˆé€¼çœŸçš„åª’ä½“æµæµé‡ã€‚ è¿™äº›ç©å®¶é€šè¿‡éµå¾ªPoissonæˆ–Paretoåˆ†å¸ƒå¼€å§‹æ¸¸æˆã€‚</li></ol><ul><li><p>ä»»åŠ¡ï¼šæ—¶é—´åºåˆ—é¢„æµ‹ï¼Œç½‘ç»œæ‰¿è½½èƒ½åŠ›é¢„æµ‹</p></li><li><p><a href="https://zenodo.org/record/3459164#.X0h2bMhLiUk">google link</a>    </p></li></ul></li><li><p>åŠ å¯†åŠ«æŒæ”»å‡»æ—¶é—´åºåˆ—æ•°æ®é›† <a href="https://www.kaggle.com/keshanijayasinghe/cryptojacking-attack-timeseries-dataset">link</a></p><ul><li>æè¿°ï¼šåŠ å¯†åŠ«æŒæ˜¯æœªç»æˆæƒä½¿ç”¨ä»–äººçš„è®¡ç®—æœºæ¥å¼€é‡‡åŠ å¯†è´§å¸ã€‚åŠ å¯†æŒ–çŸ¿ä»£ç åœ¨åå°è¿è¡Œï¼Œ<br>å› ä¸ºæ¯«æ— æˆ’å¿ƒçš„å—å®³è€…ä¼šæ­£å¸¸ä½¿ç”¨ä»–ä»¬çš„è®¡ç®—æœºã€‚ä»–ä»¬å¯èƒ½ä¼šæ³¨æ„åˆ°çš„å”¯ä¸€è¿¹è±¡æ˜¯æ€§èƒ½é™ä½æˆ–æ‰§è¡Œæ»åã€‚<br>æœ€è¿‘ï¼Œç”±äºæœåŠ¡å™¨çš„å¼ºå¤§è®¡ç®—èƒ½åŠ›å’Œè®¸å¤šé…ç½®ä¸å½“çš„æœåŠ¡å™¨è®¾ç½®ï¼Œæ”»å‡»è€…å·²å°†ç›®æ ‡ä»ä¸ªäººè®¡ç®—æœºè½¬ç§»åˆ°äº‘æœåŠ¡å™¨ã€‚<br>å°½ç®¡å·²ç»è®¾è®¡äº†è®¸å¤šç»Ÿè®¡æ–¹æ³•æ¥è¿›è¡Œå¯†ç åŠ«æŒæ”»å‡»æ£€æµ‹ï¼Œä½†æ˜¯è®¾è®¡å…·æœ‰ä½è®¡ç®—å¼€é”€çš„å®æ—¶æ£€æµ‹å™¨ä»ç„¶æ˜¯ä¸»è¦é—®é¢˜ä¹‹ä¸€ã€‚<br>å¦ä¸€æ–¹é¢ï¼Œå¯¹æ–°æ£€æµ‹ç®—æ³•å’ŒæŠ€æœ¯çš„è¯„ä¼°åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºè®¾è®¡è‰¯å¥½çš„æ•°æ®é›†çš„å­˜åœ¨ã€‚</li></ul><p><strong>æ•°æ®åŒ…æ‹¬ï¼š</strong>ä¸‰ä¸ªcsvæ–‡ä»¶ï¼ˆå¼‚å¸¸ã€æ­£å¸¸ã€å®Œæ•´ï¼‰ï¼Œæ¯ä¸ªæ–‡ä»¶åˆ†åˆ«æœ‰å…³äºæœåŠ¡å™¨çš„å¤šç§æŒ‡æ ‡ï¼ˆCPU, MEM, DISKï¼‰</p><ul><li><p>ä»»åŠ¡ï¼šå¼‚å¸¸æ£€æµ‹ï¼ˆä½†æ˜¯å¥½åƒæ²¡æœ‰ç»™å‡ºå¼‚å¸¸çš„å½¢å¼ã€å¯¹åº”æ¯ä¸ªæ—¶é—´æˆ³çš„æ ‡ç­¾ï¼‰</p></li><li><p><a href="https://datasetsearch.research.google.com/save?query=coronavirus%20covid-19&docid=Ylcq22ZqJG16NqBSAAAAAA==">google link</a></p></li></ul></li><li><p><strong>ä¸é”™çš„æ•°æ®é›†</strong> Antarex HPC (High Performance computer) æ•…éšœæ•°æ®é›† <a href="https://zenodo.org/record/2553224#.X0iokshLiUk">link</a></p><ul><li>æè¿°ï¼šAntarexæ•°æ®é›†åŒ…å«åœ¨è¿›è¡Œæ•…éšœæ³¨å…¥æ—¶ä»ä½äºè‹é»ä¸–è”é‚¦ç†å·¥å­¦é™¢çš„åŒåå®éªŒHPCç³»ç»Ÿæ”¶é›†çš„è·Ÿè¸ªæ•°æ®ï¼Œç›®çš„æ˜¯å¯¹HPCç³»ç»Ÿè¿›è¡ŒåŸºäºæœºå™¨å­¦ä¹ çš„æ•…éšœæ£€æµ‹ç ”ç©¶ã€‚<br>ä¸ºäº†è·å–æ•°æ®ï¼Œæˆ‘ä»¬æ‰§è¡Œäº†åŸºå‡†æµ‹è¯•åº”ç”¨ç¨‹åºï¼Œå¹¶åŒæ—¶é€šè¿‡ä¸“ç”¨ç¨‹åºåœ¨ç‰¹å®šæ—¶é—´åœ¨ç³»ç»Ÿä¸­æ³¨å…¥äº†é”™è¯¯ï¼Œä»è€Œè§¦å‘äº†åº”ç”¨ç¨‹åºè¡Œä¸ºçš„å¼‚å¸¸ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ä¸­æ¶µç›–äº†å¹¿æ³›çš„æ•…éšœï¼Œ<br>ä»ç¡¬ä»¶æ•…éšœåˆ°é…ç½®é”™è¯¯ï¼Œæœ€åæ˜¯ç”±å…¶ä»–è¿‡ç¨‹çš„å¹²æ‰°å¯¼è‡´çš„æ€§èƒ½å¼‚å¸¸ã€‚è¿™æ˜¯é€šè¿‡ä½œè€…å¼€å‘çš„FINJæ•…éšœæ³¨å…¥å·¥å…·å®ç°çš„ã€‚</li></ul><p><strong>æ•°æ®é›†åŒ…å«ä¸¤ç§ç±»å‹çš„æ•°æ®ï¼š</strong> ä¸€ç§ç±»å‹çš„æ•°æ®æ˜¯æŒ‡ä¸€ç³»åˆ—CSVæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶éƒ½åŒ…å«ä¸€ç»„é€šè¿‡LDMS HPCç›‘è§†æ¡†æ¶é‡‡æ ·çš„ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ã€‚<br>å¦ä¸€ç§ç±»å‹æ˜¯æŒ‡æ—¥å¿—æ–‡ä»¶ï¼Œè¯¦ç»†æè¿°æ•°æ®é›†ä¸­æ¯ä¸ªæ—¶é—´ç‚¹çš„ç³»ç»ŸçŠ¶æ€ï¼ˆå³å½“å‰æ­£åœ¨è¿è¡Œçš„åŸºå‡†æµ‹è¯•åº”ç”¨ç¨‹åºæˆ–å·²æ³¨å…¥çš„æ•…éšœç¨‹åºï¼‰ã€‚<br>è¿™ç§ç»“æ„ä½¿ç ”ç©¶äººå‘˜å¯ä»¥å¯¹æ•°æ®é›†è¿›è¡Œå¹¿æ³›çš„ç ”ç©¶ã€‚è€Œä¸”ï¼Œç”±äºæˆ‘ä»¬æ˜¯é€šè¿‡æµå¼ä¼ è¾“è¿ç»­æ•°æ®æ”¶é›†æ•°æ®é›†çš„ï¼Œ<br>å› æ­¤åŸºäºè¯¥æ•°æ®é›†çš„ä»»ä½•ç ”ç©¶éƒ½å¯ä»¥è½»æ¾åœ°ä»¥åœ¨çº¿æ–¹å¼åœ¨çœŸå®çš„HPCç³»ç»Ÿä¸Šé‡ç°ã€‚<br><strong>æ•°æ®é›†åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š</strong>ç¬¬ä¸€éƒ¨åˆ†ä»…åŒ…æ‹¬ä¸CPUå’Œå†…å­˜ç›¸å…³çš„åŸºå‡†æµ‹è¯•åº”ç”¨ç¨‹åºå’Œæ•…éšœç¨‹åºï¼Œè€Œç¬¬äºŒéƒ¨åˆ†åˆ™ä»…ä¸ç¡¬ç›˜é©±åŠ¨å™¨ç›¸å…³ã€‚</p><ul><li><p>æ•°æ®æè¿°ï¼šæ³¨å…¥å¼‚å¸¸ç”¨çš„è„šæœ¬ä¹Ÿåœ¨æ•°æ®é›†ä¸­å¯ä»¥æ‰¾åˆ°</p></li><li><p>ä»»åŠ¡ï¼šå¼‚å¸¸æ£€æµ‹</p></li><li><p><a href="https://datasetsearch.research.google.com/save?query=coronavirus%20covid-19&docid=8mGvdORXdb72rgUVAAAAAA==">google link</a></p></li></ul></li></ol><h2 id="ä¸€èˆ¬çš„ç›‘è§†æ•°æ®"><a href="#ä¸€èˆ¬çš„ç›‘è§†æ•°æ®" class="headerlink" title="ä¸€èˆ¬çš„ç›‘è§†æ•°æ®"></a>ä¸€èˆ¬çš„ç›‘è§†æ•°æ®</h2><ol><li><p><a href="https://catalog.data.gov/dataset/scalable-time-series-change-detection-for-biomass-monitoring-using-gaussian-process">ç”Ÿç‰©é‡ç›‘æµ‹æ•°æ®é›†, TIME SERIES CHANGE DETECTION FOR BIOMASS MONITORING</a></p><ul><li>æè¿°ï¼šç”Ÿç‰©é‡ç›‘æµ‹ï¼Œç‰¹åˆ«æ˜¯æ£€æµ‹æŸä¸ªåœ°ç†åŒºåŸŸçš„ç”Ÿç‰©é‡æˆ–æ¤è¢«å˜åŒ–ï¼Œå¯¹äºç ”ç©¶ç³»ç»Ÿçš„ç¢³å¾ªç¯è‡³å…³é‡è¦ï¼Œå¹¶ä¸”åœ¨ç†è§£æ°”å€™å˜åŒ–åŠå…¶å½±å“æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li><li>ä»»åŠ¡ï¼štime series change point detection</li><li><a href="https://datasetsearch.research.google.com/search?query=monitoring,%20time%20series&docid=tv/EXqiscaRktAxXAAAAAA==">google link</a></li></ul></li><li><p>ç½‘ç»œæµé‡<br><a href="https://www.kaggle.com/crawford/computer-network-traffic">https://www.kaggle.com/crawford/computer-network-traffic</a></p></li><li><p>æ°´æ³µæ•…éšœ<br><a href="https://www.kaggle.com/nphantawee/pump-sensor-data">https://www.kaggle.com/nphantawee/pump-sensor-data</a></p></li><li><p>SLAç®¡ç†<br><a href="https://www.kaggle.com/imenbenyahia/clearwatervnf-virtual-ip-multimedia-ip-system?select=bono-io.read_kbytes_sec.csv">https://www.kaggle.com/imenbenyahia/clearwatervnf-virtual-ip-multimedia-ip-system?select=bono-io.read_kbytes_sec.csv</a></p></li></ol><ol start="5"><li><p>åŸºäºå¤šä¼ æ„Ÿå™¨æ•°æ®çš„æ¶²å‹è¯•éªŒå°çš„çŠ¶æ€è¯„ä¼° <a href="https://www.kaggle.com/jjacostupa/condition-monitoring-of-hydraulic-systems">link</a></p><ul><li><p>æè¿°ï¼šè¯¥æ•°æ®é›†æ˜¯é€šè¿‡æ¶²å‹è¯•éªŒå°å®éªŒè·å¾—çš„ã€‚è¯¥è¯•éªŒå°ç”±ä¸€ä¸ªä¸»è¦å·¥ä½œè£…ç½®å’Œä¸€ä¸ªè¾…åŠ©å†·å´è¿‡æ»¤å›è·¯ç»„æˆï¼Œå®ƒä»¬é€šè¿‡æ²¹ç®±[1]ï¼Œ[2]è¿æ¥ã€‚è¯¥ç³»ç»Ÿå‘¨æœŸæ€§åœ°é‡å¤æ’å®šçš„è´Ÿè½½å¾ªç¯ï¼ˆæŒç»­æ—¶é—´ä¸º60ç§’ï¼‰ï¼Œ<br>å¹¶åœ¨å®šé‡æ”¹å˜å››ä¸ªæ¶²å‹ç»„ä»¶ï¼ˆå†·å´å™¨ï¼Œé˜€é—¨ï¼Œæ³µå’Œè“„èƒ½å™¨ï¼‰çš„çŠ¶æ€çš„åŒæ—¶ï¼Œæµ‹é‡å‹åŠ›ï¼Œä½“ç§¯æµé‡å’Œæ¸©åº¦ç­‰è¿‡ç¨‹å€¼ã€‚</p></li><li><p>ä»»åŠ¡ï¼šå¼‚å¸¸æ£€æµ‹ã€åˆ†ç±»ã€é¢„æµ‹</p></li><li><p><a href="https://datasetsearch.research.google.com/save?query=coronavirus%20covid-19&docid=pCOC5zkeO5a8CSeGAAAAAA==">google link</a></p></li></ul></li><li><p>å·¥ä¸šæµæ°´çº¿åˆ€ç‰‡ç£¨æŸæƒ…å†µæ•°æ®é›† <a href="https://www.kaggle.com/inIT-OWL/one-year-industrial-component-degradation?select=01-04T184424_001_mode1.csv">link</a></p><ul><li>å¥½åƒæ²¡æœ‰æ˜ç¡®labelï¼Œéœ€è¦è”ç³»ä½œè€…</li></ul></li></ol><h2 id="å…¶ä»–æœ‰è¶£çš„æ•°æ®é›†"><a href="#å…¶ä»–æœ‰è¶£çš„æ•°æ®é›†" class="headerlink" title="å…¶ä»–æœ‰è¶£çš„æ•°æ®é›†"></a>å…¶ä»–æœ‰è¶£çš„æ•°æ®é›†</h2><ol><li>è¾¹ç¼˜è®¡ç®—æ•°æ®é›†<br>è¾¹ç¼˜æœåŠ¡å™¨çš„ä½ç½®ä¸ç”¨æˆ·çš„ä½ç½®<br><a href="https://www.kaggle.com/salmaneunus/edge-computing-edge-servers">https://www.kaggle.com/salmaneunus/edge-computing-edge-servers</a>?</li></ol><ol start="2"><li><p><a href="https://www.kaggle.com/ntnu-testimon/paysim1">äº¤æ˜“æ¬ºè¯ˆæ£€æµ‹æ•°æ®é›†</a></p><ul><li>æè¿°ï¼šPaySimåŸºäºä»ä¸€ä¸ªéæ´²å›½å®¶/åœ°åŒºå®æ–½çš„ç§»åŠ¨è´§å¸æœåŠ¡çš„ä¸€ä¸ªæœˆè´¢åŠ¡æ—¥å¿—ä¸­æå–çš„çœŸå®äº¤æ˜“æ ·æœ¬æ¥æ¨¡æ‹Ÿç§»åŠ¨è´§å¸äº¤æ˜“ã€‚åŸå§‹æ—¥å¿—æ˜¯ç”±ä¸€å®¶è·¨å›½å…¬å¸æä¾›çš„ï¼Œè¯¥å…¬å¸æ˜¯ç§»åŠ¨é‡‘èæœåŠ¡çš„æä¾›å•†ï¼Œè¯¥æœåŠ¡ç›®å‰åœ¨å…¨çƒ14ä¸ªä»¥ä¸Šçš„å›½å®¶/åœ°åŒºä¸­è¿è¡Œã€‚</li><li>ä»»åŠ¡ï¼šäº¤æ˜“æ¬ºè¯ˆæ£€æµ‹</li><li>å½¢å¼ï¼šraw data, åŒ…å«äº¤æ˜“é‡‘é¢ã€äº¤æ˜“å®¢æˆ·ã€åˆå§‹ä½™é¢ä¸æ–°ä½™é¢ã€æ ‡ç­¾ï¼ˆæ˜¯å¦ä¸ºæ¬ºè¯ˆäº¤æ˜“ï¼‰</li><li><a href="https://datasetsearch.research.google.com/save?query=coronavirus%20covid-19&docid=LUidW2DnXG+T/EA2AAAAAA==">google link</a></li></ul></li></ol><ol start="3"><li><p>Data for: A study on leading machine learning techniques for high order fuzzy time series forecasting <a href="https://data.mendeley.com/datasets/xc6c8xr564/1">link</a></p><ul><li>æè¿°ï¼š14ä¸ªå­æ•°æ®é›†çš„åˆé›†ï¼Œå…·ä½“å‚è§ <a href="https://www.sciencedirect.com/science/article/pii/S095219761930226X#fig5">link</a></li><li>ä»»åŠ¡ï¼šæ—¶é—´åºåˆ—é¢„æµ‹</li><li><a href="https://datasetsearch.research.google.com/save?query=coronavirus%20covid-19&docid=6GYYktv0MMeQl5jGAAAAAA==">google link</a></li></ul></li><li><p>BPIæŒ‘æˆ˜ <a href="https://www.narcis.nl/dataset/RecordID/uuid%3Ac3e5d162-0cfd-4bb0-bd82-af5268819c35/Language/EN">link</a></p><ul><li>æè¿°ï¼š <strong>The challenge is to design a (draft) predictive model, which can be used to implement in a BI environment.</strong> </li></ul><p> <strong>The purpose of this predictive model will be to support Business Change Management in implementing software releases</strong><br> <strong>with less impact on the Service Desk and/or IT Operations.</strong> We have prepared several case-files with anonymous<br> information from Rabobank Netherlands Group ICT for this challenge. The files contain record details from an ITIL<br> Service Management tool called HP Service Manager. We provide you with extracts in CSV with the Interaction-,<br> Incident- or Change-number as case ID. Next to these case-files, we provide you with an Activity-log, related to<br> the Incident-cases. There is also a document detailing the data in the CSV file and providing background to the<br> Service Management tool.</p><ul><li><a href="https://datasetsearch.research.google.com/save?query=coronavirus%20covid-19&docid=ICMC800RzMnUdhDMAAAAAA==">google link</a></li></ul></li></ol><ol start="5"><li><pre><code>ä¸»æœºç½‘ç»œæµé‡æ—¶é—´åºåˆ—2019/01 [link](https://zenodo.org/record/2669079#.X0i2AMhLiUk)</code></pre></li></ol><pre><code>+ æè¿°ï¼šæ•°æ®é›†äº2019å¹´ 1 æœˆåœ¨ä¸€ä¸ªæœˆçš„æ—¶é—´å†…æ”¶é›†ã€‚æ”¶é›†IPæµçš„è§‚å¯Ÿç‚¹ä½äºå¤§å­¦æ ¡å›­ç½‘ç»œçš„è¾¹ç•Œã€‚æ ¡å›­å¤§å­¦ç½‘ç»œå¯ä½¿ç”¨16 CIDR IPv4ç½‘ç»œèŒƒå›´ï¼Œå¹¶ä¸”åŒ…å«ä»è¿æ¥å®¿èˆçš„éƒ¨åˆ†ï¼ˆé€šè¿‡æœåŠ¡å™¨éƒ¨åˆ†ï¼‰åˆ°åŒ…å«å¤§å­¦ç®¡ç†äººå‘˜çš„å·¥ä½œç«™çš„éƒ¨åˆ†åœ¨å†…çš„å„ç§ç½‘ç»œéƒ¨åˆ†ã€‚ç”¨äºåˆ›å»ºæ•°æ®é›†çš„åŸå§‹IPæµçš„å¤§å°è¶…è¿‡860GBã€‚æˆ‘ä»¬æ•°æ®é›†ä¸­çš„ä¸»æœºç”±å…¶æºIPv4åœ°å€æ ‡è¯†ã€‚  + æ•°æ®é›†åŒ…å«ä»¥ä¸‹å˜é‡ï¼š    - èšåˆ -ä½¿ç”¨å‡å€¼/æœ€å¤§/æœ€å°èšåˆå‡½æ•°ä»ä¸€å°æ—¶ä¸ç›¸äº¤çš„çª—å£ä¸­èšåˆçš„äº”åˆ†é’Ÿæ€»ä½“ç§¯ä¸­åˆ›å»º        - æµæ•°ï¼ˆFLï¼‰ -ç»™å®šæºIPçš„æµæ•°         - æ•°æ®åŒ…æ•°ï¼ˆPKTï¼‰ -ç»™å®šæºIPçš„æ•°æ®åŒ…æ•°        - å­—èŠ‚æ•°ï¼ˆBYTï¼‰ -ç»™å®šæºIPçš„æ•°æ®åŒ…æ•°        - æµé‡æŒç»­æ—¶é—´ï¼ˆDURï¼‰ -å¹³å‡æµé‡æŒç»­æ—¶é—´ï¼ˆä»¥ç§’ä¸ºå•ä½ï¼‰    - ä¸é‡å¤è®¡æ•°  -ä½¿ç”¨å‡å€¼/æœ€å¤§å€¼/æœ€å°å€¼èšåˆå‡½æ•°ï¼Œåœ¨ä¸€å°æ—¶ä¸ç›¸äº¤çš„çª—å£ä¸­èšåˆçš„äº”åˆ†é’Ÿçª—å£ä¸­æ¯ä¸ªå˜é‡çš„ä¸é‡å¤å€¼è®¡æ•°        - å¯¹ç­‰ä½“æ•°ï¼ˆPEERï¼‰ -ç»™å®šæºIPçš„ä¸åŒé€šä¿¡å¯¹ç­‰ä½“æ•°        - ç«¯å£æ•°ï¼ˆPORTSï¼‰ -ç»™å®šæºIPçš„ä¸åŒç›®æ ‡ç«¯å£æ•°        - åè®®æ•°ï¼ˆPROTOï¼‰ -ç»™å®šæºIPçš„ä¸åŒé€šä¿¡åè®®æ•°        - ASå·æ•°é‡ï¼ˆASï¼‰ -ç»™å®šæºIPçš„ä¸åŒç›®æ ‡ASå·æ•°é‡        - å›½å®¶æ•°é‡ï¼ˆCTRYï¼‰ -ç»™å®šæºIPçš„ä¸åŒç›®æ ‡å›½å®¶/åœ°åŒºçš„æ•°é‡    - æ ‡ç­¾        - èŒƒå›´ï¼ˆRNGï¼‰ -ä¸»æœºæ‰€å±çš„ç½‘ç»œèŒƒå›´ï¼ˆåŒ¿åï¼‰        - å•ä½ï¼ˆUNTï¼‰ -æ‹¥æœ‰ç½‘ç»œèŒƒå›´çš„ç®¡ç†å•ä½        - å­å•ä½ï¼ˆSUB-UNTï¼‰ -è¯¥å•ä½çš„å­å•ä½        + ä½†æ˜¯å¥½åƒæ²¡æœ‰è§„å®šå…·ä½“çš„ä»»åŠ¡ï¼Œlabeléƒ½æ˜¯ä¸»æœºæ‰€å±çš„å•ä½ï¼Œéš¾é“æ˜¯åˆ†ç±»</code></pre><ol start="6"><li><p>ç”¨äºåº”ç”¨ç¨‹åºçº§ç›‘è§†çš„ä¸åŒå¤šæ ¸å¤„ç†å™¨å¯¹è¿è¡Œæ—¶å¼€é”€çš„å½±å“çš„æ¯”è¾ƒ <a href="https://zenodo.org/record/7619#.X0i4jMhLiUk">link</a></p><ul><li>æè¿°ï¼šè¿ç»­è¿è¡Œçš„è½¯ä»¶ç³»ç»Ÿéœ€è¦åº”ç”¨ç¨‹åºçº§ç›‘è§†ï¼Œä»¥åœ¨è¿è¡Œæ—¶ä¿æŒå…¶æ€§èƒ½å’Œå¯ç”¨æ€§ã€‚è½¯ä»¶ç³»ç»Ÿçš„æ€§èƒ½ç›‘è§†éœ€è¦å°†æ—¶é—´åºåˆ—<br>æ•°æ®å­˜å‚¨åœ¨ç›‘è§†æ—¥å¿—æˆ–æµä¸­ã€‚è¿™æ ·çš„ç›‘è§†å¯èƒ½ä¼šå¯¼è‡´è¢«ç›‘è§†ç³»ç»Ÿçš„å¤§é‡è¿è¡Œæ—¶å¼€é”€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šæ ¸å¤„ç†å™¨å¯¹<br>Kiekeråº”ç”¨ç¨‹åºçº§ç›‘è§†æ¡†æ¶çš„å¼€é”€çš„å½±å“ã€‚æˆ‘ä»¬å°†ç›‘æ§å¼€é”€åˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼Œå¹¶ä½¿ç”¨å¾®åŸºå‡†å¯¹å—æ§å®éªŒå®¤è¿›è¡Œå¹¿æ³›çš„å®éªŒï¼Œ<br>ä»¥é‡åŒ–åœ¨å—æ§å’Œå¯é‡å¤æ¡ä»¶ä¸‹ç›‘æ§å¼€é”€çš„è¿™äº›éƒ¨åˆ†çš„ç»“æœã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œé€šè¿‡å¼‚æ­¥å†™å…¥ç›‘è§†æ—¥å¿—ï¼Œå¯ä»¥åœ¨å¤šæ ¸å¤„ç†å™¨ä¸Šè¿›<br>ä¸€æ­¥å‡å°‘Kiekeræ¡†æ¶å·²ç»å¾ˆä½çš„å¼€é”€ã€‚</li><li>æä¾›äº†æŸä¸ªå®éªŒåœ¨å¤šä¸ªCPUä¸Šçš„è®°å½•ï¼ŒåŒ…æ‹¬AMD, Intel, T6360, ä»»åŠ¡æœªçŸ¥</li></ul></li><li><p>å¤§å‹RAIDç£ç›˜ç³»ç»Ÿçš„ç£ç›˜æ›¿æ¢æ—¥å¿—æ–‡ä»¶ç¤ºä¾‹ï¼Œç”¨äºè¿›è¡Œé¢„æµ‹æ€§ç»´æŠ¤åˆ†æ <a href="https://zenodo.org/record/2580162#.X0i568hLiUk">link</a></p><ul><li>æè¿°ï¼šåœ¨ä¸€ä¸ªç£ç›˜ç³»ç»Ÿä¸­æ›´æ¢å…¶ä¸­çš„ä¸€ä¸ªç£ç›˜ï¼Œå¹¶è®°å½•ç³»ç»Ÿçš„æ—¥å¿—æ–‡ä»¶ã€‚è§‚å¯Ÿç£ç›˜çƒ­æ’æ‹”å¯¹ç³»ç»Ÿè¿è¡Œçš„å½±å“ã€‚</li><li>å®˜æ–¹æä¾›çš„ä¸€äº›è§‚å¯Ÿ<ol><li>éšç€æ—¶é—´çš„æµé€ï¼Œé”™è¯¯ä¼šåœ¨åˆ†ç»„çš„ç¾¤é›†ä¸­å‘ç”Ÿï¼Œä»è€Œå¯ä»¥è¿›è¡Œé¢„æµ‹æ€§ç»´æŠ¤ã€‚</li><li>æ—¶é—´å’Œç©ºé—´ä¸Šçš„ç²’åº¦ï¼šçƒ­ç£ç›˜äº¤æ¢ä¸ä¼šå¹²æ‰°RAIDç³»ç»Ÿä¸­çš„ç³»ç»Ÿå¯ç”¨æ€§ã€‚ å…¨æœºæ¶äº¤æ¢å¯ä»¥ã€‚ æ›´ç³Ÿç³•çš„æ˜¯ï¼Œé€šè¿‡åœ¨æŸä¸ªç‰¹<br>å®šæ—¶é—´è´­ä¹°å®Œæ•´çš„ï¼Œåºå¤§çš„ç³»ç»Ÿï¼Œä¸€å®¶å…¬å¸å°†åœ¨ç³»ç»Ÿç”Ÿå‘½å‘¨æœŸçš„æœ€åèµ°å‘åˆ›ä¼¤æ€§äº‹ä»¶ã€‚ åœ¨è¾ƒå°çš„æ—¶é—´é—´éš”å†…å®‰è£…è¾ƒå°çš„ç³»<br>ç»Ÿç»„ä»¶ï¼ˆé˜…è¯»ï¼šæœºæ¶ï¼‰ä¸ä¼šå±åŠæ“ä½œçš„è¿ç»­æ€§ã€‚</li></ol></li></ul></li><li><p>Application Detection through Rich Monitoring Data <a href="https://springernature.figshare.com/articles/Artifact_for_Taxonomist_Application_Detection_through_Rich_Monitoring_Data/6384248">link</a></p><ul><li>æè¿°ï¼šThe related study develops a technique named â€˜Taxonomistâ€™ to identify applications running on supercomputers,<br>using machine learning to classify known applications and detect unknown applications. </li></ul><p> <strong>The technique uses monitoring data such as CPU and memory usage metrics and hardware counters collected from</strong><br> <strong>supercomputers. The aims of this technique include providing an alternative to â€˜naiveâ€™ application detection</strong><br> <strong>methods based on names of processes and scripts, and helping prevent fraud, waste and abuse in supercomputers.</strong></p><ul><li>ä»»åŠ¡ï¼šåº”è¯¥æ˜¯åˆ†ç±»ï¼Œè¿˜æœ‰å¯èƒ½æ˜¯one-class classification</li><li><a href="https://datasetsearch.research.google.com/save?query=coronavirus%20covid-19&docid=95bGlb7wMSmRtTfiAAAAAA==">google link</a></li></ul></li><li><p>å±€åŸŸç½‘ç½‘ç»œç¨³å®šæ€§ - æµ‹é‡æ— çº¿ä¸åŸºäºä»¥å¤ªç½‘çš„å±€åŸŸç½‘çš„å“åº”æ—¶é—´ <a href="https://www.kaggle.com/garystafford/ping-data">link</a></p><ul><li>æè¿°ï¼šç›®çš„æ˜¯åœ¨è¿æ¥åˆ°Internetæ—¶æ•è·å¹¶å¯è§†åŒ–LANç½‘ç»œçš„æ—¶åºå˜åŒ–ã€‚ä¸ºæ­¤ï¼Œä»¥10ç§’ä¸ºé—´éš”ä»ç½‘ç»œä¸Šçš„ä¸€ç³»åˆ—IoTæ”¶é›†è®¾å¤‡æ”¶é›†ping<br>å“åº”æ—¶é—´ã€‚å®šæ—¶æ˜¯ä»2.4 GHzæ— çº¿å’Œ100 Mbpsä»¥å¤ªç½‘æ”¶é›†çš„ã€‚æµ‹é‡äº†ä»è®¾å¤‡åˆ°æœ¬åœ°Internetè·¯ç”±å™¨ä»¥åŠInternetä¸Šç¬¬ä¸€è·³æœåŠ¡å™¨çš„æ—¶é—´ã€‚<br>æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„é—´éš™è¡¨ç¤ºLANä¸­æ–­æˆ–è·¯ç”±å™¨è®¿é—®Internetçš„èƒ½åŠ›ä¸­æ–­ã€‚<br>pingå®ç”¨ç¨‹åºä½¿ç”¨ICMPåè®®çš„å¼ºåˆ¶æ€§ECHO_REQUESTæ•°æ®æŠ¥ECHO_RESPONSEä»ä¸»æœºæˆ–ç½‘å…³è·å–ICMP ã€‚</li><li>æ²¡æœ‰ç»™å®šlabelï¼Œä½†æ˜¯åº”è¯¥æ˜¯ä¸€ä¸ªåˆ†ç±»é—®é¢˜</li><li><a href="https://datasetsearch.research.google.com/save?query=coronavirus%20covid-19&docid=J1jGnseIcAjUTiedAAAAAA==">google link</a></li></ul></li></ol><hr><p>SMAP (Soil Moisture Active Passive satellite) and MSL (Mars Science Laboratory rover) are two public datasets from NASA [6]. Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom Soderstrom. 2018. Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic Thresholding. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &#38; Data Mining (KDD â€™18). ACM, New York, NY, USA, 387â€“395.</p>]]></content>
      
      
      
        <tags>
            
            <tag> survey </tag>
            
            <tag> time series </tag>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in SIGKDD 2020 (2020.08.23)</title>
      <link href="/uncategorized/paperlistfile/KDD2020/"/>
      <url>/uncategorized/paperlistfile/KDD2020/</url>
      
        <content type="html"><![CDATA[<p>Accepted paper list: <a href="https://www.kdd.org/kdd2020/accepted-papers">Link</a></p><span id="more"></span><h2 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h2><ul><li><p>A Geometric Approach to Time Series Chains Improves Robustness</p><p>Authors: Makoto Imamura: Tokai University; Takaaki Nakamura: Mitsubishi Electric Corporation; Eamonn Keogh: University of California - Riverside</p></li><li><p>Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks</p><p>Authors: Zonghan Wu: University of Technology Sydney; Shirui Pan: Monash University; Guodong Long: University of Technology Sydney; Jing Jiang: University of Technology Sydney; Xiaojun Chang: Monash University; Chengqi Zhang: University of Technology Sydney</p></li><li><p>Fast R-STL: Efficient and Robust Seasonal-Trend Decompositionfor Time Series with Complex Patterns</p><p>Authors: Qingsong Wen: Alibaba Group U.S.; Zhe Zhang: Alibaba Group U.S.; Yan Li: Alibaba Group U.S.; Liang Sun: Alibaba Group U.S.</p></li><li><p>Fitbit for Chickens? Time Series Data Mining Can Increase the Productivity of Poultry Farms</p><p>Authors: Alireza Abdoli: University of California Riverside; Sara Alaee: University of California Riverside; Shima Imani: University of California Riverside; Amy Murillo: University of California Riverside ; Alec Gerry: UC Riverside; Leslie Hickle: FarmSense Inc; Eamonn Keogh: UC Riverside</p></li><li><p>USAD : UnSupervised Anomaly Detection on multivariate time series</p><p>Authors: Julien Audibert: Orange EURECOM; Pietro Michiardi: EURECOM; FrÃ©dÃ©ric Guyard: Orange Labs; SÃ©bastien Marti: Orange; Maria A. Zuluaga: EURECOM</p></li><li><p>HiTANet: Hierarchical Time-Aware Attention Networks for Risk Prediction on Electronic Health Records</p><p>Authors: Junyu Luo: The Pennsylvania State University; Muchao Ye: The Pennsylvania State University; Cao Xiao: IQVIA; Fenglong Ma: The Pennsylvania State University</p></li><li><p>Identifying Sepsis Subphenotypes via Time-Aware Multi-Modal Auto-Encoder</p><p>Authors: Changchang Yin: The Ohio State University; Ruoqi Liu: The Ohio State University; Dongdong Zhang: The Ohio State University; Ping Zhang: The Ohio State University</p></li><li><p>Local Motif Clustering on Time-Evolving Graphs</p><p>Authors: Dongqi Fu: University of Illinois at Urbana-Champaign; Dawei Zhou: University of Illinois at Urbana-Champaign; Jingrui He: University of Illinois at Urbana-Champaign</p></li><li><p>Multi-Source Deep Domain Adaptation with Weak Supervision for Time-Series Sensor Data</p><p>Authors: Garrett Wilson: Washington State University; Janardhan Rao Doppa: Washington State University; Diane J. Cook: Washington State University</p></li><li><p>Sliding Sketches: A Framework using Time Zones for Data Stream Processing in Sliding Windows</p><p>Authors: Xiangyang Gou: Peking University; Long He: Peking University; Yinda Zhang: Peking University; Ke Wang: Peking University; Xilai Liu: Peking University; Tong Yang: Peking University; Yi Wang: Southern University of Science and Technology; Bin Cui: Peking University</p></li></ul><ul><li><p>Attention based multi-modal new product sales time-series forecasting</p><p>Authors: Vijay Ekambaram: IBM Research; Kushagra Manglik: IBM Research; Sumanta Mukherjee: IBM Research; Surya Shravan Kumar Sajja: IBM Research; Satyam Dwivedi: IBM Research; Vikas Raykar: IBM Research</p></li></ul><ul><li><p>BusTr: predicting bus travel times from real-time traffic</p><p>Authors: Richard Barnes: UC Berkeley; Senaka Buthpitiya: Google Research; James Cook: N ne; Alex Fabrikant: Google Research; Andrew Tomkins: Google Research; Fangzhou Xu: Google Research</p></li></ul><ul><li><p>Calendar Graph Neural Networks for Modeling Time Structures in Spatiotemporal User Behaviors</p><p>Authors: Daheng Wang: University of Notre Dame; Meng Jiang: University of Notre Dame; Munira Syed: University of Notre Dame; Oliver Conway: Conde Nast; Vishal Juneja: Conde Nast; Sriram Subramanian: Conde Nast; Nitesh V. Chawla: University of Notre Dame</p></li></ul><ul><li><p>HetETA: Heterogeneous Information Network Embedding for Estimating Time of Arrival</p><p>Authors: Huiting Hong: AI Labs Didi Chuxing Beijing China ; Yucheng Lin: AI Labs Didi Chuxing Beijing China ; Xiaoqing Yang: AI Labs Didi Chuxing Beijing China ; Zang Li: AI Labs Didi Chuxing Beijing China ; Jieping Ye: AI Labs Didi Chuxing Beijing China ; Kun Fu: AI Labs Didi Chuxing Beijing China ; Zheng Wang: AI Labs Didi Chuxing Beijing China ; Xiaohu Qie: Technology Ecosystem Development Didi Chuxing Beijing China</p></li></ul><ul><li><p>Heidegger: Interpretable Temporal Causal Discovery</p><p>Authors: Mehrdad Mansouri: Simon Fraser University; Ali Arab: Simon Fraser University; Zahra Zohrevand: Simon Fraser University; Martin Eser: Simon Fraser University</p></li></ul><ul><li><p>LogPar: Logistic PARAFAC2 Factorization for Temporal Binary Data with Missing Values</p><p>Authors: Kejing Yin: Hong Kong Baptist University; Ardavan Afshar: Georgia Institute of Technology; Joyce Ho: Emory University; William Cheung: Hong Kong Baptist University; Chao Zhang: Georgia Institute of Technology; Jimeng Sun: University of Illinois Urbana-Champaign</p></li></ul><ul><li><p>Predicting Temporal Sets with Deep Neural Networks</p><p>Authors: Le Yu: Beihang University; Leilei Sun: Beihang University; Bowen Du: Beihang University; Chuanren Liu: University of Tennessee; Hui Xiong: Rutgers University; Weifeng Lv: Beihang University</p></li></ul><h2 id="missing-value-or-irregularly-sampled-time-series"><a href="#missing-value-or-irregularly-sampled-time-series" class="headerlink" title="missing value or irregularly sampled time series"></a>missing value or irregularly sampled time series</h2><ul><li><p>Missing Value Imputation for Mixed Data via Gaussian Copula</p><p>Authors: Yuxuan Zhao: Cornell University; Madeleine Udell: Cornell University</p></li></ul><ul><li><p>LogPar: Logistic PARAFAC2 Factorization for Temporal Binary Data with Missing Values</p><p>Authors: Kejing Yin: Hong Kong Baptist University; Ardavan Afshar: Georgia Institute of Technology; Joyce Ho: Emory University; William Cheung: Hong Kong Baptist University; Chao Zhang: Georgia Institute of Technology; Jimeng Sun: University of Illinois Urbana-Champaign</p></li></ul><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><ul><li><p>Recurrent Halting Chain for Early Multi-label Classification</p><p>Authors: Thomas Hartvigsen: Worcester Polytechnic Institute; Cansu Sen: Worcester Polytechnic Institute; Xiangnan Kong: Worcester Polytechnic Institute; Elke Rundensteiner: Worcester Polytechnic Institute</p></li><li><p>Recurrent Networks for Guided Multi-Attention Classification</p><p>Authors: Xin Dai: Worcester Polytechnic Institute; Xiangnan Kong: Worcester Polytechnic Institute; Tian Guo: Worcester Polytechnic Institute; John Lee: Worcester Polytechnic Institute; Xinyue Liu: Worcester Polytechnic Institute; Constance Moore: University of Massachusetts Medical School</p></li><li><p>A Self-Evolving Mutually-Operative Recurrent Network-based Model for Online Tool Condition Monitoring in Delay Scenario</p><p>Authors: Monidipa Das: Nayang Technological University NTU Singapore ; Mahardhika Pratama: Nanyang Technological University NTU ; Tegoeh Tjahjowidodo: KU Leuven</p></li><li><p>A Sleeping, Recovering Bandit Algorithm for Optimizing Recurring Notifications</p><p>Authors: Kevin Yancey: Duolingo; Burr Settles: Duolingo</p></li><li><p>Hypergraph Convolutional Recurrent Neural Network</p><p>Authors: Jaehyuk Yi: KAIST; Jinkyoo Park: KAIST</p></li></ul><h2 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a>Anomaly Detection</h2><ul><li><p>Isolation Distributional Kernel: A new tool for kernel based anomaly detection</p><p>Authors: Kai Ming Ting: Nanjing University; Takashi Washio: Osaka University; Bi-Cun Xu: Nanjing University; Zhi-Hua Zhou: Nanjing University</p></li><li><p>USAD : UnSupervised Anomaly Detection on multivariate time series</p><p>Authors: Julien Audibert: Orange EURECOM; Pietro Michiardi: EURECOM; FrÃ©dÃ©ric Guyard: Orange Labs; SÃ©bastien Marti: Orange; Maria A. Zuluaga: EURECOM</p></li><li><p>Generic Outlier Detection in Multi-Armed Bandit</p><p>Authors: Yikun Ban: University of Illinois at Urbana-Champaign; Jingrui He: University of Illinois at Urbana-Champaign</p></li><li><p>Ultrafast Local Outlier Detection from a Data Stream with Stationary Region Skipping</p><p>Authors: Susik Yoon: Korea Advanced Institute of Science and Technology; Jae-Gil Lee: Korea Advanced Institute of Science and Technology; Byung Suk Lee: University of Vermont</p></li><li><p>Interleaved Sequence RNNs for Fraud Detection <em>æ˜¯ä¸€ä¸ªå…³äºé“¶è¡Œå¡æ¬ºè¯ˆæ£€æµ‹çš„ï¼Œæ•°æ®æ˜¯å¤šç»´éå‡åŒ€é‡‡æ ·åºåˆ—ã€‚è¿™ä¸ªæ£€æµ‹é—®é¢˜è¢«è½¬åŒ–ä¸ºä¸€ä¸ªç›‘ç£å­¦ä¹ é—®é¢˜ã€‚</em></p><p>Authors: Bernardo Branco: Feedzai; Pedro Abreu: QuantumBlack a McKinsey company ; Ana Sofia Gomes: Feedzai; Mariana Almeida: Cleverly; JoÃ£o Tiago AscensÃ£o: Feedzai; Pedro Bizarro: Feedzai</p></li></ul><ul><li><p>Grounding Visual Concepts for Multimedia Event Detection and Multimedia Event Captioning in Zero-shot Setting</p><p>Authors: Zhihui Li: University of New South Wales; Xiaojun Chang: Monash University; Lina Yao: University of New South Wales; Shirui Pan: Monash University; Zongyuan Ge: Monash University; Huaxiang Zhang: Shandong Normal University</p></li><li><p>Multi-class Data Description for Out-of-distribution Detection</p><p>Authors: Dongha Lee: Pohang University of Science and Technology; Sehun Yu: Pohang University of Science and Technology; Hwanjo Yu: Pohang University of Science and Technology</p></li><li><p>CrowdQuake: A Networked System of Low-Cost Sensors for Earthquake Detection via Deep Learning</p><p>Authors: Xin Huang: Florida Institute of Technology; Jangsoo Lee: Kyungpook National University; Young-Woo Kwon: Kyungpook National University; Chul-Ho Lee: Florida Institute of Technology</p></li></ul><ul><li><p>DATE: Dual Attentive Tree-aware Embedding for Customs Fraud Detection <em>æ˜¯ä¸€ä¸ªå…³äºäº¤æ˜“å¼‚å¸¸æ£€æµ‹çš„ï¼Œæ•°æ®é›†æ˜¯äº¤æ˜“è®°å½•ï¼ŒåŸºäºä¸€ä¸ªTree-awareçš„ç»“æ„è¿›è¡Œç‰¹å¾æå–</em></p><p>Authors: Sundong Kim: Institute for Basic Science; Yu-Che Tsai: National Cheng Kung University; Karandeep Singh: Institute for Basic Science; Yeonsoo Choi: World Customs Organization; Etim Ibok: Nigeria Customs Service; Cheng-Te Li: National Cheng Kung University; Meeyoung Cha: Institute for Basic Science</p></li></ul><h2 id="Change-point-detection"><a href="#Change-point-detection" class="headerlink" title="Change point detection"></a>Change point detection</h2><ul><li><p>A Non-Iterative Quantile Change Detection Method in Mixture Model with Heavy-Tailed Components</p><p>Authors: Yuantong Li: Purdue University; Qi Ma: North Carolina State University; Sujit Ghosh: North Carolina State University</p></li></ul><ul><li><p>Laplacian Change Point Detection for Dynamic Graphs</p><p>Authors: Shenyang Huang: McGill University, Quebec Institute for Artificial Intelligence (Mila); Yasmeen Hitti: McGill University, Quebec Institute for Artificial Intelligence (Mila); Guillaume Rabusseau: University of Montreal, Quebec Institute for Artificial Intelligence (Mila); Reihaneh Rabbany: McGill University, Quebec Institute for Artificial Intelligence (Mila)</p></li></ul><h2 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h2><ul><li><p>Interleaved Sequence RNNs for Fraud Detection</p><p>Authors: Bernardo Branco: Feedzai; Pedro Abreu: QuantumBlack a McKinsey company ; Ana Sofia Gomes: Feedzai; Mariana Almeida: Cleverly; JoÃ£o Tiago AscensÃ£o: Feedzai; Pedro Bizarro: Feedzai</p></li></ul><h2 id="Interpretable"><a href="#Interpretable" class="headerlink" title="Interpretable"></a>Interpretable</h2><ul><li><p>Adversarial Infidelity Learning for Model Interpretation</p><p>Authors: Jian Liang: Cloud and Smart Industries Group, Tencent, China; Bing Bai: Cloud and Smart Industries Group, Tencent, China; Yuren Cao: Cloud and Smart Industries Group, Tencent, China; Kun Bai: Cloud and Smart Industries Group, Tencent, China; Fei Wang: Cornell University</p></li><li><p>Heidegger: Interpretable Temporal Causal Discovery</p><p>Authors: Mehrdad Mansouri: Simon Fraser University; Ali Arab: Simon Fraser University; Zahra Zohrevand: Simon Fraser University; Martin Eser: Simon Fraser University</p></li><li><p>INPREM: An Interpretable and Trustworthy Predictive Model for Healthcare</p><p>Authors: Xianli Zhang: Xiâ€™an Jiaotong University; Buyue Qian: Xiâ€™an Jiaotong University; Shilei Cao: Tencent Jarvis Lab; Yang Li: Xiâ€™an Jiaotong University; Hang Chen: Xiâ€™an Jiaotong University; Yefeng Zheng: Tencent Jarvis Lab; Ian Davidson: University of California - Davis</p></li><li><p>Interpretability is a Kind of Safety: An Interpreter-based Ensemble for Adversary Defense</p><p>Authors: Jingyuan Wang: Beihang University; Yufan Wu: Beihang University; Mingxuan Li: Beihang University; Xin Lin: Beihang University; Junjie Wu: Beihang University; Chao Li: Beihang University</p></li><li><p>Malicious Attacks against Deep Reinforcement Learning Interpretations</p><p>Authors: Mengdi Huai: University of Virginia; Jianhui Sun: University of Virginia; Renqin Cai: University of Virginia; Liuyi Yao: University of New York at Buffalo; Aidong Zhang: University of Virginia</p></li></ul><ul><li><p>GRACE: Generating Concise and Informative Contrastive Sample to Explain Neural Network Model</p><p>Authors: Thai Le: The Pennsylvania State University; Suhang Wang: The Pennsylvania State University; Dongwon Lee: The Pennsylvania State University</p></li><li><p>xGAIL: Explainable Generative Adversarial Imitation Learning for Explainable Human Decision Analysis</p><p>Authors: Menghai Pan: Worcester Polytechnic Institute; Weixiao Huang: Worcester Polytechnic Institute; Yanhua Li: Worcester Polytechnic Institute (WPI); Xun Zhou: University of Iowa; Jun Luo: Lenovo Group Limited</p></li></ul><ul><li><p>Explainable classification of brain networks via contrast subgraphs</p><p>Authors: Tommaso Lanciano: La Sapienza University of Rome; Francesco Bonchi: Fondazione ISI; Aristides Gionis: KTH Royal Institute of Technology</p></li></ul><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><ul><li><p>High-Dimensional Similarity Search with Quantum-Assisted Variational Autoencoder</p><p>Authors: Nicholas Gao: NASA Ames Research Center; Max Wilson: NASA Ames Research Center; Thomas Vandal: NASA Ames Research Center; Walter Vinci: NASA Ames Research Center; Ramakrishna Nemani: NASA Ames Research Center; Eleanor Rieffel: NASA Ames Research Center</p></li></ul><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><ul><li><p>Cascade-LSTM: A Tree-Structured Neural Classifier for Detecting Misinformation Cascades</p><p>Authors: Francesco Ducci: ETH Zurich; Mathias Kraus: ETH Zurich; Stefan Feuerriegel: ETH Zurich</p></li></ul><h2 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h2><ul><li><p>NodeAug: Semi-Supervised Node Classification with Data Augmentation</p><p>Authors: Yiwei Wang: National University of Singapore; Wei Wang: National University of Singapore; Yuxuan Liang: National University of Singapore; Yujun Cai: Nanyang Technological University; Juncheng Liu: National University of Singapore; Bryan Hooi: National University of Singapore</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Self-supervised learning survey</title>
      <link href="/uncategorized/notes/self_supervised_learning_survey/"/>
      <url>/uncategorized/notes/self_supervised_learning_survey/</url>
      
        <content type="html"><![CDATA[<h1 id="Survey-Self-supervised-learning-Generative-or-Contrastive"><a href="#Survey-Self-supervised-learning-Generative-or-Contrastive" class="headerlink" title="Survey: Self-supervised learning: Generative or Contrastive"></a>Survey: Self-supervised learning: Generative or Contrastive</h1><p>æ¸…åå”æ°å›¢é˜Ÿçš„å·¥ä½œï¼Œæ¯”è¾ƒå®Œæ•´ã€‚</p><p><a href="https://www.aliyundrive.com/s/Hf99EegM1pd">PDF FILE (åŒ…å«ä¸€äº›ç¬”è®°)</a></p><span id="more"></span><hr><p>è‡ªç›‘ç£ä»»åŠ¡çš„æ–¹æ³•å…±åˆ†ä¸º3ç±»ï¼Œåˆ†åˆ«ä¸ºç”Ÿæˆå¼ï¼ˆgenerativeï¼‰ã€åˆ¤åˆ«å¼ï¼ˆcontrastiveï¼‰ã€å¯¹æŠ—å¼ï¼ˆgenerative-contrastive (adversarial)ï¼‰ï¼Œä»–ä»¬çš„åŒºåˆ«å¦‚ä¸‹ã€‚</p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img_profile/20200730194624.png" alt="compare" style="zoom:67%;" /><h2 id="GENERATIVE-SELF-SUPERVISED-LEARNING"><a href="#GENERATIVE-SELF-SUPERVISED-LEARNING" class="headerlink" title="GENERATIVE SELF-SUPERVISED LEARNING"></a>GENERATIVE SELF-SUPERVISED LEARNING</h2><h3 id="Auto-regressive-AR-Model"><a href="#Auto-regressive-AR-Model" class="headerlink" title="Auto-regressive (AR) Model"></a>Auto-regressive (AR) Model</h3><ul><li><p>pros: The advantage of auto-regressive models is that it can model the context dependency well.</p></li><li><p>cons: However, one shortcoming of the AR model is that the token at each position can only access its context from one direction.</p></li></ul><h3 id="Flow-based-Model"><a href="#Flow-based-Model" class="headerlink" title="Flow-based Model"></a>Flow-based Model</h3><h3 id="Auto-encoding-AE-Model"><a href="#Auto-encoding-AE-Model" class="headerlink" title="Auto-encoding (AE) Model"></a>Auto-encoding (AE) Model</h3><ol><li>Basic AE Model</li><li>Context Prediction Model (CPM): The idea of the Context Prediction Model (CPM) is predicting contextual information based on inputs.</li><li>The idea of denoising autoencoder models is that representation should be robust to the introduction of noise. The masked language model (MLM) can be regarded as a denoising because its input masks predicted tokens.</li><li>Variational AE Model</li></ol><h3 id="Hybrid-Generative-Models"><a href="#Hybrid-Generative-Models" class="headerlink" title="Hybrid Generative Models"></a>Hybrid Generative Models</h3><ol><li>Combining AR and AE Model.</li><li>Combining AE and Flow-based Models</li></ol><h2 id="CONTRASTIVE-SELF-SUPERVISED-LEARNING-Contrastive-learning-aims-at-â€œlearn-to-compareâ€"><a href="#CONTRASTIVE-SELF-SUPERVISED-LEARNING-Contrastive-learning-aims-at-â€œlearn-to-compareâ€" class="headerlink" title="CONTRASTIVE SELF-SUPERVISED LEARNING: Contrastive learning aims at â€œlearn to compareâ€."></a>CONTRASTIVE SELF-SUPERVISED LEARNING: Contrastive learning aims at â€œlearn to compareâ€.</h2><h3 id="context-instance-contrast-å³å¯¹æ¯”æŸä¸ªæ ·æœ¬åŠå…¶å®ƒçš„è¯­å¢ƒ"><a href="#context-instance-contrast-å³å¯¹æ¯”æŸä¸ªæ ·æœ¬åŠå…¶å®ƒçš„è¯­å¢ƒ" class="headerlink" title="context-instance contrast: å³å¯¹æ¯”æŸä¸ªæ ·æœ¬åŠå…¶å®ƒçš„è¯­å¢ƒ"></a>context-instance contrast: å³å¯¹æ¯”æŸä¸ªæ ·æœ¬åŠå…¶å®ƒçš„è¯­å¢ƒ</h3><ol><li>Predict Relative Position (PRP): ç›´è§‚çš„ç†è§£ï¼Œæ˜¯å°†åŸå§‹æ•°æ®çš„å¥½å‡ ä¸ªéƒ¨åˆ†ç”¨æŸç§æ–¹å¼æ‰“ä¹±ï¼Œç„¶åä½¿ç”¨ä¸€ä¸ªè¾…åŠ©çš„ä»»åŠ¡å°è¯•æ¢å¤åŸæœ‰çš„æ•°æ®ã€‚</li></ol><p>ä¾‹ï¼šåœ¨CVä¸­ï¼Œå°†æŸä¸ªæ ·æœ¬ï¼Œa) åˆ†å‰²æˆå‡ ä¸ªéƒ¨åˆ†ï¼Œç„¶åæ‰“ä¹±é¡ºåºï¼Œé™„åŠ ä»»åŠ¡ä¸ºæ¢å¤é¡ºåºï¼›b) åšæ—‹è½¬ï¼Œé™„åŠ ä»»åŠ¡æ˜¯å°†æ—‹è½¬åçš„æ ·æœ¬æ¢å¤ï¼›c) åˆ†å‰²åå®Œæˆä¸€ä¸ªæ‹¼å›¾æ¸¸æˆã€‚</p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img_profile/20200730193851.png" style="zoom:67%;" /><ol start="2"><li>Maximize Mutual Informationï¼šç›´è§‚æ¥è®²ï¼Œå³ä½¿ç”¨äº’ä¿¡æ¯æ¥è¡¨å¾æŸä¸ªinstanceæ˜¯å¦å±äºä¸€ä¸ªcontextï¼Œç»™å®šä¸€ä¸ªcontext-instance pairï¼Œå¹¶ç»™å®šå…¶æ˜¯å¦å±äºåŒä¸€ä¸ªæ ·æœ¬çš„æ ‡ç­¾ï¼ˆ0ï¼šä¸å±äºï¼Œ1ï¼šå±äºï¼‰ï¼Œåˆ™æœ€å°åŒ–æ­£æ ·æœ¬çš„äº’ä¿¡æ¯ï¼Œå¹¶æœ€å¤§åŒ–è´Ÿæ ·æœ¬çš„äº’ä¿¡æ¯ã€‚</li></ol><p>cons: The [132] provides empirical evidence that the success of the models mentioned above is only loosely connected to MI by showing that an upper bound MI estimator leads to ill-conditioned and lower performance representation. Instead, more should be attributed to encoder architecture and a negative sampling strategy related to metric learning.</p><blockquote><p>Some examples:</p><p><strong>(CV)</strong> maximize the MI between a local patch and its global context.</p><p><strong>(speech)</strong> CPC maximize the association between a segment of audio and its context audio.</p><p><strong>(NLP)</strong> maximize the mutual information between a global representation of a sentence and n-grams in it.</p><p><strong>(Graph)</strong> Deep Graph InfoMax (DGI) [139] considers a nodeâ€™s representation as the local feature and the average of randomly samples 2-hop neighbors as context.</p></blockquote><h3 id="context-context-contrast-å³å¯¹æ¯”ä¸¤ä¸ªç‹¬ç«‹çš„æ ·æœ¬"><a href="#context-context-contrast-å³å¯¹æ¯”ä¸¤ä¸ªç‹¬ç«‹çš„æ ·æœ¬" class="headerlink" title="context-context contrast: å³å¯¹æ¯”ä¸¤ä¸ªç‹¬ç«‹çš„æ ·æœ¬"></a>context-context contrast: å³å¯¹æ¯”ä¸¤ä¸ªç‹¬ç«‹çš„æ ·æœ¬</h3><p>å¯¹äºä¸€ä¸ªæ ·æœ¬ï¼Œé¦–å…ˆç”Ÿæˆä»–çš„å¤šä¸ªå‰¯æœ¬ï¼ˆé€šè¿‡å„ç§åŠ å™ª/æ•°æ®å¢å¼ºçš„æ–¹å¼ï¼‰ï¼Œç„¶åæœ€å°åŒ–è¿™å‡ ä¸ªæ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå¹¶æœ€å¤§åŒ–è¿™äº›æ ·æœ¬ä¸å¦ä¸€ä¸ªç‹¬ç«‹æ ·æœ¬çš„ç›¸ä¼¼åº¦ã€‚<br>ä¾‹ï¼šåœ¨cvä¸­ï¼Œå¤šä¸ªå‰¯æœ¬æ˜¯é€šè¿‡è£å‰ªã€é¢œè‰²è½¬æ¢ã€æ—‹è½¬ç­‰æ–¹å¼ç”Ÿæˆçš„ã€‚</p><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img_profile/20200730194302.png" style="zoom: 67%;" />        <h2 id="GENERATIVE-CONTRASTIVE-ADVERSARIAL-SELF-SUPERVISED-LEARNING"><a href="#GENERATIVE-CONTRASTIVE-ADVERSARIAL-SELF-SUPERVISED-LEARNING" class="headerlink" title="GENERATIVE-CONTRASTIVE (ADVERSARIAL) SELF-SUPERVISED LEARNING"></a>GENERATIVE-CONTRASTIVE (ADVERSARIAL) SELF-SUPERVISED LEARNING</h2><p>pros: 1) A reason for the generative modelâ€™s success in self-supervised learning is its ability to fit the data distribution. 2) GANs are designed to serve for human-level understanding. 3) GANs focus on capturing the complete information of the sample.</p><ol><li><p>Generate with Complete Input: å°†å®Œæ•´çš„æ ·æœ¬é€å…¥ç½‘ç»œå¹¶è¿›è¡Œå‹ç¼©ä¸é‡å»ºï¼Œç”±discriminatoråˆ¤åˆ«é‡å»ºæ•°æ®ä¸åŸå§‹æ•°æ®çš„å·®å¼‚ã€‚</p></li><li><p>Recover with Partial Input: å°†ç»è¿‡å¤„ç†ï¼ˆåŠ å™ªã€è½¬æ¢ï¼‰çš„æ ·æœ¬é€å…¥ç½‘ç»œè¿›è¡Œé‡å»ºï¼Œç”±discriminatoråˆ¤åˆ«é‡å»ºæ•°æ®ä¸åŸå§‹å®Œæ•´æ•°æ®çš„å·®å¼‚ã€‚ä»è¿™ä¸ªè§’åº¦ä¸Šæ¥è®²ï¼Œä¸åˆ¤åˆ«å¼æ–¹æ³•éå¸¸åƒï¼Œä½†æ˜¯äºŒè€…å­¦ä¹ åˆ†å¸ƒçš„æ–¹å¼ä¸åŒï¼Œdiscriminatorçš„å¤æ‚åº¦ä¹Ÿä¸åŒã€‚</p></li></ol><img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img_profile/20200730195129.png" style="zoom: 50%;" />]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> self-supervised learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in SIRIR 2020 (2020.07.25)</title>
      <link href="/uncategorized/paperlistfile/SIGIR2020/"/>
      <url>/uncategorized/paperlistfile/SIGIR2020/</url>
      
        <content type="html"><![CDATA[<p>å¤§å¤šæ–‡ç« éƒ½æ˜¯å…³äºNLPä¸æ¨èç³»ç»Ÿçš„ã€‚ç›¸å…³çš„ä¸å¤šã€‚</p><p>Accepted paper list: <a href="https://sigir.org/sigir2020/accepted-papers/">Link</a></p><span id="more"></span><h2 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h2><ul><li><p>Dynamic Clustering with Discrete Time Event Prediction</p><p>Karan Aggarwal: University of Minnesota; Georgios Theocharous: Adobe Research; Anup Rao: Yale University</p></li></ul><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><ul><li><p>A Deep Recurrent Survival Model for Unbiased Ranking</p><p>Jiarui Jin: Shanghai Jiao Tong University; Yuchen Fang: Shanghai Jiao Tong University; Weinan Zhang: Shanghai Jiao Tong University; Kan Ren: Microsoft; Guorui Zhou: Alibaba; Jian Xu: Alibaba; Yong Yu: Shanghai Jiao Tong University; Jun Wang: University College London; Xiaoqiang Zhu: Alibaba; Kun Gai: Alibaba</p></li></ul><h2 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a>Anomaly Detection</h2><p>N/A</p><h2 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h2><ul><li><p>Dual Sequential Network for Temporal Sets Prediction</p><p>Leilei Sun: Beihang University; Yansong Bai: Beihang University; Bowen Du: Beihang University; Chuanren Liu: University of Tennessee; Hui Xiong: Rutgers University; Weifeng Lv: Beihang University</p></li><li><p>Crowdsourced Text Sequence Aggregation based on Hybrid Reliability and Representation</p><p>Jiyi Li: University of Yamanashi</p></li><li><p>Neural Hierarchical Factorization Machines for Userâ€™s Event Sequence Analysis</p><p>Dongbo Xi: Institute of Computing Technology, Chinese Academy of Sciences; Fuzhen Zhuang: Institute of Computing Technology, Chinese Academy of Sciences; Bowen Song: Ant Financial Services Group; Yongchun Zhu: Institute of Computing Technology, Chinese Academy of Sciences; Shuai Chen: Ant Financial Services Group; Dan Hong: Ant Financial Services Group; Tao Chen: Ant Financial Group; Xi Gu: Ant Financial Services Group; Qing He: Institute of Computing Technology, CAS</p></li></ul><h2 id="Interpretable"><a href="#Interpretable" class="headerlink" title="Interpretable"></a>Interpretable</h2><ul><li><p>Towards Explainable Retrieval Models for Precision Medicine Literature Search</p><p>Jiaming Qu: University of North Carolina at Chapel Hill; Jaime Arguello: University of North Carolina at Chapel Hill; Yue Wang: University of North Carolina at Chapel Hill</p></li></ul><ul><li><p>The Curious Case of IR Explainability: Explaining Document Scores within and across Ranking Models</p><p>Procheta Sen: Dublin City University; Manisha Verma: Verizon Media, New York, NY, USA; Debasis Ganguly: IBM Ireland Research Lab; Gareth Jones: Dublin City University</p></li></ul><ul><li><p>Biomedical Information Retrieval Incorporating Knowledge Graph for Explainable Precision Medicine</p><p>Zuoxi Yang: South China University of Technology; Shoubin Dong: South China University of Technology</p></li></ul><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><p>N/A</p><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>N/A</p><hr><h2 id="Interpretable-Recommendation"><a href="#Interpretable-Recommendation" class="headerlink" title="Interpretable + Recommendation"></a>Interpretable + Recommendation</h2><ul><li><p>Explaining Recommendations in Heterogeneous Networks</p><p>Azin Ghazimatin: Max Planck Institute for Informatics</p></li></ul><ul><li><p>Fairness-Aware Explainable Recommendation over Knowledge Graphs</p><p>Zuohui Fu: Rutgers University; Yikun Xian: Rutgers University; Ruoyuan Gao: Rutgers University; Jieyu Zhao: University of California, Los Angeles; Qiaoying Huang: Rutgers University; Yingqiang Ge: Rutgers University; Shuyuan Xu: Rutgers University; Shijie Geng: Rutgers University; Chirag Shah: University of Washington; Yongfeng Zhang: Rutgers University; Gerard de Melo: Rutgers University</p></li></ul><ul><li><p>Try This Instead: Personalized and Interpretable Substitute Recommendation</p><p>Tong Chen: The University of Queensland; Hongzhi Yin: The University of Queensland; Guanhua Ye: The University of Queensland; Zi Huang: The University of Queensland; Yang Wang: Hefei University of Technology; Meng Wang: Hefei University of Technology</p></li><li><p>Neural Concept Map Generation for Effective Document Classification with Interpretable Structured Summarization</p><p>Carl Yang: University of Illinois at Urbana-Champaign; Jieyu Zhang: University of Illinois at Urbana-Champaign; Haonan Wang: University of Illinois at Urbana-Champaign; Bangzheng Li: University of Illinois at Urbana-Champaign; Jiawei Han: University of Illinois at Urbana-Champaign</p></li></ul><h2 id="Sequence-Recommendation"><a href="#Sequence-Recommendation" class="headerlink" title="Sequence + Recommendation"></a>Sequence + Recommendation</h2><ul><li><p>Sentiment-guided Sequential Recommendation</p><p>Lin Zheng: Department of Computer Science, College of Engineering, Shantou University; Naicheng Guo: Department of Computer Science, College of Engineering, Shantou University; Weihao Chen: Department of Computer Science, College of Engineering, Shantou University; Jin Yu: Department of Computer Science, College of Engineering, Shantou University; Dazhi Jiang: Department of Computer Science, College of Engineering, Shantou University</p></li></ul><ul><li><p>Sequential-based Adversarial Optimisation for Personalised Top-N Item Recommendation</p><p>Jarana Manotumruksa: University College London; Emine Yilmaz: University College London</p></li></ul><ul><li><p>Bridging Hierarchical and Sequential Context Modeling for Question-driven Extractive Answer Summarization</p><p>Yang Deng: The Chinese University of Hong Kong; Wenxuan Zhang: The Chinese University of Hong Kong; Yaliang Li: Alibaba Group; Min Yang: The Chinese Academy of Sciences; Wai Lam: The Chinese University of Hong Kong; Ying Shen: Sun Yat-Sen University</p></li><li><p>Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation</p><p>Fajie Yuan: Tencent; Xiangnan He: University of Science and Technology of China; Alexandros Karatzoglou: Google; Liguang Zhang: Tencent</p></li></ul><ul><li><p>Time Matters: Sequential Recommendation with Complex Temporal Information</p><p>Wenwen Ye: Tsinghua University; Shuaiqiang Wang: JD.COM; Xu Chen: Tsinghua University; Xuepeng Wang: JD.COM; Zheng Qin: Tsinghua University; Dawei Yin: Baidu Inc.</p></li></ul><ul><li><p>Group-Aware Long- and Short-Term Graph Representation Learning for Sequential Group Recommendation</p><p>Wen Wang: East China Normal University; Wei Zhang: East China Normal University; Jun Rao: Search Product Center, WeChat Search Application Department, Tencent; Zhijie Qiu: Search Product Center, WeChat Search Application Department, Tencent; Bo Zhang: Search Product Center, WeChat Search Application Department, Tencent; Leyu Lin: Search Product Center, WeChat Search Application Department, Tencent; Hongyuan Zha: Georgia Institute of Technology</p></li></ul><ul><li><p>KERL: A Knowledge-Guided Reinforcement Learning Model for  Sequential Recommendation</p><p>Pengfei Wang: Beijing University of Posts and Telecommunications; Yu Fan: Beijing University of Posts and Telecommunications; Long Xia: School of Information Technology, York University; Wayne Xin Zhao: School of Information, Renmin University of China; Shaozhang Niu: BUPT; Jimmy Huang: School of Information Technology, York University</p></li></ul><ul><li><p>Sequential Recommendation with Self-attentive Multi-adversarial Network</p><p>Ruiyang Ren: Renmin University of China; Zhaoyang Liu: Alibaba Group; Yaliang Li: Alibaba Group; Wayne Xin Zhao: Renmin University of China; Hui Wang: Renmin University of China; Bolin Ding: Alibaba Group; Ji-Rong Wen: Renmin University of China</p></li><li><p>A General Network Compression Framework for Sequential Recommender Systems</p><p>Yang Sun: Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Fajie Yuan: Tencent; Min Yang: Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Guoao Wei: Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Zhou Zhao: Zhejiang University; Duo Liu: Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</p></li><li><p>Next-item Recommendation with Sequential Hypergraphs</p><p>Jianling Wang: Texas A&amp;M University; Kaize Ding: Arizona State University; Liangjie Hong: Etsy Inc.; Huan Liu: Arizona State University; James Caverlee: Texas A&amp;M University</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in ICML 2020 (2020.07.12)</title>
      <link href="/uncategorized/paperlistfile/ICML2020/"/>
      <url>/uncategorized/paperlistfile/ICML2020/</url>
      
        <content type="html"><![CDATA[<p><a href="https://icml.cc/Conferences/2020/Schedule?type=Poster">Link</a></p><p><a href="https://icml.cc/Conferences/2020/Schedule">Schedule</a></p><span id="more"></span><h2 id="Anomaly"><a href="#Anomaly" class="headerlink" title="Anomaly"></a>Anomaly</h2><ul><li><p><strong>[å¤§æ¦‚çœ‹äº†çœ‹ï¼Œä¸å¤ªæ¨è]</strong> Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure</p><p>John Sipple</p><p>CONTRIBUTIONS: </p><ul><li>a scalable approach to detecting multidimensional outliers, robust under multimodal conditions,</li><li>an alternative to one-class anomaly detection using negative sampling, and</li><li>a novel application of Integrated Gradients for anomaly interpretation.</li></ul></li><li><p>Robust Outlier Arm Identification</p><p>Yinglun Zhu Â· Sumeet Katariya Â· Robert Nowak</p></li></ul><h2 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h2><ul><li><p>Population-Based Black-Box Optimization for Biological Sequence Design</p><p>Christof Angermueller Â· David Belanger Â· Andreea Gane Â· Zelda Mariet Â· David Dohan Â· Kevin Murphy Â· Lucy Colwell Â· D. Sculley</p></li><li><p>A Chance-Constrained Generative Framework for Sequence Optimization</p><p>Xianggen Liu Â· Jian Peng Â· Qiang Liu Â· Sen Song</p></li><li><p>An EM Approach to Non-autoregressive Conditional Sequence Generation</p><p>Zhiqing Sun Â· Yiming Yang</p></li><li><p>CAUSE: Learning Granger Causality from Event Sequences using Attribution Methods</p><p>Wei Zhang Â· Thomas Panum Â· Somesh Jha Â· Prasad Chalasani Â· David Page</p></li><li><p>Imputer: Sequence Modelling via Imputation and Dynamic Programming</p><p>William Chan Â· Chitwan Saharia Â· Geoffrey Hinton Â· Mohammad Norouzi Â· Navdeep Jaitly</p></li><li><p>Incremental Sampling Without Replacement for Sequence Models</p><p>Kensen Shi Â· David Bieber Â· Charles Sutton</p></li><li><p>Sequence Generation with Mixed Representations</p><p>Lijun Wu Â· Shufang Xie Â· Yingce Xia Â· Yang Fan Â· Jian-Huang Lai Â· Tao Qin Â· Tie-Yan Liu</p></li></ul><h2 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h2><ul><li><p><strong>[å·²è¯»]</strong> Learning From Irregularly-Sampled Time Series: A Missing Data Perspective</p><p>Steven Cheng-Xian Li Â· Benjamin M Marlin</p></li><li><p><strong>[å·²è¯»]</strong> Set Functions for Time Series</p><p>Max Horn Â· Michael Moor Â· Christian Bock Â· Bastian Rieck Â· Karsten Borgwardt</p><p><em>æ‰€è§£å†³çš„é—®é¢˜ï¼šé’ˆå¯¹éè§„åˆ™é‡‡æ ·çš„æ—¶é—´åºåˆ—çš„åˆ†ç±»é—®é¢˜ï¼Œæ–‡ä¸­çš„å¤§æ„æ˜¯è¿˜å¯ä»¥è¿›è¡Œå…¶ä»–ä¸‹æ¸¸ä»»åŠ¡ï¼Œè€Œéä»…æœ‰åˆ†ç±»ï¼ˆæ›´æ¢ç›¸åº”çš„losså‡½æ•°å³å¯ï¼‰ã€‚Dataset: MIMIC-III Mortality Prediction, Physionet 2012 Mortality Prediction Challenge, Physionet 2019 Sepsis Early Prediction Challenge</em></p><p><em>ä¸€ä¸ªéå¸¸è§„çš„åšæ³•ï¼Œå…¶ç†è®ºä¾æ®æ˜¯differentiable set function learning, å…·å¤‡å¯è§£é‡Šæ€§æ˜¯å…¶ä¼˜ç‚¹ï¼Œ<br>åœ¨å…·ä½“åšæ³•æ–¹é¢ï¼Œpipelineå¦‚ä¸‹ï¼š*<em>å¤šç»´æ—¶é—´åºåˆ—</em></em> -&gt; <strong>encoding</strong> (ä¸ä½¿ç”¨ç±»ä¼¼seq2seqçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ç§ç±»ä¼¼é¢‘ç‡åŸŸåˆ†è§£çš„å›ºå®šencodeingæ–¹å¼) -&gt; <strong>Embedding+Aggregation+Attention</strong> (è¿™ä¸€æ­¥ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼Œattentionä¸ºscaled dot-product attention with multiple heads) -&gt; <strong>classification</strong> (ç¥ç»ç½‘ç»œ)ã€‚*</p><p><em>åœ¨æ€§èƒ½æ–¹é¢ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åˆ†ç±»ç²¾åº¦å¹¶ä¸èƒ½è¾¾åˆ°state-of-the-artï¼Œåªæ˜¯å’Œbaselineæœ‰ç«äº‰æ€§ï¼Œè€Œåœ¨å¯è§£é‡Šæ€§ã€å†…å­˜å ç”¨ã€è¿è¡Œæ—¶é—´ä¸Šæœ‰ä¼˜å¼‚çš„æ€§èƒ½ã€‚<br>BTW, baseline: GRU-D, GRU-SIMPLE, IP-NETS, PHASED-LSTM, TRANSFORMER, SEFT-ATTN</em></p></li><li><p>Spectral Subsampling MCMC for Stationary Time Series</p><p>Robert Salomone Â· Matias Quiroz Â· Robert kohn Â· Mattias Villani Â· Minh-Ngoc Tran</p></li><li><p>Temporal Logic Point Processes</p><p>Shuang Li Â· Lu Wang Â· Ruizhi Zhang Â· xiaofu Chang Â· Xuqin Liu Â· Yao Xie Â· Yuan Qi Â· Le Song</p></li><li><p>Gradient Temporal-Difference Learning with Regularized Corrections</p><p>Sina Ghiassian Â· Andrew Patterson Â· Shivam Garg Â· Dhawal Gupta Â· Adam White Â· Martha White</p></li><li><p>Time Series Deconfounder: Estimating Treatment Effects over Time in the Presence of Hidden Confounders</p><p>Ioana Bica Â· Ahmed Alaa Â· Mihaela van der Schaar</p></li></ul><h2 id="missing-value"><a href="#missing-value" class="headerlink" title="missing value"></a>missing value</h2><ul><li><p><strong>[å·²è¯»]</strong> Learning from Irregularly-Sampled Time Series: A Missing Data Perspective</p><p>Steven Cheng-Xian Li Â· Benjamin M Marlin</p><p><em>è§£å†³çš„é—®é¢˜:</em> ä¸è§„åˆ™é‡‡æ ·ã€æœ‰ç¼ºå¤±æ—¶é—´åºåˆ—å»ºæ¨¡é—®é¢˜ã€ä¸Šè¿°æ—¶é—´åºåˆ—çš„åˆ†ç±»é—®é¢˜ã€‚</p><p><em>åˆ›æ–°ä¸ç‹¬ç‰¹:</em> æå‡ºP-VAEä¸P-GANï¼ˆPartial â€“ VAE/GANï¼‰ã€‚ å°†ç¼ºå¤±å€¼è¡¥å…¨é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªæ’å€¼é—®é¢˜</p><p><em>é‡‡ç”¨çš„æ–¹æ³•:</em> </p><ul><li>åœ¨P-VAEä¸­ï¼Œå°†å¯è§‚æµ‹åˆ°çš„ç‚¹é›†åˆTè§†ä¸ºä»æŸä¸ªåˆ†å¸ƒä¸Šé‡‡æ ·çš„éšæœºå˜é‡ï¼Œå°†VAEä¸­çš„å…³äºxçš„æ¦‚ç‡åˆ†å¸ƒè½¬åŒ–ä¸ºx.tçš„è”åˆåˆ†å¸ƒã€‚å¹¶ä»…ç›‘ç£å¯è§‚æµ‹éƒ¨åˆ†çš„é‡å»ºè¯¯å·®ä¸ç”Ÿæˆè¯¯å·®</li><li>åœ¨P-GANä¸­ï¼Œåˆ†åˆ«å‘åˆ¤åˆ«ä¸­é€å…¥ï¼ˆx, t, zï¼‰å¯¹ï¼Œä»¥ä½¿å¾—æœ‰ç¼ºå¤±çš„çœŸå®æ•°æ®ä¸ç”Ÿæˆçš„å®Œæ•´æ•°æ®æœ‰ç›¸ä¼¼çš„éšç©ºé—´è¡¨è¾¾ã€ç›¸ä¼¼çš„å¯è§‚æµ‹æ•°æ®ã€‚</li></ul><p><em>ä¸ºä½•é€‰æ‹©/å¦‚ä½•åº”ç”¨:</em> æœ¬æ–‡æ‰€æå‡ºçš„P-VAEä¸­çš„ç†è®ºæ¨å¯¼éƒ¨åˆ†å€¼çš„åç»­å·¥ä½œå€Ÿé‰´</p><p><em>æ•°æ®é›†</em></p><ul><li>å›¾åƒæ•°æ®é›†ï¼šMNISTã€CelebA</li><li>æ—¶é—´åºåˆ—æ•°æ®é›†ï¼šMIMIC-IIï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰</li></ul></li></ul><ul><li><p>Missing Data Imputation using Optimal Transport</p><p>Boris Muzellec Â· Julie Josse Â· Claire Boyer Â· Marco Cuturi</p></li><li><p>Learning with Missing Values [workshop]</p><p>Julie Josse Â· Jes Frellsen Â· Pierre-Alexandre Mattei Â· Gael Varoquaux</p></li><li><p>Full Law Identification in Graphical Models of Missing Data: Completeness Results</p><p>Razieh Nabi Â· Rohit Bhattacharya Â· Ilya Shpitser</p></li></ul><h2 id="Recurrent"><a href="#Recurrent" class="headerlink" title="Recurrent"></a>Recurrent</h2><ul><li><p>A general recurrent state space framework for modeling neural dynamics during decision-making</p><p>David Zoltowski Â· Jonathan Pillow Â· Scott Linderman</p></li><li><p>Improving the Gating Mechanism of Recurrent Neural Networks</p><p>Albert Gu Â· Caglar Gulcehre Â· Thomas Paine Â· Matthew Hoffman Â· Razvan Pascanu</p></li><li><p>Approximating Stacked and Bidirectional Recurrent Architectures with the Delayed Recurrent Neural Network</p><p>Javier Turek Â· Shailee Jain Â· Vy Vo Â· Mihai CapotÄƒ Â· Alexander Huth Â· Theodore Willke</p></li><li><p>Transformation of ReLU-based recurrent neural networks from discrete-time to continuous-time</p><p>Zahra Monfared Â· Daniel Durstewitz</p></li><li><p>Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules</p><p>Sarthak Mittal Â· Alex Lamb Â· Anirudh Goyal Â· Vikram Voleti Â· Murray Shanahan Â· Guillaume Lajoie Â· Michael Mozer Â· Yoshua Bengio</p></li><li><p>VideoOneNet: Bidirectional Convolutional Recurrent OneNet with Trainable Data Steps for Video Processing</p><p>ZoltÃ¡n Milacski Â· BarnabÃ¡s PÃ³czos Â· Andras Lorincz</p></li><li><p>Frequentist Uncertainty in Recurrent Neural Networks via Blockwise Influence Functions</p><p>Ahmed Alaa Â· Mihaela van der Schaar</p></li></ul><h2 id="Interpretable"><a href="#Interpretable" class="headerlink" title="Interpretable"></a>Interpretable</h2><ul><li><p><strong>[å·²è¯»]</strong> Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure</p><p>John Sipple</p></li><li><p><strong>[mark-interpre-2]</strong> The Many Shapley Values for Model Explanation</p><p>Mukund Sundararajan Â· Amir Najmi</p><ol><li>Shapley Valuesæ˜¯ä¸€ç§åŸºäºåšå¼ˆè®ºçš„æœºå™¨å­¦ä¹ å½’å› æ–¹æ³•ï¼ˆå‡ ä¸ªç©å®¶æ¶ˆè´¹ä¸€ä¸ªæœåŠ¡ï¼Œå³äº§ç”Ÿcostï¼Œshapley valueå³ä¸ºcoståˆ†é…åˆ°æ¯ä¸ªç©å®¶èº«ä¸Šçš„é‡ï¼Œè”ç³»â€”â€”æ¨¡å‹è¢«è§†ä¸ºcost functionï¼Œè¾“å…¥æ ·æœ¬çš„featureå³ä¸ºplayersï¼Œåˆ™åˆ†æ‘Šåˆ°æ¯ä¸ªplayerçš„costï¼ˆshapley valueï¼‰å³ä¸ºå½’å› çš„æƒé‡ï¼‰</li><li>Shapley Valuesæœ‰æ—¶ä¼šè¾“å‡ºä¸å”¯ä¸€çš„è§£é‡Šï¼Œè¿™ä½¿å¾—è§£é‡Šä¸é€‚ç”¨ï¼Œæœ¬è®ºæ–‡è§£å†³çš„æ˜¯åŸºäºShapley Valuesè¿›è¡Œè¾“å…¥å½’å› å¹¶ä½¿å…¶ä¿æŒ<strong>è§£é‡Šå”¯ä¸€æ€§</strong>çš„é—®é¢˜ã€‚</li></ol></li><li><p><strong>[mark-interpre-1]</strong> Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation</p><p>Wenxian Shi Â· Hao Zhou Â· Ning Miao Â· Lei Li</p><ol><li>æ–‡ç« è§£å†³çš„æ˜¯ï¼Œå½“Exponential Family Mixture modelä½œä¸ºéšç©ºé—´çš„å…ˆéªŒçš„æ—¶å€™å¯èƒ½å‡ºç°çš„æ¨¡å¼å´©æºƒé—®é¢˜ï¼Œç”¨ä¼ ç»Ÿçš„VAEè®­ç»ƒæ–¹æ³•å¯èƒ½å¯¼è‡´æ¨¡å¼å´©æºƒé—®é¢˜</li><li>è¯¥æ–‡ç« çš„è§£å†³æ–¹æ³•æ˜¯ï¼Œåœ¨losså‡½æ•°ä¸­å¢åŠ ä¸€ä¸ªåˆ†æ•£é¡¹ï¼Œä½¿å¾—éšç©ºé—´ä¸­çš„æ¯ä¸€ç°‡ä¹‹é—´èƒ½å¤Ÿæ›´åˆ†æ•£ã€‚</li></ol><p>ä¸¤ä¸ªæ¶‰åŠåˆ°çš„æ¦‚å¿µï¼š</p><ul><li>åéªŒå´©æºƒï¼ˆposterior collapseï¼‰ï¼šå°½ç®¡å·²æœ‰ä¸€ä¸ªå…ˆéªŒçš„éšç©ºé—´åˆ†å¸ƒï¼Œä½†æ¨¡å‹ç”Ÿæˆçš„éšç©ºé—´å˜é‡çš„åéªŒåˆ†å¸ƒè¿œåç¦»äºå…ˆéªŒ</li><li>æ¨¡å¼å´©æºƒï¼ˆmode collapseï¼‰ï¼šå…ˆéªŒçš„åœ¨éšç©ºé—´ä¸­æœ‰å¤šä¸ªç°‡çš„åˆ†å¸ƒï¼Œä½†æ¨¡å‹è®­ç»ƒåå„ä¸ªç°‡ä¹‹é—´ä»…æœ‰å¾®å°çš„å˜åŒ–ï¼Œä½¿å¾—å¤šä¸ªç°‡æœ€ç»ˆåç¼©åˆ°ä¸€ä¸ªç°‡ä¸­</li></ul></li><li><p><strong>[mark-interpre-1: å¯ä»¥çœ‹çœ‹]</strong> Explaining Groups of Points in Low-Dimensional Representations</p><p>Gregory Plumb Â· Jonathan Terhorst Â· Sriram Sankararaman Â· Ameet Talwalkar</p><p>è¿™ç¯‡æ–‡ç« ä¸»è¦æ¢ç©¶ä¸€ä¸ªé—®é¢˜ï¼šåœ¨æ•°æ®é›†æ ·æœ¬çš„ä½ç»´è¡¨å¾ç©ºé—´ä¸­ï¼Œæ¯ä¸€ç°‡ä¹‹é—´åˆ°åº•æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿå…·ä½“æ¥è®²ï¼Œç ”ç©¶ä¸¤ä¸ªå…·ä½“çš„é—®é¢˜ï¼š1ï¼‰groupAä¸groupBçš„éšç©ºé—´è¡¨å¾åˆ°åº•æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿå³ï¼šæˆ‘ä»¬èƒ½å¦æ‰¾åˆ°ä¸€ç§åœ¨æ˜¾ç©ºé—´çš„å˜æ¢ï¼Œä½¿å¾—æ ·æœ¬ä»groupAå˜æ¢åˆ°groupBï¼Ÿ2ï¼‰å¯¹äºgroupAä¸­çš„æ‰€æœ‰ç‚¹ï¼Œæˆ‘ä»¬èƒ½å¦æ‰¾å‡ºä¸€ç§é€šç”¨çš„æè¿°ï¼Œä»¥å›ç­”â€œæ»¡è¶³ä»€ä¹ˆæ¡ä»¶çš„æ ·æœ¬èƒ½å¤Ÿç®—ä½œæ˜¯groupAçš„â€ï¼Œå¹¶ä¸”ï¼Œè¿™ç§æè¿°åœ¨æ¯ä¸ªgroupä¸Šéƒ½æ˜¯æˆç«‹çš„ï¼ˆåŸè¯ï¼šwe want to find a explanation that works for all of the points in Group A and because we want the complete set of explanations to be consistent (i.e., symmetrical and transitive) among all the groups.ï¼‰ã€‚</p></li><li><p><strong>[mark-interpre-3]</strong> DeepCoDA: personalized interpretability for compositional health data</p><p>Thomas Quinn Â· Dang Nguyen Â· Santu Rana Â· Sunil Gupta Â· Svetha Venkatesh</p><p>æ²¡å¤ªçœ‹æ‡‚è®²äº†ä»€ä¹ˆï¼Œä½†æ˜¯å¤§æ¦‚å°±æ˜¯åŸºäºself-explaining neural networkæ„å»ºäº†ä¸€ç§ï¼Œåœ¨ç”Ÿç‰©å­¦ä¸Šå¯è§£é‡Šçš„æ¨¡å‹ã€‚å¹¶å…·æœ‰ä¸€äº›è‰¯å¥½çš„æ€§è´¨ã€‚</p><ul><li>A model should select the best log-ratio transformation automatically.</li><li>A model should have linear interpretability.</li><li>A model should have personalized interpretability.</li></ul></li></ul><hr><ul><li><p>Proper Network Interpretability Helps Adversarial Robustness in Classification</p><p>Akhilan Boopathy Â· Sijia Liu Â· Gaoyuan Zhang Â· Cynthia Liu Â· Pin-Yu Chen Â· Shiyu Chang Â· Luca Daniel</p><p>å¼€å‘ä¸€ç§è§£é‡Šæ–¹æ³•ï¼Œå¯¹è¾“å…¥ä¸Šçš„å¾®å°æ‰°åŠ¨é²æ£’ï¼Œå³å³ä½¿æœ‰å¹²æ‰°ï¼Œå…¶ç”Ÿæˆçš„è§£é‡Šä¹Ÿä¸ä¼šå¤§ä¸ä¸€æ ·ï¼Œä¸”ä½¿å¾—ç½‘ç»œçš„åˆ†ç±»ç»“æœéƒ½å¯¹æ‰°åŠ¨é²æ£’</p></li><li><p><strong>[ç®€å•çœ‹äº†çœ‹ï¼Œå‚è€ƒæ„ä¹‰ä¸å¤§]</strong> Robust and Stable Black Box Explanations</p><p>Himabindu Lakkaraju Â· Nino Arsov Â· Osbert Bastani</p><p>æ–‡ç« æœ¬èº«åšçš„æ˜¯ï¼šé’ˆå¯¹æ•°æ®åˆ†å¸ƒåç§»åçš„æ•°æ®é›†ï¼Œå¦‚ä½•ç”Ÿæˆçš„é²æ£’çš„å¯è§£é‡Šæ–¹æ³•ï¼Œè¿™ç§åç§»ç›´è§‰ä¸Šæ¥è®²æ˜¯ï¼šåœ¨è¾“å…¥æ•°æ®ä¸ŠåŠ ä¸€ä¸ªå¾®å°çš„æ‰°åŠ¨ï¼Œå¯è§£é‡Šæ–¹æ³•ä¸åº”å½“å› ä¸ºè¿™äº›æ‰°åŠ¨è€Œæœ‰å·¨å¤§çš„å˜åŒ–ï¼ˆç„¶è€Œç°æœ‰çš„æ–¹æ³•å¯¹è¿™äº›æ‰°åŠ¨å¹¶ä¸é²æ£’ï¼‰</p><ul><li>we propose <strong>a novel minimax objective</strong> that can be used to construct <strong>robust black box explanations for a given family of interpretable models</strong>. This objective encodes the goal of returning the highest fidelity explanation with respect to the<br>worst-case over a set of distribution shifts.</li><li>we propose <strong>a set of distribution shifts</strong> that captures our intuition about the kinds of shifts to which interpretations should be robust. In particular, this set includes shifts that contain <strong>perturbations to a small number of covariates</strong>.</li><li>we propose algorithms for optimizing this objective in two settings: (i) explanations such as linear models with continuous parameters that can be optimized using gradient descent, in which case we use adversarial training (Goodfellow et al., 2015), and (ii) explanations such as decision sets with discrete parameters, in which case we use a sampling-based approximation in conjunction with submodular optimization (Lakkaraju et al., 2016).</li></ul></li></ul><hr><ul><li><p><strong>[ç®€å•çœ‹äº†çœ‹ï¼Œå‚è€ƒæ„ä¹‰ä¸å¤§]</strong> Unsupervised Discovery of Interpretable Directions in the GAN Latent Space</p><p>Andrey Voynov Â· Artem Babenko</p><p>GANçš„éšç©ºé—´ä¸­é€šå¸¸åŒ…å«æœ‰ä¸€äº›å…·æœ‰è¯­ä¹‰ä¿¡æ¯çš„æ–¹å‘ï¼Œåœ¨è¿™äº›æ–¹å‘ä¸Šè¿›è¡Œäººç±»å¯ä»¥ç†è§£çš„éšç©ºé—´è¡¨ç¤ºçš„æ“æ§ï¼Œå¯ä»¥ç”Ÿæˆå—æ§çš„å›¾åƒå˜æ¢ï¼ˆåœ¨æŸä¸ªæ–¹å‘ä¸Šçš„ç§»åŠ¨å¯èƒ½ä»£è¡¨äº†å›¾åƒçš„é¥¿zoomingæˆ–è€…recoloringï¼Œåœ¨éšç©ºé—´ä¸­æ‰¾åˆ°è¿™äº›æ–¹å‘ï¼Œå¹¶åœ¨è¿™äº›æ–¹å‘ä¸Šç”Ÿæˆæ ·æœ¬ï¼Œæœ‰åŠ©äºæˆ‘ä»¬æ–¹ä¾¿çš„è¿›è¡Œæ ·æœ¬å˜æ¢ã€‚ï¼‰ï¼Œä½†æ˜¯ç›®å‰ä¸ºæ­¢ï¼Œå‘ç°è¿™äº›æ–¹å‘çš„è¿‡ç¨‹ä¸€èˆ¬éƒ½æ˜¯â€œin a supervised manner, requiring human labels, pretrained models, or some form of self-supervisionâ€ï¼Œ</p><p>In this paper, we introduce an <strong>unsupervised method to identify interpretable directions in the latent space</strong> of a <strong>pretrained GAN model</strong>. By a simple <strong>model-agnostic</strong> procedure, we find directions corresponding to sensible semantic manipulations without any form of (self-)supervision. Furthermore, we reveal several <strong>non-trivial findings</strong>, which would be difficult to obtain by existing methods, e.g., a direction corresponding to background removal. (ç”¨äºå»é™¤å›¾ç‰‡èƒŒæ™¯çš„æ–¹å‘)ã€‚</p></li><li><p>Interpretations are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge</p><p>Laura Rieger Â· Chandan Singh Â· William Murdoch Â· Bin Yu</p><p>ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ–¹æ³•çš„è§£é‡Šï¼Œéœ€è¦æä¾›ä»¥ä¸‹ä¸¤ç§ä½œç”¨ï¼š1ï¼‰provide insight into a modelï¼Œ2ï¼‰suggest a corresponding action in order to achieve an objectiveã€‚ä½†ä¸€èˆ¬çš„å¯è§£é‡Šæ–¹æ³•åšåˆ°ç¬¬ä¸€æ­¥å°±åœæ­¢äº†ï¼Œå³è§£é‡Šçš„å†…å®¹å¹¶æ²¡æœ‰è¢«ç”¨äºæé«˜ç½‘ç»œçš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡åˆ†è§£è§£é‡Šæƒ©ç½šï¼ˆCDEPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ä»ä¸šäººå‘˜èƒ½å¤Ÿåˆ©ç”¨ç°æœ‰è§£é‡Šæ–¹æ³•æ¥æé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§çš„æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼Œå½“æ˜¾ç¤ºå‡ºæ¨¡å‹å¯¹æŸäº›featureçš„é‡è¦æ€§åˆ†é…ä¸æ­£ç¡®æ—¶ï¼ŒCDEPä½¿ä»ä¸šäººå‘˜å¯ä»¥é€šè¿‡è§£é‡Šå°†é¢†åŸŸçŸ¥è¯†æ’å…¥æ¨¡å‹ä¸­æ¥çº æ­£è¿™äº›é”™è¯¯ã€‚</p></li><li><p><strong>[ç®€å•çœ‹äº†çœ‹ï¼Œå‚è€ƒæ„ä¹‰ä¸å¤§]</strong> Interpreting Robust Optimization via Adversarial Influence Functions</p><p>Zhun Deng Â· Cynthia Dwork Â· Jialiang Wang Â· Linjun Zhang</p><p>ä¸»è¦æ˜¯ç”¨adversarial influence functionæ¥ç†è§£é²æ£’ä¼˜åŒ–çš„è¿‡ç¨‹çš„ï¼Œä¸æ˜¯å¹¿ä¹‰çš„ç¥ç»ç½‘ç»œçš„ã€‚</p></li><li><p><strong>[è®²Shapley-value-based explanationsçš„ç¼ºç‚¹çš„ï¼Œå‚è€ƒæ„ä¹‰ä¸å¤§]</strong> Problems with Shapley-value-based explanations as feature importance measures</p><p>Indra Kumar Â· Suresh Venkatasubramanian Â· Carlos Scheidegger Â· Sorelle Friedler</p><p><em>æ‘˜è¦çš„ç¿»è¯‘ï¼š</em> é€šè¿‡åšå¼ˆè®ºçš„æ–¹å¼å½’å› ç½‘ç»œè¾“å‡ºåˆ°featureä¸Šçš„æ–¹æ³•å·²æˆä¸ºâ€œè§£é‡Šâ€æœºå™¨å­¦ä¹ æ¨¡å‹çš„ä¸€ç§æµè¡Œæ–¹å¼ã€‚è¿™äº›æ–¹æ³•å®šä¹‰äº†æ¨¡å‹ç‰¹å¾ä¹‹é—´çš„åä½œæ¸¸æˆï¼Œå¹¶ä½¿ç”¨æŸç§å½¢å¼çš„æ¸¸æˆå”¯ä¸€Shapleyå€¼åœ¨è¿™äº›è¾“å…¥å…ƒç´ ä¹‹é—´åˆ†é…å½±å“ã€‚è¿™äº›æ–¹æ³•çš„è®ºæ®æœ‰ä¸¤ä¸ªæ”¯æŸ±ï¼šå®ƒä»¬ç†æƒ³çš„æ•°å­¦ç‰¹æ€§ï¼Œä»¥åŠå®ƒä»¬å¯¹ç‰¹å®šè§£é‡ŠåŠ¨æœºçš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå½“Shapleyå€¼ç”¨äºç‰¹å¾é‡è¦æ€§æ—¶ä¼šå‡ºç°æ•°å­¦é—®é¢˜ï¼Œç¼“è§£è¿™äº›é—®é¢˜çš„è§£å†³æ–¹æ¡ˆå¿…ç„¶ä¼šå¯¼è‡´è¿›ä¸€æ­¥çš„å¤æ‚æ€§ï¼Œä¾‹å¦‚å› æœæ¨ç†çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨å…¶ä»–æ–‡çŒ®æ¥è®ºè¯ï¼Œå¤æ™®åˆ©ä»·å€¼è§‚å¹¶æœªæä¾›é€‚åˆä»¥äººä¸ºä¸­å¿ƒçš„å¯è§£é‡Šæ€§ç›®æ ‡çš„è§£é‡Šï¼ˆ<strong>Shapley values do not provide explanations which suit human-centric goals of explainability</strong>ï¼‰ã€‚</p></li><li><p>Explainable k-Means and k-Medians Clustering</p><p>Michal Moshkovitz Â· Sanjoy Dasgupta Â· Cyrus Rashtchian Â· Nave Frost</p><p>èšç±»æ˜¯å‡ ä½•æ•°æ®æ— ç›‘ç£å­¦ä¹ çš„ä¸€ç§æµè¡Œå½¢å¼ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè®¸å¤šèšç±»ç®—æ³•å¯¼è‡´éš¾ä»¥è§£é‡Šçš„èšç±»åˆ†é…ï¼Œéƒ¨åˆ†åŸå› æ˜¯å› ä¸ºå®ƒä»¬ä»¥å¤æ‚çš„æ–¹å¼ä¾èµ–äºæ•°æ®çš„æ‰€æœ‰ç‰¹å¾ã€‚ä¸ºäº†æé«˜å¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬è€ƒè™‘ä½¿ç”¨è¾ƒå°çš„å†³ç­–æ ‘å°†æ•°æ®é›†åˆ’åˆ†ä¸ºèšç±»ï¼Œä»¥ä¾¿å¯ä»¥ä»¥ç®€å•çš„æ–¹å¼å¯¹èšç±»è¿›è¡Œè¡¨å¾ã€‚æˆ‘ä»¬ä»ç†è®ºçš„è§’åº¦ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡Ä·-è¡¨ç¤ºå’Œ Ä·-ä¸­ä½æ•°ç›®æ ‡ï¼šæ˜¯å¦å¿…é¡»å­˜åœ¨ä¸€ä¸ªæ ‘æœ¨è¯±å¯¼çš„èšç±»ï¼Œå…¶æˆæœ¬å¯ä¸æœ€ä½³æ— çº¦æŸèšç±»çš„æˆæœ¬ç›¸åª²ç¾ï¼›å¦‚æœæ˜¯ï¼Œé‚£ä¹ˆå¦‚ä½•æ‰¾åˆ°å®ƒï¼Ÿåœ¨è´Ÿé¢ç»“æœæ–¹é¢ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œé¦–å…ˆï¼Œæµè¡Œçš„è‡ªä¸Šè€Œä¸‹çš„å†³ç­–æ ‘ç®—æ³•å¯èƒ½ä¼šå¯¼è‡´ä»¥ä»»æ„å¤§çš„ä»£ä»·è¿›è¡Œèšç±»ï¼›å…¶æ¬¡ï¼Œä»»ä½•ç”±æ ‘å¼•èµ·çš„èšç±»é€šå¸¸éƒ½å¿…é¡»å¯¼è‡´Î© ï¼ˆå¯¹æ•°k ï¼‰ä¸æœ€ä½³èšç±»ç›¸æ¯”çš„è¿‘ä¼¼å› å­ã€‚åœ¨ç§¯ææ–¹é¢ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æœ‰æ•ˆçš„ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿ç”¨å¸¦æœ‰Ä·æ ‘å¶ã€‚å¯¹äºä¸¤ä¸ªå‡å€¼/ä¸­ä½æ•°ï¼Œæˆ‘ä»¬è¡¨æ˜å•ä¸ªé˜ˆå€¼å‰Šå‡è¶³ä»¥å®ç°å¸¸æ•°å› å­è¿‘ä¼¼ï¼Œå¹¶ä¸”ç»™å‡ºäº†å‡ ä¹åŒ¹é…çš„ä¸‹é™ã€‚å¯¹äºä¸€èˆ¬Ä· â‰¥ 2ï¼Œæˆ‘ä»¬çš„ç®—æ³•æ˜¯ Ã” ï¼ˆÄ· ï¼‰ é€¼è¿‘æœ€ä½³ Ä·-ä¸­ä½æ•°å’Œ O ï¼ˆÄ·2ï¼‰ é€¼è¿‘æœ€ä½³ Ä·-æ‰‹æ®µã€‚åœ¨æˆ‘ä»¬è¿›è¡Œå·¥ä½œä¹‹å‰ï¼Œè¿˜æ²¡æœ‰ä»»ä½•ç®—æ³•èƒ½è¯æ˜ä¸å°ºå¯¸å’Œè¾“å…¥å¤§å°æ— å…³çš„ä¿è¯ã€‚</p></li></ul><hr><ul><li><p>Multiresolution Tensor Learning for Efficient and Interpretable Spatial Analysis</p><p>Jung Yeon Park Â· Kenneth Carr Â· Stephan Zheng Â· Yisong Yue Â· Rose Yu</p></li><li><p>Interpretable Off-Policy Evaluation in Reinforcement Learning by Highlighting Influential Transitions</p><p>Omer Gottesman Â· Joseph Futoma Â· Yao Liu Â· Sonali Parbhoo Â· Leo Celi Â· Emma Brunskill Â· Finale Doshi-Velez</p></li><li><p>Explainable and Discourse Topic-aware Neural Language Understanding</p><p>Yatin Chaudhary Â· Hinrich Schuetze Â· Pankaj Gupta</p></li><li><p>When Explanations Lie: Why Many Modified BP Attributions Fail</p><p>Leon Sixt Â· Maximilian Granz Â· Tim Landgraf</p></li></ul><ul><li><p>Fairwashing explanations with off-manifold detergent</p><p>Christopher Anders Â· Plamen Pasliev Â· Ann-Kathrin Dombrowski Â· Klaus-robert Mueller Â· Pan Kessel</p></li></ul><ul><li><p>XXAI: Extending Explainable AI Beyond Deep Models and Classifiers [workshop]</p><p>Wojciech Samek Â· Andreas Holzinger Â· Ruth Fong Â· Taesup Moon Â· Klaus-robert Mueller</p></li></ul><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><ul><li><p>Encoding Musical Style with Transformer Autoencoders</p><p>Kristy Choi Â· Curtis Hawthorne Â· Ian Simon Â· Monica Dinculescu Â· Jesse Engel</p></li><li><p>Forecasting sequential data using Consistent Koopman Autoencoders</p><p>Omri Azencot Â· N. Benjamin Erichson Â· Vanessa Lin Â· Michael Mahoney</p></li><li><p>Latent Bernoulli Autoencoder</p><p>Jiri Fajtl Â· Vasileios Argyriou Â· Dorothy Monekosso Â· Paolo Remagnino</p></li><li><p>Perceptual Generative Autoencoders</p><p>Zijun Zhang Â· Ruixiang ZHANG Â· Zongpeng Li Â· Yoshua Bengio Â· Liam Paull</p></li><li><p>Rate-distortion optimization guided autoencoder for isometric embedding in Euclidean latent space</p><p>Keizo Kato Â· Jing Zhou Â· Tomotake Sasaki Â· Akira Nakagawa</p></li><li><p>Educating Text Autoencoders: Latent Representation Guidance via Denoising</p><p>Tianxiao Shen Â· Jonas Mueller Â· Regina Barzilay Â· Tommi Jaakkola</p></li><li><p>Topological Autoencoders</p><p>Michael Moor Â· Max Horn Â· Bastian Rieck Â· Karsten Borgwardt</p></li><li><p>ControlVAE: Controllable Variational Autoencoder</p><p>Huajie Shao Â· Shuochao Yao Â· Dachun Sun Â· Aston Zhang Â· Shengzhong Liu Â· Dongxin Liu Â· Jun Wang Â· Tarek Abdelzaher</p></li><li><p>Learning Autoencoders with Relational Regularization</p><p>Hongteng Xu Â· Dixin Luo Â· Ricardo Henao Â· Svati Shah Â· Lawrence Carin</p></li></ul><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><ul><li><p>Do RNN and LSTM have Long Memory?</p><p>Jingyu Zhao Â· Feiqing Huang Â· Jia Lv Â· Yanjie Duan Â· Zhen Qin Â· Guodong Li Â· Guangjian Tian</p></li></ul><h2 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h2><ul><li>On the Generalization Effects of Linear Transformations in Data AugmentationSen Wu Â· Hongyang Zhang Â· Gregory Valiant Â· Christopher Re</li><li>Improving Molecular Design by Stochastic Iterative Target AugmentationKevin Yang Â· Wengong Jin Â· Kyle Swanson Â· Regina Barzilay Â· Tommi Jaakkola</li></ul><ul><li> VFlow: More Expressive Generative Flows with Variational Data Augmentation</li></ul><p>  Jianfei Chen Â· Cheng Lu Â· Biqi Chenli Â· Jun Zhu Â· Tian Tian</p><ul><li><p><strong>[å·²è¯»]</strong> Self-supervised Label Augmentation via Input Transformations</p><p>Hankook Lee Â· Sung Ju Hwang Â· Jinwoo Shin</p><p>ä¸€ç§åœ¨å…¨ç›‘ç£çš„è®¾å®šä¸‹ï¼Œåšæ•°æ®å¢å¼ºçš„æ–¹æ¡ˆã€‚æ‰€æå‡ºçš„æ–¹æ³•ï¼š</p><ol><li>åŒºåˆ«äºä¼ ç»Ÿçš„self-supervised learning taskï¼ˆmulti-task leanring: è®¾å®šä¸¤ä¸ªä»»åŠ¡ï¼Œä¸€ä¸ªåŸå§‹ä»»åŠ¡ä¸€ä¸ªé™„åŠ ä»»åŠ¡ï¼Œä½¿ç”¨ä¸¤ä¸ªå­ç½‘ç»œè¿›è¡Œå®ç°ï¼Œlosså‡½æ•°ä¸ºä¸¤ä¸ªä»»åŠ¡çš„å’Œï¼‰ï¼Œ<br>æœ¬æ–‡æå‡ºçš„æ–¹æ³•ç›´æ¥åœ¨åŒä¸€ä¸ªç½‘ç»œä¸­åŒæ—¶è¿›è¡ŒåŸå§‹ä»»åŠ¡+é™„åŠ ä»»åŠ¡ã€‚</li></ol><p>å³ï¼šå¯¹äºä¸€ä¸ªå›¾ç‰‡åˆ†ç±»ä»»åŠ¡ï¼Œä¼ ç»Ÿæ–¹æ³•ï¼šå­ç½‘ç»œ1ç”¨äºåˆ†ç±»ï¼Œå­ç½‘ç»œ2ç”¨äºåŒºåˆ†æ•°æ®å¢å¼ºæ–¹æ³•ã€‚æœ¬æ–‡æ–¹æ³•ï¼šè®­ç»ƒä¸€ä¸ª$N+M$åˆ†ç±»çš„ç½‘ç»œï¼Œå‰è€…ä¸ºæ•°æ®ç±»åˆ«æ•°ï¼Œåè€…ä¸ºå¢å¼ºæ–¹æ³•æ•°ã€‚ç„¶åå¯¹ç½‘ç»œçš„è¾“å‡ºè¿›è¡Œèåˆï¼Œå­¦ä¹ ä¸€ä¸ªæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åå°†è¯¥æ¦‚ç‡çš„lossåŠ å…¥åˆ°æ€»çš„lossä¸­ã€‚</p></li><li><p>Distribution Augmentation for Generative Modeling</p><p>Heewoo Jun Â· Rewon Child Â· Mark Chen Â· John Schulman Â· Aditya Ramesh Â· Alec Radford Â·<br>Ilya Sutskever</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in ACL 2020 (2020.07.06)</title>
      <link href="/uncategorized/paperlistfile/ACL2020/"/>
      <url>/uncategorized/paperlistfile/ACL2020/</url>
      
        <content type="html"><![CDATA[<p><a href="https://acl2020.org/program/accepted/">Link</a></p><span id="more"></span><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><ul><li><p>Generating Informative Conversational Response using Recurrent Knowledge-Interaction and Knowledge-Copy</p><p>Xiexiong Lin, Weiyu Jian, Jianshan He, Taifeng Wang and Wei Chu</p></li><li><p>MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning</p><p>Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara Berg and Mohit Bansal</p></li><li><p>Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension</p><p>Hongyu Gong, Yelong Shen, Dian Yu, Jianshu Chen and Dong Yu</p></li><li><p>Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment</p><p>Forrest Davis and Marten van Schijndel</p></li><li><p>Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction</p><p>Shaowei Chen, Jie Liu, Yu Wang, Wenzheng Zhang and Ziming Chi</p></li></ul><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><ul><li><p>Autoencoding Pixies: Amortised Variational Inference with Graph Convolutions for Functional Distributional Semantics</p><p>Guy Emerson</p></li><li><p>Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder</p><p>Daya Guo, Duyu Tang, Nan Duan, Jian Yin, Daxin Jiang and Ming Zhou</p></li><li><p>Semi-Supervised Semantic Dependency Parsing Using CRF Autoencoders</p><p>Zixia Jia, Youmi Ma, Jiong Cai and Kewei Tu</p></li><li><p>Autoencoding Keyword Correlation Graph for Document Clustering</p><p>Billy Chiu, Sunil Kumar Sahu, Derek Thomas, Neha Sengupta and Mohammady Mahdy</p></li><li><p>Crossing Variational Autoencoders for Answer Retrieval</p><p>Wenhao Yu, Lingfei Wu, Qingkai Zeng, Shu Tao, Yu Deng and Meng Jiang</p></li><li><p>Interpretable Operational Risk Classification with Semi-Supervised Variational Autoencoder</p><p>Fan Zhou, Shengming Zhang and Yi Yang</p></li><li><p>SCAR: Sentence Compression using Autoencoders for Reconstruction</p><p>Chanakya Malireddy, Tirth Maniar and Manish Shrivastava</p></li></ul><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><ul><li><p>Inducing Grammar from Long Short-Term Memory Networks by Shapley Decomposition</p><p>Yuhui Zhang and Allen Nie</p></li></ul><h2 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h2><ul><li><p>A Study of Non-autoregressive Model for Sequence Generation</p><p>Yi Ren, Jinglin Liu, Xu Tan, Zhou Zhao, Sheng Zhao and Tie-Yan Liu</p></li><li><p><strong>å·²è¯»</strong> BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</p><p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov and Luke Zettlemoyer</p></li><li><p>Conditional Augmentation for Aspect Term Extraction via Masked Sequence-to-Sequence Generation</p><p>Kun Li, Chengbo Chen, Xiaojun Quan, Qing Ling and Yan Song</p></li><li><p>DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking</p><p>Christopher Hidey, Tuhin Chakrabarty, Tariq Alhindi, Siddharth Varia, Kriste Krstovski, Mona Diab and Smaranda Muresan</p></li><li><p>Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks</p><p>Fynn SchrÃ¶der and Chris Biemann</p></li><li><p>Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation</p><p>Junliang Guo, Linli Xu and Enhong Chen</p></li><li><p>Location Attention for Extrapolation to Longer Sequences</p><p>Yann Dubois, Gautier Dagan, Dieuwke Hupkes and Elia Bruni</p></li><li><p>NAT: Noise-Aware Training for Robust Neural Sequence Labeling</p><p>Marcin Namysl, Sven Behnke and Joachim KÃ¶hler</p></li><li><p>SeqVAT: Virtual Adversarial Training for Semi-Supervised Sequence Labeling</p><p>Luoxin Chen, Weitong Ruan, Xinyue Liu and Jianhua Lu</p></li><li><p>Structure-Level Knowledge Distillation For Multilingual Sequence Labeling</p><p>Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Fei Huang and Kewei Tu</p></li><li><p>Enriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing</p><p>Daniel FernÃ¡ndez-GonzÃ¡lez and Carlos GÃ³mez-RodrÃ­guez</p></li><li><p>Low Resource Sequence Tagging using Sentence Reconstruction</p><p>Tal Perl, Sriram Chaudhury and Raja Giryes</p></li><li><p>Embeddings of Label Components for Sequence Labeling: A Case Study of Fine-grained Named Entity Recognition</p><p>Takuma Kato, Kaori Abe, Hiroki Ouchi, Shumpei Miyawaki, Jun Suzuki and Kentaro Inui</p></li></ul><h2 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h2><ul><li><p>AdvAug: Robust Adversarial Augmentation for Neural Machine Translation</p><p>Yong Cheng, Lu Jiang, Wolfgang Macherey and Jacob Eisenstein</p></li><li><p>Conditional Augmentation for Aspect Term Extraction via Masked Sequence-to-Sequence Generation</p><p>Kun Li, Chengbo Chen, Xiaojun Quan, Qing Ling and Yan Song</p></li><li><p>Good-Enough Compositional Data Augmentation</p><p>Jacob Andreas</p><p>ç”±äºè¯­è¨€ä»»åŠ¡ä¸­çš„æŸäº›æ¨¡å¼å…·æœ‰é€šç”¨æ€§ï¼Œä¸ºäº†è®©ç¥ç»ç½‘ç»œå­¦ä¹ åˆ°è¿™äº›é€šç”¨æ€§ï¼Œä»è€Œæå‡ºè¿™ç§å¢å¼ºæ–¹æ³•ï¼Œå…·ä½“æ–¹æ³•ï¼š</p></li></ul><ol><li>åˆ†ææ•°æ®é›†ä¸­çš„è¯­è¨€æ¨¡å¼ï¼Œå³åœ¨åŒæ ·çš„è¯­è¨€ç¯å¢ƒä¸­å‡ºç°çš„ä¸åŒè¯å¥ï¼Œè¿™äº›ä¸åŒå­—å¥å°±æ˜¯éœ€è¦è¢«å­¦ä¹ åˆ°çš„é€šç”¨æ€§ï¼Œä¸‹é¢æ˜¯ä¸€å¯¹ä¾‹å­ã€‚<ol><li><strong>She</strong> <em>picks</em> <strong>the wug</strong> <em>up</em> <strong>in</strong> Fresno.</li><li><strong>She</strong> <em>puts</em> <strong>the wug</strong> <em>down</em> <strong>in</strong> Tempe.</li></ol></li><li>åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œç²—ä½“éƒ¨åˆ†ä»£è¡¨ç€åŒæ ·çš„è¯­è¨€ç¯å¢ƒï¼Œåˆ™æ–œä½“éƒ¨åˆ†åˆ™ä¸ºéœ€è¦å­¦ä¹ åˆ°çš„é€šç”¨æ€§ï¼Œåœ¨ç½‘ç»œå—åˆ°1çš„å¥å­çš„æ—¶å€™ï¼Œä¹Ÿéœ€è¦å…·å¤‡æ¨å¯¼å‡º2ä¸­æ–œä½“éƒ¨åˆ†å†…å®¹çš„èƒ½åŠ›ã€‚</li></ol><ul><li><p>Review-based Question Generation with Adaptive Instance Transfer and Augmentation</p><p>Qian Yu, Lidong Bing, Qiong Zhang, Wai Lam and Luo Si</p></li><li><p>Logic-Guided Data Augmentation and Regularization for Consistent Question Answering</p><p>Akari Asai and Hannaneh Hajishirzi</p></li><li><p>Parallel Data Augmentation for Formality Style Transfer</p><p>Yi Zhang, Tao Ge and Xu SUN</p></li><li><p>Syntactic Data Augmentation Increases Robustness to Inference Heuristics</p><p>Junghyun Min, R. Thomas McCoy, Dipanjan Das, Emily Pitler and Tal Linzen</p></li><li><p>Noise-Based Augmentation Techniques for Emotion Datasets: What do we Recommend?</p><p>Mimansa Jaiswal and Emily Mower Provost</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>è¶…å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆTBçº§åˆ«ä»¥ä¸Šï¼‰è°ƒç ”</title>
      <link href="/uncategorized/surveys/extra-large_dataset/"/>
      <url>/uncategorized/surveys/extra-large_dataset/</url>
      
        <content type="html"><![CDATA[<p>æœ¬è°ƒç ”æ€»ç»“äº†TBçº§åˆ«ä»¥ä¸Šæ•°æ®é›†çš„ç®€å•è¯´æ˜ä¸åœ°å€ï¼Œä¸é™æ•°æ®ç±»å‹ã€‚å…¶ä¸­åŒ…å«äº†ä¸€äº›å‹ç¼©åä¸è¶³1TBçš„æ•°æ®é›†ï¼Œä»–ä»¬çš„åŸå§‹æ–‡ä»¶å¤§å°å‡ä¸º1TBä»¥ä¸Šã€‚</p><span id="more"></span><h2 id="æ–‡æœ¬æ•°æ®"><a href="#æ–‡æœ¬æ•°æ®" class="headerlink" title="æ–‡æœ¬æ•°æ®"></a>æ–‡æœ¬æ•°æ®</h2><ul><li><p>CC-100ï¼šå¤§è§„æ¨¡CommonCrawl<code>æ–‡æœ¬æ•°æ®é›†</code>ï¼ŒåŒ…æ‹¬100ç§è¯­è¨€(ç”¨äºè®­ç»ƒXLM-R)çš„<code>2.5TB</code>å¹²å‡€çš„æ— ç›‘ç£æ–‡æœ¬è¯­æ–™ï¼›</p><ul><li>download and paper: <a href="http://data.statmt.org/cc-100/">http://data.statmt.org/cc-100/</a></li><li>institution: Facebook AI</li></ul></li><li><p>äº’è”ç½‘è¯­æ–™åº“(SogouT)ï¼šæ¥è‡ªäº’è”ç½‘å„ç§ç±»å‹çš„1.3äº¿ä¸ªåŸå§‹ç½‘é¡µ, å‹ç¼©å‰çš„å¤§å°è¶…è¿‡äº†<code>5TB</code></p><ul><li>download: <a href="http://www.sogou.com/labs/resource/t.php">http://www.sogou.com/labs/resource/t.php</a></li><li>institution: Sogou Lab</li></ul></li><li><p>AWSçˆ¬è™«æ•°æ®ï¼šæ”¶é›†äº†ä»2008ä»¥æ¥æŠ“å–çš„50äº¿ä¸ªç½‘é¡µçš„æ•°æ®ã€‚å…¶ä¸­è‡ª2013å¹´å¼€å§‹ï¼Œæ‰€æœ‰çˆ¬è™«åªæŒç»­ä¸€ä¸ªæœˆï¼Œæ•°æ®ä»¥WARCæ–‡ä»¶æ ¼å¼å­˜å‚¨ã€‚<br>ä»2012å¹´å¼€å§‹ï¼ŒæŠ“å–çš„æ•°æ®è¿˜åŒ…å«å…ƒæ•°æ®ï¼ˆWATï¼‰å’Œæ–‡æœ¬æ•°æ®ï¼ˆWETï¼‰æå–ï¼Œå¤§å¤§ç®€åŒ–äº†æ•°æ®å¤„ç†ï¼Œå¤§å°çº¦<code>541 TB</code></p><ul><li>download: <a href="https://registry.opendata.aws/commoncrawl/">https://registry.opendata.aws/commoncrawl/</a></li><li>institution: Amazon AWS</li></ul></li><li><p>Google Books Ngramsï¼šåŒ…å«åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­å‡ºç°è¶…è¿‡40æ¬¡çš„n-gramï¼Œä¼˜åŒ–äº†å¿«é€ŸæŸ¥è¯¢å°ç»„çŸ­è¯­çš„ç”¨æ³•ï¼Œ<code>2.2TB</code></p><ul><li>downloadï¼š<a href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html">http://storage.googleapis.com/books/ngrams/books/datasetsv2.html</a></li><li>institution: Google</li></ul></li><li><p>1.7 billion comments on Reddit: The dataset is ~1.7 billion JSON objects complete with the comment, score,<br>author, subreddit, position in comment tree and other fields that are available through Redditâ€™s API.<br>This dataset is over <code>1 terabyte</code> uncompressed.</p><ul><li>download: <a href="https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/">https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/</a></li></ul></li></ul><h2 id="è§†é¢‘-å›¾åƒæ•°æ®"><a href="#è§†é¢‘-å›¾åƒæ•°æ®" class="headerlink" title="è§†é¢‘/å›¾åƒæ•°æ®"></a>è§†é¢‘/å›¾åƒæ•°æ®</h2><ul><li><p>YouTube-8M: A Large-Scale Video Classification Benchmark. åŒ…å«ä¸‰ä¸ªçº§åˆ«çš„è§†é¢‘æ•°æ®åŠå…¶æ ‡æ³¨ï¼š<br>Segment-rated frame-level features dataset, Frame-level features dataset (<code>1.53TB</code>), Video-level features dataset (<code>31GB</code>)</p><ul><li>download: <a href="http://research.google.com/youtube8m/download.html">http://research.google.com/youtube8m/download.html</a></li><li>paper: YouTube-8M: A Large-Scale Video Classification Benchmark (<a href="https://arxiv.org/abs/1609.08675">https://arxiv.org/abs/1609.08675</a>)</li><li>institution: Google</li></ul></li><li><p>StarCraft: Brood War replay dataset yet, with 65646 games. The full dataset <code>after compression</code> is <code>365 GB</code>,<br>1535 million frames, and 496 million player actions. The entire frame data was dumped out at 8 frames per second.</p><ul><li>download: <a href="https://github.com/TorchCraft/StarData">https://github.com/TorchCraft/StarData</a></li><li>paper: <a href="https://arxiv.org/abs/1708.02139">https://arxiv.org/abs/1708.02139</a></li><li>institution: Facebook AI</li></ul></li><li><p>è‡ªåŠ¨é©¾é©¶æ•°æ®é›†: è¿™äº›æ•°æ®æ¶µç›–äº†Waymoåœ¨æ”¶é›†çš„å¤šç§ç¯å¢ƒä¿¡æ¯ï¼ŒåŒ…æ‹¬ç™½å¤©ä¸é»‘å¤œã€é»„æ˜å’Œé»æ˜ã€é˜³å…‰å’Œé›¨å¤©ï¼Œæ¶µç›–äº†åŸå¸‚ä¸­å¿ƒå’ŒéƒŠåŒºã€‚æ•°æ®é›†å¤§å°çº¦ä¸º<code>1.35TB</code></p><ul><li>download: <a href="https://waymo.com/open">https://waymo.com/open</a></li><li>institution: Waymo</li></ul></li><li><p>å¤§è§„æ¨¡ Deepfake æ•°æ®é›† DFDCï¼š3426 åå¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡å¹³å‡å½•åˆ¶ 14.4 ä¸ªè§†é¢‘ï¼Œå¤§éƒ¨åˆ†è§†é¢‘çš„åˆ†è¾¨ç‡ä¸º1080pï¼›48,190 ä¸ªè§†é¢‘ï¼Œ<br>æ¯ä¸ªè§†é¢‘çš„å¹³å‡é•¿åº¦ä¸º 68.8ç§’ï¼Œå…±è®¡é•¿åº¦ 38.4å¤©ï¼›åŸå§‹æ•°æ®è¶…è¿‡<code>25TB</code>ã€‚</p><ul><li>download: <a href="https://www.kaggle.com/c/deepfake-detection-challenge/data">https://www.kaggle.com/c/deepfake-detection-challenge/data</a></li><li>institution: Facebook AI</li></ul></li><li><p>Open Images Dataset: Open Images is a dataset of ~9 million URLs to images that have been annotated with labels<br>spanning over 6000 categories, <code>561GB after compressed, 18TB uncompressed</code>.</p><ul><li>download: <a href="https://github.com/cvdfoundation/open-images-dataset#download-images-with-bounding-boxes-annotations">https://github.com/cvdfoundation/open-images-dataset#download-images-with-bounding-boxes-annotations</a></li></ul></li></ul><h2 id="å…¶ä»–æ•°æ®é›†"><a href="#å…¶ä»–æ•°æ®é›†" class="headerlink" title="å…¶ä»–æ•°æ®é›†"></a>å…¶ä»–æ•°æ®é›†</h2><p>è¿™äº›æ•°æ®é›†æ— æ³•å‡†ç¡®çš„åˆ—å‡ºå¤§å°æˆ–ä¸‹è½½åœ°å€ï¼Œä½†éƒ½æ˜¯å…¬å¼€çš„æ•°æ®é›†ï¼Œå…¶å¤§å°å‚è€ƒäº†ä¸€äº›å·²å‘è¡¨è®ºæ–‡ä¸­çš„æè¿°</p><ul><li>Sloan Digital Sky Survey (SDSS) dataset: one of the largest astronomical catalogs publicly accessible (<code>more than 3.4TB</code>).<ul><li>download: <a href="https://www.sdss.org/dr13/">https://www.sdss.org/dr13/</a></li><li>paper: <em>Dawson, K. S., Kneib, J. P., Percival, W. J., Alam, S., Albareti, F. D., Anderson, S. F., â€¦ &amp; Zou, H. (2016). The SDSS-IV extended Baryon Oscillation Spectroscopic Survey: overview and early data. The Astronomical Journal, 151(2), 44.</em></li><li>find this dataset in: <em>Yan, Y., Cao, L., Kulhman, C., &amp; Rundensteiner, E. (2017, August). Distributed local outlier detection in big data. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1225-1234).</em></li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> survey </tag>
            
            <tag> dataset </tag>
            
            <tag> extreme-large </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>è°ƒç ”ï¼šç¾å›½COVID19æ„ŸæŸ“/æ­»äº¡ã€å†å²æµæ„Ÿæ„ŸæŸ“/æ­»äº¡</title>
      <link href="/uncategorized/surveys/cov19_data/"/>
      <url>/uncategorized/surveys/cov19_data/</url>
      
        <content type="html"><![CDATA[<p>ä»¥ä¸‹æ•°æ®å‡æ¥æºäºç¾å›½ç–¾ç—…æ§åˆ¶ä¸é¢„é˜²ä¸­å¿ƒï¼ˆCDCï¼‰ã€‚</p><span id="more"></span><h2 id="COVID19æ•°æ®"><a href="#COVID19æ•°æ®" class="headerlink" title="COVID19æ•°æ®"></a>COVID19æ•°æ®</h2><ul><li><p><a href="https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/cases-in-us.html">COVID19 å®æ—¶æ„ŸæŸ“æ•°æ®</a></p></li><li><p><a href="https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/previouscases.html">COVID19 å†å²æ„ŸæŸ“æ•°æ®</a></p></li></ul><h2 id="æµæ„Ÿæ•°æ®"><a href="#æµæ„Ÿæ•°æ®" class="headerlink" title="æµæ„Ÿæ•°æ®"></a>æµæ„Ÿæ•°æ®</h2><ul><li><p><a href="https://www.cdc.gov/flu/weekly/index.htm">æ¯å‘¨æµæ„Ÿç›‘æµ‹æŠ¥å‘Š</a>ï¼Œå†…å®¹åŒ…æ‹¬ï¼š</p><ul><li>ç¾å›½ç—…æ¯’å­¦ç›‘æµ‹ï¼š<ol><li>æ¥è‡ªä¸´åºŠå®éªŒå®¤çš„æ•°æ®ï¼ˆæ£€æµ‹å‡ºçš„å¯¹æµæ„Ÿå‘ˆé˜³æ€§çš„æ ‡æœ¬çš„ç™¾åˆ†æ¯”ï¼‰ç”¨äºç›‘æµ‹æµæ„Ÿæ´»åŠ¨æ˜¯åœ¨å¢åŠ è¿˜æ˜¯åœ¨å‡å°‘ã€‚</li><li>æ¥è‡ªå…¬å…±å«ç”Ÿå®éªŒå®¤çš„æ•°æ®ç”¨äºç›‘æµ‹å±äºæ¯ç§æµæ„Ÿäºšå‹/è¡€ç»Ÿçš„å¾ªç¯ç—…æ¯’çš„æ¯”ä¾‹ã€‚</li><li>æ–°å‹ç”²å‹æµæ„Ÿç—…æ¯’ï¼šå¤å¨å¤·æŠ¥å‘Šäº†ä¸€ç§äººç±»æ„ŸæŸ“æ–°å‹ç”²å‹æµæ„Ÿç—…æ¯’çš„æƒ…å†µã€‚æ­¤äººæ„ŸæŸ“äº†ç”²å‹H3N2æµæ„Ÿç—…æ¯’ï¼ˆAï¼ˆH3N2ï¼‰vï¼‰ã€‚è¯¥æ‚£è€…ä¸º18å²ä»¥ä¸‹çš„å­©å­ï¼Œæœªä½é™¢ä¸”å·²åº·å¤ã€‚å°½ç®¡è¿„ä»Šå°šæ— çŒªæ¥è§¦çš„æŠ¥é“ï¼Œä½†æœ‰å…³æ‚£è€…æ„ŸæŸ“æºçš„è°ƒæŸ¥ä»åœ¨è¿›è¡Œä¸­ã€‚è¿™æ˜¯è‡ª2018å¹´ä»¥æ¥åœ¨ç¾å›½é¦–æ¬¡å‘ç°çš„ç”²å‹H3N2æµæ„Ÿç—…æ¯’æ„ŸæŸ“ã€‚</li></ol></li><li>é—¨è¯Šç–¾ç—…ç›‘æµ‹<ol><li>ILINetï¼šåœ¨ç¬¬30å‘¨çš„æ•´ä¸ªç¾å›½èŒƒå›´å†…ï¼Œé€šè¿‡ç¾å›½é—¨è¯Šæµæ„Ÿæ ·ç–¾ç—…ç›‘æµ‹ç½‘ç»œï¼ˆILINetï¼‰æŠ¥å‘Šçš„æ‚£è€…å°±è¯Šäººæ•°ä¸­æœ‰1.2ï¼…æ˜¯ç”±äºæµæ„Ÿæ ·ç–¾ç—…ï¼ˆILIï¼‰å¼•èµ·çš„ã€‚è¯¥ç™¾åˆ†æ¯”ä½äºå›½å®¶åŸºå‡†çš„2.4ï¼…ã€‚</li><li>ILIæ´»åŠ¨å›¾ï¼šåœ¨ILINetä¸­æ”¶é›†çš„æ•°æ®ç”¨äºæŒ‰å·äº§ç”ŸILIæ´»æ€§*çš„åº¦é‡ã€‚</li></ol></li><li>æµæ„Ÿç›¸å…³ä½é™¢ï¼šæµæ„Ÿä½é™¢ç›‘æ§ç½‘ç»œï¼ˆFluSurv-NETï¼‰åœ¨æ–°å…´æ„ŸæŸ“è®¡åˆ’ï¼ˆEIPï¼‰å·å’Œæµæ„Ÿä½é™¢ç›‘æ§é¡¹ç›®ï¼ˆIHSPï¼‰å„ä¸ªå·å¯¹å®éªŒå®¤ç¡®è®¤çš„æµæ„Ÿç›¸å…³ä½é™¢è¿›è¡ŒåŸºäºå¹´é¾„çš„æ‰€æœ‰ç›‘è§†ã€‚</li><li>è‚ºç‚å’Œæµæ„Ÿï¼ˆPï¼†Iï¼‰æ­»äº¡ç‡ç›‘æµ‹ï¼šæ ¹æ®2020å¹´7æœˆ30æ—¥ç¾å›½å›½å®¶å«ç”Ÿç»Ÿè®¡ä¸­å¿ƒï¼ˆNCHSï¼‰çš„æ­»äº¡ç‡ç›‘æµ‹æ•°æ®ï¼Œæˆªè‡³2020å¹´7æœˆ25æ—¥çš„ä¸€å‘¨ï¼ˆç¬¬30å‘¨ï¼‰å†…å‘ç”Ÿçš„æ­»äº¡ä¸­æœ‰6.4ï¼…å½’å› äºPï¼†Iã€‚è¯¥ç™¾åˆ†æ¯”é«˜äºç¬¬30å‘¨æµè¡Œé˜ˆå€¼5.6ï¼…ã€‚</li><li>æµæ„Ÿç›¸å…³çš„å„¿ç«¥æ­»äº¡ç‡ï¼šCDCåœ¨2019å¹´çš„ç¬¬30å‘¨æŠ¥å‘Šäº†1ä¾‹ä¸æµæ„Ÿç›¸å…³çš„å°å„¿æ­»äº¡ï¼Œè¯¥æ­»äº¡ä¸Aï¼ˆH1N1ï¼‰pdm09æµæ„Ÿç—…æ¯’æœ‰å…³ï¼Œå¹¶åœ¨ç¬¬9å‘¨ï¼ˆæˆªè‡³2020å¹´2æœˆ29æ—¥çš„ä¸€å‘¨ï¼‰å†…å‘ç”Ÿã€‚</li></ul></li><li><p><a href="https://www.cdc.gov/flu/weekly/fluviewinteractive.htm">å†å²æµæ„Ÿæ•°æ®</a>ï¼Œå†…å®¹åŒ…æ‹¬ï¼š</p><ul><li><p>ILIä¸ç—…æ¯’ç›‘æµ‹æ•°æ®</p><ol><li>å›½å®¶ï¼Œåœ°åŒºå’Œå·çº§åˆ«çš„é—¨è¯Šç–¾ç—…å’Œç—…æ¯’ç›‘æµ‹</li><li>å…¬å…±å«ç”Ÿå®éªŒå®¤æŠ¥å‘Šçš„æµæ„Ÿé˜³æ€§æ£€æµ‹çš„å¹´é¾„ç»„åˆ†å¸ƒ</li></ol></li><li><p>ä½é™¢æ²»ç–—æ•°æ®</p><ol><li>å®éªŒå®¤ç¡®è¯Šçš„æµæ„Ÿä½é™¢æ²»ç–—</li><li>å…·æœ‰ç‰¹å¾çš„å®éªŒå®¤ç¡®è®¤çš„æµæ„Ÿä½é™¢æ²»ç–—</li></ol></li><li><p>ILIæ´»åŠ¨æŒ‡æ ‡å›¾ï¼šè¯¥åº”ç”¨ç¨‹åºä½¿ç”¨é€šè¿‡ILINetæ”¶é›†çš„æ•°æ®æŒ‰çŠ¶æ€æ˜¾ç¤ºILIæ´»åŠ¨ã€‚è¯¥äº¤äº’å¼å·¥å…·ä½¿ç”¨æˆ·å¯ä»¥æŸ¥çœ‹å¤šä¸ªå­£èŠ‚ç‰¹å®šäºå·çš„æ´»åŠ¨æ°´å¹³ï¼Œå¹¶å¯ä»¥ç›´è§‚è¡¨ç¤ºå„ä¸ªå·ä¹‹é—´çš„ç›¸å¯¹æ´»åŠ¨ã€‚</p></li><li><p>æµæ„Ÿç›¸å…³çš„å°å„¿æ­»äº¡ç‡</p></li><li><p>ç¾å›½å›½å®¶å«ç”Ÿç»Ÿè®¡æ­»äº¡ç‡ç›‘æµ‹ç³»ç»Ÿä¸­å¿ƒçš„è‚ºç‚å’Œæµæ„Ÿæ­»äº¡ç‡ç›‘æµ‹</p></li><li><p>äººç±»æ„ŸæŸ“æ–°å‹ç”²å‹æµæ„Ÿç—…æ¯’</p></li><li><p>å·å’Œåœ°åŒºæµè¡Œç—…å­¦å®¶å¯¹æµæ„Ÿåœ°ç†ä¼ æ’­çš„æŠ¥é“</p></li></ul></li><li><p><a href="https://www.cdc.gov/flu/weekly/pastreports.htm">å†å²æ¯å‘¨æµæ„Ÿç›‘æµ‹æŠ¥å‘Š</a></p></li></ul><h2 id="æ•°æ®é›†"><a href="#æ•°æ®é›†" class="headerlink" title="æ•°æ®é›†"></a>æ•°æ®é›†</h2><ul><li><p><a href="https://predict.cdc.gov/post/59973fe26f7559750d84a843">CDCå…³äº2017-18æµæ„Ÿå­£èŠ‚é¢„æµ‹æ¯”èµ›çš„æ•°æ®ä¸å®˜æ–¹æŒ‡å¯¼æ–‡ä»¶</a></p></li><li><p><a href="https://www.cdc.gov/flu/weekly/flusight/index.html">CDCä¸Šå…³äºæµæ„Ÿé¢„æµ‹çš„æ–¹æ³•ä»‹ç»</a></p></li><li><p><a href="https://aminer.cn/data-covid19/?lang=zh">å­¦æœ¯ç½‘ç«™Amineræ•´ç†çš„COVID19æ•°æ®ã€ä¿¡æ¯ã€çŸ¥è¯†</a></p></li><li><p><a href="https://covid-19.aminer.cn/kg/?lang=zh">COVID19çš„ç›¸å…³çŸ¥è¯†å›¾è°±</a></p></li><li><p><a href="https://github.com/CSSEGISandData/COVID-19">COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University</a></p></li><li><p><a href="http://www.dxy.cn/">ä¸é¦™å›­</a></p></li><li><p><a href="https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports/">WHO</a></p></li><li><p><a href="https://www.worldometers.info/coronavirus/country/hungary/">Statistical reports of COVID-19 cases and mortality rate of Hungary</a></p></li><li><p><a href="https://github.com/CSSEGISandData/COVID-19">Novel Coronavirus (COVID-19) Cases</a></p></li><li><p><a href="https://www.kaggle.com/jcyzag/covid19-lockdown-dates-by-country">COVID-19 Lockdown dates by country</a></p></li><li><p><a href="https://ourworldindata.org/coronavirus-source-data">Hannah Ritchie</a></p></li><li><p><a href="https://www.kaggle.com/sudalairajkumar/covid19-in-india">Covid-19 in India</a></p></li><li><p><a href="https://ieee-dataport.org/open-access/coronavirus-covid-19-geo-tagged-tweets-dataset">CORONAVIRUS (COVID-19) GEO-TAGGED TWEETS DATASET</a></p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> survey </tag>
            
            <tag> dataset </tag>
            
            <tag> covid19 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>è°ƒç ”ï¼šç¾å›½COVID19æ„ŸæŸ“/æ­»äº¡ã€å†å²æµæ„Ÿæ„ŸæŸ“/æ­»äº¡</title>
      <link href="/uncategorized/surveys/cov19_papers/"/>
      <url>/uncategorized/surveys/cov19_papers/</url>
      
        <content type="html"><![CDATA[<h2 id="è°ƒç ”-å…³äºæµæ„Ÿé¢„æµ‹"><a href="#è°ƒç ”-å…³äºæµæ„Ÿé¢„æµ‹" class="headerlink" title="è°ƒç ”: å…³äºæµæ„Ÿé¢„æµ‹"></a>è°ƒç ”: å…³äºæµæ„Ÿé¢„æµ‹</h2><h3 id="æ•°æ®"><a href="#æ•°æ®" class="headerlink" title="æ•°æ®"></a>æ•°æ®</h3><ul><li><p><a href="https://predict.cdc.gov/post/59973fe26f7559750d84a843">CDCå…³äº2017-18æµæ„Ÿå­£èŠ‚é¢„æµ‹æ¯”èµ›çš„æ•°æ®ä¸å®˜æ–¹æŒ‡å¯¼æ–‡ä»¶</a></p></li><li><p><a href="https://www.cdc.gov/flu/weekly/flusight/index.html">CDCä¸Šå…³äºæµæ„Ÿé¢„æµ‹çš„æ–¹æ³•ä»‹ç»</a></p></li><li><p><a href="https://aminer.cn/data-covid19/?lang=zh">å­¦æœ¯ç½‘ç«™Amineræ•´ç†çš„COVID19æ•°æ®ã€ä¿¡æ¯ã€çŸ¥è¯†</a></p></li></ul><span id="more"></span><ul><li><p><a href="https://covid-19.aminer.cn/kg/?lang=zh">COVID19çš„ç›¸å…³çŸ¥è¯†å›¾è°±</a></p></li><li><p><a href="https://github.com/CSSEGISandData/COVID-19">COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University</a></p></li><li><p><a href="http://www.dxy.cn/">ä¸é¦™å›­</a></p></li><li><p><a href="https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports/">WHO</a></p></li><li><p><a href="https://www.worldometers.info/coronavirus/country/hungary/">Statistical reports of COVID-19 cases and mortality rate of Hungary</a></p></li><li><p><a href="https://github.com/CSSEGISandData/COVID-19">Novel Coronavirus (COVID-19) Cases</a></p></li><li><p><a href="https://www.kaggle.com/jcyzag/covid19-lockdown-dates-by-country">COVID-19 Lockdown dates by country</a></p></li><li><p><a href="https://ourworldindata.org/coronavirus-source-data">Hannah Ritchie</a></p></li><li><p><a href="https://www.kaggle.com/sudalairajkumar/covid19-in-india">Covid-19 in India</a></p></li><li><p><a href="https://ieee-dataport.org/open-access/coronavirus-covid-19-geo-tagged-tweets-dataset">CORONAVIRUS (COVID-19) GEO-TAGGED TWEETS DATASET</a></p></li></ul><h3 id="è®ºæ–‡"><a href="#è®ºæ–‡" class="headerlink" title="è®ºæ–‡"></a>è®ºæ–‡</h3><h4 id="SURVEY"><a href="#SURVEY" class="headerlink" title="SURVEY"></a>SURVEY</h4><ul><li><p>Artificial Intelligence Against Covid-19: An Early Review, IZA Discussion Paper. cite: 22</p></li><li><p>Role of intelligent computing in COVID-19 prognosis: A state-of-the-art reviewï¼ŒChaos, Solitons &amp; Fractals. cite: 3</p></li><li><p>Applications of machine learning and artificial intelligence for Covid-19 (SARS-CoV-2) pandemic: A reviewï¼ŒChaos, Solitons &amp; Fractals. cite: 3</p></li><li><p>Forecasting Models for Coronavirus (COVID-19): A Survey of the State-of-the-Art, teahrXiv. cite: 2</p></li><li><p>Automated Detection and Forecasting of COVID-19 using Deep Learning Techniques: A Reviewï¼ŒarXiv. cite: 1</p></li><li><p>Deep learning methods for forecasting COVID-19 time-Series data: A Comparative study, Chaos, Solitons &amp; Fractals. cite: 0</p></li><li><p>Review of Big Data Analytics, Artificial Intelligence and Nature-Inspired Computing Models towards Accurate Detection of COVID-19 Pandemic Cases and Contact Tracing, International Journal of Environmental Research and Public Health. cite: 0</p></li><li><p>COVID-19 Open Source Data Sets: A Comprehensive Survey, medRxiv. cite: 0</p></li></ul><h4 id="paper"><a href="#paper" class="headerlink" title="paper"></a>paper</h4><p>åšç–«æƒ…é¢„æµ‹çš„å·¥ä½œä¸»è¦æœ‰ä¸‰å¤§ç±»æ–¹æ³•ï¼š</p><ol><li>åŸºäºç»å…¸çš„ä¼ æŸ“ç—…æ¨¡å‹çš„æ–¹æ³•ï¼Œæˆ–åŸºäºè¿™äº›æ¨¡å‹è¿›è¡Œæ”¹åŠ¨ï¼Œæ­¤ç±»æ–¹æ³•è€ƒè™‘äº†ä¼ æŸ“ç—…çš„å¤šç§å› ç´ ï¼Œä¾‹å¦‚R0ã€äººå£æµåŠ¨ç­‰ã€‚ç»å…¸ä¼ æŸ“ç—…æ¨¡å‹åŒ…æ‹¬ï¼šsub-epidemic wave modelï¼Œextended susceptible-infected-removed model(eSIR)ï¼Œ Susceptible-Infectious-Recovered-Dead model(SIDR), SEIRD model(susceptible individuals, asymptomatic infected, symptomatic infected, recovered, and deceased)</li><li>ç»å…¸çš„æ—¶é—´åºåˆ—æ–¹æ³•ã€æœºå™¨å­¦ä¹ æ–¹æ³•æˆ–å˜ä½“ï¼ŒåŒ…æ‹¬ï¼šlogistic growth modelï¼ŒARIMAï¼ŒWavelet-based modelç­‰</li><li>åŸºäºå¾ªç¯ç¥ç»ç½‘ç»œç­‰æ–¹æ³•ï¼Œè¾“å…¥ç½‘è·¯çš„æ•°æ®ä¸ºæ—¶é—´åºåˆ—ï¼ŒåŒ…æ‹¬ï¼šLSTM, GRU, RNN åŠå…¶å˜ç§ï¼Œå¦‚stacked-RNN(LSTM/GRU), multi-layer RNN(LSTM/GRU)</li><li>åŸºäºå¤–éƒ¨çŸ¥è¯†/ä¿¡æ¯çš„æ–¹æ³•ï¼Œå¤–éƒ¨ä¿¡æ¯åŒ…æ‹¬ï¼šç¤¾äº¤ç½‘ç»œä¿¡æ¯ï¼Œæœç´¢å¼•æ“æŒ‡æ•°å¦‚baidu search indexï¼Œåœ°åŒºæ°”å€™ä¿¡æ¯ï¼Œç©ºé—´ä¿¡æ¯ï¼ˆæŸåœ°åŒºè·ç¦»ç–«æƒ…éœ‡ä¸­çš„è·ç¦»ç­‰ï¼‰ï¼Œæ™ºèƒ½è¡¨ä¸­çš„äººä½“å¥åº·æ•°æ®ç­‰ã€‚æ­¤ç±»æ–¹æ³•ç»“åˆäº†å¤–éƒ¨ä¿¡æ¯ï¼Œèƒ½å¤Ÿæ›´ç²¾ç¡®çš„åˆ»ç”»ç–«æƒ…çš„å‘å±•æ€åŠ¿ï¼Œä¸”åœ¨å®˜æ–¹æ•°å­—æ»åçš„æƒ…å†µä¸‹ï¼Œè¿˜èƒ½åšå‡ºæ¯”è¾ƒå‡†ç¡®çš„é¢„æµ‹ã€‚æ­¤ç±»ä¿¡æ¯è¿˜åº”ç”¨äº†åŸºäºRNNç­‰ç»“æ„ï¼Œæå–æ—¶é—´åºåˆ—çš„åŠ¨æ€ã€‚éƒ¨åˆ†è®ºæ–‡æå‡ºå°†æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸ä¼ ç»Ÿä¼ æŸ“ç—…æ¨¡å‹ç»“åˆï¼Œåˆ»ç”»ç–«æƒ…ä¼ æ’­æ€åŠ¿ã€‚</li></ol><h5 id="Traditional-models-ARIMA-logistic-regression-epidemic-model-and-so-on"><a href="#Traditional-models-ARIMA-logistic-regression-epidemic-model-and-so-on" class="headerlink" title="Traditional models (ARIMA, logistic regression, epidemic model and so on)"></a>Traditional models (ARIMA, logistic regression, epidemic model and so on)</h5><p>æœ¬èŠ‚ä¸­çš„è®ºæ–‡å¤§è‡´æŒ‰ç…§ â€œåŸºäºä¼ æŸ“ç—…æ¨¡å‹çš„æ–¹æ³•â€ -&gt; â€œåŸºäºä¼ ç»Ÿæ—¶é—´åºåˆ—ä¸æœºå™¨å­¦ä¹ æ–¹æ³•â€ çš„é¡ºåºæ’åˆ—ã€‚ç”±äºç›¸å…³å·¥ä½œä¸­åŒ…å«äº†å¤§é‡åœ¨é¢„å°æœ¬ç½‘ç«™ä¸Šå‘è¡¨çš„è®ºæ–‡ï¼Œå› æ­¤å°†æ­£å¼å‘è¡¨ä¸æœªæ­£å¼å‘è¡¨çš„è®ºæ–‡åˆ†å¼€äº†ã€‚</p><h6 id="peer-reviewed-papers"><a href="#peer-reviewed-papers" class="headerlink" title="peer-reviewed papers"></a>peer-reviewed papers</h6><p>Real-time forecasts of the COVID-19 epidemic in China from February 5th to February 24th, 2020, Infectious Disease Modelling</p><ul><li>æ•°æ®ï¼šdaily reported cumulative confirmed cases for the 2019-nCoV outbreak for each Chinese province from the National Health Commission of China.</li><li>æ¨¡å‹ï¼šgeneralized logistic growth model, the Richards growth model, and a sub-epidemic wave model.</li><li>cite: 182</li></ul><p>Short-term Forecasts of the COVID-19 Epidemic in Guangdong and Zhejiang, China: February 13â€“23, 2020ï¼ŒJ Clin Med</p><ul><li>æ•°æ®ï¼šthe cumulative cases for 34 provinces, including municipalities, autonomous regions, and special administrative regionsï¼Œ reported case data each day at 12 pm (GMT-5) from the initial date of reporting, 22 January 2020, to 13 February 2020.</li><li>æ¨¡å‹ï¼š<strong>The generalized logistic growth model (GLM)</strong> and the <strong>Richards model extend the simple logistic growth model</strong> with an additional scaling parameterï¼Œ <strong>sub-epidemic model</strong></li><li>cite: 61</li></ul><p>Extended SIR Prediction of the Epidemics Trend of COVID-19 in Italy and Compared With Hunan, Chinaï¼ŒFrontiers in medicine</p><ul><li>æ•°æ®ï¼špublicly available dataset of COVID-19 provided by the Johns Hopkins University. This dataset includes many countriesâ€™ daily count of confirmed cases, recovered cases, and deaths. As time-series data, it is available from 22 January 2020.</li><li>æ¨¡å‹ï¼šAn <strong>infectious disease dynamic extended susceptible-infected-removed (eSIR) model</strong>ï¼Œ The basic reproductive number was estimated using <strong>Markov Chain Monte Carlo methods</strong></li><li>cite: 24</li></ul><p>Data-based analysis, modelling and forecasting of the COVID-19 outbreak,  PloS one</p><ul><li>æ•°æ®ï¼špublicly available data from the 11th of January 2020 to the 10th of February 2020.</li><li>æ¨¡å‹ï¼šSusceptible-Infectious-Recovered-Dead (SIDR) model</li><li>cite: 148</li></ul><p>A fractional-order model for the novel coronavirus (COVID-19) outbreakï¼ŒNonlinear Dynamicsï¼ŒSpringer.</p><ul><li>æ•°æ®ï¼šItalyâ€™s data from February 24 to April 7, reported by WHO. We consider the number of the infected (I) and deceased (D) individuals</li><li>æ¨¡å‹ï¼š<strong>fractional-order</strong> susceptible individuals, asymptomatic infected, symptomatic infected, recovered, and deceased <strong>(SEIRD)</strong> model.</li><li>cite: 3</li></ul><p>Reconstructing and forecasting the COVID-19 epidemic in the United States using a 5-parameter logistic growth model, Global health research and policy, pp. 252020.</p><ul><li>æ•°æ®ï¼šmulative cases of COVID-19 in the U.S. from January 22 to April 6, 2020</li><li>æ¨¡å‹ï¼š5-parameter logistic growth model</li><li>cite: 2</li></ul><p>Estimation of COVID-19 prevalence in Italy, Spain, and France, Science of The Total Environment, Volume 729, 10 August 2020, 138817, ELSEVIER</p><ul><li>æ•°æ®ï¼šThe prevalence data of COVID-19 was taken from the </li><li>æ¨¡å‹ï¼šARIMA</li><li>cite: 43</li></ul><p>Real-time forecasts and risk assessment of novel coronavirus (COVID-19) cases: A data-driven analysis, Chaos, Solitons &amp; Fractals, Volume 135, June 2020, 109850, ELSEVIER</p><ul><li>æ•°æ®ï¼šthe daily figures of confirmed cases for five different countries, namely Canada, France, India, South Korea, and the UK. The datasets are retrieved by the Global Change Data Lab1).</li><li>æ¨¡å‹ï¼šhybrid ARIMA-WBF(Wavelet-based forecasting model) model</li><li>cite: 40</li></ul><p>COVID-19 virus outbreak forecasting of registered and recovered cases after sixty day lockdown in Italy: A data driven model approachï¼ŒJournal of Microbiology, Immunology and Infection.</p><ul><li>æ•°æ®ï¼šCOVID-19 infected patient data has extracted from the Italian Health Ministry website includes registered and recovered cases from mid February to end March.</li><li>æ¨¡å‹ï¼šseasonal ARIMA</li><li>cite: 34</li></ul><p>Prediction of Number of Cases of 2019 Novel Coronavirus (COVID-19) Using Social Media Search Indexï¼ŒIJERPH</p><ul><li>æ•°æ®ï¼šBaidu Search Index in Social Mediaï¼ŒNumber of New Suspected Infection Cases</li><li>æ¨¡å‹ï¼šä¸€ä¸ªå‡½æ•°æ¨¡å‹ï¼Œè‡ªå˜é‡ä¸ºBaidu Search Indexï¼Œå› å˜é‡ä¸ºç¡®è¯Šæ•°é‡ï¼Œè¯¥å‡½æ•°çš„å‚æ•°é€šè¿‡Subset selectionï¼ŒForward selectionï¼ŒRidge regressionï¼ŒLasso regressionå’ŒElastic netæ¥ä¼°è®¡</li><li>cite: 16</li></ul><p>Time series modelling to forecast the confirmed and recovered cases of COVID-19, Travel Medicine and Infectious Disease, ELSEVIER</p><ul><li>æ•°æ®ï¼š the number of confirmed and recovered COVID-19 cases in the world from 21-Apr-2020 up to 30-Apr-2020,</li><li>æ¨¡å‹ï¼š<strong>Autoregressive time series models</strong> based on <strong>two-piece scale mixture normal distributions</strong>, called TPâ€“SMNâ€“AR models(includes the symmetric Gaussian and asymmetric heavy-tailed non-Gaussian autoregressive time series models. )</li><li>cite: 10</li></ul><p>Prediction of the COVID-19 Pandemic for the Top 15 Affected Countries: Advanced Autoregressive Integrated Moving Average (ARIMA) Model, JMIR public health and surveillance</p><ul><li>æ•°æ®ï¼šWe have collected the case data for each day at given stipulated times, from January 21, 2020, to April 24, 2020. </li><li>æ¨¡å‹ï¼šARIMA</li><li>cite: 10</li></ul><p>ALGORITHM OF HYBRID GMDH-NETWORK CONSTRUCTION FOR TIME SERIES FORECASTï¼ŒElectronics and Control Systems</p><ul><li>æ•°æ®ï¼š the data of the COVID-19 pandemic, collected by Johns Hopkins University.</li><li>æ¨¡å‹ï¼šHybridization is achieved through the use of various neurons: <strong>classical, nonlinearAdaline, R-neuron, W-neuron, Wavelet-neuron</strong>.</li><li>cite: 0</li></ul><h6 id="preprint-paper-not-peer-reviewed"><a href="#preprint-paper-not-peer-reviewed" class="headerlink" title="preprint paper (not peer-reviewed)"></a>preprint paper (not peer-reviewed)</h6><p>Prediction of COVID-19 Spreading Profiles in South Korea, Italy and Iran by Data-Driven Coding, medRxiv, 2020.</p><ul><li>æ•°æ®ï¼šthe  number  of  infected  cases,  thecumulative  number  of  infected  cases,  the  number  of  recovered  cases,  and  death  tolls,  forindividual cities and regions in South Korea, Italy and Mazandran, from February 19, 2020,to March 6, 2020. </li><li>æ¨¡å‹ï¼šthe set of parameters of the augmented Susceptible-Exposed-Infected-Removed <strong>(SEIR)</strong> model</li><li>cite: 3</li></ul><p>Evaluation of the incidence of COVID-19 and of the efficacy of contention measures in Spain: a data-driven approach, medRxiv, 2020.</p><ul><li>æ•°æ®ï¼šwe  feed  the  model  with  the  available  data  as  of  February  28th,  2020. We consider that each province (there are 52 in Spain, see appendix B) is represented by a subpopulation.</li><li>æ¨¡å‹ï¼šStochastic  SEIR-meta population  models  </li><li>cite: 1</li></ul><p>Epidemic analysis of COVID-19 in China by dynamical modeling, MedRxivï¼Œ2020</p><ul><li>æ•°æ®ï¼š01/20åˆ°02/09çš„NHCçš„å…¬å¼€æ•°æ®ï¼Œæˆ‘ä»¬é¢„æµ‹äº†5ä¸ªä¸åŒåŒºåŸŸçš„æµè¡Œé«˜å³°å’Œå¯èƒ½çš„ç»“æŸæ—¶é—´ã€‚</li><li>æ¨¡å‹ï¼šgeneralized SEIR model </li><li>cite: 154</li></ul><p>Explicit Modeling of 2019-nCoV Epidemic Trend based on Mobile Phone Data in Mainland Chinaï¼Œ medRxiv, 2020</p><ul><li>æ•°æ®ï¼šdaily outbreak data of 2019-nCoV Pneumonia in 334 prefecture-level cities in mainland China from January 11to February 2,2020. China Unicom mobile phone database is used to obtainthe inter-city  human  mobility.Household Registered Population at 2017 year-end derived from census data wasused to approximate  the number of local residentsineach city during 2020 Spring Festival</li><li>æ¨¡å‹ï¼šthe proposed modification of SIR model</li><li>cite: 5</li></ul><p>AutoSEIR: Accurate Forecasting from Real-time Epidemic Data Using Machine Learning, medRxiv, 2020</p><ul><li>æ•°æ®ï¼šå„ä¸ªåœ°åŒºçš„ç–«æƒ…æ•°æ®ï¼Œä»¥åŠå¤šä¸ªåœ°åŒºçš„å°é”æ—¥æœŸä¿¡æ¯ã€‚</li><li>æ¨¡å‹ï¼š In this paper we propose a novel approach to epidemiological parameter fitting and epidemic forecasting, based on an <strong>extended version of the SEIR compartmental model</strong> and on <strong>an auto-differentiation technique for partially observable ODEs</strong> (Ordinary Differential Equations). </li><li>cite: 0</li></ul><p>An epidemiological forecast model and software assessing interventions on COVID-19 epidemic in China [R software], medRxiv, 2020</p><ul><li>æ•°æ®ï¼šthe COVID-19 data collected from the public website DXY.cn</li><li>æ¨¡å‹ï¼šMarkov SIR infectious disease process</li><li>cite: 18</li></ul><p>Covid-19 spread: Reproduction of data and prediction using a SIR model on Euclidean networkï¼Œ arXiv, 2020</p><ul><li>æ•°æ®ï¼šthe COVID-19 dataï¼Œthe distance from the epicenter.</li><li>æ¨¡å‹ï¼š Susceptible-Infected-Removed (SIR) modelï¼ŒEuclidean network</li><li>cite: 16</li></ul><p>Forecasting the Worldwide Spread of COVID-19 based on Logistic Model and SEIR Modelï¼ŒmedRxiv, 2020</p><ul><li>æ•°æ®ï¼šCoronavirus COVID-19 Global Cases published by the Center for Systems Science and Engineering (CSSE) of Johns Hopkins University</li><li>æ¨¡å‹ï¼šLogistic Model and SEIR Model</li><li>cite: 9</li></ul><p>Analysis and Prediction of COVID-19 Pandemic in Pakistan using Time-dependent SIR Modelï¼ŒarXiv, 2020</p><ul><li>æ•°æ®ï¼šThe data used in this paper is taken from the John Hopkins University dashboard</li><li>æ¨¡å‹ï¼šThe <strong>time-dependent Susceptible-Infected-Recovered (SIR)</strong> model</li><li>cite: 5</li></ul><p>A data driven analysis and forecast of an SEIARD epidemic model for COVID-19 in Mexico, arXiv, 2020</p><ul><li>æ•°æ®ï¼šCOVID-19 data in Mexico(infected, asymptomatic, recovered)</li><li>æ¨¡å‹ï¼š<strong>SEIARD mathematical model</strong>(susceptible (S(t)), exposed (E(t)), infected (I(t)), asymptomatic (A(t)), recovered (R(t)) and dead (D(t)).)</li><li>cite: 5</li></ul><p>Simple model for Covid-19 epidemics - back-casting in China and forecasting in the US, medRxiv, 2020</p><ul><li>æ•°æ®ï¼šmulative cases of COVID-19 in the U.S. from January 22 to April 6, 2020</li><li>æ¨¡å‹ï¼šLogistic Model</li><li>cite: 0</li></ul><p>Application of ARIMA and Holt-Winters forecasting model to predict the spreading of COVID-19 for India and its statesï¼Œ medRxiv</p><ul><li>æ•°æ®ï¼š the publicly available dataset from Kaggle as a perspective to India and its five states such as Odisha, Delhi, Maharashtra, Andhra Pradesh and West Bengal.</li><li>æ¨¡å‹ï¼š In this paper, the <strong>ARIMA</strong> (Auto regressive integrated moving average) and <strong>Holt-Winters time series exponential smoothing</strong> are used to develop an efficient 20- days ahead short-term forecast model to predict the effect of COVID-19 epidemic.</li><li>cite: 0</li></ul><p>Data-driven Simulation and Optimization for Covid-19 Exit Strategiesï¼Œ arXiv, 2020</p><ul><li>æ•°æ®</li><li>æ¨¡å‹ï¼š We have therefore built a pandemic simulation and forecasting toolkit that combines a <strong>deep learning estimation of the epidemiological parameters</strong> of the disease in order to predict the cases and deaths, and a <strong>genetic algorithm</strong> component searching for optimal trade-offs/policies between constraints and objectives set by decision-makers. </li><li>cite: 1</li></ul><p>Kalman Filter Based Short Term Prediction Model for COVID-19 Spreadï¼Œ medRxiv, 2020</p><ul><li>æ•°æ®ï¼šthis article is written after various studies and analysis on the latest data on COVID-19 spread, which also includes the demographic and environmental factors. </li><li>æ¨¡å‹ï¼šAfter gathering data from various resources, all data are integrated and passed into different <strong>Machine Learning Models</strong> to check the fit. <strong>Ensemble Learning Technique,Random Forest</strong>, gives a good evaluation score on the test data.</li><li>cite: 1</li></ul><p>Spatiotemporal Dynamics, Nowcasting and Forecasting of COVID-19 in the United Statesï¼ŒarXivï¼Œ2020</p><ul><li>æ•°æ®ï¼šdata from the reported confirmed COVID-19 infections and deaths at the county levelï¼ŒHealth Department Website in each state or region, the New York Times (NYT, 2020), the COVID-19 Data Repository by the Center for Systems Science and Engineering at Johns Hopkins University (CSSE, 2020), and the COVID Tracking Project (Atlantic, 2020).</li><li>æ¨¡å‹ï¼ša class of nonparametric <strong>space-time models</strong>ï¼Œwe consider the exponential families of distributions</li><li>cite: 4</li></ul><hr><h5 id="Deep-learning-and-other-methods"><a href="#Deep-learning-and-other-methods" class="headerlink" title="Deep learning and other methods"></a>Deep learning and other methods</h5><p>æœ¬ç« å¤§è‡´æŒ‰ç…§ â€œèåˆæ¨¡å‹ï¼ˆæ·±åº¦å­¦ä¹ ä¸ä¼ ç»Ÿä¼ æŸ“ç—…æ¨¡å‹/æ—¶é—´åºåˆ—æ¨¡å‹çš„ç»“åˆ  æˆ–  æ—¶é—´åºåˆ—ä¿¡æ¯ä¸å…¶ä»–ä¿¡æ¯çš„ç»“åˆï¼Œå…¶ä»–ä¿¡æ¯åŒ…æ‹¬ï¼šæ°”å€™ä¿¡æ¯ã€ç¤¾äº¤åª’ä½“ä¿¡æ¯ã€äººä½“å¥åº·æ•°æ®ç­‰ï¼‰â€ -&gt; â€œä»…åˆ©ç”¨æ—¶é—´åºåˆ—ä¿¡æ¯çš„æ·±åº¦å­¦ä¹ æ¨¡å‹â€ æ¥æ’åˆ—ã€‚ </p><h6 id="peer-reviewed-papers-1"><a href="#peer-reviewed-papers-1" class="headerlink" title="peer-reviewed papers"></a>peer-reviewed papers</h6><p>Forecasting Brazilian and American COVID-19 cases based on artificial intelligence coupled with climatic exogenous variablesï¼Œ Chaos, Solitons &amp; Fractalsï¼ŒVolume 139, October 2020, 110027ï¼Œ ELSEVIER</p><ul><li>æ•°æ®ï¼šæ”¶é›†çš„æ•°æ®é›†æ¶‰åŠåˆ°2020å¹´4æœˆ28æ—¥åœ¨å·´è¥¿å’Œç¾å›½äº”ä¸ªå·å‘ç”Ÿçš„COVID-19ç´¯ç§¯æ¡ˆä¾‹ã€‚å¯¹äºå·´è¥¿è¯­å¢ƒï¼Œè¯¥æ•°æ®é›†æ˜¯ä»APIï¼ˆåº”ç”¨ç¨‹åºæ¥å£ï¼‰[37]ä¸­æ”¶é›†çš„ï¼Œè¯¥API æ£€ç´¢æ¥è‡ªå·´è¥¿æ‰€æœ‰27ä¸ªå·å«ç”Ÿå±€çš„æœ‰å…³COVID-19ç—…ä¾‹çš„æ¯æ—¥ä¿¡æ¯ã€‚ç¾å›½è€Œè¨€ï¼Œè¯¥æ•°æ®é›†æ˜¯ä»çº¦ç¿°Â·éœæ™®é‡‘æ–¯å¤§å­¦ç³»ç»Ÿç§‘å­¦ä¸å·¥ç¨‹ä¸­å¿ƒï¼ˆCSSEï¼‰æä¾›çš„Githubä¸Šçš„â€œ COVID-19æ•°æ®å­˜å‚¨åº“â€ä¸­æ”¶é›†çš„[38]ã€‚</li><li>æ¨¡å‹ï¼šæ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ç»“åˆäº†æ°”å€™å¤–æºè¾“å…¥ï¼Œä»¥å¤šæ—¥é¢„æŠ¥ç­–ç•¥è¯„ä¼°äº†AIæ¨¡å‹ã€‚In this paper, <strong>Bayesian regression neural network, cubist regression, k-nearest neighbors, quantile random forest, and support vector regression</strong>, are used stand-alone, and coupled with the recent <strong>pre-processing variational mode decomposition (VMD)</strong> employed to decompose the time series into several intrinsic mode functions.</li><li>cite: 1</li></ul><p>Time Series Analysis and Forecast of the COVID-19 Pandemic in India using Genetic Programmingï¼Œ Chaos, Solitons &amp; Fractals Volume 138, September 2020, 109945ã€‚ELSEVIER</p><ul><li>æ•°æ®ï¼šå°åº¦çš„COVID19æ•°æ®</li><li>æ¨¡å‹ï¼š it has been found that the proposed <strong>GEP(Genetic Evolutionary Programming )-based models</strong> use simple linkage functions and are highly reliable for time series prediction of COVID-19 cases in India.</li><li>cite: 7</li></ul><p>Modified SEIR and AI prediction of the epidemics trend of COVID-19 in China under public health interventions, Journal of Thoracic Dosease</p><ul><li>æ•°æ®ï¼šMigration index based on the daily number of inbound and outbound events by rail, air and road trafficï¼Œmost updated COVID-19 epidemiological dataï¼Œ The 2003 SARS epidemic data between April and June 2003 across the whole of China retrieved from an archived news-site</li><li>æ¨¡å‹ï¼šModified SEIR modelï¼Œartificial intelligence (AI) approach, trained on the 2003 SARS data (LSTM)</li><li>cite: 199</li></ul><p>Predicting COVID-19 in China Using Hybrid AI Modelï¼Œ IEEE Transactions on Cyberneticsï¼ŒIEEE</p><ul><li>æ•°æ®ï¼šmost of the data mainly come from the national and provincial health commissions and include the numbers of people who are infected, suspected, cured, and have died. data for NLP are obtained from dxy.com [36], social media, and news media.</li><li>æ¨¡å‹ï¼š<strong>improved susceptible-infected (ISI) model</strong> ï¼Œthe <strong>natural language processing (NLP) module</strong> and the <strong>long short-term memory (LSTM) network</strong> are embedded into the ISI model to build the hybrid AI model for COVID-19 prediction.</li><li>cite: 7</li></ul><p>Learning from Large-Scale Wearable Device Data for Predicting Epidemics Trend of COVID-19ï¼ŒDiscrete Dynamics in Nature and Societyï¼ŒHindawi</p><ul><li>æ•°æ®ï¼šæ—¥å¸¸æµ‹é‡åŒ…æ‹¬RHRï¼Œæ´»åŠ¨å’Œç¡çœ æ—¶é—´ï¼Œè¿™æ˜¯ç”Ÿç†å¼‚å¸¸æ£€æµ‹çš„åŸºç¡€ã€‚</li><li>æ¨¡å‹ï¼šwe propose a framework, which is based on the <strong>heart rate and sleep data collected from wearable devices</strong>, to predict the epidemic trend of COVID-19 in different countries and cities. In addition to a physiological anomaly detection algorithm defined based on data from wearable devices, <strong>an online neural network</strong> prediction modelling methodology combining both detected physiological anomaly rate and historical COVID-19 infection rate is explored.</li><li>cite: 4</li></ul><p>COVID-19 Pandemic Prediction for Hungary; A Hybrid Machine Learning Approachï¼Œ Mathematics, MDPI</p><ul><li>æ•°æ®ï¼šThe actual dataset includes the daily cases and the number of deaths from 4 March to 28 April. </li><li>æ¨¡å‹ï¼š The hybrid machine learning methods of <strong>adaptive network-based fuzzy inference system (ANFIS)</strong> and <strong>multi-layered perceptron-imperialist competitive algorithm (MLP-ICA)</strong> are proposed to predict time series of infected individuals and mortality rate. </li><li>cite: 4</li></ul><p>Forecasting the prevalence of COVID-19 outbreak in Egypt using nonlinear autoregressive artificial neural networks, Process Safety and Environmental Protection, ELSEVIER</p><ul><li>æ•°æ®ï¼šåŸƒåŠå«ç”Ÿå’Œäººå£éƒ¨æŠ¥å‘Šçš„å…³äºCOVID-19ç¡®è¯Šç—…ä¾‹çš„å®˜æ–¹æ•°æ®è¢«ç”¨äºæœ¬ç ”ç©¶ï¼Œç ”ç©¶æ—¶é—´ä¸º2020å¹´3æœˆ1æ—¥è‡³5æœˆ10æ—¥</li><li>æ¨¡å‹ï¼šARIMAï¼ŒNARANNï¼ˆéçº¿æ€§è‡ªå›å½’äººå·¥ç¥ç»ç½‘ç»œï¼‰</li><li>cite: 2</li></ul><p>Multiple Ensemble Neural Network Models with Fuzzy Response Aggregation for Predicting COVID-19 Time Series: The Case of Mexico, Healthcare, dblp, 2020</p><ul><li>æ•°æ®ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªæ¥è‡ªCOVID-19ç¡®è¯Šç—…ä¾‹å’Œæ­»äº¡ç—…ä¾‹çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬å¢¨è¥¿å“¥çš„12ä¸ªå·å’Œè¯¥å›½çš„æ€»æ•°æ®</li><li>æ¨¡å‹ï¼ša multiple <strong>ensemble neural network</strong> model with fuzzy response aggregation for the COVID-19 time series is presented. Ensemble neural networks are composed of a set of modules, which are used to produce several predictions under different conditions. The modules are simple neural networks. <strong>Fuzzy logic</strong> is then used to aggregate the responses of several predictor modules</li><li>cite: 1</li></ul><p>Predicting the growth and trend of COVID-19 pandemic using <strong>machine learning and cloud computing</strong>, Internet of Things, Elsevier</p><ul><li>æ•°æ®ï¼šHannah Ritchie, httpsï¼š//ourworldindata.org/coronavirus-source-dataã€‚</li><li>æ¨¡å‹ï¼šWe show that using iterative weighting for fitting Generalized Inverse Weibull distribution, a better fit can be obtained to develop a prediction framework. </li><li>cite: 15</li></ul><p>COVID-19 Outbreak Prediction with Machine Learning, SSRN</p><ul><li>æ•°æ®ï¼š Data were collected from <a href="https://www.worldometers.info/coronavirus/country">https://www.worldometers.info/coronavirus/country</a> for five countries, including Italy, Germany, Iran, USA, and China on total cases over 30 days.</li><li>æ¨¡å‹ï¼šAmong a wide range of machine learning models investigated, two models showed promising results (i.e., <strong>multi-layered perceptron, MLP, and adaptive network-based fuzzy inference system, ANFIS</strong>).Paper further suggests that real novelty in outbreak prediction can be realized through integrating machine learning and SEIR models.</li><li>cite: 11</li></ul><p>A Methodological Approach for Predicting COVID-19 Epidemic Using EEMD-ANN Hybrid Modelï¼ŒInternet of Thingsï¼ŒElsevier, 2020</p><ul><li>æ•°æ®ï¼šThe cumulative global data of daily level information by country were retrieved from the Center for Systems Science and Engineering (CSSE) at the Johns Hopkins University GitHub repository (<a href="https://github.com/CSSEGISandData/COVID-19">https://github.com/CSSEGISandData/COVID-19</a>) accessed on 19/05/2020. This study focuses only on the number of global cumulative confirm cases of COVID-19, the number of cumulative death and recover cases from the period January 22, 2020, to May 18, 2020,</li><li>æ¨¡å‹ï¼š The primary objective of this study is to propose <strong>a hybrid model that incorporates ensemble empirical mode decomposition (EEMD) and artificial neural network (ANN) for predicting the COVID-19 epidemic</strong>. A real-time COVID-19 time series data has been used on the window periods January 22, 2020, to May 18, 2020. The time-series data first decomposed using EEMD to produce sub-signals and make original data denoised, and ANN architecture has built to train the denoised data.</li><li>cite: 0</li></ul><p>Partial derivative Nonlinear Global Pandemic Machine Learning prediction of COVID 19ï¼ŒChaos, Solitons &amp; Fractalsï¼ŒVolume 139, October 2020, 110056ï¼ŒELSEVIER</p><ul><li>æ•°æ®ï¼šbenchmark dataset Indian COVID-19 dataset obtained from <a href="https://www.kaggle.com/sudalairajkumar/covid19-in-india">https://www.kaggle.com/sudalairajkumar/covid19-in-india</a>. </li><li>æ¨¡å‹ï¼šwe propose <strong>a partial derivative regression and nonlinear machine learning (PDR-NML)</strong> method for global pandemic prediction of COVID-19. We used a Progressive Partial Derivative Linear Regression model to search for the best parameters in the dataset in a computationally efficient manner. Next, a Nonlinear Global Pandemic Machine Learning model was applied to the normalized features for making accurate predictions. </li><li>cite: 1</li></ul><p>Prediction and analysis of COVID-19 positive cases using deep learning models: A descriptive case study of Indiaï¼ŒChaos, Solitons &amp; Fractals, Volume 139, October 2020, 110017ï¼ŒELSEVIER</p><ul><li>æ•°æ®ï¼š Ministry of Health and Family Welfare (Government of India) [9]. </li><li>æ¨¡å‹ï¼šRecurrent neural network (RNN) based long-short term memory (LSTM) variants such as <strong>Deep LSTM, Convolutional LSTM and Bi-directional LSTM</strong> are applied on Indian dataset to predict the number of positive cases. </li><li>cite: 3</li></ul><p>Composite Monte Carlo decision making under high uncertainty of novel coronavirus epidemic using hybridized deep learning and fuzzy rule induction, Applied Soft Computing, Volume 93, August 2020, 106282, ELSEVIER</p><ul><li>æ•°æ®ï¼šThe data come from mainly two sources: one source is known to be deterministic in nature which is harvested from CDCP in the form of time-series starting from 25 Jan 2020 to 25 Feb 2020.</li><li>æ¨¡å‹ï¼š  <strong>Composite Monte-Carlo (CMC) simulation</strong> is a forecasting method which extrapolates available data which are broken down from multiple correlated/casual micro-data sources into many possible future outcomes by drawing random samples from some probability distributions. In this paper, a case study of using <strong>CMC that is enhanced by deep learning network and fuzzy rule induction</strong> for gaining better stochastic insights about the epidemic development is experimented. Instead of applying simplistic and uniform assumptions for a MC which is a common practice, a deep learning-based CMC is used in conjunction of fuzzy rule induction techniques. </li><li>cite: 25</li></ul><p>Statistical Explorations and Univariate Timeseries Analysis on COVID-19 Datasets to Understand the Trend of Disease Spreading and Deathï¼ŒSensors Volume 20  Issue 11, MDPI</p><ul><li>æ•°æ®ï¼š we used datasets from 1 January 2020, to 22 April 2020. </li><li>æ¨¡å‹ï¼švanilla, stacked, and bidirectional LSTM modelsï¼Œ  multilayer LSTM models</li><li>cite: 0</li></ul><p>Covid-19 Predictions Using a Gauss Model, Based on Data from April 2ï¼Œ  Physics  Volume 2  Issue 2ï¼Œ MDPI</p><ul><li>æ•°æ®ï¼šå¤šä¸ªå›½å®¶çš„æ„ŸæŸ“æ›²çº¿</li><li>æ¨¡å‹ï¼šWe study a <strong>Gauss model (GM)</strong>, a map from time to the bell-shaped Gaussian function to model the deaths per day and country, as a simple, analytically tractable model to make predictions on the coronavirus epidemic. <strong>Justified by the sigmoidal nature of a pandemic</strong>, i.e., initial exponential spread to eventual saturation, and an agent-based modelï¼Œwe apply the GM to existing data, as of 2 April 2020, from 25 countries during first corona pandemic wave and study the modelâ€™s predictions. </li><li>cite: 6</li></ul><p>Artificial Neural Network Modeling of Novel Coronavirus (COVID-19) Incidence Rates across the Continental United States,  IJERPH  Volume 17  Issue 12, MDPI</p><ul><li>æ•°æ®ï¼šIn this study, we compiled a database of 57 candidate variables that may predict county-level cumulative disease incidence as a dependent variable. From January 22 to April 25, 2020, cumulative numbers of confirmed cases of COVID-19 across the continental United States were collected at the county level from USAFacts (usafacts.org) and normalized by populations. The counties (n = 3109) were considered as samples that represent the status of the disease in the US. In this study, socioeconomic (such as household income, income inequalities, and unemployment rate), behavioral (such as smoking), environmental (such as temperature, precipitation, and air pollution), topographic (such as altitude, and terrain slope), and demographic (such as proportions of age groups, race, gender, and access to primary care) factors were prepared at the county level and were used as explanatory variables.</li><li>æ¨¡å‹ï¼šfeature selection + ANN</li><li>cite: 1</li></ul><p>Time series forecasting of COVID-19 transmission in Canada using LSTM networks, Chaos, Solitons &amp; Fractals Volume 135, June 2020, 109864, ELSEVIER</p><ul><li>æ•°æ®ï¼šThe data set also includes number of fatalities and recovered patients by the end of each day. The dataset is available in the time series format with date, month and year so that the temporal components are not neglected. </li><li>æ¨¡å‹ï¼šBi-LSTM</li><li>cite: 22</li></ul><p>Modeling and prediction of COVID-19 in Mexico applying mathematical and computational modelsï¼ŒChaos, Solitons &amp; Fractalsï¼ŒVolume 138, September 2020, 109946ï¼ŒELSEVIER</p><ul><li>æ•°æ®ï¼šThe data used comes from the â€œDaily Technical Reportâ€ [18] issued by the Mexican Ministry of Health.</li><li>æ¨¡å‹ï¼š<strong>Gompertz and Logistic models</strong>ï¼ŒAn <strong>ANN model</strong> with seven neurons in the hidden layer and using a TANSIG function</li><li>cite: 9</li></ul><h6 id="preprint-paper-not-peer-reviewed-1"><a href="#preprint-paper-not-peer-reviewed-1" class="headerlink" title="preprint paper (not peer-reviewed)"></a>preprint paper (not peer-reviewed)</h6><p>A machine learning methodology for real-time forecasting of the 2019-2020 COVID-19 outbreak using Internet searches, news alerts, and estimates from mechanistic modelsï¼ŒarXiv, 2020</p><ul><li>æ•°æ®ï¼š(a) official health reports from Chinese Center Disease for Control and Prevention (China CDC), (b) COVID-19-related internet search activity from Baidu, (c) news media activity reported by Media Cloud, and (d) daily forecasts of COVID-19 activity from GLEAM</li><li>æ¨¡å‹ï¼š*<em><strong>Augmented ARGONetï¼Œ</strong></em></li><li>cite: 14</li></ul><p>Examining COVID-19 Forecasting using Spatio-Temporal Graph Neural Networksï¼Œ arxiv, 2020</p><ul><li>æ•°æ®ï¼šthe NewYork Times (NYT)COVID-19 dataset1, the Google COVID-19 Aggregated Mobility Research Dataset, and the Google Community Mobility Reports2. The Aggregated Mobility Research Dataset helps us understand the quantity of movement, while the Community Mobility Reports helps us understand the dynamics of various types of movement.</li><li>æ¨¡å‹ï¼šthe proposed approach learns from <strong>a single large-scale spatio-temporal graph</strong>, where nodes represent the region-level human mobility, spatial edges represent the human mobility based inter-region connectivity, and temporal edges represent node features through time.</li><li>cite: 1</li></ul><p>Prediction of the COVID-19 Epidemic Trends Based on SEIR and AI Modelsï¼Œ medRxiv, 2020</p><ul><li>æ•°æ®ï¼šThe epidemic dataï¼ŒThe urban migration indexThe population density, per capita GDP and other urban data of all provinces in Chinaï¼Œthe distance from each province to Wuhanï¼ŒThe average temperature of each province</li><li>æ¨¡å‹ï¼š We proposed an <strong>SEIR(Susceptible-Exposed- Infectious-Removed) model</strong> to analyze the epidemic trend in Wuhan and use the <strong>AI model</strong> to analyze the epidemic trend in non-Wuhan areas. </li><li>cite: 0</li></ul><p>Novel Spatiotemporal Feature Extraction Parallel Deep Neural Network for Forecasting Confirmed Cases of Coronavirus Disease 2019ï¼Œ medRxiv, 2020</p><ul><li>æ•°æ®ï¼šExperimental data were collected from Germany, Italy, and Spain regarding six factors: the daily number of newly confirmed cases, deaths, and recovered cases and the accumulated number of confirmed cases, deaths, and recovered cases.</li><li>æ¨¡å‹ï¼š This study proposed a novel <strong>deep neural network framework, COVID-19Net</strong>, which parallelly combines a <strong>convolutional neural network (CNN) and bidirectional gated recurrent units (GRUs)</strong>. Three European countries with severe outbreaks were studied Germany, Italy, and Spain to extract spatiotemporal feature and predict the number of confirmed cases.</li><li>cite: 0</li></ul><p>Hawkes process modeling of COVID-19 with mobility leading indicators and spatial covariates, medRxiv, 2020</p><ul><li>æ•°æ®ï¼šCovid-19 daily cases and deaths reported by The New York Timesï¼Œ Google mobility index reportsï¼Œ County-level demographic covariates</li><li>æ¨¡å‹ï¼š<strong>Hawkes process</strong> with <strong>spatio-temporal</strong> covariates for modeling COVID-19 case and death data.</li><li>cite: 0</li></ul><p>COVID-19 Epidemic Analysis using Machine Learning and Deep Learning Algorithms, medRxiv, 2020</p><ul><li>æ•°æ®ï¼šThe day to day prevalence data of COVID-2019 from January 22, 2020, to April 1, 2020,The dataset consists of daily case reports and daily time series summary tables.</li><li>æ¨¡å‹ï¼špolynomial regression (PR)ï¼ŒSVRï¼ŒDNNï¼ŒLSTM</li><li>cite: 5</li></ul><p>An Improved Method for the Fitting and Prediction of the Number of COVID-19 Confirmed Cases Based on LSTMï¼ŒarXiv 2020 </p><ul><li>æ•°æ®ï¼šNovel Coronavirus (COVID-19) Casesï¼š<a href="https://github.com/CSSEGISandData/COVID-19%EF%BC%8C">https://github.com/CSSEGISandData/COVID-19ï¼Œ</a> COVID-19 Lockdown dates by country: <a href="https://www.kaggle.com/jcyzag/covid19-lockdown-dates-by-country">https://www.kaggle.com/jcyzag/covid19-lockdown-dates-by-country</a>.</li><li>æ¨¡å‹ï¼šLSTMï¼Œç„¶åæ ¹æ®standard deviation of last n daysæ¥è°ƒæ•´é¢„æµ‹çš„ç»“æœï¼Œä»¥ä½“ç°åœ°åŒºæ€§çš„ç–«æƒ…å‘å±•è¶‹åŠ¿çš„å·®å¼‚æ€§</li><li>cite: 0</li></ul><p>Predicting the epidemic curve of the coronavirus (SARS-CoV-2) disease (COVID-19) using artificial intelligenceï¼ŒmedRxiv, 2020</p><ul><li>æ•°æ®ï¼špublicly available datasets of World Health Organization and Johns Hopkins University</li><li>æ¨¡å‹ï¼šthen have used recurrent neural networks <strong>(RNNs)</strong> with gated recurring units <strong>(Long Short-Term Memory â€“ LSTM units)</strong> to create 2 Prediction Models. Information collected in the first t time-steps were aggregated with a fully connected (dense) neural network layer and a consequent regression output layer to determine the next predicted value.</li><li>cite: 1</li></ul><p>Worldwide and Regional Forecasting of Coronavirus (Covid-19) Spread using a Deep Learning Model, medRxiv, 2020</p><ul><li>æ•°æ®ï¼štime series data of daily reports of Chinese Centre for Disease Control and Prevention from 10 January 2020 to 3 April 2020 [4]. The data includes the total number of daily new cases of Covid-19, the total number of deaths and the total number of recoveries in China. (Europe and Middle East, worldwideâ€¦)</li><li>æ¨¡å‹ï¼šThe proposed deep learning architecture consists of <strong>Long Short Term Memory (LSTM) layer, dropout layer, and fully connected layers</strong> to predict regional and worldwide forecasts. </li><li>cite: 0</li></ul><p>COVID-19 Pandemic Prediction for Hungary; a Hybrid Machine Learning Approachï¼Œ medRxiv, 2020</p><ul><li>æ•°æ®ï¼šDataset is related to the statistical reports of COVID-19 cases and mortality rate of Hungary which is available at: <a href="https://www.worldometers.info/coronavirus/country/hungary/">https://www.worldometers.info/coronavirus/country/hungary/</a>. Figure</li><li>æ¨¡å‹ï¼šthis study proposes a hybrid machine learning approach to predict the COVID-19 and we exemplify its potential using data from Hungary. The hybrid machine learning methods of <strong>adaptive network-based fuzzy inference system (ANFIS)</strong> and <strong>multi-layered perceptron-imperialist competitive algorithm (MLP-ICA)</strong> are used to predict time series of infected individuals and mortality rate.</li><li>cite: 4</li></ul><p>Forecasting Covid-19 dynamics in Brazil: a data driven approachï¼Œ arXiv, 2020</p><ul><li>æ•°æ®ï¼šcovid-19 time series</li><li>æ¨¡å‹ï¼šThen, our main approach consists in <strong>an initial clustering of the world regions</strong> for which data is available and where the pandemic is at an advanced stage, based on a set of <strong>manually engineered features</strong> representing a countryâ€™s response to the early spread of the pandemic. A <strong>Modified Auto-Encoder network</strong> is then trained from these clusters and learns to predict future data for Brazilian states. These predictions are used to estimate important statistics about the disease, such as peaks.</li><li>cite: 0</li></ul><p>Artificial Intelligence Forecasting of Covid-19 in China, arXiv, 2020</p><ul><li>æ•°æ®ï¼šthe confirmed cases of Covid-19 across China which were collected from January 11 to February 27, 2020 by WHO.</li><li>æ¨¡å‹ï¼š<strong>a modified stacked auto-encoder</strong> for modeling the transmission dynamics of the epidemics. We used the <strong>latent variables in the auto-encoder and clustering algorithms</strong> to group the provinces/cities for investigating the transmission structure.</li><li>cite: 62</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
            <tag> survey </tag>
            
            <tag> covid19 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in ICLR 2020 (2020.04.26)</title>
      <link href="/uncategorized/paperlistfile/ICLR2020/"/>
      <url>/uncategorized/paperlistfile/ICLR2020/</url>
      
        <content type="html"><![CDATA[<p><a href="https://openreview.net/group?id=ICLR.cc/2020/Conference">Link</a></p><span id="more"></span><h2 id="Anomaly"><a href="#Anomaly" class="headerlink" title="Anomaly"></a>Anomaly</h2><ul><li><p><strong>å·²è¯»</strong> Deep Semi-Supervised Anomaly Detection </p><p>Lukas Ruff, Robert A. Vandermeulen, Nico GÃ¶rnitz, Alexander Binder, Emmanuel MÃ¼ller, Klaus-Robert MÃ¼ller, Marius Kloft</p></li><li><p><strong>å·²è¯»</strong>Iterative energy-based projection on a normal data manifold for anomaly localization </p><p>David Dehaene, Oriel Frigo, SÃ©bastien Combrexelle, Pierre Eline</p><p><strong>ä»»åŠ¡</strong>ï¼šæ–‡ç« ä¸­è¯´æ˜¯å¼‚å¸¸å®šä½ï¼Œå…¶å®ä¹Ÿå¯ä»¥çœ‹åšå¼‚å¸¸å½’å› ï¼Œä¸”é’ˆå¯¹ä»¥VAE/AEä¸ºåŸºæœ¬ç»“æ„çš„å¼‚å¸¸æ£€æµ‹å™¨å‡æœ‰æ•ˆï¼ˆä¸ªäººç†è§£å¯¹ä½¿ç”¨ONE CLASSæ–¹æ³•è®­ç»ƒã€å€ŸåŠ©é‡å»ºè¯¯å·®è®¡ç®—å¼‚å¸¸å¾—åˆ†çš„æ–¹æ³•å‡æœ‰æ•ˆï¼‰ã€‚</p><p><strong>åŸºæœ¬æµç¨‹</strong>ï¼š</p><ol><li><p>æœ¬æ–‡å‡è®¾ä½¿ç”¨ä¸€ä¸ªVAEç»“æ„åšONE CLASSè®­ç»ƒï¼Œå³å»ºæ¨¡æ­£å¸¸æ•°æ®çš„é‡å»ºä¸åˆ†å¸ƒã€‚Losså‡½æ•°ä¸º: $L = L_{reconstruction} + L_{KL(p,q)}$</p></li><li><p>ç”±äºä¸Šè¿°çš„losså‡½æ•°æœ€å°åŒ–äº†ç½‘ç»œå¯¹äºæ­£å¸¸æ ·æœ¬çš„ç›¸åº”ã€‚å› æ­¤ï¼Œé’ˆå¯¹ä»»æ„ä¸€ä¸ªå¼‚å¸¸æ ·æœ¬$x_a$ï¼Œå°†ç½‘ç»œå¯¹å…¶çš„ç›¸åº”$L(x)$åšæœ€å°åŒ–ï¼Œå³å¯å°†$x_a$è½¬æ¢å®ƒçš„æ­£å¸¸ç‰ˆæœ¬$x_n$ã€‚åœ¨è¿™ç§å‡è®¾ä¸‹ï¼Œç½‘ç»œçš„losså‡½æ•°å³ä¸ºæ ·æœ¬çš„å¼‚å¸¸å¾—åˆ†ã€‚</p></li><li><p>å®šä¹‰å¼‚å¸¸å®šä½çš„æœ€ä¼˜åŒ–ç›®æ ‡$E = L$ï¼Œç”±äºæœ‰å·¥ä½œè¡¨æ˜$L_{KL}$å¯¹è®¡ç®—å¼‚å¸¸å¾—åˆ†æœ‰è´Ÿé¢ä½œç”¨ï¼Œå› æ­¤å°†losså‡½æ•°ä¸­çš„æ­¤é¡¹å»æ‰ã€‚å³$E = L_{reconstruction}$ã€‚</p></li><li><p>ç”±äºå®šä½çš„ç›®æ ‡æ˜¯ä»…ä½¿å¾—æ ·æœ¬ä¸­å¼‚å¸¸çš„éƒ¨åˆ†è¢«æŒ‡å‡ºã€‚è€Œéæ”¹å˜æ•´ä¸ªæ ·æœ¬çš„å½¢æ€åˆ°ç±»ä¼¼è®­ç»ƒæ ·æœ¬çš„æ ·å­ï¼Œæ‰€ä»¥è¦åŠ ä¸Šæ­£åˆ™é¡¹ï¼Œæ§åˆ¶æ­£å¸¸ç‰ˆæœ¬çš„æ ·æœ¬ä¸åŸå¼‚å¸¸æ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå³ $L_{regularization} = \left| x_n - x_a \right| <em>0 $ï¼Œç„¶è€Œç”±äºL0ä¸å¯å¯¼ï¼Œå› æ­¤ä½¿ç”¨L1ä½œä¸ºè¿‘ä¼¼ï¼Œå³$L</em>{regularization} = \left| x_n - x_a \right|<em>1 $ã€‚å³ç”¨äºå®šä½å¼‚å¸¸çš„ä¼˜åŒ–ç›®æ ‡energyä¸º$E = L</em>{reconstruction} + \left| x_n - x_a \right|<em>1 $ï¼Œä¸Šå¼ä¸­çš„$x_n$ç­‰åŒäºä¸‹é¢è¡¨è¿°çš„$x</em>{old}$ã€‚</p></li><li><p>å¦‚ä½•è¿­ä»£å¼çš„å°†$x_a$è½¬ä¸º$x_n$ï¼Ÿå®šä¹‰æ¢¯åº¦ä¸‹é™æ“ä½œ$x_{new} = x_{old} - \alpha * \nabla _x (E)$</p></li><li><p>ç”±äºä»…éœ€è¦å¯¹å¼‚å¸¸çš„åƒç´ è¿›è¡Œæ›´æ–°ï¼Œæ‰€ä»¥åœ¨ä¸Šè¿°å…¬å¼ä¸ŠåŠ ä¸€å±‚Maskï¼Œå³$x_{new} = x_{old} - \alpha * (\nabla <em>x (E) \odot Mask)$ï¼Œä½†æ˜¯åœ¨æ— ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œ$Mask$æ˜¯æœªçŸ¥çš„ï¼Œå› æ­¤ä½¿ç”¨åƒç´ çº§çš„é‡å»ºè¯¯å·®æ¥ä»£æ›¿ã€‚åˆ™æ ·æœ¬æ›´æ–°å…¬å¼ä¸º$x</em>{new} = x_{old} - \alpha * (\nabla <em>x (E) \odot (x</em>{old} - f_{VAE}(x_{old}))^2)$</p></li></ol><blockquote><p>Optimizing the energy this way, a pixel where the reconstruction error is high will update faster, whereas a pixel with good reconstruction<br>will not change easily. This prevents the image to update its pixels where the reconstruction is already good, even with a high learning rate.</p></blockquote></li><li><p><strong>å·²è¯»</strong> Robust anomaly detection and backdoor attack detection via differential privacy </p><p>Min Du, Ruoxi Jia, Dawn Song</p></li><li><p>Classification-Based Anomaly Detection for General Data <em>General Data: å¤šç§æ•°æ®ç±»å‹ï¼Œå›¾åƒã€xxxç­‰ç­‰</em></p><p>Liron Bergman, Yedid Hoshen </p></li><li><p><strong>å·²è¯»</strong> Robust Subspace Recovery Layer for Unsupervised Anomaly Detection</p><p>Chieh-Hsin Lai, Dongmian Zou, Gilad Lerman</p></li><li><p>Attention Guided Anomaly Detection and Localization in ImagesArXiv2019</p><p>Shashanka VenkataramananKuan-Chuan PengRajat Vikram SinghAbhijit Mahalanobis</p></li></ul><h2 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h2><ul><li><p>Are Transformers universal approximators of sequence-to-sequence functions? </p><p>Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, Sanjiv Kumar</p></li><li><p>DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling </p></li><li><p>Revisiting Self-Training for Neural Sequence Generation </p><p>Junxian He, Jiatao Gu, Jiajun Shen, Marcâ€™Aurelio Ranzato</p></li><li><p>Adaptive Correlated Monte Carlo for Contextual Categorical Sequence Generation </p><p>Xinjie Fan, Yizhe Zhang, Zhendong Wang, Mingyuan Zhou</p></li><li><p>Compressive Transformers for Long-Range Sequence Modelling </p><p>Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, Timothy P. Lillicrap</p></li><li><p>Model-based reinforcement learning for biological sequence design </p><p>Christof Angermueller, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy, Lucy Colwell</p></li><li><p>Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models </p><p>Xisen Jin, Zhongyu Wei, Junyi Du, Xiangyang Xue, Xiang Ren</p></li></ul><h2 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h2><ul><li><p><strong>å·²è¯»</strong> N-BEATS: Neural basis expansion analysis for interpretable time series forecasting <em>å¯è§£é‡Šæ€§ä¸Šçš„å·¥ä½œ</em></p><p>Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio</p></li><li><p>Dynamic Time Lag Regression: Predicting What &amp; When </p><p>Mandar Chandorkar, Cyril Furtlehner, Bala Poduval, Enrico Camporeale, Michele Sebag</p></li><li><p>Intensity-Free Learning of Temporal Point Processes </p><p>Oleksandr Shchur, Marin BiloÅ¡, Stephan GÃ¼nnemann</p></li></ul><h2 id="Recurrent"><a href="#Recurrent" class="headerlink" title="Recurrent"></a>Recurrent</h2><ul><li><p>Variational Recurrent Models for Solving Partially Observable Control Tasks </p><p>Dongqi Han, Kenji Doya, Jun Tani</p></li><li><p>One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation </p><p>Shunshi Zhang, Bradly C. Stadie</p></li><li><p>Recurrent neural circuits for contour detection </p><p>Drew Linsley*, Junkyung Kim*, Alekh Ashok, Thomas Serre</p></li><li><p>Improved memory in recurrent neural networks with sequential non-normal dynamics <em>æ˜¯å¦å¯ä»¥ä½¿ç”¨non-normalæ¥æè¿°æˆ‘ä»¬çš„å·¥ä½œä¸­çš„ç³»ç»Ÿ</em></p><p>Emin Orhan, Xaq Pitkow</p></li><li><p>Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality </p><p>Saurabh Khanna, Vincent Y. F. Tan</p></li><li><p>Decoding As Dynamic Programming For Recurrent Autoregressive Models </p><p>Najam Zaidi, Trevor Cohn, Gholamreza Haffari</p></li><li><p>Training Recurrent Neural Networks Online by Learning Explicit State Variables </p><p>Somjit Nath, Vincent Liu, Alan Chan, Xin Li, Adam White, Martha White</p></li><li><p>Understanding Generalization in Recurrent Neural Networks </p><p>Zhuozhuo Tu, Fengxiang He, Dacheng Tao</p></li><li><p>RNNs Incrementally Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients? </p><p>Anil Kag, Ziming Zhang, Venkatesh Saligrama </p></li><li><p>Implementing Inductive bias for different navigation tasks through diverse RNN attrractors </p><p>Tie XU, Omri Barak</p></li><li><p>Symplectic Recurrent Neural Networks </p><p>Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, LÃ©on Bottou</p></li></ul><h2 id="Interpretable"><a href="#Interpretable" class="headerlink" title="Interpretable"></a>Interpretable</h2><ul><li><p>Overlearning Reveals Sensitive Attributes </p><p>Congzheng Song, Vitaly Shmatikov</p></li><li><p>Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution </p><p>Nikaash Puri, Sukriti Verma, Piyush Gupta, Dhruv Kayastha, Shripad Deshmukh, Balaji Krishnamurthy, Sameer Singh</p></li></ul><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><ul><li><p><strong>å·²è¯»</strong> From Variational to Deterministic Autoencoders</p><p>Partha Ghosh, Mehdi S. M. Sajjadi, Antonio Vergari, Michael Black, Bernhard Scholkopf</p></li><li><p>MIXED-CURVATURE VARIATIONAL AUTOENCODERS</p></li><li><p>Mogrifier LSTM </p><p>GÃ¡bor Melis, TomÃ¡Å¡ KoÄiskÃ½, Phil Blunsom</p></li></ul><h2 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h2><ul><li><p>ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring </p><p>David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, Colin Raffel</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Time series similarity learning</title>
      <link href="/uncategorized/surveys/timesimilar/"/>
      <url>/uncategorized/surveys/timesimilar/</url>
      
        <content type="html"><![CDATA[<h2 id="è°ƒç ”"><a href="#è°ƒç ”" class="headerlink" title="è°ƒç ”"></a>è°ƒç ”</h2><ul><li>NIPS19 - Learning Representations for Time Series Clustering</li></ul><p><strong>æµç¨‹</strong></p><ol><li>æœ¬è´¨ä¸Šæ˜¯åœ¨éšç©ºé—´å†…å­¦ä¹ ä¸€ä¸ªåˆ©äºåˆ†ç°‡çš„è¡¨å¾ï¼Œç„¶ååˆ©ç”¨è¯¥è¡¨å¾ç»“åˆä¼ ç»Ÿçš„èšç±»æ–¹æ³•ï¼Œåšèšç±»ä»»åŠ¡<ol><li>ä½¿ç”¨auto-encoderçš„ç»“æ„ï¼Œæ±‚å¾—æ—¶é—´åºåˆ—åœ¨éšç©ºé—´ä¸­çš„è¡¨è¾¾</li><li>ä¸ºäº†ä½¿å¾—encoderå‡ºçš„ç»“æœæ›´é€‚åˆäºæ‰§è¡Œèšç±»ä»»åŠ¡ï¼Œä½¿ç”¨k-meanså»ºæ¨¡äº†éšç©ºé—´ä¸­çš„æ•°æ®ï¼ˆæœ¬è´¨ä¸Šæ˜¯æ— ç›‘ç£çš„ä¼˜åŒ–ç±»é—´è·ç¦»ä¸ç±»å†…è·ç¦»ï¼‰ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­ä½¿ç”¨äº†å¯å¾®åˆ†çš„k-means loss</li><li>ä¸ºäº†ä½¿å¾—encoderæ›´å…·æœ‰è¡¨å¾èƒ½åŠ›ï¼Œæ·»åŠ äº†ä¸€é¡¹ä»»åŠ¡ï¼Œå³åˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨ä¸€ä¸ªå­ç½‘ç»œæ¥åˆ†ç±»encoderç¼–ç çš„çœŸå®æ•°æ®ä¸å‡æ•°æ®ã€‚</li></ol></li><li>ä½¿ç”¨UCRèšç±»ä»»åŠ¡è¯„ä¼°æ•ˆæœ</li></ol><p><strong>æˆ‘è®¤ä¸ºçš„ç¼ºç‚¹</strong></p><ol><li>K-meansçš„éšç©ºé—´å»ºæ¨¡èƒ½åŠ›è¿œå¼±ä¸GMM</li></ol><span id="more"></span><hr><ul><li>KDD19 - NeuralWarp: Time-Series Similarity with Warping Networks</li></ul><p><strong>æµç¨‹</strong></p><ol><li>æœ¬è´¨ä¸Šæ˜¯å°†DTWçš„æ€æƒ³å¼•å…¥åˆ°äº†éšç©ºé—´ï¼Œå¹¶å°†DTWä¸­çš„dynamic programmingæœç´¢æœ€ä¼˜å¯¹é½çš„è¿‡ç¨‹ï¼Œæ›¿æ¢ä¸ºäº†ä¸€ä¸ªå¯å­¦ä¹ çš„çŸ©é˜µï¼Œä»è€Œå°†DTWçš„æ€æƒ³èå…¥æ·±åº¦å­¦ä¹ ä¸­ã€‚<ol><li>å°†æ˜¾ç©ºé—´çš„æ—¶é—´åºåˆ—X(T)ç”¨è¿‡auto-encoderæ˜ å°„åˆ°éšç©ºé—´ä¸­çš„Z(N*T)ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªç½‘ç»œç”¨äºå­¦ä¹ ç»™å®šçš„ä¸€ä¸ªæ—¶é—´åºåˆ—å¯¹ä¹‹é—´çš„å¯¹é½æ–¹å¼</li><li>å­¦ä¹ çš„ç›®æ ‡å‡½æ•°æ˜¯ï¼šæœ€å°åŒ–åŒç±»æ—¶é—´åºåˆ—å¯¹çš„latent dtwè·ç¦»ï¼ŒåŒæ—¶æœ€å¤§åŒ–ä¸åŒç±»æ—¶é—´åºåˆ—å¯¹çš„latent dtwè·ç¦»ã€‚</li><li>é€šè¿‡å­¦ä¹ å¯å¾—åˆ°ä¸€ä¸ªç”¨äºåœ¨éšç©ºé—´åšå¯¹é½çš„ç¥ç»ç½‘ç»œï¼Œä¸”ç½‘ç»œçš„è¾“å‡ºä¸ºç»™å®šçš„æ—¶é—´åºåˆ—å¯¹çš„è·ç¦»</li></ol></li><li>é€šè¿‡åˆ†ç±»ä»»åŠ¡è¯„ä¼°æ€§èƒ½</li></ol><p><strong>æˆ‘è®¤ä¸ºçš„ç¼ºç‚¹</strong></p><ol><li>æœ‰ç›‘ç£å­¦ä¹ </li></ol><hr><ul><li>ICLR19 - SOM-VAE: INTERPRETABLE DISCRETE REPRESENTATION LEARNING ON TIME SERIES [code]</li></ul><p><strong>æµç¨‹</strong></p><ol><li>æœ¬è´¨ä¸Šæ˜¯åœ¨éšç©ºé—´å»ºç«‹äº†ä¸€ä¸ªSOMï¼ˆself-organized mapï¼‰å¹¶ä¸ºSOMçš„èŠ‚ç‚¹å»ºç«‹äº†markov modelï¼Œä»¥æä¾›å¯è§£é‡Šçš„è¡¨å¾ï¼ˆä»æ¦‚ç‡ä¸æ‹“æ‰‘çš„è§’åº¦ï¼Œå…¶ä¸­SOMæä¾›äº†éšç©ºé—´è¡¨è¾¾çš„æ‹“æ‰‘ç»“æ„ï¼Œmarkov modelæä¾›äº†æ¦‚ç‡æ¨¡å‹ï¼‰<ol><li>é¦–å…ˆå°†æ—¶é—´åºåˆ—ä½¿ç”¨auto-encoderé‡å»º</li><li>ç„¶ååœ¨éšç©ºé—´ä½¿ç”¨SOMè¿›è¡Œæ‹Ÿåˆï¼Œå°†éšç©ºé—´çš„æ•°æ®ç‚¹å»ºæ¨¡ä¸ºå…·æœ‰Kä¸ªèŠ‚ç‚¹çš„SOM</li><li>å°†éšç©ºé—´ä¸­çš„æ¯ä¸ªæ•°æ®ç‚¹å½’ç±»åˆ°ä¸å…¶æœ€è¿‘çš„SOMèŠ‚ç‚¹ä¸Šï¼Œå†ä¾æ‰˜æ•°æ®å»ºç«‹Markov model</li><li>ç½‘ç»œé€šè¿‡é‡å»ºè¯¯å·®/éšç©ºé—´è¡¨è¾¾ä¸SOMèŠ‚ç‚¹ä¹‹é—´çš„è·ç¦»/å¯è§‚å¯Ÿæ•°æ®çš„è½¬ç§»æ¦‚ç‡ å…±åŒä¼˜åŒ–</li></ol></li><li>ä½¿ç”¨MNISTå’ŒFASION-MNISTåšèšç±»ä»»åŠ¡</li></ol><p><strong>æˆ‘è®¤ä¸ºçš„ç¼ºç‚¹</strong></p><ol><li>æ²¡æƒ³å¥½</li></ol><hr><ul><li>ICML19 workshop - Warping Resilient Time Series Embeddings</li></ul><p><strong>æµç¨‹</strong></p><ol><li>æœ¬è´¨ä¸Šæ˜¯é€šè¿‡ç¥ç»ç½‘ç»œæ¥å­¦ä¹ ä¸€ç§å¯¹æ˜¾ç©ºé—´æ—¶é—´åºåˆ—çš„ç¿˜æ›²ä¸æ•æ„Ÿçš„éšç©ºé—´è¡¨è¾¾<ol><li>é¦–å…ˆå®šä¹‰äº†ä¸¤ç§æ—¶é—´åºåˆ—çš„ç¿˜æ›²æ–¹æ³•ï¼Œç”¨äºä¸ºåŸå§‹çš„æ—¶é—´åºåˆ—ç”Ÿæˆä¸€äº›ç¿˜æ›²åçš„æ ·æœ¬</li><li>ä½¿ç”¨ä¸¤ä¸ªauto-encoderåˆ†åˆ«å­¦ä¹ å·¦ç¿˜æ›²åçš„æ—¶é—´åºåˆ—çš„éšç©ºé—´è¡¨è¾¾ï¼Œå’Œå³ç¿˜æ›²åçš„æ—¶é—´åºåˆ—éšç©ºé—´è¡¨è¾¾ã€‚</li><li>ä½¿ç”¨ä¸€é¡¹losså‡½æ•°ï¼Œä½¿å¾—åœ¨æ˜¾ç©ºé—´åšäº†ç¿˜æ›²çš„æ ·æœ¬åœ¨éšç©ºé—´çš„è¡¨è¾¾å°½å¯èƒ½ç›¸è¿‘</li><li>ç½‘ç»œçš„è¾“å‡ºå°±æ˜¯ä¸€ä¸ªå¯¹ç¿˜æ›²ä¸æ•æ„Ÿçš„embedding</li></ol></li><li>ä½¿ç”¨ä¸€éƒ¨åˆ†UCRåšåˆ†ç±»ä»»åŠ¡</li></ol><p><strong>æˆ‘è®¤ä¸ºçš„ç¼ºç‚¹</strong></p><ol><li>ä»…å¯¹æ—¶é—´åºåˆ—çš„ç¿˜æ›²ä¸æ•æ„Ÿï¼Œä½†å¯¹æ›´å¤æ‚çš„æ—¶é—´åºåˆ—çš„è¡¨å¾èƒ½åŠ›å¯èƒ½ä¼šå¾ˆå¼±</li></ol><hr><ul><li>KDD19 workshop - A Formally Robust Time Series Distance Metric [code]</li></ul><p><strong>æµç¨‹</strong></p><ol><li>æœ¬è´¨ä¸Šæ˜¯æƒ³å¯»æ±‚ä¸€ç§å¯¹æ•°æ®æ±¡æŸ“æ›´åŠ é²æ£’çš„æ˜¾ç©ºé—´è·ç¦»åº¦é‡æ–¹æ³•ï¼Œèšç„¦äºä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œ<br>æ‰€è°“å°æ±¡æŸ“æŒ‡ï¼šåœ¨class içš„æ ·æœ¬xä¸Šå¢åŠ ä¸€ä¸ªå°çš„æ±¡æŸ“åºåˆ—kï¼Œä¸€ç§é²æ£’çš„è·ç¦»åº¦é‡æ–¹æ³•èƒ½å¤Ÿåšåˆ°ï¼šd(x_i, x_i+k) &lt; d(x_i, x_j)ï¼Œ<br>å…¶ä¸­x_jæ˜¯class jçš„æ ·æœ¬ </li><li>é€šè¿‡å°†6ä¸­é²æ£’æ€§ä¸ç²¾ç¡®æ€§å…·æœ‰ä¸åŒæ€§è´¨çš„è·ç¦»åº¦é‡æ–¹å¼ç»Ÿä¸€åˆ°0-1çš„è·ç¦»åº¦é‡èŒƒå›´åï¼Œå°†6ä¸­è·ç¦»æ–¹æ³•ç»“åˆèµ·æ¥ï¼Œç”Ÿæˆä¸€ç§æ–°çš„è·ç¦»åº¦é‡æ–¹å¼</li><li>ä½¿ç”¨UCRåšåˆ†ç±»ä»»åŠ¡</li></ol><p>å…¶å®æˆ‘ä»¬çš„æ–¹æ³•ä¸€å®šç¨‹åº¦ä¸Šä¹Ÿåœ¨è§£å†³è¿™ç§å¯¹æ±¡æŸ“åºåˆ—çš„é²æ£’æ€§ï¼Œè€Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯auto-encoderæ¥è¿‡æ»¤æ±¡æŸ“åºåˆ—å¯¹åŸå§‹åºåˆ—åœ¨éšç©ºé—´ä¸Šçš„å½±å“çš„ã€‚</p><hr><ul><li>IJCAI19 - Similarity Preserving Representation Learning for Time Series Clustering [code]</li></ul><p><strong>æµç¨‹</strong></p><ol><li>æœ¬è´¨ä¸Šæ˜¯æå‡ºä¸€ç§èƒ½å¤Ÿä¿ç•™æ—¶é—´åºåˆ—åœ¨æ˜¾ç©ºé—´ç›¸ä¼¼åº¦çš„embeddingæ–¹æ³•</li><li>ç»™å®šä¸€ä¸ªæ•°æ®é›†Dï¼ŒåŒ…å«Nä¸ªæ ·æœ¬ï¼Œé¦–å…ˆåœ¨æ˜¾ç©ºé—´ä¸­åˆ©ç”¨DTWï¼ˆæˆ–å…¶ä»–ç®—æ³•ï¼‰å»ºç«‹å…¶ä»–ä»¬çš„è·ç¦»çŸ©é˜µï¼ˆN*Nï¼‰</li><li>ç„¶è€Œå½“Nå¾ˆå¤§çš„æ—¶å€™ï¼Œå»ºç«‹è¿™æ ·ä¸€ä¸ªè·ç¦»çŸ©é˜µæ˜¯è´¹æ—¶çš„ï¼Œæ‰€ä»¥ä»–æå‡ºï¼Œéƒ¨åˆ†è§‚å¯Ÿçš„è·ç¦»çŸ©é˜µè¶³ä»¥ä¼°è®¡å‡ºå®Œæ•´çš„è·ç¦»çŸ©é˜µï¼Œå› æ­¤åœ¨è®¡ç®—æ ·æœ¬xiçš„æ—¶å€™ï¼Œå¹¶ä¸å°†å…¶ä¸å…¶ä»–N-1ä¸ªæ ·æœ¬è¿›è¡Œè·ç¦»è®¡ç®—ï¼Œè€Œæ˜¯å°†å…¶ä¸éšæœºé‡‡æ ·çš„Kä¸ªæ ·æœ¬è¿›è¡Œè·ç¦»è®¡ç®—</li><li>å¾—åˆ°äº†è·ç¦»åº¦é‡çŸ©é˜µçš„æ—¶å€™ï¼Œé€šè¿‡è§£ä¸€ä¸ªå…·æœ‰è¿‘ä¼¼æœ€ä¼˜è§£çš„ä¼˜åŒ–ç›®æ ‡ï¼Œä»è€Œè·å¾—æ ·æœ¬çš„ä½ç»´è¡¨ç¤º</li></ol><p><strong>æˆ‘è®¤ä¸ºçš„ç¼ºç‚¹</strong>ï¼š</p><ol><li>å½“æ—¶é—´åºåˆ—åœ¨æ˜¾ç©ºé—´çš„ç›¸ä¼¼åº¦èƒ½å¤Ÿæ­ç¤ºæ•°æ®çš„ç°‡æ—¶ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆï¼Œä½†å½“æ˜¾ç©ºé—´çš„è·ç¦»åº¦é‡æ— æ³•åŒºåˆ†æ•°æ®å†…åœ¨çš„æ€§è´¨çš„æ—¶å€™ï¼Œè¯¥æ–¹æ³•å°†ä¼šå¤±æ•ˆ</li></ol><hr><ul><li>NIPS18 - Autowarp: Learning a Warping Distance from Unlabeled Time Series Using Sequence Autoencoders</li></ul><p><strong>æµç¨‹</strong></p><ol><li>å‡è®¾æ•°æ®åœ¨auto-encoderæ˜ å°„ä¸‹çš„éšç©ºé—´ä¸­ï¼Œèƒ½å¤Ÿembeddingå‡ºæ—¶é—´åºåˆ—çš„å†…åœ¨ç‰¹æ€§ï¼Œå¹¶è¿›è¡Œéƒ¨åˆ†çš„åˆ†ç°‡ã€‚</li><li>å°†æ˜¾ç©ºé—´ä¸­çš„å››ä¸ªå¸¸ç”¨çš„è·ç¦»åº¦é‡å‚æ•°åŒ–ä¸ºä¸€ä¸ªè·ç¦»åº¦é‡æ—ï¼Œå…¶å‚æ•°ä¸ºalpha/beta/gamma<ol><li>é¦–å…ˆå°†æ•°æ®é€šè¿‡auto-encoderå‹ç¼©åˆ°ä¸€ä¸ªéšç©ºé—´</li><li>ç„¶åé€šè¿‡ä¼˜åŒ–alpha/beta/gammaä»¥åœ¨è·ç¦»åº¦é‡æ—é‡Œå¯»æ‰¾ä¸€ä¸ªæ°å½“çš„è·ç¦»åº¦é‡ï¼Œèƒ½å¤Ÿä½¿å¾—æ•°æ®åœ¨æ˜¾ç©ºé—´ä¸­çš„ç›¸ä¼¼æ€§å…³ç³»ä¸åœ¨éšç©ºé—´ä¸­çš„è¿‘ä¼¼ã€‚ï¼ˆè¿™ä¸ªè¿‡ç¨‹é€šè¿‡latent betaCVâ€”â€”ä¸€ç§æ— éœ€GTçš„èšç±»æ€§èƒ½è¡¡é‡æ–¹æ³•ï¼Œæ¥è¯„ä¼°æ˜¾ç©ºé—´ä¸­çš„ç›¸ä¼¼æ€§ä¸éšç©ºé—´ä¸­çš„ç›¸ä¼¼æ€§å…³ç³»æ˜¯å¦è¿‘ä¼¼ï¼‰</li></ol></li><li>æœ¬å·¥ä½œçš„ç›®æ ‡è¿˜æ˜¯æ±‚å¾—ä¸€ä¸ªæ˜¾ç©ºé—´çš„æ°å½“çš„è·ç¦»åº¦é‡æ–¹å¼ï¼Œå¹¶ä»¥éšç©ºé—´çš„æ¬§æ°è·ç¦»åº¦é‡ç”Ÿæˆçš„ç°‡ä¿¡æ¯ä½œä¸ºå‚è€ƒï¼Œä»¥é¿å…äº†æ²¡æœ‰GTçš„å›°å¢ƒ</li><li>é€šè¿‡èšç±»ä»»åŠ¡è¯„ä¼°æ€§èƒ½</li></ol><p><strong>æˆ‘è®¤ä¸ºçš„ç¼ºç‚¹</strong></p><ol><li>è¿‡äºä¾èµ–éšç©ºé—´æ‰€æä¾›çš„åˆ†ç°‡ä¿¡æ¯ï¼Œå½“éšç©ºé—´è¡¨è¾¾ä¸è¶³ä»¥æ”¯æŒåˆ†ç°‡çš„æ—¶å€™ï¼Œè¯¥æ–¹æ³•è¿‘ä¹å¤±æ•ˆã€‚</li><li>ä¸¤æ®µå¼çš„ç®—æ³•ï¼Œå¯»æ‰¾è·ç¦»åº¦é‡çš„ä¼˜åŒ–è¿‡ç¨‹ä¸auto-encoderçš„ä¼˜åŒ–è¿‡ç¨‹åˆ†ç¦»ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³</li></ol><hr><h2 id="å…¶ä»–"><a href="#å…¶ä»–" class="headerlink" title="å…¶ä»–"></a>å…¶ä»–</h2><ul><li><p>KDD 17 workshop - DECADE: A deep metric learning model for multivariate time series</p><p>  ä¸»è¦è§£å†³å¤šç»´æ—¶é—´åºåˆ—è®¡ç®—DTWè·ç¦»çš„é—®é¢˜ï¼Œä¸»è¦æ–¹æ³•ï¼šå…ˆå°†é«˜ç»´æ•°æ®æ˜ å°„åˆ°éšç©ºé—´ï¼Œç„¶ååœ¨éšç©ºé—´è®¡ç®—æ¯ä¸€ç»´æ•°æ®å¯¹åº”æ‰€æœ‰å…¶ä»–ç»´åº¦çš„alignment pathï¼Œç„¶åå®šä¹‰äº†ä¸€ç§æ–°çš„å¯¹é½æ–¹æ³•expected alignmentï¼Œå…¶æœ¬è´¨æ˜¯å¯¹mdtwä¸­æ‰€æœ‰å¯èƒ½çš„alignment pathä¸Šçš„è·ç¦»åšå¹³å‡ã€‚</p><p>  è¯¥æ–¹æ³•æ˜¯æœ‰ç›‘ç£å­¦ä¹ ï¼Œæœ€å°åŒ–åŒç±»è§çš„è·ç¦»ï¼Œæœ€å¤§åŒ–ä¸åŒç±»ä¹‹é—´çš„è·ç¦»</p></li><li><p>NIPS 16 - Graphical Time Warping for Joint Alignment of Multiple Curves</p><p>  DTWåœ¨å›¾åƒç­‰æ•°æ®ä¸Šçš„æ¨å¹¿ï¼Œæœ¬æ–‡æå‡ºDTWçš„dynamic programmingè¿‡ç¨‹å¯ä»¥è§†ä¸ºgraphä¸Šçš„network flow problemï¼Œå¹¶ä½¿ç”¨max flow algorithmè¿›è¡Œæ±‚è§£ã€‚åˆ©ç”¨ä¸Šè¿°é—®é¢˜è½¬æ¢æ–¹æ³•ï¼Œè§£å†³äº†å›¾åƒï¼ˆå¯è§†ä¸ºå¤šç»´æ—¶é—´åºåˆ—ï¼‰ä¸Šåƒç´ ä¹‹é—´çš„è·ç¦»åº¦é‡é—®é¢˜ã€‚</p></li><li><p>KDD17 - Tripoles: A New Class of Relationships in Time Series Data</p><p>  å®šä¹‰äº†ä¸€ç§åœ¨ä¸‰ä¸ªæ—¶é—´åºåˆ—ä¹‹é—´çš„å…³ç³»ï¼Œè¯¥å…³ç³»æœ‰åˆ©äºå‘ç°æ—¶é—´åºåˆ—ä¸­çš„æ–°ç‰¹æ€§ï¼ˆå¦‚æ–°çš„å…¨çƒæ°”è±¡ç±»å‹ç­‰ï¼‰</p></li><li><p>ICML17 - Soft-DTW: a Differentiable Loss Function for Time-Series</p><p>  æå‡ºsmoothed formulation of DTWï¼Œæ˜¯ä¸€ç§å¯å¾®åˆ†çš„DTWæ±‚è§£æ–¹å¼ï¼Œè¿™ä½¿å¾—DTWèƒ½å¤ŸåµŒå…¥åˆ°æ·±åº¦å­¦ä¹ çš„æ¡†æ¶ä¸­</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
            <tag> survey </tag>
            
            <tag> time series </tag>
            
            <tag> similarity learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>æ¿€æ´»å‡½æ•°</title>
      <link href="/uncategorized/surveys/activations/"/>
      <url>/uncategorized/surveys/activations/</url>
      
        <content type="html"><![CDATA[<p>ï¼ˆæœ‰å¾…æ•´ç†ï¼‰</p><p>å“¦å“¦å“¦ï¼Œä¹Ÿå°±æ˜¯å¯ä»¥ç”¨æ¿€æ´»å‡½æ•°å®ç°ä¸€äº›æ•°å­¦æ“ä½œçš„æ„æ€ï¼ŒæŠŠç‰¹å®šèŒƒå›´çš„å€¼ï¼ˆå°äºé›¶å¾—å€¼ï¼‰æ˜ å°„åˆ°å¦ä¸€ä¸ªç‰¹å®šçš„å€¼ï¼ˆ0ï¼‰ã€‚ã€æˆ‘å€’æœ‰çœ‹åˆ°ä¸€ä¸ªç”¨reluæŠŠç‰¹å®šçš„å€¼å˜ä¸º0çš„ï¼Œæ¯”å¦‚gcnè‡ªåŠ¨å­¦ä¹ ä¸´æ¥çŸ©é˜µçš„æ—¶å€™ï¼ŒæŠŠå°çš„é¡¹å˜ä¸º0ã€‘</p><span id="more"></span><p><a href="https://www.cnblogs.com/nxf-rabbit75/p/9276412.html">https://www.cnblogs.com/nxf-rabbit75/p/9276412.html</a></p><p><a href="https://www.cnblogs.com/makefile/p/activation-function.html">https://www.cnblogs.com/makefile/p/activation-function.html</a></p><p><a href="https://www.cnblogs.com/kang06/p/9371420.html">https://www.cnblogs.com/kang06/p/9371420.html</a></p><p><a href="https://recomm.cnblogs.com/blogpost/9378079">https://recomm.cnblogs.com/blogpost/9378079</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> survey </tag>
            
            <tag> neural network </tag>
            
            <tag> activation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>å¦‚ä½•åœ¨ç§‘å­¦ä¼šè®®ä½œæŠ¥å‘Š</title>
      <link href="/uncategorized/notes/nature/"/>
      <url>/uncategorized/notes/nature/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.nature.com/articles/d41586-020-00838-3?utm_source=twitter&utm_medium=social&utm_content=organic&utm_campaign=NGMT_USG_JC01_GL_Nature">åŸæ–‡é“¾æ¥</a></p><span id="more"></span><ol><li><p>äº†è§£ä½ çš„å¬ä¼—ã€‚äº†è§£ä»–ä»¬çš„çŸ¥è¯†æ°´å¹³ï¼Œå¯¹ä½ æŠ¥å‘Šå†…å®¹çš„äº†è§£ç¨‹åº¦åŠä»¥å‰æ˜¯å¦å¬è¿‡ä½ çš„æŠ¥å‘Šï¼›</p></li><li><p>åœ¨æŠ¥å‘Šä¹‹å‰è¦æƒ³å¥½æŠ¥å‘Šçš„ä¸­å¿ƒæ€æƒ³ï¼ŒæŠŠè¦ä¼ è¾¾çš„æ ¸å¿ƒæ€æƒ³ç”¨ä¸€ä¸¤å¥è¯å†™å‡ºæ¥ã€‚æŠŠä¸ä¸­å¿ƒæ€æƒ³æ— å…³çš„å¹»ç¯ç‰‡å…¨éƒ¨åˆ æ‰ï¼›</p></li><li><p>å¬ä¼—åœ¨æŠ¥å‘Šæœ€å¼€å§‹çš„æ—¶å€™ï¼Œç²¾åŠ›æ˜¯æœ€é›†ä¸­çš„ã€‚å› æ­¤åœ¨æŠ¥å‘Šæœ€å¼€å§‹å°±éœ€è¦ä¼ è¾¾ç»™å¬ä¼—æŠ¥å‘Šçš„ä¸­å¿ƒæ€æƒ³ã€‚é¿å…åœ¨ä¸€å¼€å§‹å°±è®²å¤æ‚çš„ç†è®ºæˆ–æŠ€æœ¯ï¼›</p></li><li><p>ç²¾ç®€åœ°æœ‰è®¡åˆ’æ€§åœ°ç»„ç»‡å¹»ç¯ç‰‡ï¼Œä»¥è®²æ•…äº‹çš„æ–¹å¼å¼€å±•æŠ¥å‘Šã€‚ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•æ˜¯åƒåœ¨æ‹ç”µå½±æˆ–è€…ç”»æ¼«ç”»æ—¶ä¸€æ ·ï¼Œåšä¸€ä¸ªæƒ…èŠ‚çš„è‰å›¾å±•ç¤ºã€‚æ‹æ‘„ç”µå½±æˆ–ç”»æ¼«ç”»æ—¶ç”±æ­¤å¯ä»¥é€‰å®šæƒ…èŠ‚ã€è§’è‰²ã€åœºæ™¯å’Œæ•ˆæœã€‚ç§‘å­¦å®¶å¯ä»¥ç”¨è¿™æ ·çš„æ–¹æ³•åœ¨ç¬”è®°æœ¬å‹¾ç”»ä¸€ä¸ªè‰å›¾æ¥è®¡åˆ’å¹»ç¯ç‰‡ä¸»çº¿ï¼Œå¤§çº²ï¼Œåˆ†å±‚ï¼Œé€’è¿›ï¼Œè½¬æŠ˜åŠæ€»ç»“ï¼›</p></li><li><p>è¦è€ƒè™‘å¬ä¼—çš„æ„Ÿå—å’Œç–²åŠ³è€å—åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é•¿è¾¾ä¸€å‘¨çš„å¤§å‹ä¼šè®®æœŸé—´ï¼Œå°½é‡å°†å¹»ç¯ç‰‡åšå¾—æ˜“äºæ¶ˆåŒ–ï¼›</p></li><li><p>ä¸è¦æœ‰è¿‡å¤šæ–‡å­—ï¼Œé¿å…è®©è§‚ä¼—é˜…è¯»ã€‚è¿‡å¤šæ–‡å­—ä¼šè®©å¬ä¼—ä¸å¾—ä¸é€‰æ‹©æ˜¯å¬è¿˜æ˜¯è¯»ã€‚æœ€å¥½çš„æ–¹æ³•æ˜¯æ”¾ç½®å‡ ä¸ªå…³é”®è¯å¹¶é€šè¿‡æ¼”è®²æ‰©å¤§å…³é”®è¯å†…å®¹ï¼›</p></li><li><p>å›¾ç‰‡å’Œå½±åƒæ¯”æ¼”è®²æ›´å…·æœ‰æ„Ÿå®˜æ€§ï¼Œæ›´å®¹æ˜“è®©äººç•™ä¸‹æ·±åˆ»å°è±¡ã€‚è¯•å›¾äººæ€§åŒ–åœ°å°†ç§‘ç ”å†…å®¹å’Œå¬ä¼—ä¸ªäººæ„Ÿå—è¿æ¥èµ·æ¥ï¼Œæ¯”å¦‚å±•ç¤ºå®éªŒå®¤ç°åœºç…§ç‰‡ï¼Œæˆ–å±•ç¤ºä¸€ä¸ªå¤æ‚å®éªŒæŠ€æœ¯çš„æ“ä½œå°è§†é¢‘åŠåŠ¨ç”»ç­‰ï¼›</p></li><li><p>ä¸è¦è®©å¹»ç¯ç‰‡å†…å®¹å¯†é›†ï¼Œæ’ç‰ˆæ—¶æ³¨æ„å¬ä¼—æ„Ÿå—ã€‚è¿‡åº¦å¯†é›†æˆ–è¿‡å°çš„å›¾ç‰‡ã€è¡¨æ ¼å’Œæ–‡å­—éå¸¸ä¸åˆ©äºä¿¡æ¯äº¤æµï¼Œè¿˜ä¼šåŠ é€Ÿå¬ä¼—çš„ç–²åŠ³å’Œéº»æœ¨ï¼›å¹»ç¯ç‰‡åˆ¶ä½œåˆ‡å¿Œå°†ä¸€ä¸ªå‘è¡¨æ–‡ç« çš„ç»„å›¾å…¨éƒ¨æ”¾åœ¨ä¸€å¼ å¹»ç¯ç‰‡ä¸Šï¼Œå°½é‡ä½¿æ¯ä¸€å¼ å¹»ç¯ç‰‡ä»…å‘ˆç°ä¸€ä¸¤ä¸ªå›¾ç‰‡å…ƒç´ ã€‚å°½é‡æ”¾å¤§å›¾ç‰‡ï¼›</p></li><li><p>è®¤çœŸè§£é‡Šä½ çš„æ•°æ®ã€‚å¦‚æœå‘éä¸“ä¸šé¢†åŸŸå¬ä¼—ä»‹ç»ï¼Œè¦è§£é‡Šæœ¯è¯­å’Œæ¯ä¸ªåæ ‡è½´ã€æ›²çº¿çš„æ„ä¹‰ï¼›åä¹‹ï¼Œå¦‚æœå‘æœ¬é¢†åŸŸä¸“å®¶ä»‹ç»ï¼Œåˆ™å¯ä»¥è·³è¿‡æœ¯è¯­ã€åæ ‡è½´å’Œä¸åŒå‚æ•°çš„æ„ä¹‰ã€‚ä½†æ— è®ºå¦‚ä½•ï¼Œå¿…é¡»è§£é‡Šå›¾å½¢è¡¨è¾¾çš„æ„ä¹‰ï¼›</p></li><li><p>æ¯å¼ å¹»ç¯ç‰‡æ ‡é¢˜ä½¿ç”¨ä¸€ä¸ªå®Œæ•´çš„è¡¨ç¤ºç»“è®ºçš„ä¸‹å®šä¹‰å¥å­ã€‚æ¯”å¦‚â€œå„å°”å°¼è¯ºç°è±¡ä½¿å†¬å¤©åœ°è¡¨é£åŠ›å‡å¼±20%â€ï¼Œè€Œéâ€œå„å°”å°¼è¯ºç°è±¡å’Œåœ°è¡¨é£åŠ›â€è¿™æ ·çš„è¯ç»„å †ç Œï¼›</p></li><li><p>ä¸€èˆ¬æ¥è®²ä¸€åˆ†é’Ÿä¸€å¼ å¹»ç¯ç‰‡ï¼Œåœ¨ä¸å¾—å·²æƒ…å†µä¸‹ï¼Œå¯ä»¥å¤šåšä¸€äº›å¹»ç¯ç‰‡ï¼Œæ‰“ç ´è¿™ä¸ªå¸¸è§„ã€‚ä½†æ˜¯å¬ä¼—åœ¨ä¸€å®šæ—¶é—´å†…æ‘„å–ä¿¡æ¯çš„èƒ½åŠ›æ˜¯æ’å®šçš„ï¼Œå› æ­¤ï¼Œå¯ä»¥å¢åŠ å¹»ç¯ç‰‡ä½†ä¸è¦å¢åŠ å†…å®¹é‡ã€‚</p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> presentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in AAAI 2020 (2020.02.07)</title>
      <link href="/uncategorized/paperlistfile/AAAI2020/"/>
      <url>/uncategorized/paperlistfile/AAAI2020/</url>
      
        <content type="html"><![CDATA[<p><a href="https://aaai.org/Conferences/AAAI-20/wp-content/uploads/2020/01/AAAI-20-Accepted-Paper-List.pdf">Link</a></p><span id="more"></span><h2 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h2><ul><li><p><strong>å·²è¯»</strong> DATA-GRU: Dual-Attention Time-Aware Gated Recurrent Unit for Irregular Multivariate Time Series</p><p>Qingxiong Tan (Department of Computer Science, Hong Kong Baptist University)*; Mang YE (Inception Institute of<br>Artificial Intelligence); Baoyao Yang (Department of Computer Science, Hong Kong Baptist University); Siqi Liu<br>(Department of Computer Science, Hong Kong Baptist University); Andy Jinhua Ma (School of Data and Computer<br>Science, Sun Yat-Sen University); Terry Cheuk-Fung Yip (Department of Medicine and Therapeutics, The Chinese<br>University of Hong Kong); Grace Lai-Hung Wong (Department of Medicine and Therapeutics, The Chinese<br>University of Hong Kong); PongChi Yuen (Department of Computer Science, Hong Kong Baptist University)</p></li><li><p>Joint Modeling of Local and Global Temporal Dynamics for Multivariate Time Series Forecasting with Missing Values</p><p>Xianfeng Tang (The Pennsylvania State University)*; Huaxiu Yao (Pennsylvania State University); Yiwei Sun (Penn<br>State University); Charu Aggarwal (IBM); Prasenjit Mitra (Pennsylvania State University ); Suhang Wang<br>(Pennsylvania State University)</p></li><li><p>Factorized Inference in Deep Markov Models for Incomplete Multimodal Time Series</p><p>Tan Zhi-Xuan (Massachusetts Institute of Technology)*; Desmond Ong (A*STAR Artificial Intelligence Initiative);<br>Harold Soh (National University of Singapore)</p></li><li><p>A Variational Point Process Approach for Social Event Sequences</p><p>Zhen Pan (University of Science and Technology of China)*; Zhenya Huang (University of Science and Technology of<br>China ); Defu Lian (University of Science and Technology of China); Enhong Chen (University of Science and<br>Technology of China)</p></li><li><p>Deep Unsupervised Binary Coding Networks for Multivariate Time Series Retrieval</p><p>Dixian Zhu (University of Iowa); Dongjin Song (NEC Labs America)*; Yuncong Chen (NEC Laboratories America,<br>Inc.); Cristian Lumezanu (NEC Labs); Wei Cheng (NEC Laboratories America); Bo Zong (NEC Labs); Jingchao Ni ( NEC<br>Laboratories America); Takehiko Mizoguchi (NEC Laboratories America, Inc.); Tianbao Yang (University of Iowa);<br>Haifeng Chen (NEC Labs)</p></li><li><p>Relation Inference among Sensor Time Series in Smart Buildings with Metric Learning</p><p>Shuheng Li (Peking University)*; Dezhi Hong (UC San Diego); Hongning Wang (University of Virginia)</p></li><li><p>Time2Graph: Revisiting Time Series Modeling with Dynamic Shapelets</p><p>Ziqiang Cheng (Zhejiang University); Yang Yang (Zhejiang University)*; Wei Wang (State Grid Huzhou Power Supply<br>Co. Ltd.); Wenjie Hu (Zhejiang University); Yueting Zhuang (Zhejiang University); guojie song (PKU, China)</p></li></ul><ul><li><p>OMuLeT: Online Multi-Lead Time Location Prediction for Hurricane Trajectory Forecasting</p><p>Ding Wang (Michigan State University)*; Boyang Liu (Michigan State University); Pang-Ning Tan (MSU); Lifeng Luo<br>(Michigan State University)</p></li><li><p>Tensorized LSTM with Adaptive Shared Memory for Learning Trends in Multivariate Time Series</p><p>Dongkuan Xu (The Pennsylvania State University)*; Wei Cheng (NEC Laboratories America); Bo Zong (NEC Labs);<br>Dongjin Song (NEC Labs America); Jingchao Ni ( NEC Laboratories America); Wenchao Yu (UCLA); Yanchi Liu<br>(Rutgers); Haifeng Chen (NEC Labs); Xiang Zhang (The Pennsylvania State University)</p></li><li><p>Block Hankel Tensor ARIMA for Multiple Short Time Series Forecasting</p><p>QIQUAN SHI (Huawei Noahâ€™s Ark Lab)*; Jiaming YIN (Tongji University); Jiajun CAI (The University of Hong Kong);<br>Andrzej Cichocki (Skolkovo Institute of Science and Technology); Tatsuya Yokota (Nagoya Institute of Technology);<br>Lei CHEN (Huawei Noahâ€™s Ark Lab); Mingxuan Yuan (Huawei); Jia Zeng (Huawei Noahâ€™s Ark Lab)</p></li><li><p>The Missing Data Encoder: Cross-Channel Image Completion with Hide-And-Seek Adversarial Network</p><p>Arnaud Dapogny (Pierre and Marie Curie University (UPMC))*; Matthieu Cord (Sorbonne University); Patrick PÃ©rez<br>(Valeo.ai)</p></li></ul><ul><li><p>Joint Modeling of Local and Global Temporal Dynamics for Multivariate Time Series Forecasting with Missing<br>Values</p><p>Xianfeng Tang (The Pennsylvania State University)*; Huaxiu Yao (Pennsylvania State University); Yiwei Sun (Penn<br>State University); Charu Aggarwal (IBM); Prasenjit Mitra (Pennsylvania State University ); Suhang Wang<br>(Pennsylvania State University)</p></li></ul><h2 id="missing-value"><a href="#missing-value" class="headerlink" title="missing value"></a>missing value</h2><ul><li><p>Random Intersection Graphs and Missing Data</p><p>Dror Salti (Ben Gurion University of The Negev)*; Yakir Berchenko (Ben Gurion University of The Negev)</p></li></ul><ul><li><p>Polynomial Matrix Completion for Missing Data Imputation and Transductive Learning</p><p>Jicong Fan (Cornell University)*; Yuqian Zhang (Cornell University); Madeleine Udell (Cornell University)</p></li></ul><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><ul><li><p>Eigenvalue Normalized Recurrent Neural Networks for Short Term Memory</p><p>Kyle Helfrich (University of Kentucky)*; Qiang Ye (University of Kentucky)</p></li><li><p>Particle Filter Recurrent Neural Networks</p><p>Xiao Ma (National University of Singapore)*; Peter Karkus (National University of Singapore); David Hsu (NUS);<br>Wee Sun Lee (National University of Singapore)</p></li><li><p>Segmenting Medical MRI via Recurrent Decoding Cell</p><p>Kai Xie (East China Normal University); ä½•è‰¯å (åŒæµå¤§å­¦); Ying Wen (East China Normal University)*</p></li><li><p>An Attentional Recurrent Neural Network for Personalized Next Location Recommendation</p><p>Qing Guo (Nanyang Technological University)*; Zhu Sun (Nanyang Technological University); Jie Zhang (Nanyang<br>Technological University); Yin-Leng Theng (Nanyang Technological University)</p></li><li><p>Weighted Automata Extraction from Recurrent Neural Networks via Regression on State Spaces</p><p>Takamasa Okudono (National Institute of Informatics)*; Masaki Waga (National Institute of Informatics); Taro<br>Sekiyama (National Institute of Informatics); Ichiro Hasuo (National Institute of Informatics &amp; SOKENDAI)</p></li><li><p>Recurrent Nested Model for Sequence Generation</p><p>Wenhao Jiang (Tencent AI Lab)*; Lin Ma (Tencent AI Lab); Wei Lu (UESTC)</p></li><li><p>Span-based Neural Buffer: Towards Efficient and Effective Utilization of Long-Distance Context for Neural Sequence Models</p><p>Yangming Li (Ant Financial Services Group)*; Kaisheng Yao (Ant Financial Services Group); Libo Qin (Research<br>Center for Social Computing and Information Retrieval, Harbin Institute of Technology); Shuang Peng (Ant Financial<br>Services Group); Yijia Liu (Alibaba Group); Xiaolong Li (Ant Financial)</p></li><li><p>Multi-Zone Unit for Recurrent Neural Networks</p><p>Fandong Meng (Tencent WeChat AI - Pattern Recognition Center Tencent Inc.)*; Jinchao Zhang (Tencent); Yang Liu<br>(Tsinghua University); Jie Zhou (Tencent)</p></li><li><p>Event-Driven Continuous Time Bayesian Networks</p><p>Debarun Bhattacharjya (IBM Research)*; Karthikeyan Shanmugam (IBM Research NY); Tian Gao (IBM Research);<br>Nicholas Mattei (Tulane University); Kush Varshney (IBM Research); Dharmashankar Subramanian (IBM Research)</p></li><li><p>Modeling Electrical Motor Dynamics using Encoder-Decoder with Recurrent Skip Connection</p><p>Sagar Verma (IIIT Delhi)*; Nicolas Henwood (Schneider Electric); Marc Castella (Telecom SudParis); Francois<br>Malrait (Schneider Electric); Jean-Christophe Pesquet (CentraleSupelec)</p></li><li><p>Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples</p><p>Minhao Cheng (UCLA)*; Jinfeng Yi (JD AI Research); Pin-Yu Chen (IBM Research); Huan Zhang (UCLA); Cho-Jui Hsieh (UCLA)</p></li><li><p>Temporal Pyramid Recurrent Neural Network</p><p>Qianli Ma (South China University of Technology)*; Zhenxi Lin (South China University of Technology); Enhuan<br>Chen (South China University of Technology); Garrison Cottrell (UC San Diego)</p></li><li><p>AirNet: A Calibration Model for Low-Cost Air Monitoring Sensors Using Dual Sequence Encoder Networks</p><p>Haomin Yu (Beijing Jiaotong University); Qingyong Li (Beijing Jiaotong University)*; YangLi-ao Geng (Beijing<br>Jiaotong University); Yingjun Zhang (Beijing Jiaotong University); Zhi Wei (New Jersey Institute of Technology)</p></li><li><p>A Skip-connected Evolving Recurrent Neural Network for Data Stream Classification under Label Latency Scenario</p><p>Monidipa Das (Nanyang Technological University); Mahardhika Pratama (Nanyang Technology University)*; Jie<br>Zhang (Nanyang Technological University); Yew Soon Ong (Nanyang Technological University, Nanyang View,<br>Singapore)</p></li><li><p><strong>å·²è¯»</strong> Not All Attention Is Needed: Gated Attention Network for Sequence Data</p><p>LANQING XUE (Hong Kong University of Science and Technology)*; Xiaopeng Li (Hong Kong U. of Sci. &amp; Tech.);<br>Nevin Zhang (HKUST)</p></li><li><p>Biologically Plausible Sequence Learning with Spiking Neural Networks</p><p>Zuozhu Liu (Singapore University of Technology and Design); Thiparat Chotibut (Chulalongkorn university)*;<br>Christopher Hillar (Rewwood Center); Shaowei Lin (SUTD)</p></li><li><p>Structured Sparsification of Gated Recurrent Neural Networks</p><p>Ekaterina Lobacheva (Samsung-HSE Laboratory, National Research University Higher School of Economics)*;<br>Nadezhda Chirkova (Samsung-HSE Laboratory, National Research University Higher School of Economics);<br>Aleksandr Markovich (National Research University Higher School of Economics); Dmitry Vetrov (National Research<br>University Higher School of Economics, Samsung AI Center Moscow)</p></li><li><p>TapNet: Multivariate Time Series Classificationwith Attentional Prototype Network</p><p>Xuchao Zhang (Virginia Tech)*; Yifeng Gao (George Mason University); Jessica Lin (George Mason University);<br>Chang-Tien Lu (Virginia Tech, USA)</p></li></ul><h2 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a>Anomaly Detection</h2><ul><li><p>Likelihood Ratios and Generative Classifiers for Unsupervised Out-of-Domain Detection In Task Oriented<br>Dialog</p><p>Varun Prashant Gangal (Carnegie Mellon University)*; Abhinav Arora (Facebook); Arash Einolghozati (Facebook);<br>Sonal Gupta (Facebook)</p></li><li><p>Self-Supervised Learning for Generalizable Out-of-Distribution Detection</p><p>Sina Mohseni (Texas A&amp;M University)*; Mandar Pitale (NVIDIA); JBS Yadawa (NVIDIA); Zhangyang Wang (TAMU)</p><p>è§£å†³çš„é—®é¢˜ï¼šä¸»ä»»åŠ¡ä¸ºæœ‰ç›‘ç£çš„out-of-distribution detectionï¼Œäºæ­¤åŒæ—¶è¦è§£å†³OOD detectioné—®é¢˜ï¼Œä¾‹å­ï¼šåœ¨ä¸€ä¸ªåˆ†ç±»åŠ¨ç‰©çš„é—®é¢˜ä¸Šï¼Œè¦æ‹’ç»å¯¹ä¸€ä¸ªäººåƒè¿›è¡Œè¯†åˆ«ï¼Œå³ä¸æä¾›ä»»ä½•ç½‘ç»œè¾“å‡ºã€‚</p><p>æ–¹æ³•ï¼šä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ–¹æ³•ï¼š</p><ol><li>å…ˆè®­ç»ƒä¸€ä¸ªCç±»åˆ†ç±»å™¨ç”¨äºåˆ†ç±»åŸå§‹ä»»åŠ¡ï¼ˆå³åŠ¨ç‰©åˆ†ç±»é—®é¢˜ï¼ŒCä¸ºåŠ¨ç‰©ç±»åˆ«ï¼‰</li><li>å†åœ¨å·²ç»è®­ç»ƒåçš„åˆ†ç±»å™¨çš„æœ€åä¸€å±‚æ·»åŠ æ–°çš„Kç±»åˆ†ç±»å±‚ï¼Œç”¨äºåšOOD detectionï¼Œå…¶ä¸­$K = C + A$ï¼ŒAä¸ºä¸€ä¸ªè¶…å‚æ•°ï¼Œæ˜¯OODæ ·æœ¬çš„æ€»ç±»åˆ«æ•°ï¼ˆå³å‡è®¾OODæ ·æœ¬å…±æœ‰Aä¸ªç±»åˆ«ï¼‰ã€‚ä¸Šè¿°ç½‘ç»œä½¿ç”¨æ··åˆæ ·æœ¬è¿›è¡Œå¾®è°ƒï¼Œå…¶ä¸­æ··åˆæ ·æœ¬æ˜¯in- å’Œout- of distribution æ ·æœ¬çš„æ··åˆã€‚OODæ ·æœ¬åœ¨è®­ç»ƒçš„æ—¶å€™å¯ä»¥è®¾ç½®ä¸ºä»»æ„çš„OODï¼ˆå³ï¼Œå¯¹äºä¸€ä¸ªåŸå§‹é—®é¢˜ä¸ºçŒ«ç‹—åˆ†ç±»çš„ç½‘ç»œï¼ŒOOD_trainå¯ä»¥è®¾ç½®ä¸ºäººè„¸ï¼Œå³ä½¿åœ¨çœŸå®ä¸–ç•Œä¸­çš„OODæ ·æœ¬ä¸æ­¢æœ‰äººè„¸ï¼‰ã€‚ä¸Šè¿°è®­ç»ƒæ–¹æ³•çš„ç›®æ ‡æ˜¯ï¼š<strong>è®©ç½‘ç»œèƒ½å¤Ÿåœ¨ä¸å¿˜è®°in-distributionæ ·æœ¬ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œè¿˜èƒ½è®°ä½OODçš„ä¸€éƒ¨åˆ†ç‰¹å¾ã€‚</strong></li><li>åœ¨æ¨ç†çš„æ—¶å€™ï¼Œä½¿ç”¨æ­¥éª¤<strong>2</strong>çš„åˆ†ç±»å±‚ï¼Œå³Kç±»çš„åˆ†ç±»å±‚ã€‚å¯¹äºä»»æ„ä¸€ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œé¦–å…ˆè®¡ç®—åœ¨Cç±»åˆ†ç±»é—®é¢˜ä¸Šçš„é¢„æµ‹ç»“æœï¼Œå³$y_{pred} = arg_{max_i}{\gamma[1:C]}$ï¼Œè®©åè®¡ç®—å…¶OOD scoreï¼Œ$Score_{OOD} = sum(\gamma[C:K])$ï¼Œå³åœ¨Aç±»OODåˆ†ç±»ä¸Šçš„SOFTMAXå“åº”æ€»å’Œã€‚</li><li>ä¸ªäººç†è§£ï¼šOODæ ·æœ¬å°†åœ¨C:Kä¸Šæœ‰è¾ƒé«˜çš„å“åº”ï¼Œæ­¤æ—¶è¯¥æ ·æœ¬çš„$y_{pred}$å°†ä¸å¯ä¿¡ï¼Œå¹¶æ‹’ç»è¯¥æ ·æœ¬çš„åˆ†ç±»ã€‚åä¹‹åˆ™æ¥å—è¯¥æ ·æœ¬çš„åˆ†ç±»ã€‚</li></ol></li><li><p>Adaptive Double Exploration Tradeoff for Outlier Detection</p><p>Xiaojin Zhang (CUHK)*; Honglei Zhuang (Google Research); Shengyu Zhang (Tencent); Yuan Zhou (UIUC)</p></li><li><p>MixedAD: A Scalable Algorithm for Detecting Mixed Anomalies in Attributed Graphs</p><p>Mengxiao Zhu (Beihang Univerisity)*; Haogang Zhu (Beihang University)</p></li><li><p>Detecting semantic anomalies</p><p>Faruk Ahmed (Mila, Universite de Montreal)*; Aaron Courville (Universite de Montreal)</p></li><li><p><strong>å·²è¯»</strong> Outlier Detection Ensemble with Embedded Feature Selection</p><p>Li Cheng (National University of Defense Technology)*; Yijie Wang (â€œ National University of Defense Technology,<br>Chinaâ€); Xinwang Liu (National University of Defense Technology); Bin Li ( National University of Defense<br>Technology)</p></li><li><p>Multi-scale Anomaly Detection on Attributed Networks</p><p>Leonardo Gutierrez Gomez (Universite catholique de Louvain)*; Alexandre Bovet (Universite catholique de<br>Louvain); Jean-Charles Delvenne (Universite catholique de Louvain)</p></li><li><p>Transfer Learning for Anomaly Detection through Localized and Unsupervised Instance Selection</p><p>Vincent Vercruyssen (KU Leuven)*; Jesse Davis (KU Leuven); Wannes Meert (KU Leuven)</p></li><li><p>MIDAS: Microcluster-Based Detector of Anomalies in Edge Streams</p><p>Siddharth Bhatia (National University of Singapore)*; Bryan Hooi (National University of Singapore); Minji Yoon<br>(Carnegie Mellon University); Kijung Shin (KAIST); Christos Faloutsos ()</p></li></ul><h2 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h2><ul><li><p>Graph Transformer for Graph-to-Sequence Learning</p><p>Deng Cai (The Chinese University of Hong Kong)*; Wai Lam (The Chinese University of Hong Kong)</p></li><li><p>Sequence Generation with Optimal-Transport-Enhanced Reinforcement Learning</p><p>Liqun Chen (Duke University)*; Ke Bai (Duke University); Chenyang Tao (Duke University); Yizhe Zhang (Microsoft<br>Research); Guoyin Wang (Duke University); Wenlin Wang (Duke Univeristy); Ricardo Henao (Duke University);<br>Lawrence Carin Duke (CS)</p></li></ul><h2 id="Interpretable"><a href="#Interpretable" class="headerlink" title="Interpretable"></a>Interpretable</h2><ul><li><p>Dynamic Network Pruning with Interpretable Layerwise Channel Selection</p><p>Yulong Wang (Tsinghua University); Xiaolu Zhang (Ant Financial Services Group); Hang Su (Tsinghua Univiersity); Bo<br>Zhang (Tsinghua University); Xiaolin Hu (Tsinghua University)*</p></li><li><p>Interpretable and Differentially Private Predictions</p><p>Frederik Harder (Max Planck Institute)*; Matthias Bauer (MPI TÃ¼bingen); Mijung Park (MPI Tuebingen)</p></li><li><p>MRI Reconstruction with Interpretable Pixel-Wise Operations Using Reinforcement Learning</p><p>wentian li (Tsinghua University)*; XIDONG FENG (department of Automation,Tsinghua University); Haotian An<br>(Tsinghua University); Xiang Yao Ng (Tsinghua University); Yu-Jin Zhang (Tsinghua University)</p></li><li><p>Interpretable rumor detection in microblogs by attending to user interactions</p><p>Serena Khoo (DSO National Laboratories)*; Hai Leong Chieu (DSO National Laboratories); Zhong Qian (Soochow<br>University); Jing Jiang (Singapore Management University)</p></li><li><p>Asymmetrical Hierarchical Networks with Attentive Interactions for Interpretable Review-Based Recommendation</p><p>Xin Dong (Rutgers University); Jingchao Ni ( NEC Laboratories America)*; Wei Cheng (NEC Laboratories America);<br>Zhengzhang Chen (NEC Laboratories America, Inc.); Bo Zong (NEC Labs); Dongjin Song (NEC Labs America); Yanchi<br>Liu (NEC Labs America); Haifeng Chen (NEC Labs); Gerard de Melo (Rutgers University)</p></li><li><p>Iteratively Questioning and Answering for Interpretable Legal Judgment Prediction</p><p>Haoxi Zhong (Tsinghua University)*; Yuzhong Wang (Tsinghua University); Cunchao Tu (Tsinghua University);<br>Tianyang Zhang (Powerlaw); Zhiyuan Liu (Tsinghua University); Maosong Sun (Tsinghua University)</p></li><li><p>Chemically Interpretable Graph Interaction Network for Prediction of Pharmacokinetic Properties of Drug-<br>like Molecules</p><p>Yashaswi Pathak (International Institute of Information Technology,Hyderabad); Siddhartha Laghuvarapu (IIIT<br>Hyderabad); Sarvesh Mehta (IIIT Hyderabad); Deva Priyakumar (IIIT Hyderabad)*</p></li><li><p>Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents</p><p>Ming Tu (JD AI Research)*; Kevin Huang (JD AI Research); Guangtao Wang (JD.com); Jing Huang (JD.COM);<br>Xiaodong He (JD AI Research); Bowen Zhou (JD)</p></li></ul><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><ul><li><p>General Partial Label Learning via Dual Bipartite Graph Autoencoder</p><p>Brian Chen (Columbia University); Bo Wu (Columbia University); Alireza Zareian (Columbia University); Hanwang<br>Zhang (Nanyang Technological University); Shih-Fu Chang (Columbia University)*</p></li><li><p>A Variational Autoencoder with Deep Embedding Model for Generalized Zero-Shot Learning</p><p>Peirong Ma (Guangzhou University); Xiao Hu (Guangzhou University)*</p></li></ul><ul><li> Graph Representation Learning via Ladder Gamma Variational Autoencoders</li></ul><p>  Arindam Sarkar (Amazon)*; Nikhil Mehta (Duke University); Piyush Rai (IIT Kanpur)</p><ul><li><p>Semi-Supervised Text Simplification with Back-Translation and Asymmetric Denoising Autoencoders</p><p>Yanbin Zhao (Shanghai Jiao Tong University)*; Lu Chen (Shanghai Jiao Tong University); Zhi Chen (Shanghai Jiao<br>Tong University); Kai Yu (Shanghai Jiao Tong University)</p></li><li><p>Draft and Edit: Automatic Storytelling Through Multi-Pass Hierarchical Conditional Variational Autoencoder</p><p>Meng Hsuan Yu (Peking University )*; Juntao Li (Peking University); Danyang Liu (Shanghai Jiao Tong University);<br>Bo Tang (Southern University of Science and Technology); Haisong Zhang (Tencent AI Lab); Dongyan Zhao (Peking<br>University); Rui Yan (Peking University)</p></li><li><p>Vector Quantization-Based Regularization for Autoencoders</p><p>Hanwei Wu (KTH Royal Institute of Technology)*; Markus Flierl (KTH Royal Institute of Technology)</p></li></ul><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><ul><li><p>SalSAC: A Video Saliency Prediction Model with Shuffled Attentions and Correlation-based ConvLSTM</p><p>Xinyi Wu (University of South Carolina); Zhenyao Wu (University of South Carolina); jinglin zhang (Nanjing<br>University of Information Science and Technology); Lili Ju (University of South Carolina); Song Wang (University of<br>South Carolina)*</p></li><li><p>Graph LSTM with Context-Gated Mechanism for Spoken Language Understanding</p><p>Linhao Zhang (Peking University)*; Dehong Ma (Peking University); Xiaodong Zhang (Peking University); Xiaohui<br>Yan (Huawei Technologies); Houfeng Wang (Peking University)</p></li><li><p>Self-Attention ConvLSTM for Spatiotemporal Prediction</p><p>Zhihui Lin (Tsinghua University)*; Maomao Li (Tsinghua university); Zhuobin Zheng ( Tsinghua University);<br>Yangyang Cheng (Tsinghua University); Chun Yuan (Tsinghua University)</p></li><li><p>Bivariate Beta-LSTM</p><p>Kyungwoo Song (KAIST)*; JoonHo Jang (KAIST); Seung jae Shin (KAIST); Il-Chul Moon (KAIST)</p></li><li><p>D2D-LSTM: LSTM-based Path Prediction of Content Diffusion Tree in Device-to-Device Social Networks</p><p>Heng Zhang (Tianjin University)*; Xiaofei Wang (College of Intelligence and Computingï¼ŒTianjin University); Jiawen<br>Chen (Tianjin University); Chenyang Wang (Tianjin University); Jianxin Li (Deakin University)</p></li><li><p>Why Attention? Analyze BiLSTM Deficiency and Its Remedies in the Case of NER</p><p>Peng-Hsuan Li (Academia Sinica)*; Tsu-Jui Fu (National Tsing Hua University); Wei-yun Ma (å°æ¹¾ä¸­ç ”é™¢)</p></li><li><p>CF-LSTM: Cascaded Feature-Based Long Short-Term Networks for Predicting Pedestrian Trajectory</p><p>Yi Xu (Xiâ€™an Jiaotong University); JING YANG (Xiâ€™an Jiaotong University); Shaoyi Du (Xiâ€™an Jiaotong Unviersity)*</p></li></ul><h2 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h2><ul><li><p>Random Erasing Data Augmentation</p><p>Zhun Zhong (Xiamen University)*; Liang Zheng (Australian National University); Guoliang Kang (CMU); Shaozi Li<br>(Xiamen University, China); Yi Yang (UTS)</p><p>è®ºæ–‡ä¸ºCNNè®­ç»ƒæå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚Random Erasingï¼Œåœ¨ä¸€å¼ å›¾ç‰‡ä¸­éšæœºçš„é€‰æ‹©ä¸€ä¸ªçŸ©å½¢æ¡†ï¼Œåœ¨éšæœºçš„ä½ç½®ä¸Šä½¿ç”¨éšæœºçš„å€¼æ¥æ“¦é™¤å›¾ç‰‡åŸæ¥çš„åƒç´ ã€‚é€šè¿‡è¯¥æ–¹æ³•èƒ½å¤Ÿç»™å›¾ç‰‡åŠ å…¥ä¸åŒç¨‹åº¦çš„é®æŒ¡ï¼Œé€šè¿‡è¿™æ ·çš„è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥å‡å°‘æ¨¡å‹è¿‡æ‹Ÿåˆçš„é£é™©åŒæ—¶å¯¹é®æŒ¡å…·æœ‰ä¸€å®šçš„é²æ£’æ€§ã€‚éšæœºæ“¦é™¤å’Œrandom croppingï¼Œrandom flippingä¸€æ ·å¯ä»¥ä½œä¸ºæ•°æ®å¢å¼ºçš„æ–¹æ³•ï¼Œåœ¨åˆ†ç±»ï¼Œæ£€æµ‹å’Œè¡Œäººé‡è¯†åˆ«é¢†åŸŸèƒ½å¤Ÿå–å¾—ä¸é”™çš„æ•ˆæœã€‚</p></li><li><p>Effective Data Augmentation with Multi-Domain Learning GANs</p><p>Shinâ€™ya Yamaguchi (NTT)*; Sekitoshi Kanai (NTT Software Innovation Center/Keio University); Takeharu Eda (NTT)</p></li></ul><ul><li><p>CONAN: Complementary Pattern Augmentation for Rare Disease Detection</p><p>Limeng Cui (Penn State University); Siddharth Biswal (Georgia Institute of Technology); Lucas Glass (IQVIA); Greg<br>Lever (IQVIA); Jimeng Sun (Georgia Tech); Cao Xiao (IQVIA)*</p><p>ç¨€æœ‰ç–¾ç—…å½±å“ç€å…¨ä¸–ç•Œäº¿ä¸‡äººæ°‘ï¼Œä½†ç”±äºå®ƒä»¬çš„æ‚£ç—…ç‡æä½ï¼ˆä»1 / 1,000åˆ°1 / 200,000æ‚£è€…ä¸ç­‰ï¼‰å¹¶ä¸”è¢«ä¸¥é‡è¯¯è¯Šï¼Œå› æ­¤éš¾ä»¥å‘ç°ã€‚æˆ‘ä»¬å¦‚ä½•å¯é åœ°æ£€æµ‹å‡ºå¦‚æ­¤ä½çš„æ‚£ç—…ç‡ï¼Ÿå¦‚ä½•è¿›ä¸€æ­¥åˆ©ç”¨è¯Šæ–­å¯èƒ½ä¸ç¡®å®šçš„æ‚£è€…æ”¹å–„æ£€æµ‹ç‡ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç½•è§ç—…æ£€æµ‹çš„äº’è¡¥æ¨¡å¼å¢å¼ºï¼ˆCONANï¼‰æ¡†æ¶ã€‚CONANç»“åˆäº†å¯¹æŠ—è®­ç»ƒå’Œæœ€å¤§åˆ©æ¶¦ç‡åˆ†ç±»çš„æ€æƒ³ã€‚å®ƒé¦–å…ˆå­¦ä¹ è‡ªæˆ‘ä¸“æ³¨å’Œåˆ†å±‚åµŒå…¥ï¼Œä»¥è¿›è¡Œæ‚£è€…æ¨¡å¼è¡¨å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘äº†äº’è¡¥çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¨¡å‹ï¼Œé€šè¿‡é¼“åŠ±å„ç±»åˆ«ä¹‹é—´çš„æœ€å¤§å·®é¢æ¥ç”Ÿæˆä¸ç¡®å®šæ‚£è€…çš„å€™é€‰é˜³æ€§å’Œé˜´æ€§æ ·æœ¬ã€‚æ­¤å¤–ï¼ŒCONANå…·æœ‰ç–¾ç—…æ£€æµ‹å™¨ï¼Œå¯åœ¨å¯¹æŠ—æ€§è®­ç»ƒæœŸé—´ç”¨ä½œè¯†åˆ«ç½•è§ç–¾ç—…çš„è¯†åˆ«å™¨ã€‚æˆ‘ä»¬åœ¨ä¸¤é¡¹ç–¾ç—…æ£€æµ‹ä»»åŠ¡ä¸Šè¯„ä¼°äº†CONANã€‚å¯¹äºä½æµè¡Œæ€§ç‚ç—‡æ€§è‚ ç—…ï¼ˆIBDï¼‰æ£€æµ‹ï¼ŒCONANæ›²çº¿ä¸‹çš„ç²¾ç¡®å¬å›é¢ç§¯ï¼ˆPR-AUCï¼‰ä¸º0.96ï¼Œç›¸å¯¹äºæœ€ä½³åŸºçº¿ï¼Œç›¸å¯¹æ”¹å–„äº†50.1ï¼…ã€‚å¯¹äºç½•è§ç–¾ç—…ç‰¹å‘æ€§è‚ºçº¤ç»´åŒ–ï¼ˆIPFï¼‰æ£€æµ‹ï¼ŒCONANè¾¾åˆ°0.22 PR-AUCï¼Œç›¸å¯¹æœ€ä½³åŸºå‡†ï¼Œç›¸å¯¹æ”¹å–„41.3ï¼…ã€‚</p></li><li><p>Nonlinear Mixup: Out-Of-Manifold Data Augmentation for Text Classification</p><p>Hongyu Guo (National Research Council Canada)*</p></li><li><p>Dialog State Tracking with Reinforced Data Augmentation</p><p>Yichun Yin (Noahâ€™s Ark Lab of Huawei)*; Lifeng Shang (Noahâ€™s Ark Lab); Xin Jiang (Huawei Noahâ€™s Ark Lab); Xiao<br>Chen (Huawei Noahâ€™s Ark Lab); Qun Liu (Huawei Noahâ€™s Ark Lab)</p></li></ul><ul><li><p>Constructing Multiple Tasks for Augmentation: Improving Neural Image Classification With K-means Features</p><p>Tao Gui (Fudan University)*; Lizhi Qing (Fudan university); Qi Zhang (Fudan University); Jiacheng Ye (fudan<br>university); Hang Yan (Fudan University); zichu fei (FUDAN University); Xuanjing Huang (â€œ Fudan University, Chinaâ€)</p></li></ul><ul><li><p>Incorporating Label Embedding and Feature Augmentation for Multi-Dimensional Classification</p><p>Haobo Wang (Zhejiang University)*; Chen Chen (Zhejiang University); Weiwei Liu (Wuhan University); Ke Chen (<br>Zhejiang University); Tianlei Hu (Zhejiang University); Gang Chen (Zhejiang University)</p><p>é€šè¿‡é›†æˆæ ‡ç­¾ä¿¡æ¯æ¥æ“çºµç‰¹å¾ç©ºé—´çš„ç‰¹å¾å¢å¼ºæ˜¯è§£å†³å¤šç»´åˆ†ç±»ï¼ˆMDCï¼‰é—®é¢˜çš„æœ€æµè¡Œç­–ç•¥ä¹‹ä¸€ã€‚ä½†æ˜¯ï¼Œé¦™è‰ç‰¹å¾å¢å¼ºæ–¹æ³•æ— æ³•è€ƒè™‘ç±»å†…çš„æ’ä»–æ€§ï¼Œå¹¶ä¸”å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„æ–°å‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†æ ‡ç­¾åµŒå…¥å’Œç‰¹å¾å¢å¼ºï¼ˆLEFAï¼‰æŠ€æœ¯æ— ç¼é›†æˆä»¥å­¦ä¹ æ ‡ç­¾ç›¸å…³æ€§ã€‚å…·ä½“åœ°ï¼ŒåŸºäºæ³¨æ„åŠ›åˆ†è§£æœºï¼Œå¼•å…¥äº†äº’ç›¸å…³æ„ŸçŸ¥ç½‘ç»œä»¥å­¦ä¹ ä½ç»´æ ‡ç­¾è¡¨ç¤ºï¼Œå…¶åŒæ—¶æç»˜äº†ç±»é—´ç›¸å…³æ€§å’Œç±»å†…æ’ä»–æ€§ã€‚ç„¶åï¼Œå¯ä»¥å°†å­¦ä¹ åˆ°çš„æ½œåœ¨æ ‡ç­¾çŸ¢é‡ç”¨äºæ‰©å¤§åŸå§‹ç‰¹å¾ç©ºé—´ã€‚</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Papers in IJCAI 2020 (2021.01)</title>
      <link href="/uncategorized/paperlistfile/IJCAI2020/"/>
      <url>/uncategorized/paperlistfile/IJCAI2020/</url>
      
        <content type="html"><![CDATA[<p><a href="http://static.ijcai.org/2020-accepted_papers.html">Link</a></p><span id="more"></span><h2 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h2><ul><li><p><strong>è®¡åˆ’é˜…è¯»</strong> A new attention mechanism to classify multivariate time series</p><p>Yifan Hao, Huiping Cao</p></li><li><p>The Squawk Bot: Joint Learning of Time Series and Text Data Modalities for Automated Financial Information Filtering</p><p>Xuan-Hong Dang, Syed Yousaf Shah, Petros Zerfos</p></li><li><p>A Quantum-inspired Entropic Kernel for Multiple Financial Time Series Analysis</p><p>Lu Bai, Lixin Cui, Yue Wang, Yuhang Jiao, Edwin R. Hancock</p></li><li><p>WATTNet: Learning to Trade FX via Hierarchical Spatio-Temporal Representation of Highly Multivariate Time Series</p><p>Michael Poli, Jinkyoo Park, Ilija Ilievski</p></li><li><p>Fair Division of Time: Multi-layered Cake Cutting</p><p>Hadi Hosseini, Ayumi Igarashi, Andrew Searns</p></li></ul><ul><li><p>A new attention mechanism to classify multivariate time series</p><p>Yifan Hao, Huiping Cao</p></li><li><p>Joint Time-Frequency and Time Domain Learning for Speech Enhancement</p><p>Chuanxin Tang, Chong Luo, Zhiyuan Zhao, Wenxuan Xie, Wenjun Zeng</p></li></ul><ul><li><p>Generating Robust Audio Adversarial Examples with Temporal Dependency</p><p>Hongting Zhang, Pan Zhou, Qiben Yan, Xiao-Yang Liu</p></li></ul><h2 id="missing-value"><a href="#missing-value" class="headerlink" title="missing value"></a>missing value</h2><ul><li><p>A Spatial Missing Value Imputation Method for Multi-view Urban Statistical Data</p><p>Yongshun Gong, Zhibin Li, Jian Zhang, Wei Liu, Bei Chen, Xiangjun Dong</p></li></ul><h2 id="Recurrent"><a href="#Recurrent" class="headerlink" title="Recurrent"></a>Recurrent</h2><ul><li><p>A Structured Latent Variable Recurrent Network with Stochastic Attention for Generating Weibo Comments</p><p>Shijie Yang, Liang Li, Shuhui Wang, Weigang Zhang, Qingming Huang, Qi Tian</p></li><li><p>Recurrent Relational Memory Network for Unsupervised Image Captioning</p><p>Dan Guo, Yang Wang, Peipei Song, Meng Wang</p></li><li><p>Recurrent Dirichlet Belief Networks for interpretable Dynamic Relational Data Modelling</p><p>Yaqiong Li, Xuhui Fan, Ling Chen, Bin Li, Zheng Yu, Scott A. Sisson</p></li></ul><h2 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h2><ul><li><p>Seq-U-Net: A One-Dimensional Causal U-Net for Efficient Sequence Modelling</p><p>Daniel Stoller, Mi Tian, Sebastian Ewert, Simon Dixon</p></li><li><p>Hierarchical Attention Based Spatial-Temporal Graph-to-Sequence Learning for Grounded Video Description</p><p>Kai Shen, Lingfei Wu, Fangli Xu, Siliang Tang, Jun Xiao, Yueting Zhuang</p></li><li><p>Discovering Subsequence Patterns for Next POI Recommendation</p><p>Kangzhi Zhao, Yong Zhang, Hongzhi Yin, Jin Wang, Kai Zheng, Xiaofang Zhou, Chunxiao Xing</p></li><li><p>Multi-Scale Group Transformer for Long Sequence Modeling in Speech Separation</p><p>Yucheng Zhao, Chong Luo, Zheng-Jun Zha, Wenjun Zeng</p></li></ul><h2 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a>Anomaly Detection</h2><ul><li><p>Towards a Hierarchical Bayesian Model of Multi-View Anomaly Detection</p><p>Zhen Wang, Chao Lan</p></li><li><p>Cross-Interaction Hierarchical Attention Networks for Urban Anomaly Prediction</p><p>Chao Huang, Chuxu Zhang, Peng Dai, Liefeng Bo</p></li><li><p>Inductive Anomaly Detection on Attributed Networks</p><p>Kaize Ding, Jundong Li, Nitin Agarwal, Huan Liu</p></li><li><p>Robustness of Autoencoders for Anomaly Detection Under Adversarial Impact</p><p>Adam Goodge, Bryan Hooi, See Kiong Ng, Wee Siong Ng</p></li></ul><h2 id="Interpretable"><a href="#Interpretable" class="headerlink" title="Interpretable"></a>Interpretable</h2><ul><li><p>Explainable Recommendation via Interpretable Feature Mapping and Evaluation of Explainability</p><p>Deng Pan, Xiangrui Li, Xin Li, Dongxiao Zhu</p></li><li><p>Interpretable Models for Understanding Immersive Simulations</p><p>Nicholas Hoernle, Kobi Gal, Barbara Grosz, Leilah Lyons, Ada Ren, Andee Rubin</p></li><li><p>Learning Interpretable Models in the Property Specification Language</p><p>Rajarshi Roy, Dana Fisman, Daniel Neider</p></li><li><p>Learning Interpretable Representations with Informative Entanglements</p><p>Ege BeyazÄ±t, Doruk Tuncel, Xu Yuan, Nian-Feng Tzeng, Xindong Wu</p></li><li><p>Logic Constrained Pointer Networks for Interpretable Textual Similarity</p><p>Subhadeep Maji, Rohan Kumar, Manish Bansal, Kalyani Roy, Pawan Goyal</p></li><li><p>Recurrent Dirichlet Belief Networks for interpretable Dynamic Relational Data Modelling</p><p>Yaqiong Li, Xuhui Fan, Ling Chen, Bin Li, Zheng Yu, Scott A. Sisson</p></li><li><p>Generating Interpretable Poverty Maps using Object Detection in Satellite Images</p><p>Kumar Ayush, Burak Uzkent, Marshall Burke, David Lobell, Stefano Ermon</p></li><li><p>Interpretable Multimodal Learning for Intelligent Regulation in Online Payment Systems</p><p>Shuoyao Wang, Diwei Zhu</p></li></ul><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><ul><li><p>Detecting Adversarial Attacks via Subset Scanning of Autoencoder Activations and Reconstruction Error</p><p>Celia Cintas, Skyler Speakman, Victor Akinwande, William Ogallo, Komminist Weldemariam, Srihari Sridharan, Edward McFowland</p></li><li><p>Aggregating Crowd Wisdom with Side Information via a Clustering-based Label-aware Autoencoder</p><p>Liâ€™ang Yin, Yunfei Liu, Weinan Zhang, Yong Yu</p></li><li><p>Diffusion Variational Autoencoders</p><p>Luis A. Perez Rey, Vlado Menkovski, Jim Portegies</p></li><li><p>Disentangled Variational Autoencoder based Multi-Label Classification with Covariance-Aware Multivariate Probit Model</p><p>Junwen Bai, Shufeng Kong, Carla Gomes</p></li></ul><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><ul><li><p>PewLSTM: Periodic LSTM with Weather-Aware Gating Mechanism for Parking Behavior Prediction</p><p>Feng Zhang, Ningxuan Feng, Yani Liu, Cheng Yang, Jidong Zhai, Shuhao Zhang, Bingsheng He, Jiazao Lin, Xiaoyong Du</p></li></ul><h2 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h2><ul><li><p>Stochastic Batch Augmentation with An Effective Distilled Dynamic Soft Label Regularizer</p><p>Qian Li, Qingyuan Hu, Yong Qi, Saiyu Qi, Jie Ma, Jian Zhang</p></li><li><p>CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot Cross-Lingual NLP</p><p>Libo Qin, Minheng Ni, Yue Zhang, Wanxiang Che</p></li><li><p>Lexical-Constraint-Aware Neural Machine Translation via Data Augmentation</p><p>Guanhua Chen, Yun Chen, Yong Wang, Victor O.K. Li</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pythonè¯­æ³•æŸ¥æ¼è¡¥ç¼º</title>
      <link href="/uncategorized/notes/python_notes/"/>
      <url>/uncategorized/notes/python_notes/</url>
      
        <content type="html"><![CDATA[<p>ä»…è®°å½•ä¸€äº›é¡¹ç›®ä¸­å¸¸ç”¨ä½†æ€»è®°ä¸ä½çš„è¯­æ³•ï¼Œå¸Œæœ›èƒ½æ—©æ—¥è®°ä½ğŸ‘ğŸ»</p><span id="more"></span><p><strong>ç›®å½•</strong></p><ul><li><a href="#%E8%BD%AC%E8%BD%BD">Some reposts</a></li><li><a href="#datetime">Datetime</a></li><li><a href="#pandas">Pandas</a> </li><li><a href="#json">Json</a></li><li><a href="#networkx">Networkx</a></li><li><a href="#warnings">Warnings</a></li><li><a href="#pypandoc">Pandoc (pypandoc)</a></li></ul><h2 id="è½¬è½½"><a href="#è½¬è½½" class="headerlink" title="è½¬è½½"></a>è½¬è½½</h2><ul><li><a href="https://mp.weixin.qq.com/s/dTrW68RjQ6K0nO5CPWUjlA">python å­—ç¬¦ä¸²ç”¨æ³•æ€»ç»“</a></li><li><a href="https://www.cnblogs.com/nmb-musen/p/10856023.html">python å†…ç½®å¼‚å¸¸æ€»ç»“</a></li><li><a href="https://www.jianshu.com/p/21cf48be6bf6">Pythonè¿›åº¦æ¡åº“â€”â€”tqdm</a></li></ul><h2 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h2><p>æ™®é€šçš„åˆ—èµ‹å€¼ï¼Œå®˜æ–¹æ–°ç‰ˆæœ¬æ¨èä½¿ç”¨<code>.loc</code>è¿›è¡Œç´¢å¼•ï¼Œå¦‚</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">df<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token string">'site_id'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span>site_id<span class="token punctuation">]</span> <span class="token operator">*</span> data_len<span class="token comment"># æ—§ç‰ˆæœ¬ï¼Œä¼šæŠ¥warning</span>df<span class="token punctuation">[</span><span class="token string">'site_id'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span>site_id<span class="token punctuation">]</span> <span class="token operator">*</span> data_len<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>æŒ‰æ¡ä»¶èµ‹å€¼DataFrame</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">data_len <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>df<span class="token punctuation">[</span>df<span class="token punctuation">[</span><span class="token string">'ulEutranCellId'</span><span class="token punctuation">]</span> <span class="token operator">==</span> user_id<span class="token punctuation">]</span><span class="token punctuation">)</span>df<span class="token punctuation">.</span>loc<span class="token punctuation">[</span>df<span class="token punctuation">[</span><span class="token string">'ulEutranCellId'</span><span class="token punctuation">]</span> <span class="token operator">==</span> user_id<span class="token punctuation">,</span> <span class="token string">'site_id'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span>site_id<span class="token punctuation">]</span> <span class="token operator">*</span> data_len<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>æ•°æ®æ›¿æ¢</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># df = df.replace(source, target)</span>df <span class="token operator">=</span> df<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>nan<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="datetime"><a href="#datetime" class="headerlink" title="datetime"></a>datetime</h2><p>å­—ç¬¦ä¸²ä¸timestampçš„è½¬æ¢</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> datetime<span class="token keyword">def</span> <span class="token function">convert_str_to_timestamp</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    d_bj <span class="token operator">=</span> datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>strptime<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">'%Y-%m-%d %H:%M:%S'</span><span class="token punctuation">)</span>    d_delta <span class="token operator">=</span> datetime<span class="token punctuation">.</span>timedelta<span class="token punctuation">(</span>days<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> hours<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span> <span class="token comment"># BJ = UTC + 8</span>    d_utc <span class="token operator">=</span> d_bj <span class="token operator">-</span> d_delta <span class="token comment"># UTC = BJ - 8</span>    timestamp <span class="token operator">=</span> <span class="token punctuation">(</span>d_utc <span class="token operator">-</span> datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">(</span><span class="token number">1970</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>total_seconds<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> timestamp<span class="token keyword">def</span> <span class="token function">convert_timestamp_to_str</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    d <span class="token operator">=</span> datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>utcfromtimestamp<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># UTCæ—¶é—´</span>    d_delta <span class="token operator">=</span> datetime<span class="token punctuation">.</span>timedelta<span class="token punctuation">(</span>days<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> hours<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span> <span class="token comment"># BJ = UTC + 8</span>    d_cest <span class="token operator">=</span> d <span class="token operator">+</span> d_delta    time_string <span class="token operator">=</span> d_cest<span class="token punctuation">.</span>strftime<span class="token punctuation">(</span><span class="token string">'%Y-%m-%d %H:%M:%S'</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> time_string<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>ğŸŒŸpandasä¸­çš„<code>pandas.datetime</code>å¯¹è±¡å¯ä»¥é€šè¿‡<code>.to_pydatetime()</code>è½¬ä¸º<code>datetime</code>å¯¹è±¡ï¼Œç„¶åå†ç”¨ä¸Šé¢çš„æ–¹æ³•ã€‚</p><h2 id="json"><a href="#json" class="headerlink" title="json"></a>json</h2><p>Python data to json</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> jsondata <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'name'</span><span class="token punctuation">:</span><span class="token string">'name'</span><span class="token punctuation">,</span><span class="token string">'email'</span><span class="token punctuation">:</span><span class="token string">'email'</span><span class="token punctuation">&#125;</span><span class="token comment"># sort_keys=True: é‡æ–°æ’åˆ—keys</span><span class="token comment"># indent=4: æ‰“å°æ—¶çš„ç¼©è¿›</span>json_str <span class="token operator">=</span> json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span>data<span class="token punctuation">,</span> sort_keys<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> indent<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>json_str <span class="token operator">=</span> json_str<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'unicode_escape'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>json_str<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>json to python data</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>json_path<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    context <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span>    f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="networkx"><a href="#networkx" class="headerlink" title="networkx"></a>networkx</h2><h3 id="Build-a-graph"><a href="#Build-a-graph" class="headerlink" title="Build a graph"></a>Build a graph</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> networkx <span class="token keyword">as</span> nxedge_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment"># [(node_idx1, node_idx2), ...]</span>edge_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'node1'</span><span class="token punctuation">,</span> <span class="token string">'node2'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'node3'</span><span class="token punctuation">,</span> <span class="token string">'node4'</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment"># [('node_name1', 'node_name2'), ...]</span><span class="token comment"># use this</span>g <span class="token operator">=</span> nx<span class="token punctuation">.</span>DiGraph<span class="token punctuation">(</span>edge_list<span class="token punctuation">)</span><span class="token comment"># or</span>g <span class="token operator">=</span> nx<span class="token punctuation">.</span>DiGraph<span class="token punctuation">(</span><span class="token punctuation">)</span>g<span class="token punctuation">.</span>add_edges_from<span class="token punctuation">(</span>edge_list<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Find-root-leaf-node"><a href="#Find-root-leaf-node" class="headerlink" title="Find root/leaf node"></a>Find root/leaf node</h3><p>éå†nodeå¹¶æ£€æŸ¥å‡º/å…¥åº¦</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">find_root_node</span><span class="token punctuation">(</span>g<span class="token punctuation">:</span> nx<span class="token punctuation">.</span>DiGraph<span class="token punctuation">)</span><span class="token punctuation">:</span>    root <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> n<span class="token punctuation">,</span> d <span class="token keyword">in</span> g<span class="token punctuation">.</span>in_degree<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> d <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span> root<span class="token punctuation">.</span>append<span class="token punctuation">(</span>n<span class="token punctuation">)</span>    <span class="token keyword">return</span> root<span class="token keyword">def</span> <span class="token function">find_leaf_node</span><span class="token punctuation">(</span>g<span class="token punctuation">:</span> nx<span class="token punctuation">.</span>DiGraph<span class="token punctuation">)</span><span class="token punctuation">:</span>    leaf <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> n<span class="token punctuation">,</span> d <span class="token keyword">in</span> g<span class="token punctuation">.</span>out_degree<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> d <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span> leaf<span class="token punctuation">.</span>append<span class="token punctuation">(</span>n<span class="token punctuation">)</span>    <span class="token keyword">return</span> leaf<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="æ·±åº¦ä¼˜å…ˆéå†å›¾"><a href="#æ·±åº¦ä¼˜å…ˆéå†å›¾" class="headerlink" title="æ·±åº¦ä¼˜å…ˆéå†å›¾"></a>æ·±åº¦ä¼˜å…ˆéå†å›¾</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># æŸ¥æ‰¾å›¾ä¸­çš„æ ¹ç»“ç‚¹</span>root_node <span class="token operator">=</span> find_root_node<span class="token punctuation">(</span>g<span class="token punctuation">)</span>all_path <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment"># æ„é€ æ·±åº¦ç”Ÿæˆæ ‘</span><span class="token keyword">for</span> root <span class="token keyword">in</span> root_node<span class="token punctuation">:</span>    t <span class="token operator">=</span> nx<span class="token punctuation">.</span>dfs_tree<span class="token punctuation">(</span>g<span class="token punctuation">,</span> source<span class="token operator">=</span>root<span class="token punctuation">)</span>    paths <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>t<span class="token punctuation">.</span>edges<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    all_path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>paths<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>paths<span class="token punctuation">)</span>    <span class="token comment"># paths, e.g. [(0, 1), (1, 2), (2, 3), (3, 4)]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="warnings"><a href="#warnings" class="headerlink" title="warnings"></a>warnings</h2><h3 id="Output-a-warning"><a href="#Output-a-warning" class="headerlink" title="Output a warning"></a>Output a warning</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> warningswarnings<span class="token punctuation">.</span>warn<span class="token punctuation">(</span><span class="token string">'Warning info.'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="Warningé‡å®šå‘"><a href="#Warningé‡å®šå‘" class="headerlink" title="Warningé‡å®šå‘"></a>Warningé‡å®šå‘</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># é‡å®šå‘warningsçš„è¾“å‡ºåˆ°stdout</span><span class="token keyword">def</span> <span class="token function">redirection</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">customwarn</span><span class="token punctuation">(</span>message<span class="token punctuation">,</span> category<span class="token punctuation">,</span> filename<span class="token punctuation">,</span> lineno<span class="token punctuation">,</span> <span class="token builtin">file</span><span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> line<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        sys<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>write<span class="token punctuation">(</span>warnings<span class="token punctuation">.</span>formatwarning<span class="token punctuation">(</span>message<span class="token punctuation">,</span> category<span class="token punctuation">,</span> filename<span class="token punctuation">,</span> lineno<span class="token punctuation">)</span><span class="token punctuation">)</span>    warnings<span class="token punctuation">.</span>showwarning <span class="token operator">=</span> customwarn<span class="token comment"># é‡å®šå‘stdout</span>__console__ <span class="token operator">=</span> sys<span class="token punctuation">.</span>stdout <span class="token comment"># ä»¥å¤‡æ¢å¤stdoutåˆ°console</span>std_out <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>output_dir<span class="token punctuation">,</span> <span class="token string">'somefile.txt'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>sys<span class="token punctuation">.</span>stdout <span class="token operator">=</span> std_outredirection<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># æ¢å¤é‡å®šå‘</span>sys<span class="token punctuation">.</span>stdout <span class="token operator">=</span> __console__<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="pypandoc"><a href="#pypandoc" class="headerlink" title="pypandoc"></a>pypandoc</h2><p>å°†markdownæ–‡æ¡£è½¬æ¢ä¸ºpdfæ–‡ä»¶</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># convert to pdf</span>pypandoc<span class="token punctuation">.</span>convert_file<span class="token punctuation">(</span>  source_file<span class="token operator">=</span>md_file<span class="token punctuation">,</span>  outputfile<span class="token operator">=</span>pdf_file<span class="token punctuation">,</span>  to<span class="token operator">=</span><span class="token string">'pdf'</span><span class="token punctuation">,</span>  <span class="token builtin">format</span><span class="token operator">=</span><span class="token string">'md'</span><span class="token punctuation">,</span>   extra_args<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'-V'</span><span class="token punctuation">,</span> <span class="token string">'geometry:margin=1cm'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> note </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Other Outstanding Papers</title>
      <link href="/uncategorized/paperlistfile/otherconf/"/>
      <url>/uncategorized/paperlistfile/otherconf/</url>
      
        <content type="html"><![CDATA[<h2 id="KDD2019"><a href="#KDD2019" class="headerlink" title="KDD2019"></a>KDD2019</h2><ul><li><p>Robust High Dimensional Stream Classification with Novel Class Detection</p><p>  Zhuoyi WANG (University of Texas at Dallas)*; Zelun Kong (University of Texas at Dallas); Swarup Chandra (University of Texas at Dallas); Hemeng Tao (The University of Texas at Dallas); Latifur Khan (The university of Texas at Dallas)</p></li><li><p>TARDIS: Distributed Indexing Framework for Big Time Series Data</p><p>  liang zhang (WPI)*; Noura S Alghamdi (WPI); Mohamed Y. Eltabakh (Worcester Polytechnic Institute); Elke Rundensteiner (WPI)</p></li></ul><span id="more"></span><ul><li><p>DBSVEC: Density-Based Clustering Using Support Vector Expansion</p><p>  Zhen Zohn Wang (Tsinghua University)*; Rui Zhang (â€ University of Melbourne, Australiaâ€); Jianzhong Qi (The University of Melbourne); Bo Yuan (Tsinghua University)</p></li><li><p>Adaptive Wavelet Clustering for Highly Noisy Data</p><p>  Zengjian Chen (Huazhong University of Science and Technology); Jiayi Liu (University of Massachusetts Amherst)*; Yihe Deng (University of California, Los Angeles); Kun He (Huazhong University of Science and Technology); John E Hopcroft (Cornell University)</p></li><li><p>DBSCAN-MS: Distributed Density-Based Clustering in Metric Spaces</p><p>  Keyu Yang (Zhejiang University); Yunjun Gao (â€ Zhejiang University, Chinaâ€)*; Rui Ma (Zhejiang University); Lu Chen (Aalborg University, Denmark); Sai Wu (Zhejiang Univ); Gang Chen (Zhejiang University)</p></li><li><p><strong>å·²è¯»</strong> Deep Anomaly Detection with Deviation Networks</p><p>  Authors: Guansong Pang (The University of Adelaide);Chunhua Shen (The University of Adelaide);Anton van den Hengel (The University of Adelaide);</p></li><li><p>dEFEND: Explainable Fake News Detection <em>æ²¡æœ‰å€Ÿé‰´æ€§ï¼Œéœ€è¦å€ŸåŠ©æ–°é—»çš„è¯„è®ºæ¥è¯†åˆ«</em></p><p>  Authors: Kai Shu (Arizona State University);Limeng Cui (The Pennsylvania State University);Suhang Wang (The Pennsylvania State University);Dongwon Lee (Penn State Univeristy);Huan Liu (Arizona State University);</p></li><li><p>Fast and Accurate Anomaly Detection in Dynamic Graphs with a Two-Pronged Approach</p><p>  Authors: Minji Yoon (Carnegie Mellon University);Bryan Hooi (Carnegie Mellon University);Kijung Shin (Carnegie Mellon University);Christos Faloutsos (Carnegie Mellon University);</p></li><li><p>Sequential Anomaly Detection using Inverse Reinforcement Learning <em>æœ‰å¾ˆå¤šçš„å…³äºå¼ºåŒ–å­¦ä¹ çš„å†…å®¹ï¼Œä¸å¤ªé€‚åˆ</em></p><p>  Authors: Min-Hwan Oh (Columbia University);Garud Iyengar (Columbia University);</p></li><li><p><strong>å·²è¯»</strong> Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network</p><p>  Authors: Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun and Dan Pei</p></li><li><p>Time-Series Anomaly Detection Service at Microsoft</p><p>  Authors: Hansheng Ren, Bixiong Xu, Yujing Wang, Chao Yi, Congrui Huang, Tony Xing, Xiaoyu Kou, Mao Yang and Jie Tong</p></li><li><p>Sets2Sets: Learning from Sequential Sets with Neural Networks</p><p>  Authors: Haoji Hu (University of Minnesota);Xiangnan He (University of Science and Technology of China);</p></li><li><p>Towards Robust and Discriminative Sequential Data Learning: When and How to Perform Adversarial Training?</p></li></ul><p><em>è¿™ä¸ªçœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸ªæœ‰ç›‘ç£çš„è¿‡ç¨‹ï¼Œç”¨åˆ†ç±»ä»»åŠ¡åšå®éªŒã€‚å¤§æ„æ˜¯ç”Ÿæˆä¸€äº›æ‰°åŠ¨ï¼Œè®©è¿™äº›æ‰°åŠ¨é™„åŠ åœ¨åŸå§‹åºåˆ—ä¸Šï¼Œç„¶åæ•™å¯¼ç½‘ç»œå»è¯†åˆ«å‡ºè¿™äº›æ‰°åŠ¨ã€‚ç½‘ç»œæŒºå¤æ‚çš„</em></p><pre><code>Authors: Xiaowei Jia (University of Minnesota);Sheng Li (University of Georgia);Handong Zhao (Adobe);Sungchul Kim (Adobe);Vipin Kumar (University of Minnesota);</code></pre><ul><li><p>HOLMES: Real-Time APT Detection through Correlation of Suspicious Information Flows <em>çœ‹ä¸€ä¸‹ï¼Œæ€ä¹ˆåšå®æ—¶çš„</em></p></li><li><p>From Anomaly Detection to Rumour Detection using Data Streams of Social Platforms</p><p>  Thanh Tam Nguyen (Ecole Polytechnique Federale de Lausanne), Matthias Weidlich (Humboldt-UniversitÃ¤t zu Berlin), Bolong Zheng (Huazhong University of Science and Technology), Hongzhi Yin (The University of Queensland), Nguyen Quoc Viet Hung (Griffith University), and Bela Stantic (Griffith University)</p></li><li><p>Efficient Discovery of Sequence Outlier Patterns <em>å¥½åƒæ˜¯åšæ—¥å¿—çš„</em></p><p>  Lei Cao (MIT), Yizhou Yan (Worcester Polytechnic Institute), Samuel Madden (MIT), Elke Rundensteiner (Worcester Polytechnic Institute), and Mathan Gopalsamy (Signify Research, Cambridge, MA USA)</p></li><li><p>NETS: Extremely Fast Outlier Detection from a Data Stream via Set-Based Processing</p><p>  Susik Yoon (KAIST), Jae-Gil Lee (KAIST), and Byung Suk Lee (University of Vermont)</p></li><li><p>GRAIL: Efficient Time-Series Representation Learning <em>ç‰¹å¤æ‚ï¼Œå¥½åƒè¿˜æ²¡å•¥ç”¨</em></p><p>  John Paparrizos (University of Chicago) and Michael Franklin (University of Chicago)</p></li></ul><hr><h2 id="ICDE2018"><a href="#ICDE2018" class="headerlink" title="ICDE2018"></a>ICDE2018</h2><ul><li><p>Polygraph: A Plug-n-Play Framework to Quantify Anomalies</p><p>  Yazeed Alabdulkarim (University of Southern California)<br>  Marwan Almaymoni (University of Southern California)<br>  Shahram Ghandeharizadeh (University of Southern California)</p></li><li><p>Efficient Learning Interpretable Shapelets for Accurate Time Series Classification</p><p>  Zicheng Fang (Fudan University)<br>  Peng Wang (Fudan University)<br>  Wei Wang (Fudan University)</p></li><li><p>Generalized Dynamic Time Warping: Unleashing the Warping Power Hidden in Point-Wise Distances</p><p>  Rodica Neamtu (Worcester Polytechnic Institute)<br>  Ramoza Ahsan (Worcester Polytechnic Institute)<br>  Elke Rundensteiner (Worcester Polytechnic Institute)<br>  Gabor Sarkozy (Worcester Polytechnic Institute)<br>  Eamonn Keogh (UC Riverside)<br>  Hoang Anh Dau (UC Riverside)<br>  Cuong Nguyen (Worcester Polytechnic Institute)<br>  Charles Lovering (Worcester Polytechnic Institute)</p></li><li><p>Ensemble Direct Density Ratio Estimation for Multistream Classification</p><p>  Swarup Chandra (University of Texas at Dallas)<br>  Ahsanul Haque (University of Texas at Dallas)<br>  Hemeng Tao (University of Texas at Dallas)<br>  Latifur Khan (University of Texas at Dallas)<br>  Jie Liu (University of Texas at Dallas)<br>  Charu Aggarwal (IBM Research)</p></li></ul><hr><h2 id="ICDE2016"><a href="#ICDE2016" class="headerlink" title="ICDE2016"></a>ICDE2016</h2><ul><li><p>A model-based approach for text clustering with outlier detection. 625-636</p></li><li><p>A new privacy-preserving solution for clustering massively distributed personal times-series. 1370-1373</p></li><li><p>Time-series classification with COTE: The collective of transformation-based ensembles. 1548-1549</p></li><li><p>Fast motif discovery in short sequences. 1158-1169</p></li></ul><hr><h2 id="ICDM2015"><a href="#ICDM2015" class="headerlink" title="ICDM2015"></a>ICDM2015</h2><ul><li><p>Time Series Segmentation to Discover Behavior Switching in Complex Physical Systems</p><p>  Zheng Han, Lehigh University;  Haifeng Chen, NEC Laboratories America;  Tan Yan, NEC Laboratories America;  Geoff Jiang, NEC Laboratories America</p></li><li><p>Missing Value Estimation for Hierarchical Time Series: A Study of Hierarchical Web Traffic</p><p>  Zitao Liu, University of Pittsburgh; Yan Yan, Yahoo! Labs; Milos Hauskrecht, University of Pittsburgh</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep leanring or machine learning on time series with mixed sampling rate</title>
      <link href="/uncategorized/surveys/mixrate/"/>
      <url>/uncategorized/surveys/mixrate/</url>
      
        <content type="html"><![CDATA[<ul><li><p>Autoregressive Convolutional Neural Networks for Asynchronous Time Series: <strong>ICML 2019</strong></p><p><strong>å¼‚ï¼š</strong></p><ol><li>å¤„ç†çš„æ˜¯å¤šç»´æ—¶é—´åºåˆ—æ•°æ®</li><li>é’ˆå¯¹éåŒæ­¥æ•°æ®ï¼Œå¹¶æœªå’Œæˆ‘ä»¬ä¸€æ ·ï¼Œè®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿç”¨äºå¤šç§é‡‡æ ·ç‡çš„æ¨¡å‹ï¼Œè€Œæ˜¯å°†[å¤šç»´æ—¶é—´åºåˆ—ï¼Œåºåˆ—æŒ‡ç¤ºå‰‚ï¼Œé‡‡æ ·é—´éš”]åˆå¹¶æˆä¸ºå¤šç»´æ—¶é—´åºåˆ—ï¼Œç„¶åè¾“å…¥åˆ°ç½‘ç»œä¸­</li><li>ä½¿ç”¨ç±»ä¼¼è‡ªå›å½’æ¨¡å‹çš„ç®—æ³•ï¼Œå¯¹æ—¶é—´åºåˆ—æ¨¡å‹è¿›è¡Œé¢„æµ‹</li><li>ä½¿ç”¨cnnè¿›è¡Œåºåˆ—å­¦ä¹ </li></ol><p><strong>åŒï¼š</strong></p><ol><li>æ²¡ä»€ä¹ˆç›¸åŒçš„</li></ol></li></ul><span id="more"></span><hr><ul><li><p>Hierarchical Deep Generative Models for Multi-Rate Multivariate Time Series: <strong>ICML 2018</strong></p><p><strong>å¼‚ï¼š</strong></p><ol><li>é’ˆå¯¹Lç§ä¸åŒé‡‡æ ·ç‡çš„æ•°æ®ï¼Œå¹¶æœªå’Œæˆ‘ä»¬ä¸€æ ·ï¼Œè®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿç”¨äºå¤šç§é‡‡æ ·ç‡çš„æ¨¡å‹ï¼Œè€Œæ˜¯ï¼š</li><li>ç”¨RNNå°†ä¸åŒé‡‡æ ·ç‡çš„æ—¶é—´åºåˆ—ä¸­çš„æ¯ä¸€ä¸ªå€¼ï¼Œembeddingåˆ°éšç©ºé—´ï¼Œh</li><li>åŸºäºæ·±åº¦é©¬å°”ç§‘å¤«æ¨¡å‹ï¼Œå»ºæ¨¡ä¸Šè¿°éšç©ºé—´å˜é‡çš„ç”Ÿæˆæ¨¡å‹</li><li>åˆ©ç”¨Auxiliary connectionsï¼Œæå–ä¸åŒé‡‡æ ·ç‡çš„æ—¶é—´åºåˆ—çš„é•¿ç¨‹/çŸ­ç¨‹è®°å¿†ï¼Œå³åºåˆ—é—´çš„å…³ç³»</li><li>è¯¥å·¥ä½œåŸºäºä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡æ¨¡æ‹Ÿæ•°æ®çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæ¥å»ºæ¨¡è‹¥é‡‡æ ·ç‡æ•°æ®ä¹‹é—´çš„å…³ç³»</li><li>é’ˆå¯¹éåŒæ­¥æ•°æ®ï¼Œå¹¶æœªå’Œæˆ‘ä»¬ä¸€æ ·ï¼Œè®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿç”¨äºå¤šç§é‡‡æ ·ç‡çš„æ¨¡å‹</li></ol><p><strong>å¼‚ï¼š</strong></p><ol><li>éƒ½è®¾ç½®äº†æŸç§æœºåˆ¶ï¼Œç”¨äºåˆ†åˆ«æå–äº†æ—¶é—´åºåˆ—ä¸­çš„long/short term dependencyï¼Œè¯¥æ–‡ç« ä½¿ç”¨çš„æ–¹æ³•ç±»ä¼¼äºæˆ‘ä»¬æ‰€é‡‡ç”¨çš„è·³è¿æœºåˆ¶</li></ol></li></ul><hr><ul><li><p>INTERPOLATION-PREDICTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES: <strong>ICLR 2019</strong></p><p><strong>å¼‚ï¼š</strong></p><ol><li>å¯¹äºDç§ä¸åŒé‡‡æ ·ç‡çš„æ—¶é—´åºåˆ—ï¼Œå…ˆè®¾ç«‹ä¸€ä¸ªå‡åŒ€çš„å‚è€ƒæ—¶é—´çº¿ï¼Œç„¶ååˆ©ç”¨æ’è¡¥ç½‘ç»œï¼ˆ INTERPOLATION NETWORKS ï¼‰åœ¨ä¸Šè¿°çš„å‚è€ƒæ—¶é—´çº¿ä¸Šè¿›è¡Œæ’è¡¥ï¼Œå°†ä¸åŒé‡‡æ ·ç‡çš„æ—¶é—´åºåˆ—æ’è¡¥æˆå‡åŒ€é‡‡æ ·çš„æ—¶é—´åºåˆ—</li><li>å†æå–å¤šç»´æ—¶é—´åºåˆ—ä¹‹é—´çš„å…³è”å…³ç³»ï¼ˆå»ºæ¨¡å¤šç»´æ—¶é—´åºåˆ—ï¼‰</li><li>ä¸Šä¸€æ­¥çš„è¾“å‡ºè¢«é€å…¥åç»­çš„é¢„æµ‹ç½‘ç»œä¸­è¿›è¡Œè¿›ä¸€æ­¥çš„è®­ç»ƒï¼ˆä»¥é¢„æµ‹ã€åˆ†ç±»ç­‰ä»»åŠ¡ä¸ºè®­ç»ƒç›®æ ‡ï¼‰</li><li>é’ˆå¯¹éåŒæ­¥æ•°æ®ï¼Œå¹¶æœªå’Œæˆ‘ä»¬ä¸€æ ·ï¼Œè®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿç”¨äºå¤šç§é‡‡æ ·ç‡çš„æ¨¡å‹</li></ol><p><strong>å¼‚ï¼š</strong></p><ol><li>éƒ½è®¾ç½®äº†æŸç§æœºåˆ¶ï¼Œç”¨äºåˆ†åˆ«æå–äº†æ—¶é—´åºåˆ—ä¸­çš„long/short term dependency</li></ol></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
            <tag> survey </tag>
            
            <tag> time series </tag>
            
            <tag> mixed sampling rate </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep leanring or machine learning on time series with missing value</title>
      <link href="/uncategorized/surveys/missvalue/"/>
      <url>/uncategorized/surveys/missvalue/</url>
      
        <content type="html"><![CDATA[<p>è¿‘å¹´æ¥ä¸€äº›é¡¶ä¼šæ–‡ç« æˆ–è€…æ˜¯æ¯”è¾ƒé‡è¦çš„æ–‡ç« ï¼Œå¹¶åˆ—å‡ºä¸æˆ‘çš„å·¥ä½œçš„å¼‚åŒç‚¹ï¼ˆNIPS20çš„æŠ•ç¨¿ï¼‰</p><span id="more"></span><hr><ul><li><p>Recurrent Neural Networks for Multivariate Time Series with Missing Values</p><p>Che, Z., Purushotham, S., Cho, K., Sontag, D., &amp; Liu, Y. <em>(2018)</em>.<br>Recurrent neural networks for multivariate time series with missing values. <strong>Scientific reports</strong>, 8(1), 1-12. cite:550</p><p><strong>å¼‚ï¼š</strong></p><ol><li><p>åˆ©ç”¨å†å²å€¼ã€é¢„æµ‹ç¼ºå¤±ä½ç½®çš„å€¼ã€‚é¢„æµ‹å€¼ = beta*ä¸Šä¸€ä¸ªå¯è§‚æµ‹å€¼ + (1-beta)*å†å²å‡å€¼ï¼Œå…¶ä¸­betaæ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ã€‚</p></li><li><p>æ²¡æœ‰åˆ©ç”¨åˆ°æ—¶é—´åºåˆ—çš„åŠ¨æ€ä¿¡æ¯ï¼ˆdynamicï¼‰ã€‚</p></li></ol><p><strong>åŒï¼š</strong></p><ol><li>ç›´æ¥å°†å¤„ç†ç¼ºå¤±å€¼çš„æ“ä½œä¸åç»­çš„ä»»åŠ¡ç»“åˆèµ·æ¥ï¼Œå³ï¼Œå¹¶ä¸æ˜¯ä¸€ä¸ªå•ç‹¬åšç¼ºå¤±å€¼å¡«å……çš„å·¥ä½œï¼Œè€Œæ˜¯å»ºç«‹ä¸€ä¸ªç”¨äºåˆ†ç±»ä¸å›å½’çš„ç½‘ç»œï¼Œè¯¥ç½‘ç»œèƒ½å¤„ç†ç¼ºå¤±å€¼</li></ol></li></ul><hr><ul><li><p>BRITS: Bidirectional Recurrent Imputation for Time Series </p><p>NIPS 2018</p><p><strong>å¼‚ï¼š</strong></p><ol><li>åˆ©ç”¨å†å²å€¼ï¼Œé¢„æµ‹ç¼ºå¤±ä½ç½®çš„å€¼ã€‚é¢„æµ‹å€¼ = f(å†å²è§‚æµ‹å€¼)ï¼Œå¯¹æ—¶é—´åºåˆ—çš„æ€§è´¨ä¸åšä»»ä½•å‡è®¾ï¼Œå…¶ä¸­fæ˜¯å¯å­¦ä¹ çš„éçº¿æ€§æ˜ å°„ã€‚<ol><li>tæ—¶åˆ»å­˜åœ¨ä¸€ä¸ªè§‚æµ‹å€¼çš„æ—¶å€™ï¼Œç›´æ¥å°†è§‚æµ‹xtå€¼ç”¨äºåç»­ä»»åŠ¡ï¼Œå¹¶å¯¹xtåŸºäºå†å²æ•°æ®åšä¸€ä¸ªé¢„æµ‹ï¼Œè¯¥é¢„æµ‹ç”¨äºç›‘ç£ç¼ºå¤±å€¼çš„å¡«å……ã€‚</li><li>tæ—¶åˆ»æ˜¯ç¼ºå¤±å€¼çš„æ—¶å€™ï¼Œç½‘ç»œç›´æ¥åŸºäºå†å²å€¼åšé¢„æµ‹x^tï¼Œå¹¶å°†è¯¥é¢„æµ‹å€¼ç”¨äºåç»­ä»»åŠ¡ã€‚</li></ol></li></ol><p><strong>åŒï¼š</strong></p><ol><li>ç›´æ¥å°†å¤„ç†ç¼ºå¤±å€¼çš„æ“ä½œä¸åç»­çš„ä»»åŠ¡ç»“åˆèµ·æ¥ï¼ˆåŒä¸Šï¼‰ã€‚</li><li>åœ¨å¡«å……ï¼ˆæˆ–åç»­ä»»åŠ¡ï¼‰çš„æ—¶å€™ï¼Œè€ƒè™‘åˆ°äº†æ—¶é—´åºåˆ—çš„åŠ¨æ€ä¿¡æ¯ï¼ˆdynamicï¼‰ã€‚</li></ol></li></ul><hr><ul><li><p>Multivariate Time Series Imputation with Generative Adversarial Networks </p><p>NIPS 2018</p><p><strong>å¼‚ï¼š</strong></p><ol><li>ä»ä»»åŠ¡çš„è§’åº¦å‡ºå‘ï¼Œä¸æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼šå¯¹äºä¸€ä¸ªé¢„æµ‹é—®é¢˜ï¼Œéœ€è¦å…ˆè®­ç»ƒä¸€ä¸ªç½‘ç»œç”¨äºæ•°æ®è¡¥å…¨ï¼Œå†è®­ç»ƒä¸€ä¸ªé¢„æµ‹ç½‘ç»œç”¨äºé¢„æµ‹ã€‚</li><li>æ¨¡å‹æ€»çš„æ¶æ„æ˜¯ä¸€ä¸ªGANï¼Œå…¶ä¸­Gä¸Déƒ½æ˜¯ç”±GRUIåœ¨ç»„æˆçš„ï¼ˆGRUIå¯ä»¥æ¥æ”¶æœ‰ç¼ºå¤±å€¼çš„æ—¶é—´åºåˆ—ä¸ä¸€ä¸ªæ ‡å¿—ç€delayçš„å˜é‡ï¼Œ<br>å¹¶æ ¹æ®å†å²å€¼è¾“å‡ºå¯¹ç¼ºå¤±ä½ç½®è¿›è¡Œè¡¥å…¨ï¼‰ã€‚<ul><li>G: åœ¨ä¸€ä¸ªæ­£æ€åˆ†å¸ƒä¸­é‡‡é›†ä¸€ä¸ªå™ªå£°å‘é‡zï¼Œç„¶åé€å…¥Gï¼Œè¾“å‡ºçš„æ˜¯ä¸€ä¸ªå®Œæ•´çš„åºåˆ—ï¼ŒGçš„æ¶æ„åº”å½“æ˜¯ä¸€ä¸ªRNN</li><li>D: ä½¿ç”¨ç½‘ç»œå°†å®Œæ•´ä¸éå®Œæ•´çš„åºåˆ—å‹ç¼©åˆ°éšç©ºé—´ï¼Œå¹¶åˆ¤æ–­å…¶ç›¸ä¼¼åº¦</li></ul></li></ol><p><strong>åŒï¼š</strong></p><ol><li>åœ¨å¡«å……ï¼ˆæˆ–åç»­ä»»åŠ¡ï¼‰çš„æ—¶å€™ï¼Œè€ƒè™‘åˆ°äº†æ—¶é—´åºåˆ—çš„åŠ¨æ€ä¿¡æ¯ï¼ˆdynamicï¼‰</li></ol></li></ul><hr><ul><li><p>E2GAN: End-to-End Generative Adversarial Network for Multivariate Time Series Imputation (IJCAI 2019)</p><p><strong>å¼‚ï¼š</strong></p><ol><li>ä»ä»»åŠ¡çš„è§’åº¦å‡ºå‘ï¼Œä¸æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼ˆåŒä¸Šï¼‰ã€‚</li><li>æ¨¡å‹æ€»çš„æ¶æ„æ˜¯ä¸€ä¸ªGANï¼Œå…¶ä¸­Gä¸Déƒ½æ˜¯ç”±GRUIåœ¨ç»„æˆçš„ï¼ˆGRUIå¯ä»¥æ¥æ”¶æœ‰ç¼ºå¤±å€¼çš„æ—¶é—´åºåˆ—ä¸ä¸€ä¸ªæ ‡å¿—ç€delayçš„å˜é‡ï¼Œ<br>å¹¶æ ¹æ®å†å²å€¼è¾“å‡ºå¯¹ç¼ºå¤±ä½ç½®è¿›è¡Œè¡¥å…¨ï¼‰<ul><li>Gï¼šå…ˆåœ¨æœ‰ç¼ºå¤±å€¼çš„ä½ç½®ä¸Šå¡«å……ä¸€äº›å™ªéŸ³ï¼Œç„¶åé€å…¥Gï¼Œè¾“å‡ºçš„æ˜¯ä¸€ä¸ªç»è¿‡è¡¥å…¨åçš„æ—¶é—´åºåˆ—ï¼ŒGçš„æ¶æ„ä¸ºseq2seq</li><li>Dï¼šä½¿ç”¨ç½‘ç»œå°†å®Œæ•´ä¸éå®Œæ•´çš„åºåˆ—å‹ç¼©åˆ°éšç©ºé—´ï¼Œå¹¶åˆ¤æ–­å…¶ç›¸ä¼¼åº¦</li></ul></li></ol><p><strong>åŒï¼š</strong></p><ol><li>åœ¨å¡«å……ï¼ˆæˆ–åç»­ä»»åŠ¡ï¼‰çš„æ—¶å€™ï¼Œè€ƒè™‘åˆ°äº†æ—¶é—´åºåˆ—çš„åŠ¨æ€ä¿¡æ¯ï¼ˆdynamicï¼‰</li></ol></li></ul><hr><ul><li><p>Temporal Belief Memory: Imputing Missing Data during RNN Training </p><p>IJCAI 2018</p><p><strong>å¼‚ï¼š</strong></p><ol><li><p>æå‡ºäº†ä¸€ç§å•å…ƒå«åšTBMï¼ŒTBMç”¨äºæ¥æ”¶å¸¦æœ‰ç¼ºå¤±å€¼çš„åºåˆ—ï¼Œå¹¶å¯¹ç¼ºå¤±ä½ç½®è¿›è¡Œå¡«å……ã€‚TBMæ˜¯è¿æ¥åœ¨LSTM cellä¹‹å‰çš„ï¼Œ<br>åœ¨æœ‰ç¼ºå¤±å€¼çš„æ—¶å€™ï¼Œå°†TBMçš„è¾“å‡ºé€å…¥LSTMï¼Œåœ¨æ²¡æœ‰ç¼ºå¤±å€¼çš„æ—¶å€™ï¼Œå°†åŸå§‹çš„æ—¶é—´åºåˆ—é€å…¥LSTM</p></li><li><p>TBMä¸­æœ‰ä¸¤ç§é—¨ï¼šmissing gate(æŒ‡ç¤ºæŸä¸ªæ—¶åˆ»æ˜¯å¦å­˜åœ¨ç¼ºå¤±å€¼), belief gateï¼ˆæŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨ä¸Šä¸€æ—¶åˆ»çš„å€¼ä½œä¸ºå¡«å……ï¼Œ<br>ä½†æ˜¯ä½¿ç”¨å½“å‰æ—¶åˆ»çš„å…¶ä»–featureçš„å‡å€¼ä½œä¸ºå¡«å……ï¼‰ã€‚</p><ol><li>å½“æœ‰ä¸€ä¸ªè§‚æµ‹å€¼çš„æ—¶å€™ï¼Œmissing gate = 0ï¼Œè§‚æµ‹å€¼ç›´æ¥è¿›å…¥LSTM</li><li>å½“æ²¡è§‚æµ‹å€¼çš„æ—¶å€™ï¼Œmissing gate = 1, æ­¤æ—¶belief gateè¿›è¡Œä¼°ç®—ï¼ŒæŒ‡ç¤ºæ˜¯å¦è®©ç¥ç»å…ƒæ”¾ç”µ(ç”Ÿç‰©å­¦çš„è§’åº¦)<ol><li>ç¥ç»å…ƒä¸æ”¾ç”µï¼šbelief gate = 0ï¼Œå‘LSTMè¾“å…¥ä¸Šä¸€æ—¶åˆ»çš„å€¼</li><li>ç¥ç»å…ƒæ”¾ç”µï¼šbelief gate = 1, å°†å½“å‰æ—¶åˆ»çš„å…¶ä»–featureçš„å‡å€¼é€å…¥LSTM</li></ol></li></ol></li><li><p>BMåœ¨è®¡ç®—å¡«å……å€¼çš„æ—¶å€™ï¼Œæ²¡æœ‰ä½¿ç”¨æ—¶é—´åºåˆ—çš„åŠ¨æ€ä¿¡æ¯ï¼ˆdynamicï¼‰ï¼Œä»…å•çº¯ä½¿ç”¨äº†ä¸Šä¸€æ—¶åˆ»çš„å€¼/å½“å‰æ—¶åˆ»çš„å…¶ä»–featureçš„å‡å€¼ä½œä¸ºå¡«å…….</p></li><li><p>ä»–åœ¨é€‰æ‹©å¡«å……ç­–ç•¥çš„æ—¶å€™ï¼ˆé€‰æ‹©å‘LSTMè¾“å…¥ä¸Šä¸€æ—¶åˆ»çš„å€¼ æˆ–å°†å½“å‰æ—¶åˆ»çš„å…¶ä»–featureçš„å‡å€¼é€å…¥LSTMï¼‰ï¼Œ<br>ä»…è€ƒè™‘äº†ç©ºç¼ºæ—¶åˆ»ä¸ä¸Šä¸€æ¬¡è§‚æµ‹å€¼ä¹‹é—´çš„å·®å€¼ï¼Œå¹¶æ²¡æœ‰è€ƒè™‘ä»»ä½•çš„å†å²æ•°æ®ä¸æ—¶é—´åºåˆ—åŠ¨æ€.</p></li></ol><p><strong>åŒï¼š</strong></p><ol><li>ç›´æ¥å°†å¤„ç†ç¼ºå¤±å€¼çš„æ“ä½œä¸åç»­çš„ä»»åŠ¡ç»“åˆèµ·æ¥.</li></ol></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
            <tag> survey </tag>
            
            <tag> time series </tag>
            
            <tag> missing value </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Augmentation</title>
      <link href="/uncategorized/surveys/dataaug/"/>
      <url>/uncategorized/surveys/dataaug/</url>
      
        <content type="html"><![CDATA[<p>Data Augmentation (Specifically for rhythmic time series, e.g. ECG, EEG, etc..)</p><p>ä¸€èˆ¬æ¥è®²ï¼Œé’ˆå¯¹ECG/EEGæ•°æ®çš„åˆ†ç±»ã€å¼‚å¸¸æ£€æµ‹ç­‰ä»»åŠ¡ï¼Œéƒ½å­˜åœ¨ä¸¥é‡çš„æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚è¿™äº›æ•°æ®å¢å¼ºæ–¹æ³•ä¸€èˆ¬ç”¨äºä¸°å¯Œæ ·æœ¬è¾ƒå°‘çš„ç±»åˆ«ã€‚<br>å¤§é‡çš„å·¥ä½œé›†ä¸­äºä½¿ç”¨GAN/VAEç­‰ç”Ÿæˆæ¨¡å‹å»ºç«‹æ•°æ®çš„åˆ†å¸ƒï¼Œç„¶åä»åˆ†å¸ƒä¸­ç”Ÿæˆé‡‡æ ·ã€‚ä»¥ä¸‹çš„æ–¹æ³•æ˜¯ä¸€äº›åœ¨æ˜¾ç©ºé—´æ•°æ®ä¸Šè¿›è¡Œæ•°æ®å¢å¼ºçš„ç®€å•æ–¹æ³•ã€‚</p><span id="more"></span><hr><h2 id="æœ‰ç”¨çš„æ–¹æ³•"><a href="#æœ‰ç”¨çš„æ–¹æ³•" class="headerlink" title="æœ‰ç”¨çš„æ–¹æ³•"></a>æœ‰ç”¨çš„æ–¹æ³•</h2><ul><li><p>BeatGAN: Anomalous Rhythm Detection using Adversarially Generated Time Series</p><p>  Zhou, B., Liu, S., Hooi, B., Cheng, X., &amp; Ye, J. <em>(2019, August)</em>. BeatGAN: Anomalous Rhythm Detection using Adversarially Generated Time Series. <strong>In IJCAI</strong> (pp. 4433-4439).</p><p>  Time warpingï¼Œæ¨¡æ‹ŸECGæ•°æ®ä¸­çš„ç¬æ—¶å¢é€Ÿä¸å‡é€Ÿï¼Œå…·ä½“æ“ä½œçš„æ—¶å€™ï¼Œéšæœºåˆ é™¤Nä¸ªç‚¹ï¼Œç„¶åå†éšæœºå¹³å‡æ’å€¼Nä¸ªç‚¹ã€‚</p></li><li><p>Convolutional recurrent neural networks for electrocardiogram classification</p><p>Zihlmann, M., Perekrestenko, D., &amp; Tschannen, M. <em>(2017, September)</em>. Convolutional recurrent neural networks for electrocardiogram classification. <strong>In 2017 Computing in Cardiology (CinC)</strong> (pp. 1-4). IEEE. cite: 82</p><p>ä¸¤ç§å¢å¼ºæ–¹æ³•ï¼š<strong>1ï¼‰</strong>åœ¨ECGä¸­ä½¿å¾—ä¸€æ®µæ•°å€¼ä¸º0ï¼Œä»¥æ¨¡æ‹ŸECGä¿¡å·çš„ä¸¢å¤±ï¼ˆè®¾å¤‡æ¥è§¦ä¸è‰¯ç­‰ï¼‰ï¼Œä»…é€‚ç”¨äºECGæ•°æ®ï¼ˆè€Œä¸”éœ€è¦ç¡®è®¤ä¸æ•°æ®é›†åŸå§‹çš„åˆ†å¸ƒæ˜¯å¦ä¸€è‡´ï¼Œå¦‚åŸå§‹æ•°æ®é›†å¹¶ä¸å­˜åœ¨è¿™ç§æƒ…å†µï¼Œåˆ™ä¹Ÿè®¸ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼‰ã€‚<br><strong>2ï¼‰</strong>æ¨¡æ‹ŸECGæ•°æ®ä¸­è¿‡é«˜æˆ–è€…è¿‡ä½çš„æ–°å¿ƒç‡ï¼ˆæ„Ÿè§‰ä¸BeatGANä¸­çš„ç›®çš„ä¸€è‡´ï¼‰</p></li><li><p>An Amplitudes-Perturbation Data Augmentation Method in Convolutional Neural Networks for EEG Decoding</p><p>Zhang, X. R., Lei, M. Y., &amp; Li, Y. <em>(2018, August)</em>. An amplitudes-perturbation data augmentation method in convolutional neural networks for EEG decoding. <strong>In 2018 5th International Conference on Information, Cybernetics, and Computational Social Systems (ICCSS)</strong> (pp. 231-235). IEEE. cite: 2</p><p>é’ˆå¯¹EEGæ•°æ®ï¼Œå°†æ•°æ®ç”¨STFTè½¬æ¢åˆ°é¢‘åŸŸï¼Œç„¶ååœ¨æŒ¯å¹…ä¸­å¢åŠ ä¸€äº›æ‰°åŠ¨ï¼ˆé‡‡æ ·è‡ªé«˜æ–¯å™ªå£°ï¼‰ï¼Œåœ¨è½¬æ¢å›æ¥åˆ°åŸå§‹åŸŸã€‚</p></li><li><p>Data Augmentation for EEG-Based Emotion Recognition with Deep Convolutional Neural Networks</p><p>Wang, F., Zhong, S. H., Peng, J., Jiang, J., &amp; Liu, Y. <em>(2018, February)</em>. Data augmentation for eeg-based emotion recognition with deep convolutional neural networks. <strong>In International Conference on Multimedia Modeling</strong> (pp. 82-93). Springer, Cham.</p><p>åœ¨å›¾åƒå¤„ç†ä¸­ä½¿ç”¨ä¸¤ç§åŸºæœ¬çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼šå‡ ä½•å˜æ¢å’Œå™ªå£°æ·»åŠ ã€‚åŒ…æ‹¬ä½ç§»ï¼Œæ¯”ä¾‹ï¼Œæ—‹è½¬/åå°„ç­‰åœ¨å†…çš„å‡ ä½•å˜æ¢å¹¶ä¸ç›´æ¥é€‚åˆäºæ‰©å¤§æˆ‘ä»¬çš„EEGæ•°æ®ã€‚ä¸å›¾åƒç›¸æ¯”ï¼ŒEEGä¿¡å·æ˜¯éšæ—¶é—´å˜åŒ–çš„è¿ç»­ä¿¡å·ã€‚å³ä½¿æ‰§è¡Œäº†ç‰¹å¾æå–ï¼Œå…¶ç‰¹å¾ä»ç„¶æ˜¯æ—¶é—´åºåˆ—ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æ—‹è½¬æˆ–ç§»åŠ¨EEGæ•°æ®ï¼Œåˆ™æ—¶åŸŸç‰¹å¾å°†è¢«ç ´åã€‚ä¸ºé¿å…æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è€ƒè™‘ä½¿ç”¨å™ªå£°æ·»åŠ æ–¹æ³•æ¥å¢å¼ºEEGæ ·æœ¬ã€‚</p></li></ul><hr><h2 id="ä¸é”™ä½†æ˜¯å¯¹æˆ‘å‚è€ƒæ„ä¹‰ä¸å¤§çš„æ–¹æ³•"><a href="#ä¸é”™ä½†æ˜¯å¯¹æˆ‘å‚è€ƒæ„ä¹‰ä¸å¤§çš„æ–¹æ³•" class="headerlink" title="ä¸é”™ä½†æ˜¯å¯¹æˆ‘å‚è€ƒæ„ä¹‰ä¸å¤§çš„æ–¹æ³•"></a>ä¸é”™ä½†æ˜¯å¯¹æˆ‘å‚è€ƒæ„ä¹‰ä¸å¤§çš„æ–¹æ³•</h2><ul><li><p>A Novel Deep Learning Approach With Data Augmentation to Classify Motor Imagery Signals</p><p>Zhang, Z., Duan, F., Sole-Casals, J., Dinares-Ferran, J., Cichocki, A., Yang, Z., &amp; Sun, Z. <em>(2019)</em>. A novel deep learning approach with data augmentation to classify motor imagery signals. <strong>IEEE Access</strong>, 7, 15945-15954. cite: 28</p><p>é¦–å…ˆå°†æ•°æ®è¿›è¡Œåˆ†è§£ï¼Œåˆ†è§£ä¸ºå¤šä¸ªå­åºåˆ—ï¼ˆå¦‚IMF1ï¼ŒIMF2ï¼ŒIMF3ç­‰ï¼‰ï¼ŒåŸå§‹åºåˆ—å¯ä»¥è§†ä¸ºå¤šä¸ªå­åºåˆ—çš„åŠ å’Œã€‚<br>æ–°çš„æ•°æ®é€šè¿‡éšæœºç»„åˆä¸Šè¿°çš„å­åºåˆ—å¹¶å°†å…¶åŠ å’Œæ¥åˆ›é€ ï¼ˆIMF1(2)+IMF2(3)+IMF3(1)ï¼‰</p></li><li><p>Comparing feature-based classifiers and convolutional neural networks to detect arrhythmia from short segments of ECG</p><p>Andreotti, F., Carr, O., Pimentel, M. A., Mahdi, A., &amp; De Vos, M. <em>(2017, September)</em>.<br>Comparing feature-based classifiers and convolutional neural networks to detect arrhythmia from short segments of ECG. <strong>In 2017 Computing in Cardiology (CinC)</strong> (pp. 1-4). IEEE. cite: 53</p><p>æ•°æ®é›†ä¸­å­˜åœ¨ä¸€ä¸ªå™ªéŸ³ç±»åˆ«ï¼ˆä¸ªäººç†è§£æ˜¯ç”±äºè®¾å¤‡ç­‰åŸå› å™ªå£°çš„å™ªéŸ³ï¼Œæ— æ³•å°†å…¶åˆ†é…åˆ°å…·ä½“å‘çš„ECGç±»åˆ«ä¸­ï¼‰<br>å°†è¿™äº›å™ªéŸ³é™„åŠ åˆ°åŸå§‹æ•°æ®ä¸Šä»¥ç”Ÿæˆæ–°çš„æ ·æœ¬</p></li></ul><hr><h2 id="å¾…ç¡®å®šæ‰€æåŠçš„æ–¹æ³•æ˜¯å¦æœ‰ç”¨"><a href="#å¾…ç¡®å®šæ‰€æåŠçš„æ–¹æ³•æ˜¯å¦æœ‰ç”¨" class="headerlink" title="å¾…ç¡®å®šæ‰€æåŠçš„æ–¹æ³•æ˜¯å¦æœ‰ç”¨"></a>å¾…ç¡®å®šæ‰€æåŠçš„æ–¹æ³•æ˜¯å¦æœ‰ç”¨</h2><ul><li>ECG Arrhythmias Detection Using Auxiliary Classifier Generative Adversarial Network and Residual NetworkWang, P., Hou, B., Shao, S., &amp; Yan, R. <em>(2019)</em>. ECG arrhythmias detection using auxiliary classifier generative adversarial network and residual network. <strong>IEEE Access</strong>, 7, 100910-100922. cite: 3æ–‡ç« æœ¬èº«æå‡ºçš„æ–¹æ³•æ˜¯åŸºäºGANçš„ï¼Œä½†åœ¨related workä¸­æåŠäº†ä¸€äº›æ— éœ€æ·±åº¦ç½‘ç»œçš„ä¼ ç»Ÿå¢å¼ºæ–¹æ³•</li><li>A novel data augmentation method to enhance deep neural networks for detection of atrial fibrillationCao, P., Li, X., Mao, K., Lu, F., Ning, G., Fang, L., &amp; Pan, Q. <em>(2020)</em>. A novel data augmentation method to enhance deep neural networks for detection of atrial fibrillation. <strong>Biomedical Signal Processing and Control</strong>, 56, 101675. cite: 4æ–‡ç« æœ¬èº«æ‰€æå‡ºçš„å¢å¼ºæ–¹æ³•ä»…é€‚ç”¨äºECGæ•°æ®ï¼ˆéœ€è¦æå–æ•°æ®çš„QPVæ®µç­‰ï¼‰ï¼Œä½†åœ¨related workä¸­æåŠäº†ä¸€äº›é€šç”¨çš„å¢å¼ºæ–¹æ³•</li></ul><hr><h2 id="ä¸€äº›-â€œæ²¡ä»€ä¹ˆç”¨â€-çš„æ–¹æ³•"><a href="#ä¸€äº›-â€œæ²¡ä»€ä¹ˆç”¨â€-çš„æ–¹æ³•" class="headerlink" title="ä¸€äº› â€œæ²¡ä»€ä¹ˆç”¨â€ çš„æ–¹æ³•"></a>ä¸€äº› â€œæ²¡ä»€ä¹ˆç”¨â€ çš„æ–¹æ³•</h2><ul><li>ECG arrhythmia classification using a 2-D convolutional neural networkJun, T. J., Nguyen, H. M., Kang, D., Kim, D., Kim, D., &amp; Kim, Y. H. <em>(2018)</em>. ECG arrhythmia classification using a 2-D convolutional neural network. <strong>arXiv preprint</strong> arXiv:1804.06812. cite: 45é¦–å…ˆå°†ECGæ•°æ®è½¬æ¢ä¸º2Dçš„å›¾åƒæ•°æ®ï¼ˆ128*128ï¼‰ï¼Œç„¶åä½¿ç”¨CNNå¯¹æ•°æ®è¿›è¡Œåˆ†ç±»ã€‚å…¶æ•°æ®å¢å¼ºæ–¹æ³•æ˜¯åŸå§‹æ•°æ®ä½¿ç”¨ä¸€å®šç­–ç•¥è¿›è¡Œè£å‰ª<br>ï¼ˆå¦‚è£å‰ªtop right / top leftéƒ¨åˆ†ç­‰ï¼‰</li></ul><ul><li>Rotational data augmentation for electroencephalographic dataKrell, M. M., &amp; Kim, S. K. <em>(2017, July)</em>. Rotational data augmentation for electroencephalographic data. <strong>In 2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</strong> (pp. 471-474). IEEE. cite: 12æœ¬æ–‡è¯´çš„æ˜¯EEGæ•°æ®çš„å¢å¼ºï¼ŒEEGæ•°æ®ç”±äºé‡‡é›†å™¨æ”¾ç½®åœ¨äººè„‘ä¸Šçš„ä½ç½®ä¸åŒï¼Œè€Œä¼šäº§ç”Ÿæ•°æ®åç§»ã€‚<br>ä½†å…¶æ–¹æ³•ä»…æ˜¯å°†EEGé‡‡é›†å™¨ä»¥å¤šç§è§’åº¦æ”¾ç½®å¹¶é‡‡é›†æ•°æ®ä»¥ä½œä¸ºå¢å¼ºæ•°æ®ã€‚</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
            <tag> survey </tag>
            
            <tag> data augmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Some Well-known Papers</title>
      <link href="/uncategorized/paperlistfile/other_papers/"/>
      <url>/uncategorized/paperlistfile/other_papers/</url>
      
        <content type="html"><![CDATA[<p> ä¸€äº›ç»å…¸çš„è®ºæ–‡</p><h2 id="VAEs"><a href="#VAEs" class="headerlink" title="VAEs"></a>VAEs</h2><ul><li><p>Auto-Encoding Variational Bayes</p><p>æå‡ºVAE</p></li><li><p>Wasserstein Auto-Encoders</p><p>æå‡ºWAE</p></li></ul><span id="more"></span><ul><li><p>ICLR20 - FROM VARIATIONAL TO DETERMINISTIC AUTOENCODERS</p><p>Github: <a href="https://github.com/ParthaEth/Regularized_autoencoders-RAE-">Link</a></p><p><strong>è§£å†³çš„é—®é¢˜</strong>: å»ºæ¨¡æ•°æ®çš„éšç©ºé—´åˆ†å¸ƒæ¨¡å‹ï¼Œæå‡ºRAEï¼ˆRegularized Autoencoderï¼‰ã€‚</p><p><strong>åˆ›æ–°ä¸ç‹¬ç‰¹</strong>: RAEæ— éœ€å‡è®¾æ•°æ®çš„éšç©ºé—´åˆ†å¸ƒç¬¦åˆä»»ä½•å…ˆéªŒåˆ†å¸ƒã€‚</p><p><strong>é‡‡ç”¨çš„æ–¹æ³•</strong></p><ol><li>å°†ä¼ ç»ŸVAEä¸­ï¼Œä»å…ˆéªŒåˆ†å¸ƒä¸­é‡‡é›†éšæœºæ•°æ®é€å…¥decoderçš„æ“ä½œï¼Œè§†ä¸ºä¸€ç§decoderçš„æ­£åˆ™é¡¹ï¼Œç”¨äºè¾…åŠ©å­¦ä¹ ä¸€ä¸ªå¹³æ»‘çš„åˆ†å¸ƒã€‚</li><li>å°†æ•°æ®çš„éšç©ºé—´åˆ†å¸ƒè§†ä¸ºæ‹¥æœ‰æŸå‡å€¼ä¸å›ºå®šæ–¹å·®çš„ä¸€ä¸ªæœªçŸ¥åˆ†å¸ƒï¼Œå¾—åˆ°æ•°æ®éšç©ºé—´è¡¨è¾¾åï¼Œå†ä¼°è®¡ä¸€ä¸ªåéªŒçš„æ•°æ®åˆ†å¸ƒã€‚</li></ol><p><strong>ä¸ºä½•é€‰æ‹©/å¦‚ä½•åº”ç”¨</strong>: æœ¬æ–‡çš„æ–¹æ³•ä½œä¸ºä¸€ç§å¼ºå¤§çš„æ•°æ®åˆ†å¸ƒä¼°è®¡æ–¹æ³•ï¼Œä¸é¢„è®¾ä»»ä½•å…ˆéªŒåˆ†å¸ƒï¼Œåœ¨å…¶ä»–å¤æ‚çš„å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šèƒ½å¤Ÿå¯¹æ•°æ®è¿›è¡Œå»ºæ¨¡ã€‚</p><p><strong>æ•°æ®é›†</strong></p><ol><li>MNIST</li><li>CIFAR</li><li>CELEBAï¼ˆäººåƒå±æ€§æ•°æ®é›†ï¼‰</li></ol></li><li><p>NIPS18 - Gaussian Process Prior Variational Autoencoders</p><p>æå‡ºGPVAEï¼Œå³éšç©ºé—´å˜é‡éµä»é«˜æ–¯è¿‡ç¨‹ï¼Œè€Œéä¸€ä¸ªæ—¶ä¸å˜çš„åˆ†å¸ƒã€‚</p></li><li><p>ICLR19 - INTERPRETABLE DISCRETE REPRESENTATION LEARNING ON TIME SERIES</p><p>æå‡ºSOM-VAE, <a href="https://github.com/ratschlab/SOM-VAE">Github</a></p><blockquote><p>High-dimensional time series are common in many domains. Since human cognition is<br>not optimized to work well in high-dimensional spaces, these areas could benefit from<br>interpretable low-dimensional representations. However, most representation learning<br>algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings<br>from data features to salient properties of the representation and non-smoothness over time.<br>To address this problem, we propose a new representation learning framework building on<br>ideas <strong>from interpretable discrete dimensionality reduction and deep generative modeling.</strong><br>This framework allows us to learn discrete representations of time series, which give rise to<br>smooth and interpretable embeddings with superior clustering performance. <strong>We introduce<br>a new way to overcome the non-differentiability in discrete representation learning and<br>present a gradient-based version of the traditional self-organizing map algorithm that is<br>more performant than the original.</strong> Furthermore, to allow for a probabilistic interpretation of<br>our method, we integrate a Markov model in the representation space. This model uncovers<br>the temporal transition structure, improves clustering performance even further and provides<br>additional explanatory insights as well as a natural representation of uncertainty.<br>We evaluate our model in terms of clustering performance and interpretability on static<br>(Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a<br>chaotic Lorenz attractor system with two macro states, as well as on a challenging real world<br>medical time series application on the eICU data set. Our learned representations compare<br>favorably with competitor methods and facilitate downstream tasks on the real world data.</p><p>â€¦</p><p><em>Time series from the data space [green] are encoded<br>by a neural network [black] time-point-wise into the latent space. The latent data manifold is approximated<br>with a self-organizing map (SOM) [red]. In order to achieve a discrete representation, every latent data point<br>($z_e$) is mapped to its closest node in the SOM ($z_q$). A Markov transition model [blue] is learned to predict<br>the next discrete representation ($z_q ^{t+1}$) given the current one ($z_q ^{t}$). The discrete representations can then be<br>decoded by another neural network back into the original data space.</em></p></blockquote></li><li><p>NIPS17 - Neural Discrete Representation Learning</p><p>Github: <a href="https://github.com/nakosung/VQ-VAE">Link</a></p><p>A useful blog: <a href="http://ameroyer.github.io/projects/2019/08/20/VQVAE.html">Link</a></p><p>Some code and its explaination: <a href="http://ameroyer.github.io/projects/2019/08/20/VQVAE.html">Link</a></p><p><strong>è§£å†³çš„é—®é¢˜</strong>: å­¦ä¹ æ•°æ®çš„éšç©ºé—´ç¦»æ•£åŒ–è¡¨ç¤ºï¼Œè¯¥ç¦»æ•£åŒ–è¡¨ç¤ºå¯ä»¥ç”¨äºå‹ç¼©ç¼–ç ã€ç”Ÿæˆæ›´æ¸…æ™°çš„å›¾åƒç­‰ã€‚</p><p><strong>åˆ›æ–°ä¸ç‹¬ç‰¹</strong>ï¼šç¬¬ä¸€ä¸ªå­¦ä¹ æ•°æ®ç¦»æ•£è¡¨ç¤ºçš„å·¥ä½œã€‚</p><p><strong>é‡‡ç”¨çš„æ–¹æ³•</strong></p><ol><li><p>é¦–å…ˆå°†æ•°æ®ä½¿ç”¨encoderæ˜ å°„åˆ°éšç©ºé—´ï¼Œç„¶åå­¦ä¹ æ•°æ®åœ¨éšç©ºé—´çš„ç±»åˆ«åˆ†å¸ƒ(<a href="https://zhuanlan.zhihu.com/p/59550457">categorical distribution</a>)ï¼Œ<br>ç„¶åä½¿ç”¨ä¸€ä¸ªç¼–ç è¡¨å¯¹è¿ç»­çš„éšç©ºé—´è¡¨ç¤ºåšè¿‘ä¼¼ï¼Œå¾—åˆ°ç¦»æ•£åŒ–çš„è¡¨ç¤ºï¼Œè¯¥ç¦»æ•£åŒ–çš„è¡¨ç¤ºè½½é€å…¥decoderè¿›è¡Œæ•°æ®è¿˜åŸï¼ˆæ–‡ç« ä¸»è¦ç€ç¬”ä¸å›¾åƒçš„ç”Ÿæˆï¼Œæ–‡ç« ä¸­ä½¿ç”¨pixelCNNä½œä¸ºdecoderï¼Œè¯¥ç»“æ„æ¥æ”¶ç¦»æ•£åŒ–çš„è¡¨ç¤ºä½œä¸ºè¾“å…¥ã€‚æ–‡ç« ä¹Ÿä»‹ç»äº†éŸ³é¢‘çš„ç”Ÿæˆï¼Œå³åºåˆ—ç”Ÿæˆã€‚ï¼‰ã€‚</p></li><li><p>è¿›è¡Œéšç©ºé—´ç¦»æ•£åŒ–çš„ä¸»è¦æ–¹å¼æ˜¯ï¼Œå°†ä¸€ä¸ªDç»´çš„å‘é‡é€šè¿‡æŸ¥è¡¨æ“ä½œï¼Œé‡åŒ–åˆ°è·ç¦»ä»–æœ€è¿‘çš„ä¸€ä¸ªæ•°å€¼ä¸Šã€‚è€Œè¯¥è¡¨ä¹Ÿå°†åœ¨è®­ç»ƒä¸­æ›´æ–°ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´åŠ æ¥è¿‘çœŸå®å€¼ã€‚</p></li><li><p>æœ¬æ–‡çš„å¤§é‡ç¬”å¢¨ç”¨åœ¨å¦‚ä½•å¯¹è¿™ä¸ªç½‘ç»œè¿›è¡Œæ›´æ–°ä¸Šï¼Œå› ä¸ºæŸ¥è¡¨çš„é‡åŒ–æ“ä½œæ˜¯æ— æ³•è¿›è¡Œæ¢¯åº¦è®¡ç®—çš„ã€‚</p></li></ol><p><strong>ä¸ºä½•é€‰æ‹©/å¦‚ä½•åº”ç”¨</strong></p><ol><li><p>æœ¬æ–‡æ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿæ›´é€¼çœŸçš„é‡å»ºæ ·æœ¬ï¼Œåª²ç¾BigGAN.</p></li><li><p>æ‰€æå‡ºçš„æ–¹æ³•é€‚ç”¨äºå»ºæ¨¡ç¦»æ•£æ•°æ®ï¼Œè€Œå¤§å¤šæ•°çš„çœŸå®ä¸–ç•Œæ•°æ®å‡ä¸ºç¦»æ•£åŒ–çš„ã€‚</p></li></ol><p><strong>æ•°æ®é›†</strong></p><ol><li><p>CIFAR10, ImageNet</p></li><li><p>VCTK dataset(è¯­éŸ³åˆ†ç±»æ•°æ®é›†)</p></li><li><p>action sequence(deepMindçš„è§†é¢‘åºåˆ—æ•°æ®é›†)</p></li></ol></li><li><p>IJCAI19 - CLVSA A Convolutional LSTM Based Variational Sequence-to-Sequence Model with Attention for Predicting Trends of Financial Markets</p><p>æå‡ºCLVSAï¼Œè¿˜æ²¡è¯»</p></li></ul><h2 id="Anomaly-detection"><a href="#Anomaly-detection" class="headerlink" title="Anomaly detection"></a>Anomaly detection</h2><blockquote><p>The work that is the most related to ours is <strong>AnoGAN (Schlegl et al., 2017)</strong>.</p><p>â€¦</p><p><strong>Bergmann et al. (2019)</strong> compare standard AE reconstructions techniques<br>to AnoGAN, and observes that AnoGANâ€™s performances on anomaly localizations tasks are<br>poorer than AEâ€™s due to the mode collapse tendency of GAN architectures. Interestingly, updates on<br>AnoGAN such as <strong>fast AnoGAN (Schlegl et al., 2019)</strong> or <strong>AnoVAEGAN (Baur et al., 2018)</strong> replaced<br>the gradient descent search of the optimal z with a learned encoder model, yielding an approach<br>very similar to the standard VAE reconstruction-based approaches, but with a reconstruction loss<br>learned by a discriminator, which is still prone to mode collapse (Thanh-Tung et al., 2019).</p></blockquote><p>  <strong>AnoGAN (Schlegl et al., 2017)</strong> Thomas Schlegl, Philipp SeebÃ¶ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg<br>  Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker<br>  discovery. In International Conference on Information Processing in Medical Imaging, pp. 146â€“157. Springer, 2017.</p><p>  <strong>Bergmann et al. (2019)</strong> Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec adâ€”a comprehensive real-world<br>  dataset for unsupervised anomaly detection. CVPR, 2019.</p><p>  <strong>fast AnoGAN (Schlegl et al., 2019)</strong> Thomas Schlegl, Philipp SeebÃ¶ck, Sebastian M. Waldstein, Georg Langs, and Ursula Schmidt-<br>  Erfurth. f-anogan: Fast unsupervised anomaly detection with generative adversarial networks.<br>  Medical Image Analysis, 54:30 â€“ 44, 2019. ISSN 1361-8415. doi: <a href="https://doi.org/10.1016/j">https://doi.org/10.1016/j</a>.<br>  media.2019.01.010.</p><p>  <strong>AnoVAEGAN (Baur et al., 2018)</strong> Christoph Baur, BenediktWiestler, Shadi Albarqouni, and Nassir Navab. Deep autoencoding models<br>  for unsupervised anomaly segmentation in brain MR images. CoRR, abs/1804.04488, 2018.</p><hr>]]></content>
      
      
      
        <tags>
            
            <tag> paper list </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
