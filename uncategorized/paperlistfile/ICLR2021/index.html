
<!DOCTYPE html>
<html>
<head>
    <title>Smileyan</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
    <link href="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">
    <script src="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"></script>
    
<link rel="stylesheet" href="../../../css/prism.css">
 
    
<script src="../../../js/prism.js"></script>

    
    
<link rel="stylesheet" href="../../../css/index.css">
 
    
<script src="../../../js/search.js"></script>



     
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
        </script>
     

     
        <script src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js"> </script>
        
<script src="../../../js/fancybox.js"></script>

    

    <script type="text/javascript">
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
        var path = "/" + search_path;
        searchFunc(path, 'local-search-input', 'local-search-result');
    </script>
<meta name="generator" content="Hexo 5.4.2"></head>
 
 
<body>
    <!-- 导航栏 -->
    <!-- 导航栏 -->
<nav class="navbar navbar-expand-md navbar-dark bg-dark mb-4 pt-2 pb-2">
    <div class="container">
        <!-- 标题 --> 
        <a class="navbar-brand navbar-expand-sm" href="/">
            <img class="nofancybox" src="/logo.png" style="width: 75px;"  alt="笑颜网">
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
                aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <!-- 左边右边导航按钮 --> 
        <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav me-auto">
                <!-- 左右两侧的导航栏 -->

 
 

    <li class="nav-item">
        <a class="nav-link" href="../../../">
           <big> <i class="fa fa-home ps-1" ></i> 主页</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../archives">
           <big> <i class="fa fa-archive ps-1" ></i> 归档</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../about">
           <big> <i class="fa fa-user ps-1" ></i> 关于</big>
        </a>
    </li>
 
 
            </ul>

            
            <form class="">
                <div class="d-flex">
                    <button class="btn text-muted fa fa-search d-none d-md-block d-lg-block" disabled></button>
                    <input id="local-search-input" class="form-control me-2 pe-4" type="search"
                            placeholder="搜索 " aria-label="Search">
                </div>
                <div id="local-search-result" style="position:absolute; padding-top: 8px; max-height: 960px; width: 480px;overflow-y: scroll; z-index: 1050;"></div>
            </form>
            
            <ul class="navbar-nav">
                <!-- 左右两侧的导航栏 -->

 
 
            </ul>
        </div>
    </div>
</nav>


    <main class="container">
        
<div class="container-fluid markdown-section">
<div class="row">
    <div class="col-md-2 d-none d-sm-none d-md-block">
        <div class="">
            <!-- 同一类型的文件 -->
<!-- 尽可能做到每篇文章只有一个类
    1. 获得 post 的类别 categories
     2.1 如果 categories.length === 0，啥也不做
     2.2 如果 categories === 1，渲染这个类的所有文章
     2.3 如果 categories.length > 1 ，则以第一个类为准渲染所有文章
-->


 
        </div>
    </div>
    <div class="col-md-8 col-sm-12">
        <!-- 博客详情 -->
        <div class="">
            <!-- ps-4 pe-4 pt-2 -->
            <p class="h2">
                <span class="post-title">
                    Related Papers in ICLR 2021 (May 04 2021)
                </span>   
            </p>

            <div class="pb-3 pt-1 pe-3">
                <i class="fa fa-calendar p-1"></i>
                2021/05/04 00:00:00 

                <i class="fal fa-pen"> </i>
                2022/04/12 18:50:37 

                <!--
                <i class="fa fa-folder-open p-1"> </i> 
                <span class="tag-hover">
                     
                </span>
                -->

                <i class="fa fa-tags p-1"> </i>
                <span class="tag-hover">
                    
                        <a href="../../../tags/paper-list/" class="link-dark text-decoration-none"> 
                            paper list 
                        </a>
                    
                </span>
            </div>

            <div class="border-bottom pb-2">
                <p><a target="_blank" rel="noopener" href="https://openreview.net/group?id=ICLR.cc/2021/Conference">Accept paper list</a></p>
<span id="more"></span>

<h2 id="Anomaly-detection-anomaly-outlier-out-of-distribution-one-class"><a href="#Anomaly-detection-anomaly-outlier-out-of-distribution-one-class" class="headerlink" title="Anomaly detection [anomaly, outlier, out-of-distribution, one-class]"></a>Anomaly detection [anomaly, outlier, out-of-distribution, one-class]</h2><ul>
<li><p><strong>已读</strong>SSD: A Unified Framework for Self-Supervised Outlier Detection </p>
<p>  Vikash Sehwag, Mung Chiang, Prateek Mittal</p>
<p>  <strong>Reviewers say:</strong><br>  这项工作调查了一个经典的无监督离群值检测问题，其中我们没有任何标签信息，需要从那些未标记的数据中学习检测模型，以将任何不一致的数据点识别为离群值。这里的关键方法是应用现有的自我监督对比特征学习方法来提取特征表示，然后应用基于聚类的方法来计算离群值。它还提供了两种利用标记的异常数据（如果可用）的方法，包括改进的马哈拉诺比斯距离方法和最近提出的监督式对比学习方法的应用。使用四个数据集评估了这些方法，包括使用一些标记的异常数据的无监督和半监督方法。</p>
<p>  <strong>解决的问题</strong>：分布外数据检测问题（Out-of-distribution detection）</p>
<p>  <strong>难点</strong></p>
<ul>
<li>现有的无监督方法在复杂数据形态（如图像数据）上性能不好</li>
<li>现有的在图像数据上性能较好的方法大多假设有很多in-distribution数据的标可以获得</li>
</ul>
<p>  <strong>创新点</strong></p>
<ul>
<li>本文提出一个无监督的OOD框架，可以不使用任何分布内数据的标签。</li>
<li>本文将few-shot设定拓展到所提出的框架中，即已知少量的OOD样本，少量有标签的OOD数据有助于提高OOD性能。</li>
<li>本文将所提出的SSD也拓展到了已知分部内数据标签的情况，使得改进后的方法比原始方案有更好的性能。</li>
</ul>
<p>  <strong>为何选择/如何应用</strong></p>
<ul>
<li>本文提出了一种完全无需标签的方法，这在大多数异常检测问题中都是可借鉴的</li>
<li>本文的关键方法在于应用现有的self-supervised contrastive learning方法对数据进行了无监督的特征提取，这为后续工作中的特征提取提供了思路。 </li>
<li>本文进一步评估了改进后的mahalanobis distance在聚类时能够显著提高性能</li>
</ul>
</li>
<li><p>Learning and Evaluating Representations for Deep One-Class Classification </p>
<p>  Kihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, Tomas Pfister</p>
<p>  <strong>One-sentence Summary:</strong> We present a two-stage framework for deep one-class classification, composed of state-of-the-art self-supervised representation learning followed by generative or discriminative one-class classifiers.</p>
<p>  <strong>Reviewers say:</strong> 本文研究了一类分类问题，提出了一种学习自我监督表示和分布增强的对比学习方法。全面的结果和分析表明，该方法是有效的，并就其起作用的潜在机制支持了他们的主张。总体而言，尽管发现新颖性不高，但审稿人认为该论文写得好，动机强/论证力强，并提供了详尽的相关工作比较和实验。几位审稿人在证明表示的统一性以及建议其他数据集方面提出了一些可能的弱点。通过有趣的讨论，作者在Mvtec数据集上提供了其他可视化效果和结果。这进一步支持了本文的论点。</p>
</li>
</ul>
<ul>
<li><p>Explainable Deep One-Class Classification </p>
<p>  Philipp Liznerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe Franks, Marius Kloft, Klaus Robert Muller</p>
<p>  <strong>One-sentence Summary:</strong> We introduce an approach to explainable deep anomaly detection based on fully convolutional neural networks. </p>
<p>  <strong>Reviewers say:</strong> 本文涉及可解释的异常检测。为此，它将超球面分类器修改为完全卷积数据描述（FCDD）。正如两个评论者所指出的那样，这是在超球面分类器中直接应用全卷积网络的方法。但是，本文还展示了如何使用具有固定高斯核的跨步转置卷积对接收场进行上采样。两者以及解决可解释的异常检测都很重要。此外，实证评估是详尽无遗的，并且与最新技术相比显示出一些好处。所以，是的，增量式的，但是对于一个非常有趣的重要案例来说，增量式的。</p>
</li>
<li><p>Multiscale Score Matching for Out-of-Distribution Detection </p>
<p>  Ahsan Mahmood, Junier Oliva, Martin Andreas Styner</p>
<p>  <strong>One-sentence Summary:</strong> Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.</p>
<p>  <strong>Reviewers say:</strong></p>
<ol>
<li>作者利用并重新调整了噪声条件评分网络（NCSN），该条件最初由Song＆Ermon（2019）引入用于生成模型，用于检测失配（OOD）图像。作者展示了分数匹配背后的直觉和基本原理，然后进行了等价的去噪自动编码器（DAE）来得出NCSN作为分数估算器，并提供了分析以证明多尺度分数分析的价值。在对SVHN和CIFAR数据集进行的实验分析中，他们证明了他们的方法（MSMA）优于以前使用OOD任务模型（ODIN，JEM，似然比）在文献中报道的发现。</li>
<li>摘要：他们提出了一种新的OOD检测方法，即MSMA，它使用了一种新的生成模型[NCSN]，并且在不同规模的似然矢量上拟合了简单密度模型的第二阶段。他们在标准OOD图像数据集（CIFAR10与OOD，SVHN与OOD等）上显示了经验良好的结果。他们能够在大多数情况下实现完美的分离，并且与以前的无监督方法相比，CIFAR10与SVHN的结果大大提高。他们显示了在医学图像中检测OOD的有趣应用，这些图像的扫描范围为9-11岁，并且OOD为&lt;9岁。</li>
<li>本文将多尺度得分估计应用于分布外检测。他们证明了多尺度估计的有用性，并采用了辅助模型来识别异常数据。所提出的方法在两个不同的设置上进行了评估，对于分布不均的检测非常有效。</li>
</ol>
</li>
</ul>
<ul>
<li><p>In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness </p>
<p>  Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, Percy Liang</p>
<p>  <strong>One-sentence Summary:</strong> Using auxiliary information as inputs hurts OOD, but using auxiliary information by pretraining and self-training improves in-distribution and OOD accuracies on real-world datasets, with theoretical guarantees in a linear multi-task setting.</p>
<p>  <strong>Reviewers say:</strong> 本文通过利用可用的辅助信息解决了在注释数据很少的情况下改善泛化的问题。作者考虑了两种选择的各自优点：在多任务或传输设置中，使用辅助信息作为补充输入或作为附加输出。对于线性回归，他们从理论上表明前者可以帮助改善分布误差，但可能会损害OOD误差，而后者可能有助于改善OOD误差。他们提出了一个将这两种选择结合起来的框架，并凭经验表明它在三个不同的数据集上做到了。</p>
</li>
</ul>
<h2 id="Heterogeneous"><a href="#Heterogeneous" class="headerlink" title="Heterogeneous"></a>Heterogeneous</h2><ul>
<li><p>Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks </p>
<p>  Timothy Castiglia, Anirban Das, Stacy Patterson</p>
</li>
<li><p>HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients </p>
<p>  Enmao Diao, Jie Ding, Vahid Tarokh</p>
</li>
</ul>
<h2 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h2><ul>
<li>Generative Time-series Modeling with Fourier Flows   Ahmed Alaa, Alex James Chan, Mihaela van der Schaar</li>
</ul>
<ul>
<li><p>Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies </p>
<p>  T. Konstantin Rusch, Siddhartha Mishra</p>
<p>  <strong>One-sentence Summary:</strong> A biologically motivated and discretized ODE based RNN for learning long-term dependencies, with rigorous bounds mitigating the exploding and vanishing gradient problem.</p>
</li>
</ul>
<ul>
<li><p>Multi-Time Attention Networks for Irregularly Sampled Time Series </p>
<p>  Satya Narayan Shukla, Benjamin Marlin</p>
<p>  <strong>One-sentence Summary:</strong> This paper presents a new deep learning architecture for learning with sparse and irregularly sampled multivariate time series.</p>
<p>  <strong>Reviewers say:</strong> 本文讨论不规则样本时间序列的分析。该方法主要基于插值。因此，作者可以研究有监督和无监督的问题。该架构由正弦波注意层，可在潜在空间中形成固定大小的界标的VAE层和RNN解码器组成。对于监督任务，作者添加了分类损失。</p>
<ul>
<li>他们在插值任务中获得了令人印象深刻的结果，而在分类任务中获得了有趣的结果。</li>
<li>在插值问题中，我们想将稳健的基线视为线性插值或类似AR的模型。即使我必须承认作者已经提出了与最近文献的模型进行的大量比较，这也将为我们提供有意义的MSE结果，以比较其他方法。</li>
<li>结果令人印象深刻，但我不知道架构的哪个部分会带来如此出色的性能</li>
</ul>
</li>
<li><p>Generative Time-series Modeling with Fourier Flows </p>
<p>  Ahmed Alaa, Alex James Chan, Mihaela van der Schaar</p>
<p>  <strong>Reviewers say:</strong></p>
<ol>
<li><p>作者提出了傅里叶域中时间序列的流量生成模型。首先将时间序列数据转换为傅立叶域。代替先前文献中提出的仿射耦合层，作者设计了仿射耦合层的频域版本。</p>
</li>
<li><p>本文介绍了一种新的卷积流体系结构，该体系结构使用DFT将生成的时间序列转换为频域。卷积是通过频谱仿射层在频域中进行乘法来完成的，该仿射层使用依赖于数据的滤波器对信号的偶数或奇数部分进行变换。所得的时域卷积具有与输入有关的权重，这是一种有趣的原始方法，与其他卷积流（例如[1]）明显不同。</p>
<ul>
<li>相关性：时间序列生成建模在从医学到金融的各个领域都有广泛的关键应用。新方法显示出非常有前途的性能，并且有可能成为时间序列生成的最新方法。</li>
<li>独创性：DFT和谱仿射层的使用在归一化流量的背景下是独创的。重要的是，DFT非常适合流动，因为它是等轴测​​图，因此具有很小的雅可比。即使在常规ConvNet体系结构中，依赖输入的卷积的使用也非常有趣。</li>
</ul>
</li>
</ol>
</li>
<li><p>Clairvoyance: A Pipeline Toolkit for Medical Time Series </p>
<p>  Daniel Jarrett, Jinsung Yoon, Ioana Bica, Zhaozhi Qian, Ari Ercole, Mihaela van der Schaar</p>
<p>  <strong>One-sentence Summary:</strong> We develop and present Clairvoyance: a pipeline toolkit for medical time series.</p>
<p>  <strong>Reviewers say:</strong> 该手稿介绍并说明了用于时间序列数据的医学机器学习的端到端软件管道，称为Clairvoyance。必须为设计和开发了这一出色的资源，以加速这些计算技术在临床实践中的采用，以支持人们的判断和决策的方式，对作者表示祝贺。该手稿擅长描述其贡献并将其与相关工作联系起来。它还包括一组令人信服的实验，这些实验是来自三个相互补充的医学环境的数据集</p>
</li>
<li><p>Discrete Graph Structure Learning for Forecasting Multiple Time Series </p>
<p>  Chao Shang, Jie Chen, Jinbo Bi</p>
<p>  <strong>One-sentence Summary:</strong> We propose a graph neural network approach that learns a graph structure to enhance the forecasting of multiple multivariate time series.</p>
<p>  <strong>Reviewers say:</strong> 本文提出了一种通过尝试通过学习的图结构估算各个维度之间的相关性来进行多元时间序列预测的方法。维度被视为图形中的节点，问题被映射到学习离散图形结构，该结构可以帮助进行下游预测任务。该论文表明，即使显式结构未知，也可以利用图神经网络（GNN）来提高预测性能。这是在以端到端的方式学习图形结构和预测体系结构的同时实现的。与在元学习框架中学习离散图结构的双层优化方法相比，所提出的方法在计算效率上更高。该方法还被要求能够通过提出确保所学习的图结构保持与已知图结构接近的正则化器来合并图结构的先验知识。与在三个真实数据集上使用的几种强基准方法相比，该方法可以提高预测性能。通常，该论文写得很好并且易于阅读。</p>
</li>
<li><p>Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding </p>
<p>  Sana Tonekaboni, Danny Eytan, Anna Goldenberg</p>
<p>  <strong>One-sentence Summary:</strong> An unsupervised representation learning framework for high-dimensional non-stationary time series </p>
<p>  <strong>Reviewers say:</strong> 本文提出了一种用于时间序列的无监督表示（嵌入）学习方法。尽管无监督的表示学习已被广​​泛研究，并在NLP和视觉等领域表现出良好的表现，但对于时间序列社区而言，它相对较新。与最近的工作（CPC和三重损失）相比，本文具有以下差异：</p>
<ol>
<li>它使用平稳性/非平稳性的统计测试来估计固定的时间窗。</li>
<li>它使用CPC和Triplet-loss中的对比学习来学习嵌入，但是还考虑到天真的负采样可能包含虚假负数，并且不利于在季节性强的时间序列上进行嵌入学习的事实。相反，它采用了“积极无标签学习”框架来解决此问题。</li>
</ol>
</li>
</ul>
<h2 id="About-deep-learning"><a href="#About-deep-learning" class="headerlink" title="About deep learning"></a>About deep learning</h2><ul>
<li><p>Understanding Over-parameterization in Generative Adversarial Networks </p>
<p>  Yogesh Balaji, Mohammadmahdi Sajedi, Neha Mukund Kalibhat, Mucong Ding, Dominik Stöger, Mahdi Soltanolkotabi, Soheil Feizi</p>
<p>  <strong>One-sentence Summary:</strong> We present an analysis of over-paramterization in GANs both theoretically and empirically.</p>
<p>  Reviewers say : 本文从理论和经验上研究了在GAN训练中增加参数数量（“过参数化”）的效果。与神经网络在监督学习中发生的情况类似，过度参数化确实有助于稳定训练动态（并凭经验提高性能）。本文为1层ReLU网络生成器的宽度提供了一个明确的阈值，以便使用线性鉴别器进行梯度上升训练可产生与全局鞍点的线性收敛速度（这对应于与数据均值）。作者还提供了一个更通用的定理，将这个结果推广到更深的网络。</p>
</li>
</ul>
<ul>
<li><p>Understanding the role of importance weighting for deep learning </p>
<p>  Da Xu, Yuting Ye, Chuanwei Ruan</p>
<p>  <strong>One-sentence Summary:</strong> We study the theoretical properties of importance weighting for deep learning.</p>
<p>  <strong>Reviewers say:</strong> 本文研究了深度学习模型中重要性加权方案对梯度下降的隐式偏差的影响。它提供了一些理论结果，可为重要权重方案对收敛极限以及收敛速度的影响提供重要见解。给出了线性分隔符和深度学习模型的结果。还研究了协变量平移设置。理论结果得到了经验论证的支持，并且也得出了关于哪些加权方案将更有用的有用见解。他们还解释了一些以前观察到的经验现象。</p>
</li>
</ul>
<h2 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h2><ul>
<li><p>On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines </p>
<p>  Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow</p>
<p>  <strong>One-sentence Summary:</strong> We provide an analysis of the fine-tuning instability of BERT-based models and present a simple method to fix it</p>
<p>  <strong>Reviewers say:</strong> 本文确定了NLP深度学习中一个主要已知问题背后的原因：在自监督式预训练之后对小型数据集进行微调可能会非常不稳定，在某些情况下，该模型需要数十次重新启动才能达到可接受的性能。然后，本文介绍了一个简单的建议修复方法。</p>
</li>
<li><p>Multi-timescale Representation Learning in LSTM Language Models </p>
<p>  Shivangi Mahto, Vy Ai Vo, Javier S. Turek, Alexander Huth</p>
<p>  <strong>One-sentence Summary:</strong> This work presents a theoretically-motivated analysis of memory and timescale in LSTM language models.</p>
<p>  <strong>Reviewers say:</strong> 本文指出自然语言中单词之间的关系通常遵循幂律。门控递归神经网络（例如LSTM）在自然语言建模方面表现出色，但是LSTM的遗忘机制是由指数衰减决定的。这项工作展示了一种工程化LSTM的遗忘机制，以模仿自然语言中表现出的幂律关系的方法。通过应用他们的技术，修改后的LSTM模型可以更好地建模稀有标记，这些标记通常跨越较长的时间范围，因此，该模型可以在频率较低的单词上获得较低的困惑度。本文的主要贡献是推导，该推导表明，在给出第一个输入令牌后，LSTM的遗忘门在零输入状态下会经历指数衰减。</p>
<p>  实验表明，从反伽马分布中绘制T是自然语言的自然拟合。然后，作者提出了利用此特性的多尺度LSTM模型。每个时间标度T是从反伽玛分布中得出的，该反伽玛分布实际上成为一个遗忘偏差项，并且在训练过程中是固定的。绘制多个T来模拟幂律。多尺度LSTM可以捕获正确的归纳偏置，以便在对频率较低的单词进行建模时表现更好，这可能有助于在内存中保留更长的时间。该论文写得很好，并且该方法的动机和解释都清楚。实验经过适当设计，结果很好地支持了主要主张。</p>
</li>
<li><p>Representation Learning for Sequence Data with Deep Autoencoding Predictive Components </p>
<p>  Junwen Bai, Weiran Wang, Yingbo Zhou, Caiming Xiong</p>
<p>  <strong>Reviewers say:</strong> 本文提出了深度自动编码预测组件（DAPC），这是一种用于序列数据的自监督表示学习方法。在这种方法中，模型学习最大化预测信息，即过去和未来时间窗口之间的相互信息。为了避免退化的解决方案，提出的方法依赖于优化蒙版重建的第二种损失。</p>
</li>
<li><p>Differentiable Segmentation of Sequences </p>
<p>  Erik Scharwächter, Jonathan Lennartz, Emmanuel Müller</p>
<p>  <strong>One-sentence Summary:</strong> We propose an architecture for effective gradient-based learning of segmented models for sequential data.</p>
</li>
<li><p>PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences </p>
<p>  Fan, Xin Yu, Yuhang Ding, Yi Yang, Mohan Kankanhalli</p>
<p>  <strong>One-sentence Summary:</strong> This paper proposes a point spatio-temporal (PST) convolution to learn representations of raw point cloud sequences by disentangling space and time.</p>
<p>  <strong>Reviewers say:</strong> 本文介绍了一种新的卷积方法来直接处理原始时空（ST）点云数据。提出的点时空（PST）卷积在“点管”上运行，并通过每个时间步的共享空间卷积解耦空间和时间，然后进行时间卷积。它还引入了转置的PST，以在编码器-解码器框架（PSTNet）中启用逐点预测。提出的实验通过使用PSTNet对点云序列进行动作识别和语义分割，证明了这些卷积的有效性，显示了对相关最新工作的改进。</p>
</li>
<li><p>Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning </p>
<p>  Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, Zhaopeng Tu</p>
<p>  <strong>Reviewers say:</strong> 本文介绍了对细粒度层的关注，以评估各个编码器层的作用并研究编码器层融合的工作原理，其中解码器层可以访问各种编码器层的信息，而与标准Transformer中的最终编码器层不同。</p>
<p>  基于以下观点：编码器嵌入层对于编码器层融合的成功至关重要，而最上层的解码器层则更加关注编码器嵌入层，因此提出了SurfaceFusion，该方法仅将编码器嵌入层连接到解码器的softmax层。 ，导致BLEU等指标获得了可观的收益。</p>
</li>
<li><p>Seq2Tens: An Efficient Representation of Sequences by Low-Rank Tensor Projections </p>
<p>  Csaba Toth, Patric Bonnier, Harald Oberhauser</p>
<p>  <strong>One-sentence Summary:</strong> An Efficient Representation of Sequences by Low-Rank Tensor Projections</p>
<p>  <strong>Reviewers say:</strong></p>
<ol>
<li>本文介绍了自由代数，这是一种经典的数学概念，是表示任意长度的顺序数据的通用工具。所提方法具有吸引人的理论特性，例如保持静态特征映射的通用性和连续设置的收敛性。作者进一步建议使用自由代数的堆叠秩1投影作为序列表示的近似值，以使其在计算上可行的神经网络层。作者通过将NN实现与FCN结合起来，以多元时间序列分类问题为基准，并以GP-VAE模型为序列数据归纳问题进行了基准，说明了该方法的灵活性和有效性。与以前的最新技术相比，所提出的方法显示出改进的结果。</li>
</ol>
<ul>
<li>启示：本文为社区提供了NN在序列数据上的通用逼近定理的扩展，以及将静态特征图转换为序列特征的通用方法。实验表明，该方法在判别和生成问题上均具有灵活性和有效性。</li>
</ul>
<ol start="2">
<li>本文针对序列数据提出了一种有趣的低秩张量表示学习模型，称为Seq2Tens。可以将所提出的模型作为Seq2Tens层插入现有的最新神经网络模型中，以提高性能，这已在本文的一些基准数据集中得到了证明。</li>
</ol>
</li>
</ul>
<h2 id="Interpretable"><a href="#Interpretable" class="headerlink" title="Interpretable"></a>Interpretable</h2><ul>
<li><p>Understanding the failure modes of out-of-distribution generalization </p>
<p>  Vaishnavh Nagarajan, Anders Andreassen, Behnam Neyshabur</p>
<p>  <strong>One-sentence Summary:</strong> In this theoretical study, we explain why machine learning models rely on spuriously correlated features in the dataset and fail at out-of-distribution generalization.</p>
</li>
<li><p>Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability </p>
<p>  Suraj Srinivas, Francois Fleuret</p>
<p>  <strong>One-sentence Summary:</strong> Input-gradients in discriminative neural net models capture information regarding an implicit density model, rather than that of the underlying discriminative model which it is intended to explain.</p>
<p>  <strong>Reviewers say:</strong> 本文从基于能量的生成模型的最新观察结果出发，从理论角度研究了可解释性文献中提出的基于梯度的归因方法。首先，作者指出了基于梯度的归因的普遍弱点，该弱点源于以下事实：输入梯度没有提供明确的解释，因为softmax输出的平移不变性使其具有任意性。然后作者提出，可以通过判别模型“包含隐式”类条件密度模型（提到的有关基于能量的生成模型的最新发现）这一事实来解释基于梯度的归因模型成功的原因。然后，他们继续阐述这个想法，该想法表明将隐式类条件生成模型与数据的“真实”生成模型对齐将如何帮助提供与基于梯度的归因相关的信息，以及如何通过一种新颖的实现方式有效地促进对齐得分匹配，以及如何将该机制实际实现为正则化成本。作者随后进行了实证研究，令人信服地证实了其理论观点的预测。首先，他们表明，通过“ GAN-test方法”提出的经过训练的判别模型，在噪声较少的意义上以及通过判别准确性而言，用分数匹配和建议的梯度范数正则化生成的样本更好。最后，他们表明，根据像素扰动测试的判别版本，基于梯度的解释的质量更好，这是一种通过扰动以相关性递增顺序排列的像素来评估梯度说明的方法。总之，本文在判别模型，基于能量的生成模型和基于梯度的解释之间建立了非常有趣的基本理论联系，使用此理论框架来解释基于梯度的解释如何克服softmax位移不变性问题（并指出在本文中），并介绍了实用的培训程序，以利用所获得的理论见解来产生更好的解释，并且在仿真中也进行了实证验证。一种通过扰乱以相关性递增顺序排列的像素来评估梯度说明的方法。</p>
</li>
</ul>
<ul>
<li><p>Getting a CLUE: A Method for Explaining Uncertainty Estimates </p>
<p>  Javier Antoran, Umang Bhatt, Tameem Adel, Adrian Weller, José Miguel Hernández-Lobato</p>
<p>  <strong>One-sentence Summary:</strong> We introduce a method to help explain uncertainties of any differentiable probabilistic model by perturbing input features.</p>
</li>
<li><p>Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking </p>
<p>  Michael Sejr Schlichtkrull, Nicola De Cao, Ivan Titov</p>
<p>  <strong>One-sentence Summary:</strong> We present a novel post-hoc interpretation method for graph neural networks, and apply it to analyse two models from the NLP literature.</p>
</li>
<li><p>Interpretable Models for Granger Causality Using Self-explaining Neural Networks </p>
<p>  Ričards Marcinkevičs, Julia E Vogt</p>
<p>  <strong>One-sentence Summary:</strong> We propose an interpretable framework for inferring Granger causality based on self-explaining neural networks.</p>
<p>  <strong>Reviewers say:</strong> 本文主要涉及在非线性动力学设置中学习多元时间序列中的格兰杰因果关系。核心方法使用带有稀疏诱导正则化器（基于弹性网和平滑度的融合套索）的矢量自回归建模，以及最近提出的具有自解释神经网络（用于解释性）。作者还通过学习对原始数据和时间反转数据稳定的格兰杰因果结构来扩充框架。详尽的经验分析是根据最近的GC基准进行的。我对本文的一些关注如下</p>
</li>
<li><p>Interpreting Knowledge Graph Relation Representation from Word Embeddings </p>
<p>  Carl Allen, Ivana Balazevic, Timothy Hospedales</p>
<p>  <strong>One-sentence Summary:</strong> Interpreting the structure of knowledge graph relation representation using insight from word embeddings.</p>
<p>  <strong>Reviewers say:</strong> 作者通过对实体之间的关系进行分类来研究单词表示模型的潜在语义属性。目的是表明即使两种类型的模型都具有不同的学习目标，词嵌入和知识图表示也可以学习共同的潜在结构。主要的贡献是对象之间的关系到目标词嵌入的映射，这种关系的分类以及对最新知识图表示的评估。研究表明，知识表示模型遵循定义的关系条件。</p>
</li>
<li><p>Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning </p>
<p>  Alihan Hüyük, Daniel Jarrett, Cem Tekin, Mihaela van der Schaar</p>
<p>  <strong>One-sentence Summary:</strong> We present a method for learning interpretable representations of behavior to enable auditing, quantifying, and understanding human decision-making processes.</p>
<p>  <strong>Reviewers say:</strong> 这项工作提出了一种理解和解释决策行为的方法。作者旨在使方法1）透明，2）能够处理部分可观察性，以及3）处理脱机数据。为此，他们开发了INTERPOLE，它使用贝叶斯技术来估计决策动态以及决策边界。在模拟域和真实域上的结果表明，他们的方法在保持行为准确性的同时解释了行为数据中的决策，并着重于解释决策动态，而不是世界的“真实”动态。</p>
</li>
<li><p>BERTology Meets Biology: Interpreting Attention in Protein Language Models </p>
<p>  Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, richard socher, Nazneen Rajani</p>
<p>  <strong>One-sentence Summary:</strong> We analyze the internal representations of protein language models, and show that attention targets structural and functional properties of protein sequences.</p>
</li>
<li><p>Representation learning for improved interpretability and classification accuracy of clinical factors from EEG </p>
<p>  Garrett Honke, Irina Higgins, Nina Thigpen, Vladimir Miskovic, Katie Link, Sunny Duan, Pramod Gupta, Julia Klawohn, Greg Hajcak</p>
<p>  <strong>One-sentence Summary:</strong> We use disentangled representations of EEG signals to improve performance on clinical classification tasks, provide interpretable recommendations for post-hoc analysis and allow for extraction of ERPs from novel single EEG trajectories.</p>
<p>  <strong>Reviewers say:</strong> 作者关注EEG信号的分类，以便根据EEG信号预测年龄，性别，抑郁和1轴失调症。经过标准的预处理和可选的平均值以获得诱发的反应后，作者将样品喂入容器中。-VAE，然后使用标准分类算法或SCAN方法来预测标签。作者报告了比基于晚期阳性潜力的常规方法更好的结果。他们还表明，他们的方法可以使用非平均EEG数据进行训练，并且在ERP上进行测试时，反之亦然。最后，作者检查学习到的表示。</p>
</li>
<li><p>Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels </p>
<p>  Xingchen Wan, Binxin Ru, Xiaowen Dong, Michael Osborne</p>
<p>  <strong>One-sentence Summary:</strong> We propose a NAS method that is sample-efficient, highly performant and interpretable.</p>
<p>  <strong>Reviewers say:</strong> 作者提出了一种新的神经体系结构搜索算法，该算法将贝叶斯优化与富有表现力且广受欢迎的Weisfeiler-Lehman（WL）图核相结合。使用WL的一个优点是源于内核计算方式的本质的可解释结果，即通过图形的传播方案。合并方程式的导数 3.2可以提取直接负责提高性能的子图。在各种实验中，作者不仅显示出已检测架构的性能提高，而且还发现了其他算法也可以找到的子图。<br>  即使我的专业知识不属于NAS领域，我仍然认为这项工作很有吸引力。它是图形内核的创新应用程序，它具有可伸缩性，而在这种情况下，可伸缩性几乎没有问题。我发现新颖性，可解释性和定量结果方面令人信服，足以建议您接受。此外，作品的结构和书面结构都很好，数字清晰易读。与其他SOTA NAS算法的比较是否质量好，是否公平，我认为具有NAS背景的审阅者的意见很有价值。</p>
</li>
<li><p>Explainable Deep One-Class Classification </p>
<p>  Philipp Liznerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe Franks, Marius Kloft, Klaus Robert Muller</p>
<p>  <strong>One-sentence Summary:</strong> We introduce an approach to explainable deep anomaly detection based on fully convolutional neural networks. </p>
</li>
<li><p>A Learning Theoretic Perspective on Local Explainability </p>
<p>  Jeffrey Li, Vaishnavh Nagarajan, Gregory Plumb, Ameet Talwalkar</p>
<p>  在本文中，我们通过局部逼近解释的角度探索了可解释机器学习与学习理论之间的联系。首先，我们解决了性能泛化的传统问题，并使用局部解释性的概念来限制模型的测试时间准确性。其次，我们探讨了解释泛化的新问题，这是越来越多的基于有限样本的局部逼近解释的重要问题。最后，我们通过经验验证了我们的理论结果，并表明它们反映了在实践中可以看到的结果。</p>
<p>  <strong>Reviewers say:</strong> 本文试图在局部可解释性和学习理论之间建立一种新颖的联系，并针对与表现概括和解释概括相关的界线提出了两个定理。本文提供了两组实证结果，以说明边界的有用性。</p>
<p>  总体而言，通过从学习理论的角度探索黑匣子机器学习模型的本地解释，本文的想法很有趣。本文提出镜像邻居保真度（MNF），作为衡量本地可解释性的新方法以及论点和结论的核心组成部分。</p>
<p>  该论文声称，MNF自然补充了常用的邻域保真度（NF），并且在评估对“现实的”高维分布数据的本地解释时，NF具有相对于NF的独特优势，而这些解释通常表现出显着的特征依赖性。但是，除了附录中的一个玩具示例外，没有任何可靠或令人信服的证据和经验实验可支持上述权利要求。</p>
</li>
<li><p>Shapley explainability on the data manifold </p>
<p>  Christopher Frye, Damien de Mijolla, Tom Begley, Laurence Cowton, Megan Stanley, Ilya Feige</p>
<p>  <strong>One-sentence Summary:</strong> We present drawbacks of model explanations that do not respect the data manifold, and introduce two methods for on-manifold explainability.</p>
<p>  <strong>Reviewers say:</strong> 本文重点讨论Shapley值的离数据流形问题，该问题是通过对分布不全的数据进行采样而创建的。目标是开发有效的方法。提出了两种主要算法：用于近似条件分布的生成模型和用于直接近似的训练监督模型。它们在实验中显示出优于原始非流形Shapley值的优势。印象实验特别有趣。使用生成模型来解决可解释性方法（不仅仅是SHAP）中的流形数据问题的总体思路通常是一个不错的方向。请注意，对于SHAP而言，该问题更为突出，因为该方法基于输入特征的所有子集的性能，并且本文很好地说明了解决基于Shapley的方法的流形外数据问题的必要性。实验结果也很全面，并为它们的有用性提供了足够的证据。似乎存在一些新奇的担忧：“到目前为止，尚缺乏一种在一般数据上估计流形Shapley值的高效方法，并且该工作的重点。” 歧管外数据在可解释性方面的问题已得到很好的研究，其SHAP方法的细节已在之前进行了讨论。<a target="_blank" rel="noopener" href="https://proceedings.icml.cc/static/paper_files/icml/2020/334-Paper.pdf">this paprt</a>作者对本文进行了非常简短的引用，但实际上并未提及它们的不同之处。除非该作品对现有文献的主要贡献是显而易见的，否则我无法改变自己的分数。如果贡献是“……支持流形方法的实验证据”，则该贡献不足以用于该场所。</p>
</li>
<li><p>Information-theoretic Probing Explains Reliance on Spurious Features </p>
<p>  Charles Lovering, Rohan Jha, Tal Linzen, Ellie Pavlick</p>
<p>  <strong>One-sentence Summary:</strong> We find that feature extractability, measured by probing classifiers, can be viewed as an inductive bias: the more extractable a feature is after pre-training, the less statistical evidence needed during fine-tuning for the model to use the feature.</p>
<p>  <strong>Reviewers say:</strong> 本文研究了来自预训练表示的特征的可抽取性与经过微调的模型使用该特征的程度之间的关系。特征的可提取性通过训练为从预训练表示中检测特征的探测分类器的最小描述长度来衡量（使用Voita和Titov的在线代码版本）。精细调整的模型使用特征的程度通过模型将虚假特征与非虚假特征（称为“目标”特征）分开所需的证据量来衡量。这里的证据是指出现虚假特征但未发生非虚假特征的示例。当存在许多此类示例时（仅高伪造率），模型更容易拒绝虚假特征并学会依赖目标特征。“</p>
<p>  本文针对合成数据和更自然的数据进行了两种实验。合成数据是符号序列，其中的任务是识别简单属性，例如符号的出现或重复。进行实验时，应在训练过程中提供不同比率的仅虚假示例，以提供越来越多的证据来证明虚假特征（符号2的存在）并支持目标特征。目标特征与标签相同，即示例对应于标签时为1，否则为0。该论文通过探测分类器的MDL报告了虚假特征和目标特征的可提取性。感兴趣的度量标准是相对MDL，其中越高表示功能越容易提取。当功能更易于提取时，模型拒绝虚假特征所需的证据较少。由于提取特征较少，因此需要更多证据。</p>
<p>  自然语言示例是通过对三种语言现象（主语-动词一致，负极性项目和填充项相关性）语法生成的示例的可接受性判断而制成的。此处的设置再次相似，只是对如何计算可提取性进行了调整。此处的主要结果是，可提取性与拒绝虚假特征所需的证据之间具有高度（负）相关性。</p>
</li>
</ul>
<ul>
<li><p>Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization </p>
<p>  Judy Borowski, Roland Simon Zimmermann, Judith Schepers, Robert Geirhos, Thomas S. A. Wallis, Matthias Bethge, Wieland Brendel</p>
<p>  <strong>One-sentence Summary:</strong> Using human psychophysical experiments, we show that natural images can be significantly more informative for interpreting neural network activations than synthetic feature visualizations.</p>
<p>  <strong>Reviewers say:</strong> </p>
<ol>
<li>本文着重于特征可视化，该可视化为给定的隐藏节点生成最大程度的激活图像，以了解CNN的内部工作原理。他们将这些图像的信息量与自然图像相比较，后者也强烈激活了指定的隐藏节点，并发现自然图像可以帮助人类更好地回答哪些其他测试自然图像也被最大程度地激活了。</li>
<li>主要思想是研究极端激活图像如何帮助人类预测CNN激活。作者通过将极度激活的图像与示例性的自然图像进行比较来实现，这些自然图像也强烈地激活了特定的特征图（并使用心理物理学测试来查看哪种图像可以更好地帮助人类）。</li>
<li>作者指出，许多可视化方法都将响应最大化与人为定义的正则化方法相结合，这些方法本质上是艺术上的选择，旨在降低图像的噪点，这是正确且正确的。这些正则化归因于它们自身对结果图像的偏见，这可能会使它们的信息量更少。还可以很好地认识到，单元可能被一个以上的语义概念高度激活，或者与其他单元（可能传递更多的信息而不是选择性地最大化单个神经元的激活）相结合而活跃。</li>
</ol>
</li>
</ul>
<ul>
<li><p>Scaling Symbolic Methods using Gradients for Neural Model Explanation </p>
<p>  Subham Sekhar Sahoo, Subhashini Venugopalan, Li Li, Rishabh Singh, Patrick Riley</p>
<p>  <strong>Reviewers say:</strong> 本文提出了一种编码最小输入特征发现问题的方法-将一种最小的特征集输入到预测所需的特征中-将其编码为一种可满足满意度模理论（SMT）求解器的形式。特别是，他们首先使用集成梯度方法对第一层神经元的影响程度进行评分。然后，他们产生并解决一个SMT问题，该问题找到了改变这些有影响的神经元的最小面罩。他们展示了他们在一些问题上的方法。</p>
</li>
<li><p><strong>【重点阅读】</strong>Evaluation of Similarity-based Explanations </p>
<p>  Kazuaki Hanawa, Sho Yokoi, Satoshi Hara, Kentaro Inui</p>
<p>  <strong>One-sentence Summary:</strong> We investigated empirically which of the relevance metrics (e.g. similarity of hidden layer, influence function, etc.) are appropriate for similarity-based explanation.</p>
<p>  <strong>Reviewers say:</strong> 这项工作提供了基于示例的解释方法中使用的相似性度量的经验评估，其目的是在训练集中为黑匣子模型的预测提供决策支持示例。本文评估了文献中流行的基于梯度的度量，例如影响函数，费舍尔核，以及基于l2，余弦距离和不同嵌入空间上的点积的简单方法。作者介绍了两个评估不同方法可靠性的新任务：相同的类测试和相同的子类测试。</p>
</li>
<li><p>Learning explanations that are hard to vary </p>
<p>  Giambattista Parascandolo, Alexander Neitz, ANTONIO ORVIETO, Luigi Gresele, Bernhard Schölkopf</p>
<p>  <strong>Reviewers say:</strong> 这项工作假定数据集中存在不变机制。使用梯度下降训练的机器学习算法通常会在示例中平均梯度。本文的观点是，通过平均梯度，信息会丢失。该方法假定，在梯度下降算法中，可以使用几何（或karcher）平均值代替算术平均值来保存有关不变机制的信息-而忽略混杂因素。在直接应用几何平均值时会遇到困难，因此开发了一种简单的启发式算法，其中包括根据梯度的符号是否在一批示例中一致（或是否达到某个一致阈值）来掩盖梯度。该算法已在合成数据集，CIFAR-10上的半合成任务以及RL算法coinbase上进行了测试。</p>
</li>
<li><p>Debiasing Concept-based Explanations with Causal Analysis </p>
<p>  Mohammad Taha Bahadori, David Heckerman</p>
<p>  <strong>One-sentence Summary:</strong> We use a technique from instrumental variables literature and remove the impact of noise and latent confounding from concept-based explanations.</p>
<p>  <strong>Reviewers say:</strong> 这项工作的重点是使用基于概念的解释进行模型的可解释性。作者认为，概念问题与功能中的混淆信息相关。他们提出了表示系统的因果图，并使用工具变量方法来消除未观察到的混杂因素的影响。该方法在综合和真实数据上进行了评估。</p>
</li>
<li><p>Evaluations and Methods for Explanation through Robustness Analysis </p>
<p>  Cheng-Yu Hsieh, Chih-Kuan Yeh, Xuanqing Liu, Pradeep Kumar Ravikumar, Seungyeon Kim, Sanjiv Kumar, Cho-Jui Hsieh</p>
<p>  <strong>One-sentence Summary:</strong> We propose a suite of objective measurements for evaluating feature based explanations by the notion of robustness analysis; we further derive new explanation that captures different characteristics of explanation comparing to existing methods.</p>
<p>  <strong>Reviewers say:</strong> 许多可解释性技术都集中在识别“最相关特征”的子集上。在这项工作中，作者建议将该集合定义为最容易遭到攻击的特征集（在<br>感）。首先，由于垂直空间LaTeX骇客攻击的数量令人沮丧，论文有点难以阅读，以至于节和段落之间的间距甚至小于句子之间的正常间距。这不是将所有内容压缩到8页的好方法。</p>
<p>  除此之外，该论文在实验评估方面非常全面，并提供了一系列适当的基准，健全性检查和人体研究。我认为这是对当前功能归因技术套件的有趣补充。从概念上讲，它非常相似。正如作者所指出的那样，有许多相关技术试图通过向它们添加噪声，将其设置为基准值或模糊它们来“去除特征”。在这里，作者改为考虑对抗性地干扰他们，他们提出了一种改进的贪婪策略，该方法似乎效果很好。对于我来说，还是有点不清楚，为什么考虑对抗性扰动比说考虑（例如考虑模糊或增加选定特征的噪声）更具说服力，但是它们的作用略有不同，而且从经验上讲，与现有技术相比，此方法可在插入和删除指标下获得收益。</p>
</li>
<li><p>Shapley Explanation Networks </p>
<p>  Rui Wang, Xiaoqian Wang, David I. Inouye</p>
<p>  <strong>One-sentence Summary:</strong> To enable new capabilities, we propose to use Shapley values as inter-layer representations in deep neural networks rather than as post-hoc explanations.</p>
</li>
</ul>
<ul>
<li><p>Shape or Texture: Understanding Discriminative Features in CNNs </p>
<p>  Md Amirul Islam, Matthew Kowal, Patrick Esser, Sen Jia, Björn Ommer, Konstantinos G. Derpanis, Neil Bruce</p>
<p>  <strong>One-sentence Summary:</strong> Exploring and quantifying shape information encoded in CNNs.</p>
<p>  <strong>Reviewers say:</strong></p>
<ol>
<li>作者试图理解的问题在对象识别，纹理/形状偏差和深度神经网络中的学习表示等领域都很有趣且相关。</li>
<li>作者提供了一组不错的受控实验，这些实验表明了其中的某些效果，并且作者在其方法中提出了科学依据（尽管并不完美），但这与该领域非常相关。相反，这不是“另一篇论文，试图克服纹理偏差而没有任何直觉，试图获得更好的数字（不幸的是，这些天在计算机视觉中已经完成）”，相反，这篇论文是关于理解纹理/形状偏差的基本机制的知识，这些机制扩展了视觉层次结构中的计算的最终阶段，这就是为什么我倾向于接受。论文中几乎所有数字都很清楚，并有助于传达作者试图表达的内容（尽管有一些澄清点）</li>
</ol>
</li>
</ul>
<h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><ul>
<li><p>VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models<br>  Zhisheng Xiao, Karsten Kreis, Jan Kautz, Arash Vahdat</p>
</li>
<li><p>Disentangled Recurrent Wasserstein Autoencoder<br>  Jun Han, Martin Renqiang Min, Ligong Han, Xuan Zhang, Li Erran Li</p>
</li>
<li><p>Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders<br>  Mangal Prakash, Alexander Krull, Florian Jug</p>
</li>
<li><p>Tomographic Auto-Encoder: Unsupervised Bayesian Recovery of Corrupted Data<br>  Francesco Tonolini, Andreas Damianou, Pablo Garcia Moreno, Roderick Murray-Smith</p>
</li>
<li><p>Unsupervised Audiovisual Synthesis via Exemplar Autoencoders<br>  Kangle Deng, Aayush Bansal, Deva Ramanan</p>
</li>
<li><p>Property Controllable Variational Autoencoder via Invertible Mutual Dependence<br>  Xiaojie Guo, Yuanqi Du, Liang Zhao</p>
</li>
<li><p>Improving relational regularized autoencoders with spherical sliced fused Gromov Wasserstein<br>  Khai Nguyen, Son Nguyen, Nhat Ho, Tung Pham, Hung Bui</p>
</li>
<li><p>Learning a Latent Search Space for Routing Problems using Variational Autoencoders<br>  André Hottung, Bhanu Bhandari, Kevin Tierney</p>
</li>
<li><p>Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation<br>  Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, Noah Smith</p>
</li>
<li><p>Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning </p>
<p>  Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, Zhaopeng Tu</p>
<p>  <strong>Reviewers say:</strong> 本文介绍了对细粒度层的关注，以评估各个编码器层的作用并研究编码器层融合的工作原理，其中解码器层可以访问各种编码器层的信息，而与标准Transformer中的最终编码器层不同。</p>
<p>  基于以下观点：编码器嵌入层对于编码器层融合的成功至关重要，而最上层的解码器层则更加关注编码器嵌入层，因此提出了SurfaceFusion，该方法仅将编码器嵌入层连接到解码器的softmax层。 ，导致BLEU等指标获得了可观的收益。</p>
</li>
</ul>
<h2 id="Missing-value-amp-irregularly-sampled-time-series"><a href="#Missing-value-amp-irregularly-sampled-time-series" class="headerlink" title="Missing value &amp; irregularly sampled time series"></a>Missing value &amp; irregularly sampled time series</h2><ul>
<li><p>Multi-Time Attention Networks for Irregularly Sampled Time Series </p>
<p>  Satya Narayan Shukla, Benjamin Marlin</p>
<p>  <strong>One-sentence Summary:</strong> This paper presents a new deep learning architecture for learning with sparse and irregularly sampled multivariate time series.</p>
<p>  <strong>Reviewers say:</strong> 本文讨论不规则样本时间序列的分析。该方法主要基于插值。因此，作者可以研究有监督和无监督的问题。该架构由正弦波注意层，可在潜在空间中形成固定大小的界标的VAE层和RNN解码器组成。对于监督任务，作者添加了分类损失。</p>
<ul>
<li>他们在插值任务中获得了令人印象深刻的结果，而在分类任务中获得了有趣的结果。</li>
<li>在插值问题中，我们想将稳健的基线视为线性插值或类似AR的模型。即使我必须承认作者已经提出了与最近文献的模型进行的大量比较，这也将为我们提供有意义的MSE结果，以比较其他方法。</li>
<li>结果令人印象深刻，但我不知道架构的哪个部分会带来如此出色的性能</li>
</ul>
</li>
<li><p>not-MIWAE: Deep Generative Modelling with Missing not at Random Data </p>
<p>  Niels Bruun Ipsen, Pierre-Alexandre Mattei, Jes Frellsen</p>
<p>  <strong>One-sentence Summary:</strong> We present an approach for building and fitting deep latent variable models (DLVMs) in cases where the missing process is dependent on the missing data.</p>
<p>  <strong>Reviewers say:</strong> 本文提出了一种对数据进行深度潜变量模型训练的方法，这些数据不是随机丢失的。为了学习深潜变量模型的参数，本文采用重要性加权变分推理技术。在各种数据集上进行的实验表明，该方法通过显式地建模随机数据中缺失的模型而有效。</p>
</li>
</ul>
<h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><ul>
<li><p>Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies </p>
<p>  T. Konstantin Rusch, Siddhartha Mishra</p>
<p>  <strong>One-sentence Summary:</strong> A biologically motivated and discretized ODE based RNN for learning long-term dependencies, with rigorous bounds mitigating the exploding and vanishing gradient problem.</p>
</li>
</ul>
<ul>
<li><p><strong>【重点阅读】</strong>Recurrent Independent Mechanisms (Spotlight)</p>
<p>  Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, Bernhard Schölkopf</p>
<p>  <strong>One-sentence Summary:</strong> Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.</p>
<p>  <strong>Reviewers say:</strong> 本文提出了一种新颖的递归网络，称为RIM，以提高对局部变化的泛化性和鲁棒性。该网络由很大程度上独立的经常性模块组成，这些模块很少被激活，并通过柔和的注意力进行交互。在一系列不同任务上进行的实验表明，RIM的推广性优于LSTM。</p>
</li>
<li><p>Disentangled Recurrent Wasserstein Autoencoder (Spotlight)</p>
<p>  Jun Han, Martin Renqiang Min, Ligong Han, Xuan Zhang, Li Erran Li</p>
<p>  <strong>One-sentence Summary:</strong> We propose the first recurrent Wasserstein Autoencoder for learning disentangled representations of sequential data with theoretical analysis.</p>
<p>  <strong>Reviewers say:</strong> 本文提出了一种学习序列数据的静态和动态潜在变量的解缠方法。在学习目标方面，本文将Wasserstein自动编码器扩展到顺序数据，这种方法新颖且动机良好。静态变量的聚合后验自然而然地出现，并且对正则化起着重要作用（这对于序列数据来说似乎是新的）。作者还研究了如何为真实情景中的弱监督学习建模其他分类变量。图形化的模型清楚地说明了主要步骤（生成和推断），并提供了严格的说明来支持它们。实验结果证明了该方法在纠缠性能和生成质量方面的优势。</p>
</li>
<li><p>The geometry of integration in text classification RNNs </p>
<p>  Kyle Aitken, Vinay Venkatesh Ramasesh, Ankush Garg, Yuan Cao, David Sussillo, Niru Maheswaranathan</p>
<p>  <strong>One-sentence Summary:</strong> We study text classification RNNs using tools from dynamical systems analysis, finding and explaining the geometry of low-dimensional attractor manifolds.</p>
<p>  <strong>Reviewers say:</strong> 继Maheswaranathan等人（2019）和Maheswaranathan＆Sussillo（2020）等最近的研究之后，本文加入了研究循环网络解决监督序列分类问题的机制的研究领域。在此过程中，本文假设并确认了递归网络（无论是GRU还是LSTM）的内部隐藏状态在读取输入时在平面（近似）吸引子上演化，相当于在处理输入序列时整合了证据。 ，并针对三种类型的问题（分类，有序分类和多标签分类）展示了这些吸引子的存在和集成动态。</p>
</li>
<li><p>Uncertainty Estimation and Calibration with Finite-State Probabilistic RNNs </p>
<p>  Cheng Wang, Carolin Lawrence, Mathias Niepert</p>
<p>  <strong>One-sentence Summary:</strong> A method to estimate and calibrate uncertainty in recurrent state transitions.</p>
<p>  <strong>Reviewers say:</strong> 本文提出了一种量化RNN不确定性的方法，这是各种应用中的重要问题。它提供了各种领域的结果，表明所提出的方法优于基线。但是，除了考虑的基线（例如协方差传播，先验网络和正交证书）以外，通过针对特定任务与SOTA方法进行比较，这些实验将大大受益。还可以通过添加理论上的解释来解释Gumbel softmax函数如何捕获基本数据和模型不确定性，从而对本文进行改进。</p>
</li>
<li><p>RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs </p>
<p>  Meng Qu, Junkun Chen, Louis-Pascal Xhonneux, Yoshua Bengio, Jian Tang</p>
<p>  <strong>One-sentence Summary:</strong> Learn Logic Rules for Reasoning on Knowledge Graphs.</p>
<p>  <strong>Reviewers say:</strong> 在这项工作中，作者举例说明了一种从知识图开始学习逻辑规则的方法。学习逻辑规则比简单地执行链接预测更有趣，因为规则是人类可读的，因此具有可解释性。该方法似乎很有趣，并且所解决的问题可能会引起广泛的关注。它似乎不是很新颖，但是对我来说似乎是有效的。这篇论文写得很好并且自成体系。此外，实验结果表明，与其他系统相比（甚至与不学习规则而仅执行链接预测的系统相比），该方法具有竞争优势。</p>
</li>
<li><p>SkipW: Resource adaptable RNN with strict upper computational limit </p>
<p>  Tsiry Mayet, Anne Lambert, Pascal Leguyadec, Francoise Le Bolzer, François Schnitzler</p>
<p>  <strong>One-sentence Summary:</strong> Skip-Window is a method to allow recurrent neural networks (RNNs) to trade off accuracy for computational cost during the analysis of a sequence while keeping a strict upper computational limit.</p>
<p>  <strong>Reviewers say:</strong> 此提交内容提供了SkipRNN（跳过窗口）的扩展，它将输入序列分割为长度为L的窗口，从中只能使用K个样本。这保证了不会超过计算预算。跳过窗口通过在每个窗口的开头并行预测L个更新概率来实现这种归纳偏差。需要在训练之前设置L，而可以在测试时修改K。该模型在两个任务中评估，即合成添加任务和人类活动识别。作者报告了小型平台中的延迟和能耗，显示了该研究方向对实际应用的影响。</p>
</li>
<li><p>Multi-timescale Representation Learning in LSTM Language Models </p>
<p>  Shivangi Mahto, Vy Ai Vo, Javier S. Turek, Alexander Huth</p>
<p>  <strong>One-sentence Summary:</strong> This work presents a theoretically-motivated analysis of memory and timescale in LSTM language models.</p>
<p>  <strong>Reviewers say:</strong> 本文指出自然语言中单词之间的关系通常遵循幂律。门控递归神经网络（例如LSTM）在自然语言建模方面表现出色，但是LSTM的遗忘机制是由指数衰减决定的。这项工作展示了一种工程化LSTM的遗忘机制，以模仿自然语言中表现出的幂律关系的方法。通过应用他们的技术，修改后的LSTM模型可以更好地建模稀有标记，这些标记通常跨越较长的时间范围，因此，该模型可以在频率较低的单词上获得较低的困惑度。本文的主要贡献是推导，该推导表明，在给出第一个输入令牌后，LSTM的遗忘门在零输入状态下会经历指数衰减。</p>
<p>  实验表明，从反伽马分布中绘制T是自然语言的自然拟合。然后，作者提出了利用此特性的多尺度LSTM模型。每个时间标度T是从反伽玛分布中得出的，该反伽玛分布实际上成为一个遗忘偏差项，并且在训练过程中是固定的。绘制多个T来模拟幂律。多尺度LSTM可以捕获正确的归纳偏置，以便在对频率较低的单词进行建模时表现更好，这可能有助于在内存中保留更长的时间。该论文写得很好，并且该方法的动机和解释都清楚。实验经过适当设计，结果很好地支持了主要主张。</p>
</li>
</ul>
<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><ul>
<li><p>Sparse Quantized Spectral Clustering (Spotlight)</p>
<p>  Zhenyu Liao, Romain Couillet, Michael W. Mahoney</p>
<p>  <strong>Reviewers say:</strong> </p>
<ol>
<li>本文对通过将非线性函数应用于随机矩阵而获得的矩阵光谱进行了很好的分析。</li>
<li>这是一篇很好的论文，表明人们可以扰动内核矩阵（或使其通过非线性变换），而不必显着修改基础本征谱，因此不会损害应用于矩阵的光谱聚类的性能。我立即可以看到的最重要的应用是稀疏化内核矩阵，以便可以有效地进行计算使用。或类似地，如作者所述，应用量化和二值化。</li>
</ol>
</li>
</ul>
<ul>
<li><p>A Critique of Self-Expressive Deep Subspace Clustering </p>
<p>  Benjamin David Haeffele, Chong You, Rene Vidal</p>
<p>  <strong>One-sentence Summary:</strong> Here we show theoretically and experimentally that there are a number of flaws with many existing self-expressive deep subspace clustering models.</p>
<p>  <strong>Reviewers say:</strong> 摘要：本文对自表达深度子空间聚类（SEDSC）模型的先前结果的重要性提出了疑问，这些模型被吹捧为线性子空间聚类（使用自表达属性）成功扩展到非线性数据结构。作者提出了一组理论结果，这些结果表明SEDSC的标准配方通常是不适的。即使增加了正则化，也显示出这种公式可以很好地产生琐碎的几何形状，不利于成功的子空间聚类。</p>
</li>
</ul>
<ul>
<li><p>Intraclass clustering: an implicit learning ability that regularizes DNNs </p>
<p>  Simon Carbonnelle, Christophe De Vleeschouwer</p>
<p>  <strong>One-sentence Summary:</strong> This paper provides empirical evidence that deep neural networks are implicitly regularized through their ability to extract meaningful clusters among the samples of a class.</p>
<p>  <strong>Reviewers say:</strong></p>
<ol>
<li>本文研究了在监督学习中训练的神经网络的类内聚类能力，发现尽管标签没有明确强制这样做，但网络仍显示出类内聚类能力。并且基于这些标准的准则与模型泛化性能很好地相关。</li>
<li>作者注意到，在分类任务中，通常存在未在粗糙类标签中明确编码的相似图像的类内组，他们称之为类内聚类。他们假设，DNN在没有明确告知的情况下识别这些类内集群的能力可能与泛化相关。然后，他们继续在一系列网络，体系结构和大量超参数配置上对此进行验证。他们会在可能的情况下建立因果关系。此外，他们表明，可以使用简单的基于方差的方法检测类内聚类，并且它在训练的早期就出现了。</li>
</ol>
</li>
<li><p>Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation </p>
<p>  Yaling Tao, Kentaro Takagi, Kouta Nakata</p>
<p>  <strong>One-sentence Summary:</strong> We present a clustering-friendly representation learning method using instance discrimination and feature decorrelation, which achieves accuracy of 81.5% and 95.4% on CIFAR-10 and ImageNet-10, respectively, far above state-of-the-art values.</p>
<p>  <strong>Reviewers say:</strong> 作者提出了一种改进的基于深度学习的表示学习方法，该方法为聚类分析提供了更有效的功能。（1）根据在几个广泛使用的数据集上进行的比较实验，将softmax公式化的正交约束的集成能够提供更稳定的潜在特征表示。（2）据了解，广泛使用的深度聚类方法用于替代优化特征表示模型参数并更新由聚类方法（例如k-means）提供的锚点，我想知道本研究中提出的方法是否可以以真正的端到端方式集成这两个步骤。（3）我对这种拟议的表征学习方法的评估指标远远超过了最新的水平，印象深刻。尽管作者提供了CIFAR-10数据集上潜在特征的一些分布图，但是ImageNet-10上的可视化又如何呢？此外，添加一些存在于原始图像空间而非潜在空间中的“真实”可视化结果可以帮助说明所提出的方法是否可以从视觉内容的角度挖掘视觉上有意义的概念。</p>
</li>
<li><p>MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering </p>
<p>  Tsung Wei Tsai, Chongxuan Li, Jun Zhu</p>
<p>  <strong>One-sentence Summary:</strong> A principled probabilistic clustering method that exploits the discriminative representations learned by contrastive learning and the semantic structures captured by a latent mixture model in a unified framework.</p>
<p>  <strong>Reviewers say:</strong> 简介：作者提出了“专家混合”类型的方法来解决无监督学习问题的聚类。该方法称为对比专家混合（MiCE），它使用对比学习作为基本模块，并将其与潜在的混合模型相结合。作者为MiCE开发了一种可扩展算法，并根据经验评估了提出的图像聚类方法。</p>
</li>
<li><p>Deep Learning meets Projective Clustering </p>
<p>  Alaa Maalouf, Harry Lang, Daniela Rus, Dan Feldman</p>
<p>  <strong>One-sentence Summary:</strong> We suggest a novel technique for compressing a fully connected layer (or an embedding layer).</p>
<p>  <strong>Reviewers say:</strong>     这项工作提出了一种基于投影聚类的新方法，用于压缩DNN的嵌入层以实现自然语言建模任务。作者表明，通过考虑一组k个子空间而不是单个子空间，可以改善压缩和模型精度之间的折衷。压缩DNN的方法是研究的活跃领域，本文提出了一种有前途的方法以及有趣的结果。</p>
<p>  评级：本文提出了一些有趣的想法来压缩嵌入层。但是，由于这是一篇经验论文，因此我希望能得到一组更全面的经验结果，并能更好地与其他相关方法进行比较。</p>
</li>
<li><p>Isotropy in the Contextual Embedding Space: Clusters and Manifolds </p>
<p>  Xingyu Cai, Jiaji Huang, Yuchen Bian, Kenneth Church</p>
<p>  <strong>One-sentence Summary:</strong> This paper reveals isotropy in the clustered contextual embedding space, and found low-dimensional manifolds in there.</p>
<p>  <strong>Reviewers say:</strong> 作者研究了自然语言的各种上下文嵌入模型的令牌嵌入空间。他们使用基于最近邻居，聚类和PCA的技术，在这些嵌入模型中报告了关于局部维数/各向异性/聚类/流形结构的各种结果，这是希望了解这些模型的科学家和从业人员普遍感兴趣的结果。这些包括在适当聚类和移动时发现嵌入中的（局部）各向同性，以及在GPT模型中出现明显的流形结构。</p>
</li>
<li><p>Deep Repulsive Clustering of Ordered Data Based on Order-Identity Decomposition </p>
<p>  Seon-Ho Lee, Chang-Su Kim</p>
<p>  <strong>One-sentence Summary:</strong> A deep clustering algorithm for ordered data is proposed based on the order-identity decomposition.</p>
<p>  <strong>Reviewers say:</strong> 作者描述了一种对有序数据进行预测的直观有效的方法。该方法使用了一种基于聚类的直观方法，该方法将数据分组为子集，子集中的项易于订购。该文件写得很清楚，并清楚地说明了该方法。本文显示了该方法的预测输出的几个示例，并显示了两个任务的结果（估计年龄，美学评分回归）。该方法在估计年龄的任务上达到了最先进的结果，并且在其他任务上具有竞争力。作者显示了关于年龄转变的进一步结果。</p>
</li>
</ul>
<h2 id="data-augmentation"><a href="#data-augmentation" class="headerlink" title="data augmentation"></a>data augmentation</h2><ul>
<li><p>Removing Undesirable Feature Contributions Using Out-of-Distribution Data </p>
<p>  Saehyung Lee, Changhwa Park, Hyungyu Lee, Jihun Yi, Jonghyun Lee, Sungroh Yoon</p>
<p>  <strong>One-sentence Summary:</strong> We propose a simple method, Out-of-distribution data Augmented Training (OAT), to leverage OOD data for adversarial and standard learning.</p>
<p>  <strong>Reviewers say:</strong> 本文研究了在训练过程中使用未标记的分布失调（OOD）数据来提高鲁棒性（和标准）准确性的效果。主要算法贡献是基于数据增强的鲁棒训练算法来训练损失，该算法经过精心设计以从其他OOD数据中受益。有趣的是，OOD数据带有随机标签，用于训练过程。如理论结果所示，这种输入OOD数据的方式有助于消除对非鲁棒特征的依赖性，从而提高鲁棒性。</p>
<p>  正如所有评论者（我都同意）所指出的那样，在训练中使用未标记的OOD数据的想法是新颖的/有趣的，并且该论文还展示了如何通过算法来做到这一点。数值结果也证实了所提出方法的有效性。</p>
</li>
<li><p><strong>【值得阅读】</strong> Negative Data Augmentation </p>
<p>  Abhishek Sinha, Kumar Ayush, Jiaming Song, Burak Uzkent, Hongxia Jin, Stefano Ermon</p>
<p>  <strong>One-sentence Summary:</strong> We propose a framework to do Negative Data Augmentation for generative models and self-supervised learning</p>
<p>  **Reviewers say:**本文研究了扩大负面实例（不仅仅是正面实例）如何改善各种表征学习任务。本文研究了许多不同的增强，并将它们应用于GAN和带有图像和视频的对比学习。</p>
<ul>
<li>优点：本文的一个主要优点是它的简单性。该方法很容易实现为几种方法，并且在本文评估的每种方法上都可获得很强的结果。这些方法基于GAN和图像和视频的对比学习进行了评估。</li>
<li><pre><code>尽管该方法的新颖性有限，但在建立一些理论结果以直观说明该方法为何起作用方面，本文做得很好。与缺乏直觉了解其工作原理的机器学习进步相比，本文在为这种方法提供一些解释和动机方面做得很好。
</code></pre>
</li>
<li>尽管本文着重于图像和视频，但相同的思想也可以扩展到其他形式，例如文本或音频。</li>
<li>实验令人信服，表明了这种想法的普遍性。实验是在几个不同的数据集上进行的。实验得到理论结果的支持，从而直观地说明了该方法为何有效。引言在确定与其他数据增强方法的差异方面做得很好，尤其是通过使用负面示例。</li>
</ul>
</li>
<li><p>CoDA: Contrast-enhanced and Diversity-promoting Data Augmentation for Natural Language Understanding </p>
<p>  Yanru Qu, Dinghan Shen, Yelong Shen, Sandra Sajeev, Weizhu Chen, Jiawei Han</p>
<p>  <strong>Reviewers say:</strong> NLP样本的增加是一项重要任务，没有明确的“适用于所有人”机制。这与计算机视觉形成鲜明对比，在计算机视觉中，存在诸如旋转，色调修改，饱和度以及多种其他技术的技术。这项工作试图通过提出一种将多种先前已知的方法仔细合并以生成各种标签保存示例的技术来解决该问题。RoBERTa上的实验结果强调了这种数据增强方法在文本分类（GLUE）下游任务中的适用性和重要性。</p>
</li>
<li><p>MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space </p>
<p>  Tsz-Him Cheung, Dit-Yan Yeung</p>
<p>  <strong>Reviewers say:</strong> 本文提出了一种使用潜在嵌入空间的统一数据扩充方法-学习连续的潜在转换空间，并找到在该空间中遍历的有效方向以进行数据增强。所提出的方法结合了现有的数据增强方法，例如对抗训练，三重丢失和联合训练。本文还确定了模型性能低下的输入示例，并创建了更难的示例来帮助模型改进其性能。在与文本，表格，时间序列和图像模态相对应的多个值上进行评估，除图像数据外，其性能均优于SOTA。</p>
<p>  本文回应了审阅者的反馈，以提供更详细的实验和更强的基准，并进行了消融研究以显示该方法不同组成部分的有效性。通过与其他SOTA方法进行彻底的经验比较，以及使用其他损失函数（例如中心损失，大边际损失和其他对比损失）作为本文中提出的三重损失的替代方法，可以进一步改善结果。</p>
</li>
<li><p>Training GANs with Stronger Augmentations via Contrastive Discriminator </p>
<p>  Jongheon Jeong, Jinwoo Shin</p>
<p>  <strong>One-sentence Summary:</strong> We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. </p>
<p>  <strong>Reviewers say:</strong> 本文旨在通过将对比性学习的原理纳入GAN鉴别器的训练中来改进生成对抗网络（GAN）的训练。与试图将GAN损失直接最小化的普通GAN不同，拟议的带有对比鉴别器（ContraD）的GAN变体使用鉴别器网络首先从给定的数据扩充和实际/生成的示例集中学习对比表示，然后训练基于学习到的对比表示的鉴别器。注意到这种混合的副作用是由于GAN训练而在对比学习中的改进。结果表明，带有对比鉴别器的GAN模型优于使用数据增强的其他技术。</p>
</li>
<li><p>Model Patching: Closing the Subgroup Performance Gap with Data Augmentation </p>
<p>  Karan Goel, Albert Gu, Yixuan Li, Christopher Re</p>
<p>  <strong>One-sentence Summary:</strong> We describe how to fix classifiers that fail on subgroups of a class using a combination of learned data augmentation &amp; consistency training to achieve subgroup invariance.</p>
<p>  <strong>Reviewers say:</strong> 本文提出了一种在分类器依赖于特定子组特征的情况下减轻图像中子组性能差距的方法。作者提出了一种数据增强方法，其中（由GANs生成的）合成示例充当所有可能子组中真实样本的实例。通过匹配原始示例和扩展示例的预测，预测模型被迫忽略鼓励不变性的亚组差异。所提出的“受控数据增强”方法（如R4所精确调用的）是相关且动机良好的，理论依据支持主要主张，并且实验结果多种多样，并证明了所提出方法的优点。正如R3正确指出的那样，“附录也非常详尽，代码组织得很好”。</p>
</li>
<li><p>SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization </p>
<p>  A F M Shahab Uddin, Mst. Sirazam Monira, Wheemyung Shin, TaeChoong Chung, Sung-Ho Bae</p>
<p>  <strong>One-sentence Summary:</strong> The proposed method carefully selects a representative image patch with the help of a saliency map and mixes this indicative patch with the target image that leads the model to learn more appropriate feature representation</p>
<p>  <strong>Reviewers say:</strong> </p>
<ol>
<li>本文提出了对数据混合的cutmix策略的一种改进，其中源补丁不是随机选择而是基于显着性选择。结果表明，在Imagenet，CIFAR 10/100上，混合和其他相关策略得到了改进，并且还可以转移到对象检测中</li>
<li>总结和贡献： 本文提出了一种新的数据增强策略来训练图像分类器和目标检测器。关键见解是使用图像显着性信号来指导混合图像时在何处裁剪和粘贴图像。本文包括对这种方法的设计空间的探索，以及多个实验结果，表明与现有的数据增强策略相比，该方法的经验优越性。</li>
</ol>
<p>  启示： 这篇论文很有趣，因为它提供了一个新的技巧，既易于理解，可以辩驳，又（现在）具有良好的经验支持（用于分类，检测和对抗攻击的鲁棒性）。</p>
<p>  创意：有限。尽管以前的工作都没有提供此处介绍的实验结果，但结果是可以预期的。这项工作是良好的A + B增量工作。</p>
</li>
<li><p>On Graph Neural Networks versus Graph-Augmented MLPs </p>
<p>  Zhengdao Chen, Lei Chen, Joan Bruna</p>
<p>  <strong>One-sentence Summary:</strong> We establish a separation in representation power between GNNs and Graph-Augmented MLPs.</p>
<p>  <strong>Reviewers say:</strong> 本文研究了图神经网络（GNN）的一种变体，即图增强MLP（GA-MLP）。与在GNN中，节点将消息发送到邻居并通过非线性MLP聚合接收到的消息不同，GA-MLP依赖于一次计算的单个增强嵌入，然后将MLP应用于新的嵌入。可以通过对输入表示应用形式为A，A ^ 2，…，A ^ k的线性变换来获得增强嵌入，从而捕获更大的邻域。本文的主要目的是证明与GNN相比，使用GA-MLP解决图形问题时的根本缺陷。沿着这些思路，主要结果可以描述如下：1）本文确定了识别非同构图的特定实例，可以通过GNN而不是GA-MLP框架来解决。2）本文对GNN与图增强MLP的表示能力进行了实验和实验评估，并根据根图上的节点级函数显示了两者之间的表达能力分离。具体来说，他们表明，可以用一定深度的GNN表示的一组函数在k中呈指数增长，而在考虑类似的GA-MLP体系结构时，函数类仅呈指数增长。他们还根据经验评估了两种模型在社区检测和步行问题计数方面的性能差异。</p>
</li>
<li><p>Explaining the Efficacy of Counterfactually Augmented Data </p>
<p>  Divyansh Kaushik, Amrith Setlur, Eduard H Hovy, Zachary Chase Lipton</p>
<p>  <strong>One-sentence Summary:</strong> We present a framework for thinking about counterfactually augmented data and make strides towards understanding its benefits in out-of-domain generalization.</p>
<p>  Reviewers says: 本文研究了反事实扩充的数据对域外泛化的影响。本文从具有高斯线性模型的结构因果模型的玩具示例开始，其中将噪声添加到因果或非因果特征上。休闲特征上的噪声增加会影响最小二乘估计，而非因果特征上的噪声却不会影响最小二乘估计。本文在这种情况和反事实文本编辑之间作了一个类比，其中假定跨度（合理值）被认为是因果关系。提出了一个假设，即在基本原理（因果关系特征）上添加噪声会导致模型依赖非理性因素（非因果关系特征）并导致较差的样本外性能，而在非理性因素中添加噪声则会导致更糟的结果。样本内性能，但更好的样本外性能。对情绪和自然语言推理（NLI）数据集的实验大多证实了这一假设，但有一些例外需要讨论。实验包括三种识别原理的方法（人工编辑，人工识别的跨度和通过自我注意识别的跨度）。在有理或无理的情况下对模型进行有无噪声训练（用随机令牌代替真实令牌）。</p>
</li>
<li><p>Reweighting Augmented Samples by Minimizing the Maximal Expected Loss </p>
<p>  Mingyang Yi, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, Zhi-Ming Ma</p>
<p>  <strong>One-sentence Summary:</strong> a new reweighting strategy on augmented samples</p>
<p>  <strong>Reviewers say:</strong> 本文提出了一种新颖的数据扩充方法。特别地，它提出了重新加权损失函数，该函数允许找到扩增样本的最佳加权。该方法在标准图像和语言任务上进行了测试，并与多种替代方法进行了比较。</p>
</li>
<li><p>Simple Augmentation Goes a Long Way: ADRL for DNN Quantization </p>
<p>  Lin Ning, Guoyang Chen, Weifeng Zhang, Xipeng Shen</p>
<p>  <strong>One-sentence Summary:</strong> Augments the neural networks in Deep Reinforcement Learning(DRL) with a complementary scheme to boost the performance of learning and solve the common low convergence problem in the early stage of DRL</p>
</li>
<li><p>On Data-Augmentation and Consistency-Based Semi-Supervised Learning </p>
<p>  Atin Ghosh, Alexandre H. Thiery</p>
<p>  <strong>One-sentence Summary:</strong> We propose a simple and natural framework leveraging the Hidden Manifold Model to study modern SSL methods.</p>
<p>  <strong>Reviewers say:</strong> 本文提供了一些有关在基于一致性正则化的半监督学习中使用数据增强的理论观点。本文使用的框架认为，高质量的数据增强应该沿着数据流形移动。这种通用视图允许将论文的思想应用于数据集（与现有技术的半监督学习算法中使用的图像特定数据增强相反）。我不知道有任何其他工作可以提出这些观点，并且事实上，本文的意义在于，它为最有效的半监督学习方法提供了新的且可能有用的观点。审稿人认为该文件清晰实用。主要关注的是，该论文仅包括玩具环境中的实验。确实，</p>
</li>
</ul>
<ul>
<li><p>Tradeoffs in Data Augmentation: An Empirical Study </p>
<p>  Raphael Gontijo-Lopes, Sylvia Smullin, Ekin Dogus Cubuk, Ethan Dyer</p>
<p>  <strong>One-sentence Summary:</strong> We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.</p>
<p>  <strong>Reviewers say:</strong> 本文研究了数据扩充问题，即通过修改现有的数据来获得新的训练示例。数据扩充在机器学习和人工智能中很流行，因为它增加了训练示例的数量。但是，其对模型性能的影响在实践中仍然未知。增强运算符（例如图像旋转）可能是有帮助的，也可能是有害的。本文介绍了两个新的指标，称为亲和力和多样性，以量化任何给定的扩增算子的效果。作者发现，具有高亲和力分数和高多样性分数的运算符可带来最佳性能改进。</p>
<p>  长处</p>
<pre><code>  - 引入的措施同时考虑了数据和模型，这在现代深度学习中是不可分割的。
  - 这些措施可以直观地解释为什么某些数据增强方法有效而另一些无效的原因。
  - 亲和力很容易计算。为了获得亲和力，需要一个在干净数据上训练的模型，并使用具有给定扩充的验证集。可以重用训练后的模型来衡量其他增强。
  - 实验是广泛的。
</code></pre>
</li>
<li><p>Combining Ensembles and Data Augmentation Can Harm Your Calibration </p>
<p>  Yeming Wen, Ghassen Jerfel, Rafael Muller, Michael W Dusenberry, Jasper Snoek, Balaji Lakshminarayanan, Dustin Tran</p>
<p>  <strong>One-sentence Summary:</strong> We found that combining ensembles and data augmentation worsens calibration than applying them individually, and we proposed a simple fix to it.</p>
<p>  <strong>Reviewers say:</strong> 这项工作分析了数据增强策略（例如MixUp）与模型集成之间在校准性能方面的交互作用。作者指出，将单个模型组合在一起时，诸如混合和标签平滑之类的策略如何减少单个模型的过分自信，从而导致校准性能下降。具体来说，所有技术都是单独采取的，通过减少过度自信来改善校准。但是，结合起来，它们会导致模型置信度不足，因此校准效果会更差。基于此分析，作者提供了一种简单的技术，可在CIFAR-10，CIFAR-10-C，CIFAR-100和CIFAR-100-C和ImageNet上产生SOTA校准性能。作者建议根据模型在特定类别上是否过度自信来动态启用和禁用MixUp，</p>
<p>  我认为这项工作提供了有用的见解以及简单有效的解决方案。另外，它写得很清楚，读起来非常容易和愉快。</p>
</li>
<li><p>GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing </p>
<p>  Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, bailin wang, Yi Chern Tan, Xinyi Yang, Dragomir Radev, richard socher, Caiming Xiong</p>
<p>  <strong>One-sentence Summary:</strong> Language model pre-training for table semantic parsing.</p>
<p>  <strong>Reviewers say:</strong> 本文提出了一种用于语义解析的预训练技术，重点是语义解析以及使其实际工作所需的技术细节。总体而言，所有审阅者都认为结果非常好，并且您看到跨多个文本到SQL数据集的良好改进。对于（a）创建用于生成综合数据的SCFG的难度提出了一些保留，但是作者似乎已经适当地解决了这一问题，并且需要付出合理的努力。（b）预训练任务是如何针对特定任务（文本到SQL）和数据集（蜘蛛）量身定制的。总体而言，我倾向于同意以下事实：由于语法是从蜘蛛衍生而来的，因此对蜘蛛的改进并不那么引人注目，但是作者正确地声称，即使收益略微减少，其他数据集的持续改进也是显而易见的。可以希望这个想法也可以推广到其他可以生成合成数据的设置，并且应该将合成数据与实际数据结合起来的详细信息很有用。</p>
</li>
</ul>
<h2 id="About-distribution"><a href="#About-distribution" class="headerlink" title="About distribution"></a>About distribution</h2><ul>
<li><p>Free Lunch for Few-shot Learning: Distribution Calibration<br>  Shuo Yang, Lu Liu, Min Xu</p>
</li>
<li><p>Improved Autoregressive Modeling with Distribution Smoothing<br>  Chenlin Meng, Jiaming Song, Yang Song, Shengjia Zhao, Stefano Ermon</p>
</li>
<li><p>Long-tailed Recognition by Routing Diverse Distribution-Aware Experts<br>  Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, Stella Yu</p>
</li>
</ul>
<ul>
<li><p>Meta-Learning of Structured Task Distributions in Humans and Machines<br>  Sreejan Kumar, Ishita Dasgupta, Jonathan Cohen, Nathaniel Daw, Thomas Griffiths</p>
</li>
<li><p>Convex Potential Flows: Universal Probability Distributions with Optimal Transport and Convex Optimization<br>  Chin-Wei Huang, Ricky T. Q. Chen, Christos Tsirigotis, Aaron Courville</p>
</li>
</ul>
 
            </div>
        </div>
        
        <div class="mt-3">
            <!-- 前一页后一页 -->
<div class="previous-next-links">
     
    <div class="previous-design-link">
        <a href="../../surveys/QTS/">
            <i style="font-size:16px;" class="fa fa-arrow-left" aria-hidden="true"></i>
            Quasi-periodic time series
        </a>
    </div>
     

     
    <div class="next-design-link">
        <a href="../WWW2021/">
            Related Papers in WWW 2021 (2021.04.19)
            <i style="font-size:16px;" class="fa fa-arrow-right" aria-hidden="true"></i>
        </a>
    </div>
 
</div> 
             
        </div>

    </div>
    <div class="col-2">
        <div class="d-none d-sm-none d-md-block sticky-top border-start">
             
    
        <div>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Anomaly-detection-anomaly-outlier-out-of-distribution-one-class"><span class="toc-number">1.</span> <span class="toc-text">Anomaly detection [anomaly, outlier, out-of-distribution, one-class]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Heterogeneous"><span class="toc-number">2.</span> <span class="toc-text">Heterogeneous</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Time-series"><span class="toc-number">3.</span> <span class="toc-text">Time series</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#About-deep-learning"><span class="toc-number">4.</span> <span class="toc-text">About deep learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sequence"><span class="toc-number">5.</span> <span class="toc-text">Sequence</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Interpretable"><span class="toc-number">6.</span> <span class="toc-text">Interpretable</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Autoencoder"><span class="toc-number">7.</span> <span class="toc-text">Autoencoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Missing-value-amp-irregularly-sampled-time-series"><span class="toc-number">8.</span> <span class="toc-text">Missing value &amp; irregularly sampled time series</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Recurrent-Neural-Network"><span class="toc-number">9.</span> <span class="toc-text">Recurrent Neural Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Clustering"><span class="toc-number">10.</span> <span class="toc-text">Clustering</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#data-augmentation"><span class="toc-number">11.</span> <span class="toc-text">data augmentation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#About-distribution"><span class="toc-number">12.</span> <span class="toc-text">About distribution</span></a></li></ol> 
        </div>
    
 

        </div>
    </div>
</div>
</div>
 
    </main>

    <!-- 底部栏 -->
    <footer class="bg-dark pt-1 pb-0 mt-5">
    <div class="container pb-3 pt-3 text-center">
        <p class="text-muted tag-hover">
            All Rights Reserved <i class="fa fa-copyright"></i> 2017-2022 笑颜网 smileyan.cn <br> 
            Powered by 
             <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://hexo.io">
                 Hexo.io
            </a> &  <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://github.com/smile-yan/hexo-theme-heyan">
               heyan
            </a>
            <br>
            <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://beian.miit.gov.cn/#/Integrated/index">
                <img src="/police.png" style="width: 18px; height: 18px; margin-top: -4px" class="nofancybox">
                湘ICP备 17012851号 
            </a>
        </p>
    </div>
</footer>
</body> 
</html>