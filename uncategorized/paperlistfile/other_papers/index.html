
<!DOCTYPE html>
<html>
<head>
    <title>Smileyan</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
    <link href="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">
    <script src="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"></script>
    
<link rel="stylesheet" href="../../../css/prism.css">
 
    
<script src="../../../js/prism.js"></script>

    
    
<link rel="stylesheet" href="../../../css/index.css">
 
    
<script src="../../../js/search.js"></script>



     
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
        </script>
     

     
        <script src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js"> </script>
        
<script src="../../../js/fancybox.js"></script>

    

    <script type="text/javascript">
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
        var path = "/" + search_path;
        searchFunc(path, 'local-search-input', 'local-search-result');
    </script>
<meta name="generator" content="Hexo 5.4.2"></head>
 
 
<body>
    <!-- 导航栏 -->
    <!-- 导航栏 -->
<nav class="navbar navbar-expand-md navbar-dark bg-dark mb-4 pt-2 pb-2">
    <div class="container">
        <!-- 标题 --> 
        <a class="navbar-brand navbar-expand-sm" href="/">
            <img class="nofancybox" src="/logo.png" style="width: 75px;"  alt="笑颜网">
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
                aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <!-- 左边右边导航按钮 --> 
        <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav me-auto">
                <!-- 左右两侧的导航栏 -->

 
 

    <li class="nav-item">
        <a class="nav-link" href="../../../">
           <big> <i class="fa fa-home ps-1" ></i> 主页</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../archives">
           <big> <i class="fa fa-archive ps-1" ></i> 归档</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../about">
           <big> <i class="fa fa-user ps-1" ></i> 关于</big>
        </a>
    </li>
 
 
            </ul>

            
            <form class="">
                <div class="d-flex">
                    <button class="btn text-muted fa fa-search d-none d-md-block d-lg-block" disabled></button>
                    <input id="local-search-input" class="form-control me-2 pe-4" type="search"
                            placeholder="搜索 " aria-label="Search">
                </div>
                <div id="local-search-result" style="position:absolute; padding-top: 8px; max-height: 960px; width: 480px;overflow-y: scroll; z-index: 1050;"></div>
            </form>
            
            <ul class="navbar-nav">
                <!-- 左右两侧的导航栏 -->

 
 
            </ul>
        </div>
    </div>
</nav>


    <main class="container">
        
<div class="container-fluid markdown-section">
<div class="row">
    <div class="col-md-2 d-none d-sm-none d-md-block">
        <div class="">
            <!-- 同一类型的文件 -->
<!-- 尽可能做到每篇文章只有一个类
    1. 获得 post 的类别 categories
     2.1 如果 categories.length === 0，啥也不做
     2.2 如果 categories === 1，渲染这个类的所有文章
     2.3 如果 categories.length > 1 ，则以第一个类为准渲染所有文章
-->


 
        </div>
    </div>
    <div class="col-md-8 col-sm-12">
        <!-- 博客详情 -->
        <div class="">
            <!-- ps-4 pe-4 pt-2 -->
            <p class="h2">
                <span class="post-title">
                    Some Well-known Papers
                </span>   
            </p>

            <div class="pb-3 pt-1 pe-3">
                <i class="fa fa-calendar p-1"></i>
                2019/01/01 00:00:00 

                <i class="fa fa-folder-open p-1"> </i> 
                <span class="tag-hover">
                     
                </span>

                <i class="fa fa-tag p-1"> </i>
                <span class="tag-hover">
                    
                        <a href="../../../tags/paper-list/" class="link-dark text-decoration-none"> 
                            paper list 
                        </a>
                    
                </span>
            </div>

            <div class="border-bottom pb-2">
                <p> 一些经典的论文</p>
<h2 id="VAEs"><a href="#VAEs" class="headerlink" title="VAEs"></a>VAEs</h2><ul>
<li><p>Auto-Encoding Variational Bayes</p>
<p>提出VAE</p>
</li>
<li><p>Wasserstein Auto-Encoders</p>
<p>提出WAE</p>
</li>
</ul>
<span id="more"></span>
<ul>
<li><p>ICLR20 - FROM VARIATIONAL TO DETERMINISTIC AUTOENCODERS</p>
<p>Github: <a target="_blank" rel="noopener" href="https://github.com/ParthaEth/Regularized_autoencoders-RAE-">Link</a></p>
<p><strong>解决的问题</strong>: 建模数据的隐空间分布模型，提出RAE（Regularized Autoencoder）。</p>
<p><strong>创新与独特</strong>: RAE无需假设数据的隐空间分布符合任何先验分布。</p>
<p><strong>采用的方法</strong></p>
<ol>
<li>将传统VAE中，从先验分布中采集随机数据送入decoder的操作，视为一种decoder的正则项，用于辅助学习一个平滑的分布。</li>
<li>将数据的隐空间分布视为拥有某均值与固定方差的一个未知分布，得到数据隐空间表达后，再估计一个后验的数据分布。</li>
</ol>
<p><strong>为何选择/如何应用</strong>: 本文的方法作为一种强大的数据分布估计方法，不预设任何先验分布，在其他复杂的异常检测任务上能够对数据进行建模。</p>
<p><strong>数据集</strong></p>
<ol>
<li>MNIST</li>
<li>CIFAR</li>
<li>CELEBA（人像属性数据集）</li>
</ol>
</li>
<li><p>NIPS18 - Gaussian Process Prior Variational Autoencoders</p>
<p>提出GPVAE，即隐空间变量遵从高斯过程，而非一个时不变的分布。</p>
</li>
<li><p>ICLR19 - INTERPRETABLE DISCRETE REPRESENTATION LEARNING ON TIME SERIES</p>
<p>提出SOM-VAE, <a target="_blank" rel="noopener" href="https://github.com/ratschlab/SOM-VAE">Github</a></p>
<blockquote>
<p>High-dimensional time series are common in many domains. Since human cognition is<br>not optimized to work well in high-dimensional spaces, these areas could benefit from<br>interpretable low-dimensional representations. However, most representation learning<br>algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings<br>from data features to salient properties of the representation and non-smoothness over time.<br>To address this problem, we propose a new representation learning framework building on<br>ideas <strong>from interpretable discrete dimensionality reduction and deep generative modeling.</strong><br>This framework allows us to learn discrete representations of time series, which give rise to<br>smooth and interpretable embeddings with superior clustering performance. <strong>We introduce<br>a new way to overcome the non-differentiability in discrete representation learning and<br>present a gradient-based version of the traditional self-organizing map algorithm that is<br>more performant than the original.</strong> Furthermore, to allow for a probabilistic interpretation of<br>our method, we integrate a Markov model in the representation space. This model uncovers<br>the temporal transition structure, improves clustering performance even further and provides<br>additional explanatory insights as well as a natural representation of uncertainty.<br>We evaluate our model in terms of clustering performance and interpretability on static<br>(Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a<br>chaotic Lorenz attractor system with two macro states, as well as on a challenging real world<br>medical time series application on the eICU data set. Our learned representations compare<br>favorably with competitor methods and facilitate downstream tasks on the real world data.</p>
<p>…</p>
<p><em>Time series from the data space [green] are encoded<br>by a neural network [black] time-point-wise into the latent space. The latent data manifold is approximated<br>with a self-organizing map (SOM) [red]. In order to achieve a discrete representation, every latent data point<br>($z_e$) is mapped to its closest node in the SOM ($z_q$). A Markov transition model [blue] is learned to predict<br>the next discrete representation ($z_q ^{t+1}$) given the current one ($z_q ^{t}$). The discrete representations can then be<br>decoded by another neural network back into the original data space.</em></p>
</blockquote>
</li>
<li><p>NIPS17 - Neural Discrete Representation Learning</p>
<p>Github: <a target="_blank" rel="noopener" href="https://github.com/nakosung/VQ-VAE">Link</a></p>
<p>A useful blog: <a target="_blank" rel="noopener" href="http://ameroyer.github.io/projects/2019/08/20/VQVAE.html">Link</a></p>
<p>Some code and its explaination: <a target="_blank" rel="noopener" href="http://ameroyer.github.io/projects/2019/08/20/VQVAE.html">Link</a></p>
<p><strong>解决的问题</strong>: 学习数据的隐空间离散化表示，该离散化表示可以用于压缩编码、生成更清晰的图像等。</p>
<p><strong>创新与独特</strong>：第一个学习数据离散表示的工作。</p>
<p><strong>采用的方法</strong></p>
<ol>
<li><p>首先将数据使用encoder映射到隐空间，然后学习数据在隐空间的类别分布(<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59550457">categorical distribution</a>)，<br>然后使用一个编码表对连续的隐空间表示做近似，得到离散化的表示，该离散化的表示载送入decoder进行数据还原（文章主要着笔与图像的生成，文章中使用pixelCNN作为decoder，该结构接收离散化的表示作为输入。文章也介绍了音频的生成，即序列生成。）。</p>
</li>
<li><p>进行隐空间离散化的主要方式是，将一个D维的向量通过查表操作，量化到距离他最近的一个数值上。而该表也将在训练中更新，使其能够更加接近真实值。</p>
</li>
<li><p>本文的大量笔墨用在如何对这个网络进行更新上，因为查表的量化操作是无法进行梯度计算的。</p>
</li>
</ol>
<p><strong>为何选择/如何应用</strong></p>
<ol>
<li><p>本文所提出的方法能够更逼真的重建样本，媲美BigGAN.</p>
</li>
<li><p>所提出的方法适用于建模离散数据，而大多数的真实世界数据均为离散化的。</p>
</li>
</ol>
<p><strong>数据集</strong></p>
<ol>
<li><p>CIFAR10, ImageNet</p>
</li>
<li><p>VCTK dataset(语音分类数据集)</p>
</li>
<li><p>action sequence(deepMind的视频序列数据集)</p>
</li>
</ol>
</li>
<li><p>IJCAI19 - CLVSA A Convolutional LSTM Based Variational Sequence-to-Sequence Model with Attention for Predicting Trends of Financial Markets</p>
<p>提出CLVSA，还没读</p>
</li>
</ul>
<h2 id="Anomaly-detection"><a href="#Anomaly-detection" class="headerlink" title="Anomaly detection"></a>Anomaly detection</h2><blockquote>
<p>The work that is the most related to ours is <strong>AnoGAN (Schlegl et al., 2017)</strong>.</p>
<p>…</p>
<p><strong>Bergmann et al. (2019)</strong> compare standard AE reconstructions techniques<br>to AnoGAN, and observes that AnoGAN’s performances on anomaly localizations tasks are<br>poorer than AE’s due to the mode collapse tendency of GAN architectures. Interestingly, updates on<br>AnoGAN such as <strong>fast AnoGAN (Schlegl et al., 2019)</strong> or <strong>AnoVAEGAN (Baur et al., 2018)</strong> replaced<br>the gradient descent search of the optimal z with a learned encoder model, yielding an approach<br>very similar to the standard VAE reconstruction-based approaches, but with a reconstruction loss<br>learned by a discriminator, which is still prone to mode collapse (Thanh-Tung et al., 2019).</p>
</blockquote>
<p>  <strong>AnoGAN (Schlegl et al., 2017)</strong> Thomas Schlegl, Philipp Seeböck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg<br>  Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker<br>  discovery. In International Conference on Information Processing in Medical Imaging, pp. 146–157. Springer, 2017.</p>
<p>  <strong>Bergmann et al. (2019)</strong> Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad—a comprehensive real-world<br>  dataset for unsupervised anomaly detection. CVPR, 2019.</p>
<p>  <strong>fast AnoGAN (Schlegl et al., 2019)</strong> Thomas Schlegl, Philipp Seeböck, Sebastian M. Waldstein, Georg Langs, and Ursula Schmidt-<br>  Erfurth. f-anogan: Fast unsupervised anomaly detection with generative adversarial networks.<br>  Medical Image Analysis, 54:30 – 44, 2019. ISSN 1361-8415. doi: <a target="_blank" rel="noopener" href="https://doi.org/10.1016/j">https://doi.org/10.1016/j</a>.<br>  media.2019.01.010.</p>
<p>  <strong>AnoVAEGAN (Baur et al., 2018)</strong> Christoph Baur, BenediktWiestler, Shadi Albarqouni, and Nassir Navab. Deep autoencoding models<br>  for unsupervised anomaly segmentation in brain MR images. CoRR, abs/1804.04488, 2018.</p>
<hr>
 
            </div>
        </div>
        
        <div class="mt-3">
            <!-- 前一页后一页 -->
<div class="previous-next-links">
     
    <div class="previous-design-link">
        <a href="../../surveys/dataaug/">
            <i style="font-size:16px;" class="fa fa-arrow-left" aria-hidden="true"></i>
            Data Augmentation
        </a>
    </div>
     

     
</div> 
             
        </div>

    </div>
    <div class="col-2">
        <div class="d-none d-sm-none d-md-block sticky-top border-start">
             
    
        <div>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#VAEs"><span class="toc-number">1.</span> <span class="toc-text">VAEs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Anomaly-detection"><span class="toc-number">2.</span> <span class="toc-text">Anomaly detection</span></a></li></ol> 
        </div>
    
 

        </div>
    </div>
</div>
</div>
 
    </main>

    <!-- 底部栏 -->
    <footer class="bg-dark pt-1 pb-0 mt-5">
    <div class="container pb-3 pt-3 text-center">
        <p class="text-muted tag-hover">
            All Rights Reserved <i class="fa fa-copyright"></i> 2017-2022 笑颜网 smileyan.cn <br> 
            Powered by 
             <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://hexo.io">
                 Hexo.io
            </a> &  <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://github.com/smile-yan/hexo-theme-heyan">
               heyan
            </a>
            <br>
            <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://beian.miit.gov.cn/#/Integrated/index">
                <img src="/police.png" style="width: 18px; height: 18px; margin-top: -4px" class="nofancybox">
                湘ICP备 17012851号 
            </a>
        </p>
    </div>
</footer>
</body> 
</html>