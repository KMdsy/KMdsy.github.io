
<!DOCTYPE html>
<html>
<head>
    <title> Here is Shaoyu </title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--ç½‘é¡µæ ‡é¢˜å·¦ä¾§æ˜¾ç¤º-->
    <link rel="icon" href="/fish.ico" type="image/x-icon">
    <!--æ”¶è—å¤¹æ˜¾ç¤ºå›¾æ ‡-->
    <link rel="shortcut icon" href="/fish.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
    <link href="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">
    <script src="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"></script>
    
<link rel="stylesheet" href="../../../css/prism.css">
 
    
<script src="../../../js/prism.js"></script>

    
    
<link rel="stylesheet" href="../../../css/index.css">
 
    
<script src="../../../js/search.js"></script>



     
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
        </script>
     

     
        <script src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js"> </script>
        
<script src="../../../js/fancybox.js"></script>

    

    <script type="text/javascript">
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
        var path = "/" + search_path;
        searchFunc(path, 'local-search-input', 'local-search-result');
    </script>
<meta name="generator" content="Hexo 5.4.2"></head>
 
 
<body>
    <!-- å¯¼èˆªæ  -->
    <!-- å¯¼èˆªæ  -->
<nav class="navbar navbar-expand-md navbar-dark bg-dark mb-4 pt-2 pb-2">
    <div class="container">
        <!-- æ ‡é¢˜ --> 
        <a class="navbar-brand navbar-expand-sm" href="/">
            <img class="nofancybox" src="/shark.svg" style="width: 65px;">
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
                aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <!-- å·¦è¾¹å³è¾¹å¯¼èˆªæŒ‰é’® --> 
        <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav me-auto">
                <!-- å·¦å³ä¸¤ä¾§çš„å¯¼èˆªæ  -->

 
 

    <li class="nav-item">
        <a class="nav-link" href="../../../">
           <big> <i class="fa fa-home ps-1" ></i> Home</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../note">
           <big> <i class="fa fa-file-text-o ps-1" ></i> Notes</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../survey">
           <big> <i class="fa fa-globe ps-1" ></i> Surveys</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../paperlist">
           <big> <i class="fa fa-cc-discover ps-1" ></i> Paper Lists</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../archives">
           <big> <i class="fa fa-archive ps-1" ></i> Archives</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../about">
           <big> <i class="fa fa-user ps-1" ></i> About</big>
        </a>
    </li>
 
 
            </ul>

            
            <form class="">
                <div class="d-flex">
                    <button class="btn text-muted fa fa-search d-none d-md-block d-lg-block" disabled></button>
                    <input id="local-search-input" class="form-control me-2 pe-4" type="search"
                            placeholder="æœç´¢ " aria-label="Search">
                </div>
                <div id="local-search-result" style="position:absolute; padding-top: 8px; max-height: 960px; width: 480px;overflow-y: scroll; z-index: 1050;"></div>
            </form>
            
            <ul class="navbar-nav">
                <!-- å·¦å³ä¸¤ä¾§çš„å¯¼èˆªæ  -->

 
 
            </ul>
        </div>
    </div>
</nav>


    <main class="container">
        
<div class="container-fluid markdown-section">
<div class="row">
    <div class="col-md-2 d-none d-sm-none d-md-block">
        <div class="">
            <!-- åŒä¸€ç±»å‹çš„æ–‡ä»¶ -->
<!-- å°½å¯èƒ½åšåˆ°æ¯ç¯‡æ–‡ç« åªæœ‰ä¸€ä¸ªç±»
    1. è·å¾— post çš„ç±»åˆ« categories
     2.1 å¦‚æœ categories.length === 0ï¼Œå•¥ä¹Ÿä¸åš
     2.2 å¦‚æœ categories === 1ï¼Œæ¸²æŸ“è¿™ä¸ªç±»çš„æ‰€æœ‰æ–‡ç« 
     2.3 å¦‚æœ categories.length > 1 ï¼Œåˆ™ä»¥ç¬¬ä¸€ä¸ªç±»ä¸ºå‡†æ¸²æŸ“æ‰€æœ‰æ–‡ç« 
-->


 
        </div>
    </div>
    <div class="col-md-8 col-sm-12">
        <!-- åšå®¢è¯¦æƒ… -->
        <div class="">
            <!-- ps-4 pe-4 pt-2 -->
            <p class="h2">
                <span class="post-title">
                    Optimization for Nondifferentiable Problem
                </span>   
            </p>

            <div class="pb-3 pt-1 pe-3">
                <i class="fa fa-calendar p-1"></i>
                2023/02/17 11:10:00 

                <i class="fa fa-pencil"> </i>
                2023/02/21 21:13:00 

                <!--
                <i class="fa fa-folder-open p-1"> </i> 
                <span class="tag-hover">
                     
                </span>
                -->

                <i class="fa fa-tags p-1"> </i>
                <span class="tag-hover">
                    
                        <a href="../../../tags/survey/" class="link-dark text-decoration-none"> 
                            survey 
                        </a>
                    
                        <a href="../../../tags/optimization/" class="link-dark text-decoration-none"> 
                            optimization 
                        </a>
                    
                </span>
            </div>

            <div class="border-bottom pb-2">
                <p><a href="/uncategorized/surveys/bilevel_optimization">ä¸Šç¯‡</a>è°ƒç ”æ€»ç»“äº†åŒå±‚ä¼˜åŒ–é—®é¢˜çš„è§£å†³èŒƒå¼ï¼Œå¹¶æ¢³ç†äº†åŒå±‚ä¼˜åŒ–åº”ç”¨åœ¨Sim2realçš„domain randomizationï¼ˆDRï¼‰é—®é¢˜ä¸Šæ—¶çš„éš¾ç‚¹åœ¨äº</p>
<ul>
<li>å¦‚ä½•è§£ä¸å¯å¾®çš„å¤–å±‚ä¼˜åŒ–é—®é¢˜</li>
<li><strong>å¦‚ä½•è½¬æ¢ä¸å¯å¾®çš„å¤–å±‚ä¼˜åŒ–é—®é¢˜ä¸ºsmoothçš„</strong></li>
</ul>
<p>æœ¬æ–‡å°†å¯¹å…¶ä¸­ â€œä¸å¯å¾®é—®é¢˜çš„æ±‚è§£â€ è¿›è¡Œè°ƒç ”ã€‚</p>
<p>æ­¤å¤–æœ¬æ–‡ä¹Ÿå°†ç®€è¦ä»‹ç»ä¸€ä¸‹å…³äºè¶…å‚ä¼˜åŒ–ï¼ˆhyperparameter optimizationï¼ŒHOï¼‰çš„å†…å®¹ï¼Œè°ƒç ”HOé—®é¢˜çš„åŸå› æ˜¯ï¼šdomain randomizationä¸HOé—®é¢˜å…·æœ‰ç›¸ä¼¼çš„formulationï¼Œä½†äºŒè€…åˆå­˜åœ¨åŒºåˆ«ï¼ˆæœ€é‡è¦çš„åŒºåˆ«åœ¨äºä¸€èˆ¬çš„HOé—®é¢˜æ˜¯differetiableçš„ï¼ŒDRé—®é¢˜åœ¨æ²¡æœ‰è½¬æ¢ä¸ºsmoothé—®é¢˜ä¹‹å‰æ˜¯ä¸å¯å¾®çš„ï¼‰ï¼Œå› æ­¤é˜…è¯»äº†ä¸€äº›æ–‡çŒ®å¹¶åšäº†ç®€è¦æ€»ç»“ã€‚</p>
<span id="more"></span>

<h2 id="1-Formulation-of-Domain-Randomization"><a href="#1-Formulation-of-Domain-Randomization" class="headerlink" title="1. Formulation of Domain Randomization"></a>1. Formulation of Domain Randomization</h2><p><a href="uncategorized/surveys/sim2real">å›é¡¾</a>DRçš„formulationå¦‚ä¸‹</p>
<p>$$
\begin{array}{rl}
\min & F(\phi, \theta) = \mathcal{L}(\theta^{*};\mathcal{D}_{real}) \\
s.t. & \theta^{*} = \arg\min_{\theta}f(\phi, \theta) = \mathbb{E}_{x \sim P_{\phi}(x)}[\mathcal{L}(\theta;\mathcal{D}_{x})]\\
var. & \phi, \theta
\end{array}
$$</p>
å¯ä»¥çœ‹åˆ°ï¼Œå¤–å±‚ä¼˜åŒ–é—®é¢˜å¯å¾®ä½†**å†…å±‚ä¼˜åŒ–å‡½æ•°ä¸å¯å¾®ï¼Œ** åŸå› åœ¨äºï¼š

<ol>
<li>é‡‡æ ·è®­ç»ƒæ•°æ® $\mathcal{D}_{x}$ çš„åˆ†å¸ƒï¼Œå…¶å‚æ•°å—æ§äºä¼˜åŒ–å˜é‡$\phi$ ã€‚</li>
<li>ä¸€èˆ¬æ¥è®²ï¼Œä»ä»¿çœŸå™¨ä¸­ç”Ÿæˆä»¿çœŸæ•°æ®çš„è¿‡ç¨‹ï¼ˆå¦‚ä»å›¾å½¢æ¸²æŸ“å¼•æ“ä¸­ç”Ÿæˆå›¾åƒï¼‰ä¸å•çº¯æ˜¯ä¸€ä¸ªåˆ†å¸ƒé‡‡æ ·é—®é¢˜ï¼Œä¸ºä¿è¯é—®é¢˜çš„ä¸€èˆ¬æ€§ï¼Œè¿™ä¸ªæ•°æ®ç”Ÿæˆè¿‡ç¨‹è¦è¢«è§†ä¸ºä¸å¯å¾®ã€‚</li>
</ol>
<blockquote>
<p>Ruiz, N., Schulter, S., &amp; Chandraker, M. Learning To Simulate. In International Conference on Learning Representations, 2019.</p>
</blockquote>
<h2 id="2-Solving-a-Nondifferential-Problem"><a href="#2-Solving-a-Nondifferential-Problem" class="headerlink" title="2. Solving a Nondifferential Problem"></a>2. Solving a Nondifferential Problem</h2><p>ä¸€èˆ¬æ¥è¯´ï¼Œä¸å¯å¾®é—®é¢˜çš„è§£å†³æ–¹æ³•æœ‰æ¬¡ <strong>æ¢¯åº¦æ³•ï¼ˆsubgradientï¼‰</strong>ã€<strong>è¿‘ç«¯æ¢¯åº¦æ³•ï¼ˆProximal Gradientï¼‰</strong>ã€‚</p>
<h3 id="2-1-Subgradient"><a href="#2-1-Subgradient" class="headerlink" title="2.1 Subgradient"></a>2.1 Subgradient</h3><p><strong>æ¬¡æ¢¯åº¦æ³•ä¸€èˆ¬ç”¨äºæ±‚è§£convex non-smoothé—®é¢˜</strong>ï¼Œæœ¬èŠ‚çš„ä¸»è¦å‚è€ƒææ–™ä¸º[1]ï¼ŒåŒæ—¶å‚è€ƒäº†åšå®¢[2]</p>
<blockquote>
<p>[1] Polyak, B. T. (1978). Subgradient methods: a survey of Soviet research. <em>Nonsmooth optimization</em>, <em>3</em>, 5-29.</p>
<p>[2] <a target="_blank" rel="noopener" href="https://blog.csdn.net/zbwgycm/article/details/104507442">https://blog.csdn.net/zbwgycm/article/details/104507442</a></p>
</blockquote>
<p><strong>æ¬¡æ¢¯åº¦æ³•é€‚ç”¨äºæ‰€æœ‰ä¸å¯å¾®çš„ç›®æ ‡å‡½æ•°ï¼Œä½†å…¶æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢</strong>ã€‚</p>
<h4 id="2-1-1-Definition"><a href="#2-1-1-Definition" class="headerlink" title="2.1.1 Definition"></a>2.1.1 Definition</h4><p>å¯¹äºå®šä¹‰åœ¨ $\mathbb{R}^n$ çš„è¿ç»­å‡¸å‡½æ•° $f(x)$ ï¼Œå½“å‘é‡ $\partial f(x) \in \mathbb{R}^n $ æ»¡è¶³ä»¥ä¸‹æ¡ä»¶æ—¶ï¼Œå¯è¢«ç§°ä¸ºå‡½æ•° $f(x)$ åœ¨ $x$ ç‚¹çš„æ¬¡æ¢¯åº¦</p>
<p>$$
f(x+y) \geq f(x)+(\partial f(x), y), \forall y \in R^{n}
$$</p>
æ³¨ï¼šè¿™é‡Œçš„ $(\cdot,\cdot)$ åº”è¯¥æ˜¯å‘é‡ç§¯çš„æ„æ€ï¼Œå‚è€ƒ[2]ä¸­ $f(y) \geq f(x)+g^{T}(y-x)$ã€‚å¯ä»¥çœ‹å‡ºï¼Œç›¸æ¯”ä¸æ¢¯åº¦çš„å®šä¹‰ï¼ˆç­‰å·ï¼‰ï¼Œä¸Šè¿°æ¡ä»¶æ›´ä¸ºæ¾å¼›ã€‚ä¸€äº›æ¬¡æ¢¯åº¦çš„æ€§è´¨å¦‚ä¸‹[2]

<ul>
<li>æ¬¡æ¢¯åº¦æ€»æ˜¯å‡ºç°åœ¨å®šä¹‰åŸŸ $dom(f)$ çš„å†…éƒ¨ã€‚</li>
<li>å¯¹äºæ‰€æœ‰ $x \in \mathbb{R}^n$ ï¼Œæ¬¡æ¢¯åº¦éƒ½å­˜åœ¨ï¼ˆä½†å¯èƒ½ä¸æ˜¯å”¯ä¸€çš„ï¼‰ã€‚å¦‚æœ $f(x)$ æ˜¯å¯å¾®çš„ï¼Œåˆ™æ¬¡æ¢¯åº¦æ˜¯å”¯ä¸€çš„ï¼Œå¹¶ä¸”ä¸æ¢¯åº¦ã€‚å„ç§ç±»å‹å‡½æ•°çš„æ¬¡æ¢¯åº¦è®¡ç®—è§„åˆ™æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„ã€‚</li>
<li>æ¬¡æ¢¯åº¦çš„å®šä¹‰å¯ä»¥æ¨å¹¿åˆ°éå‡¸å‡½æ•°ä¸­ï¼Œä½† <strong>éå‡¸å‡½æ•°çš„æ¬¡æ¢¯åº¦å¯èƒ½ä¸å­˜åœ¨</strong> ã€‚</li>
</ul>
<h4 id="2-1-2-Gradient-Decent"><a href="#2-1-2-Gradient-Decent" class="headerlink" title="2.1.2 Gradient Decent"></a>2.1.2 Gradient Decent</h4><p>ç±»æ¯”äºæ¢¯åº¦ä¸‹é™æ³•ï¼Œæ¬¡æ¢¯åº¦æ³•åªæ˜¯å°†å…¶ä¸­çš„æ¢¯åº¦æ›¿æ¢ä¸ºæ¬¡æ¢¯åº¦ï¼Œå…¶ä»–æ­¥éª¤ä¸å˜ï¼Œå…¶æ›´æ–°å¦‚ä¸‹ï¼š</p>
<p>$$
x_{k+1} = x_{k} - \gamma_{k} \frac{\partial f(x_k)}{\| \partial f(x_k) \|}
$$</p>
å…¶ä¸­ $\gamma_k$ ä¸º step sizeï¼Œä¸Šå¼çš„ç®€æ´å†™æ³•ä¸º $x_{k+1} = x_k - \gamma_k \cdot g_{k-1}$ã€‚æ¢¯åº¦æ³•å’Œæ¬¡æ¢¯åº¦æ³•ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œä¸€èˆ¬æ¥è¯´ï¼Œæ–¹å‘ $ \partial f(x_k)$ ä¸æ˜¯ $x_k$ å¤„çš„ä¸‹é™æ–¹å‘ï¼Œå³ï¼š**å¹¶éä¸¥æ ¼ä¸‹é™** ï¼›ä¸å¯å¾®å‡½æ•°çš„ $f(x_k)$ çš„å€¼**ä¸ä¼šå•è°ƒå‡å°**ã€‚ 

<h4 id="2-1-3-Error-and-Convergence"><a href="#2-1-3-Error-and-Convergence" class="headerlink" title="2.1.3 Error and Convergence"></a>2.1.3 Error and Convergence</h4><p>Convergence rate $O(1/\epsilon^2)$ï¼Œæ…¢äºæ¢¯åº¦ä¸‹é™çš„ $O(1/\epsilon)$ ã€‚$k$ æ¬¡è¿­ä»£åçš„error levelä¸º $O(1/\sqrt{k})$ã€‚</p>
<h3 id="2-2-Proximal-Gradient"><a href="#2-2-Proximal-Gradient" class="headerlink" title="2.2 Proximal Gradient"></a>2.2 Proximal Gradient</h3><p>æœ¬èŠ‚çš„ä¸»è¦å‚è€ƒææ–™å¦‚ä¸‹</p>
<blockquote>
<p>[1] Schmidt, M., Roux, N., &amp; Bach, F. (2011). Convergence rates of inexact proximal-gradient methods for convex optimization. <em>Advances in neural information processing systems</em>, <em>24</em>.</p>
<p>[2] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/82622940">https://zhuanlan.zhihu.com/p/82622940</a></p>
</blockquote>
<p>è¿‘ç«¯æ¢¯åº¦æ³•é€‚ç”¨äºâ€œæ•´ä½“ä¼˜åŒ–ç›®æ ‡ä¸å¯å¾®åˆ†ï¼Œä½†å¯ä»¥<strong>åˆ†è§£ä¸ºéƒ¨åˆ†å¯å¾®ã€éƒ¨åˆ†ä¸å¯å¾®</strong>â€çš„ç›®æ ‡å‡½æ•°ã€‚å¯¹æ¯”æ¬¡æ¢¯åº¦æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œè¯¯å·®ã€‚</p>
<h4 id="2-2-1-Problem-Statement"><a href="#2-2-1-Problem-Statement" class="headerlink" title="2.2.1 Problem Statement"></a>2.2.1 Problem Statement</h4><p>è¿‘ç«¯æ¢¯åº¦ç”¨äºè§£å†³ä¸€ç±»å¤åˆçš„ä¸å¯å¾®é—®é¢˜ï¼Œå¯ä»¥è¡¨ç¤ºå¦‚ä¸‹</p>
<p>$$
\underset{x \in \mathbb{R}^d}{\operatorname{minimize}} f(x):=g(x)+h(x)
$$</p>
å…¶ä¸­ $g,h$ å‡ä¸ºå‡¸å‡½æ•°ï¼ˆconvexï¼‰ï¼Œä½† $g$ æ˜¯ smooth çš„ï¼Œ$h$ ä¸ºä¸å¯å¾®çš„ésmoothé¡¹ã€‚

<h4 id="2-2-2-Definition"><a href="#2-2-2-Definition" class="headerlink" title="2.2.2 Definition"></a>2.2.2 Definition</h4><p>è¿‘ç«¯æ¢¯åº¦ç®—æ³•çš„åŸºç¡€æ˜¯ä»¥ä¸‹çš„è¿‘ç«¯ç®—å­ï¼ˆproximity operatorï¼‰ï¼Œå…¶å®šä¹‰å¦‚ä¸‹</p>
<p>$$
\operatorname{prox}_L(y)=\underset{x \in \mathbb{R}^d}{\arg \min } \frac{L}{2}\|x-y\|^2+h(x)
$$</p>
å…¶ä¸­ $L$ æ˜¯å‡½æ•° $g$ çš„  Lipschitz å¸¸æ•°ã€‚æœ¬è´¨ä¸Šï¼Œ**è¿‘ç«¯ç®—å­ç”¨å¹³æ»‘é¡¹ $g$ çš„äºŒæ¬¡è¿‘æ‹Ÿå®šä¹‰äº†å˜é‡å‘æœ€å°å€¼çš„æ›´æ–°æ–¹å‘**ã€‚

<p>ä¸Šå¼è§£è¯»ï¼šä¸Šå¼çš„è‡ªå˜é‡æ˜¯ $y$ï¼Œç›®æ ‡æ˜¯ç»™å®šä¸€ä¸ª $y$ï¼Œæ‰¾åˆ°ä½¿å¾—åé¢çš„å¼å­æœ€å°åŒ–çš„ $x$ã€‚å¯ä»¥çœ‹å‡ºï¼Œç”±äºåé¢çš„æœ€å°åŒ–é—®é¢˜å…¶ä¼˜åŒ–å˜é‡æ˜¯ $x$ï¼Œå› æ­¤è¿‘ç«¯ç®—å­çš„å½¢å¼ä¸ $h$ é¡¹å¯†åˆ‡ç›¸å…³ã€‚</p>
<p>ğŸŒŸ å¯¹äºå‡ ç§ç‰¹æ®Šçš„ $h$ å½¢å¼ï¼Œä¾‹å¦‚ $l_1$-normï¼Œä¸Šé¢çš„è¿‘ç«¯ç®—å­æ˜¯å­˜åœ¨è§£æè§£çš„ï¼ˆè¯¦è§[1]ä¸­çš„å‚è€ƒæ–‡çŒ®[5, 6]ä»¥åŠçŸ¥ä¹æ–‡ç« ï¼‰ã€‚<strong>ç„¶è€Œï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œé‚»è¿‘ç®—å­å¯èƒ½æ²¡æœ‰è§£æè§£ï¼Œæˆ–è€…ç²¾ç¡®è®¡ç®—è¯¥è§£å¯èƒ½éå¸¸æ˜‚è´µã€‚</strong></p>
<h4 id="2-2-3-Proximal-Gradient-Descent"><a href="#2-2-3-Proximal-Gradient-Descent" class="headerlink" title="2.2.3 Proximal Gradient Descent"></a>2.2.3 Proximal Gradient Descent</h4><p>å€ŸåŠ©è¿‘ç«¯æ¢¯åº¦åšä¼˜åŒ–æ—¶çš„å‚æ•°æ›´æ–°æ–¹æ³•å¦‚ä¸‹ï¼ˆgeneralized formï¼‰</p>
<p>$$
x_k=\operatorname{prox}_L\left[y_{k-1}-(1 / L)\left(\nabla g\left(y_{k-1}\right)+e_k\right)\right]
$$</p>
å…¶ä¸­ $e_k$ æ˜¯è®¡ç®—æ¢¯åº¦æ—¶å¼•å…¥çš„è¯¯å·®ï¼Œæ­¤å¤–ï¼Œå¦‚æœè¿‘ç«¯ç®—å­è¢«ä¸ç²¾ç¡®æ±‚è§£ï¼Œ$x_k$ å°†è¿˜å­˜åœ¨ä¸€ä¸ªç”±æ­¤å¯¼è‡´çš„è¯¯å·®é¡¹ $\epsilon_k$ã€‚

<p>ä¸Šå¼çš„ä¸€ç§æ›´å®¹æ˜“ç†è§£çš„å†™æ³•ä¸ºï¼ˆbasic gradient descentï¼‰ï¼Œå…¶ä¸­ $t$ ä¸ºè¿­ä»£çš„æ­¥é•¿â‰¥</p>
<p>$$
x_k=\operatorname{prox}_L\left[x_{k-1}-t\nabla g\left(x_{k-1}\right)\right]
$$</p>
ç›¸æ¯” generalized formï¼Œbasic gradient descent å– $y_k = x_k$ï¼Œåœ¨**accelerated proximal-gradient method**ä¸­ï¼Œ$y_k=x_k+\beta_k\left(x_k-x_{k-1}\right)$ã€‚

<h4 id="2-2-4-Error"><a href="#2-2-4-Error" class="headerlink" title="2.2.4 Error"></a>2.2.4 Error</h4><p>$k$ æ¬¡è¿­ä»£åçš„error levelä¸º $O(1/k)$ã€‚accelerated proximal-gradient çš„ error level ä¸º $O(k^2)$ã€‚</p>
<h2 id="3-Hyperparameter-Optimization"><a href="#3-Hyperparameter-Optimization" class="headerlink" title="3. Hyperparameter Optimization"></a>3. Hyperparameter Optimization</h2><p>æœ¬èŠ‚å†…å®¹å‚è€ƒ</p>
<blockquote>
<p>Bao, F., Wu, G., Li, C., Zhu, J., &amp; Zhang, B. (2021). Stability and generalization of bilevel programming in hyperparameter optimization. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 4529-4541.</p>
</blockquote>
<p>HO é—®é¢˜çš„formulationå¯ä»¥å†™ä¸º</p>
<p>$$
\begin{array}{rl}
\hat{\lambda}\left(S^{t r}, S^{v a l}\right) & \approx \underset{\lambda \in \Lambda}{\arg \min } \hat{R}^{v a l}\left(\lambda, \hat{\theta}\left(\lambda, S^{t r}\right), S^{v a l}\right) \\
\text{where} \quad \hat{\theta}\left(\lambda, S^{t r}\right) & \approx \underset{\theta \in \Theta}{\arg \min } \hat{R}^{t r}\left(\lambda, \theta, S^{t r}\right)
\end{array}
$$</p>
ä¸Šå¼é‡‡ç”¨çº¦ç­‰äºç¬¦å·ï¼Œæ˜¯å› ä¸ºï¼šåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ˆä¾‹å¦‚ï¼Œç¥ç»ç½‘ç»œå†…å±‚ä¼˜åŒ–å˜é‡ï¼‰ï¼Œä¸Šå¼ä¸­çš„å†…éƒ¨å’Œå¤–éƒ¨é—®é¢˜çš„**å…¨å±€æœ€ä¼˜å€¼æ˜¯éš¾ä»¥å®ç°**çš„ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œä»¥æŸç§æ–¹å¼ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ï¼ˆéšæœºï¼‰æ¢¯åº¦ä¸‹é™ï¼‰å¯¹å…¶è¿›è¡Œè¿‘ä¼¼ã€‚

<p>ğŸŒŸæ³¨æ„ï¼šHOé—®é¢˜ä¸å­˜åœ¨DRé—®é¢˜ä¸­æ‰€æåˆ°çš„ä¸¤ä¸ªé˜¶æ®µï¼Œå› æ­¤<strong>å¤§å¤šæ•°å¯ä»¥è®¤ä¸ºæ˜¯å¯å¾®</strong>çš„ã€‚ä¸”å¤§å¤šæ•°æ˜¯å¸¦çº¦æŸçš„ä¼˜åŒ–ï¼ˆå¦‚æ•´æ•°çº¦æŸç­‰ï¼‰ã€‚</p>
<p><strong>HOçš„è§£å†³æ–¹æ³•ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ç§ï¼š</strong></p>
<ul>
<li>Unrolled differentiationï¼šåœ¨å†…å¤–å±‚ä¸Šæ‰§è¡Œæœ‰é™æ­¥æ•°çš„æ¢¯åº¦ä¸‹é™ã€‚æ³¨æ„ï¼š<strong>è¿™é‡Œçš„ $\theta$ ç›¸å¯¹äº $\lambda$ æ˜¯å¯å¾®çš„</strong>ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™ã€‚å…·ä½“çš„åˆ†æï¼šå°†å†…å±‚å˜é‡ï¼ˆç¥ç»ç½‘ç»œå‚æ•°ï¼‰è§†ä¸ºä¸€ä¸ªè¶³å¤Ÿå¤§çš„çŸ©é˜µï¼Œåˆ™å¤–å±‚å˜é‡ï¼ˆè¶…å‚ï¼‰çš„å˜åŒ–å¯¼è‡´çš„å†…å±‚å˜é‡å˜åŒ–ï¼ˆå¦‚ç¥ç»ç½‘ç»œå±‚æ•°å˜åŒ–ï¼‰å¯ä»¥è¢«è§†ä¸ºçŸ©é˜µä¸maskçš„ä¹˜ç§¯ã€‚å› æ­¤äºŒè€…å­˜åœ¨å‡½æ•°å…³ç³» $\theta = g(\lambda)$ï¼Œè¿™ç§å‡½æ•°å…³ç³»æ˜¯å¯å¾®çš„ã€‚</li>
<li>Cross-validationï¼šCVæ˜¯HOçš„ç»å…¸æ–¹æ³•ã€‚å®ƒé¦–å…ˆé€šè¿‡ ç½‘æ ¼æœç´¢ æˆ– éšæœºæœç´¢ è·å¾—ä¸€ç»„æœ‰é™çš„è¶…å‚æ•°ï¼Œé€šå¸¸æ˜¯ $\Lambda$ çš„å­é›†ã€‚ç„¶åï¼Œå®ƒè®­ç»ƒå†…å±‚é—®é¢˜ï¼Œä»¥è·å¾—ç»™å®šè¶…å‚æ•°çš„ç›¸åº”å‚æ•° $\theta$ ã€‚æœ€åï¼Œæ ¹æ®<strong>éªŒè¯è¯¯å·®</strong>é€‰æ‹©æœ€ä½³ $(\theta^{\star}, \lambda^{\star})$ å¯¹ã€‚</li>
<li>Implicit gradientï¼šéšå¼æ¢¯åº¦æ–¹æ³•ç›´æ¥ä¼°è®¡å¤–å±‚é—®é¢˜çš„æ¢¯åº¦ï¼Œè¿™é€šå¸¸æ¶‰åŠä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œå¦‚å…±è½­æ¢¯åº¦ã€Neumannè¿‘ä¼¼ï¼Œä»¥ä¼°è®¡HessiançŸ©é˜µçš„é€†ã€‚</li>
<li>Bayesian optimizationï¼šè´å¶æ–¯ä¼˜åŒ–å°†å¤–å±‚é—®é¢˜è§†ä¸ºä»é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰é‡‡æ ·çš„<strong>é»‘ç®±å‡½æ•°</strong>ï¼Œå¹¶åœ¨è¯„ä¼°æ–°çš„è¶…å‚æ•°æ—¶æ›´æ–°GPçš„åéªŒã€‚</li>
<li>Hypernetworksï¼šå­¦ä¹ åœ¨ç»™å®šè¶…å‚æ•°çš„æƒ…å†µä¸‹è¾“å‡ºè¿‘ä¼¼æœ€ä¼˜å‡è®¾çš„ä»£ç†å‡½æ•°ã€‚</li>
</ul>
 
            </div>
        </div>
        
        <div class="mt-3">
            <!-- å‰ä¸€é¡µåä¸€é¡µ -->
<div class="previous-next-links">
     
    <div class="previous-design-link">
        <a href="../../paperlistfile/Hotpaper%20in%202021-2022%20(Anomaly%20detection,%20Failure%20detection)/">
            <i style="font-size:16px;" class="fa fa-arrow-left" aria-hidden="true"></i>
            Hotpaper in 2021-2022 (Anomaly detection / Failure detection)
        </a>
    </div>
     

     
    <div class="next-design-link">
        <a href="../bilevel_optimization/">
            Paradigm of Bi-level Optimization
            <i style="font-size:16px;" class="fa fa-arrow-right" aria-hidden="true"></i>
        </a>
    </div>
 
</div> 
             
        </div>

    </div>
    <div class="col-2">
        <div class="d-none d-sm-none d-md-block sticky-top border-start">
             
    
        <div>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Formulation-of-Domain-Randomization"><span class="toc-number">1.</span> <span class="toc-text">1. Formulation of Domain Randomization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Solving-a-Nondifferential-Problem"><span class="toc-number">2.</span> <span class="toc-text">2. Solving a Nondifferential Problem</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Subgradient"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 Subgradient</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Proximal-Gradient"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 Proximal Gradient</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Hyperparameter-Optimization"><span class="toc-number">3.</span> <span class="toc-text">3. Hyperparameter Optimization</span></a></li></ol> 
        </div>
    
 

        </div>
    </div>
</div>
</div>
 
    </main>

    <!-- åº•éƒ¨æ  -->
    <footer class="bg-dark pt-1 pb-0 mt-5">
    <div class="container pb-3 pt-3 text-center">
        <p class="text-muted tag-hover">
            Powered by 
             <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://hexo.io">
                 Hexo.io
            </a> &  <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://github.com/smile-yan/hexo-theme-heyan">
               heyan
            </a>
            <br>
        </p>
    </div>
</footer>
</body> 
</html>