
<!DOCTYPE html>
<html>
<head>
    <title> Here is Shaoyu </title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--网页标题左侧显示-->
    <link rel="icon" href="/fish.ico" type="image/x-icon">
    <!--收藏夹显示图标-->
    <link rel="shortcut icon" href="/fish.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
    <link href="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">
    <script src="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"></script>
    
<link rel="stylesheet" href="../../../css/prism.css">
 
    
<script src="../../../js/prism.js"></script>

    
    
<link rel="stylesheet" href="../../../css/index.css">
 
    
<script src="../../../js/search.js"></script>



     
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
        </script>
     

     
        <script src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js"> </script>
        
<script src="../../../js/fancybox.js"></script>

    

    <script type="text/javascript">
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
        var path = "/" + search_path;
        searchFunc(path, 'local-search-input', 'local-search-result');
    </script>
<meta name="generator" content="Hexo 5.4.2"></head>
 
 
<body>
    <!-- 导航栏 -->
    <!-- 导航栏 -->
<nav class="navbar navbar-expand-md navbar-dark bg-dark mb-4 pt-2 pb-2">
    <div class="container">
        <!-- 标题 --> 
        <a class="navbar-brand navbar-expand-sm" href="/">
            <img class="nofancybox" src="/shark.svg" style="width: 65px;">
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
                aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <!-- 左边右边导航按钮 --> 
        <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav me-auto">
                <!-- 左右两侧的导航栏 -->

 
 

    <li class="nav-item">
        <a class="nav-link" href="../../../">
           <big> <i class="fa fa-home ps-1" ></i> Home</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../note">
           <big> <i class="fa fa-file-text-o ps-1" ></i> Notes</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../survey">
           <big> <i class="fa fa-globe ps-1" ></i> Surveys</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../paperlist">
           <big> <i class="fa fa-cc-discover ps-1" ></i> Paper Lists</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../archives">
           <big> <i class="fa fa-archive ps-1" ></i> Archives</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../about">
           <big> <i class="fa fa-user ps-1" ></i> About</big>
        </a>
    </li>
 
 
            </ul>

            
            <form class="">
                <div class="d-flex">
                    <button class="btn text-muted fa fa-search d-none d-md-block d-lg-block" disabled></button>
                    <input id="local-search-input" class="form-control me-2 pe-4" type="search"
                            placeholder="搜索 " aria-label="Search">
                </div>
                <div id="local-search-result" style="position:absolute; padding-top: 8px; max-height: 960px; width: 480px;overflow-y: scroll; z-index: 1050;"></div>
            </form>
            
            <ul class="navbar-nav">
                <!-- 左右两侧的导航栏 -->

 
 
            </ul>
        </div>
    </div>
</nav>


    <main class="container">
        
<div class="container-fluid markdown-section">
<div class="row">
    <div class="col-md-2 d-none d-sm-none d-md-block">
        <div class="">
            <!-- 同一类型的文件 -->
<!-- 尽可能做到每篇文章只有一个类
    1. 获得 post 的类别 categories
     2.1 如果 categories.length === 0，啥也不做
     2.2 如果 categories === 1，渲染这个类的所有文章
     2.3 如果 categories.length > 1 ，则以第一个类为准渲染所有文章
-->


 
        </div>
    </div>
    <div class="col-md-8 col-sm-12">
        <!-- 博客详情 -->
        <div class="">
            <!-- ps-4 pe-4 pt-2 -->
            <p class="h2">
                <span class="post-title">
                    Optimization for Nondifferentiable Problem
                </span>   
            </p>

            <div class="pb-3 pt-1 pe-3">
                <i class="fa fa-calendar p-1"></i>
                2023/02/17 11:10:00 

                <i class="fa fa-pencil"> </i>
                2023/02/21 21:13:00 

                <!--
                <i class="fa fa-folder-open p-1"> </i> 
                <span class="tag-hover">
                     
                </span>
                -->

                <i class="fa fa-tags p-1"> </i>
                <span class="tag-hover">
                    
                        <a href="../../../tags/survey/" class="link-dark text-decoration-none"> 
                            survey 
                        </a>
                    
                        <a href="../../../tags/optimization/" class="link-dark text-decoration-none"> 
                            optimization 
                        </a>
                    
                </span>
            </div>

            <div class="border-bottom pb-2">
                <p><a href="/uncategorized/surveys/bilevel_optimization">上篇</a>调研总结了双层优化问题的解决范式，并梳理了双层优化应用在Sim2real的domain randomization（DR）问题上时的难点在于</p>
<ul>
<li>如何解不可微的外层优化问题</li>
<li><strong>如何转换不可微的外层优化问题为smooth的</strong></li>
</ul>
<p>本文将对其中 “不可微问题的求解” 进行调研。</p>
<p>此外本文也将简要介绍一下关于超参优化（hyperparameter optimization，HO）的内容，调研HO问题的原因是：domain randomization与HO问题具有相似的formulation，但二者又存在区别（最重要的区别在于一般的HO问题是differetiable的，DR问题在没有转换为smooth问题之前是不可微的），因此阅读了一些文献并做了简要总结。</p>
<span id="more"></span>

<h2 id="1-Formulation-of-Domain-Randomization"><a href="#1-Formulation-of-Domain-Randomization" class="headerlink" title="1. Formulation of Domain Randomization"></a>1. Formulation of Domain Randomization</h2><p><a href="uncategorized/surveys/sim2real">回顾</a>DR的formulation如下</p>
<p>$$
\begin{array}{rl}
\min & F(\phi, \theta) = \mathcal{L}(\theta^{*};\mathcal{D}_{real}) \\
s.t. & \theta^{*} = \arg\min_{\theta}f(\phi, \theta) = \mathbb{E}_{x \sim P_{\phi}(x)}[\mathcal{L}(\theta;\mathcal{D}_{x})]\\
var. & \phi, \theta
\end{array}
$$</p>
可以看到，外层优化问题可微但**内层优化函数不可微，** 原因在于：

<ol>
<li>采样训练数据 $\mathcal{D}_{x}$ 的分布，其参数受控于优化变量$\phi$ 。</li>
<li>一般来讲，从仿真器中生成仿真数据的过程（如从图形渲染引擎中生成图像）不单纯是一个分布采样问题，为保证问题的一般性，这个数据生成过程要被视为不可微。</li>
</ol>
<blockquote>
<p>Ruiz, N., Schulter, S., &amp; Chandraker, M. Learning To Simulate. In International Conference on Learning Representations, 2019.</p>
</blockquote>
<h2 id="2-Solving-a-Nondifferential-Problem"><a href="#2-Solving-a-Nondifferential-Problem" class="headerlink" title="2. Solving a Nondifferential Problem"></a>2. Solving a Nondifferential Problem</h2><p>一般来说，不可微问题的解决方法有次 <strong>梯度法（subgradient）</strong>、<strong>近端梯度法（Proximal Gradient）</strong>。</p>
<h3 id="2-1-Subgradient"><a href="#2-1-Subgradient" class="headerlink" title="2.1 Subgradient"></a>2.1 Subgradient</h3><p><strong>次梯度法一般用于求解convex non-smooth问题</strong>，本节的主要参考材料为[1]，同时参考了博客[2]</p>
<blockquote>
<p>[1] Polyak, B. T. (1978). Subgradient methods: a survey of Soviet research. <em>Nonsmooth optimization</em>, <em>3</em>, 5-29.</p>
<p>[2] <a target="_blank" rel="noopener" href="https://blog.csdn.net/zbwgycm/article/details/104507442">https://blog.csdn.net/zbwgycm/article/details/104507442</a></p>
</blockquote>
<p><strong>次梯度法适用于所有不可微的目标函数，但其收敛速度较慢</strong>。</p>
<h4 id="2-1-1-Definition"><a href="#2-1-1-Definition" class="headerlink" title="2.1.1 Definition"></a>2.1.1 Definition</h4><p>对于定义在 $\mathbb{R}^n$ 的连续凸函数 $f(x)$ ，当向量 $\partial f(x) \in \mathbb{R}^n $ 满足以下条件时，可被称为函数 $f(x)$ 在 $x$ 点的次梯度</p>
<p>$$
f(x+y) \geq f(x)+(\partial f(x), y), \forall y \in R^{n}
$$</p>
注：这里的 $(\cdot,\cdot)$ 应该是向量积的意思，参考[2]中 $f(y) \geq f(x)+g^{T}(y-x)$。可以看出，相比与梯度的定义（等号），上述条件更为松弛。一些次梯度的性质如下[2]

<ul>
<li>次梯度总是出现在定义域 $dom(f)$ 的内部。</li>
<li>对于所有 $x \in \mathbb{R}^n$ ，次梯度都存在（但可能不是唯一的）。如果 $f(x)$ 是可微的，则次梯度是唯一的，并且与梯度。各种类型函数的次梯度计算规则是众所周知的。</li>
<li>次梯度的定义可以推广到非凸函数中，但 <strong>非凸函数的次梯度可能不存在</strong> 。</li>
</ul>
<h4 id="2-1-2-Gradient-Decent"><a href="#2-1-2-Gradient-Decent" class="headerlink" title="2.1.2 Gradient Decent"></a>2.1.2 Gradient Decent</h4><p>类比于梯度下降法，次梯度法只是将其中的梯度替换为次梯度，其他步骤不变，其更新如下：</p>
<p>$$
x_{k+1} = x_{k} - \gamma_{k} \frac{\partial f(x_k)}{\| \partial f(x_k) \|}
$$</p>
其中 $\gamma_k$ 为 step size，上式的简洁写法为 $x_{k+1} = x_k - \gamma_k \cdot g_{k-1}$。梯度法和次梯度法之间的主要区别在于，一般来说，方向 $ \partial f(x_k)$ 不是 $x_k$ 处的下降方向，即：**并非严格下降** ；不可微函数的 $f(x_k)$ 的值**不会单调减小**。 

<h4 id="2-1-3-Error-and-Convergence"><a href="#2-1-3-Error-and-Convergence" class="headerlink" title="2.1.3 Error and Convergence"></a>2.1.3 Error and Convergence</h4><p>Convergence rate $O(1/\epsilon^2)$，慢于梯度下降的 $O(1/\epsilon)$ 。$k$ 次迭代后的error level为 $O(1/\sqrt{k})$。</p>
<h3 id="2-2-Proximal-Gradient"><a href="#2-2-Proximal-Gradient" class="headerlink" title="2.2 Proximal Gradient"></a>2.2 Proximal Gradient</h3><p>本节的主要参考材料如下</p>
<blockquote>
<p>[1] Schmidt, M., Roux, N., &amp; Bach, F. (2011). Convergence rates of inexact proximal-gradient methods for convex optimization. <em>Advances in neural information processing systems</em>, <em>24</em>.</p>
<p>[2] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/82622940">https://zhuanlan.zhihu.com/p/82622940</a></p>
</blockquote>
<p>近端梯度法适用于“整体优化目标不可微分，但可以<strong>分解为部分可微、部分不可微</strong>”的目标函数。对比次梯度法，该方法具有更快的收敛速度和误差。</p>
<h4 id="2-2-1-Problem-Statement"><a href="#2-2-1-Problem-Statement" class="headerlink" title="2.2.1 Problem Statement"></a>2.2.1 Problem Statement</h4><p>近端梯度用于解决一类复合的不可微问题，可以表示如下</p>
<p>$$
\underset{x \in \mathbb{R}^d}{\operatorname{minimize}} f(x):=g(x)+h(x)
$$</p>
其中 $g,h$ 均为凸函数（convex），但 $g$ 是 smooth 的，$h$ 为不可微的非smooth项。

<h4 id="2-2-2-Definition"><a href="#2-2-2-Definition" class="headerlink" title="2.2.2 Definition"></a>2.2.2 Definition</h4><p>近端梯度算法的基础是以下的近端算子（proximity operator），其定义如下</p>
<p>$$
\operatorname{prox}_L(y)=\underset{x \in \mathbb{R}^d}{\arg \min } \frac{L}{2}\|x-y\|^2+h(x)
$$</p>
其中 $L$ 是函数 $g$ 的  Lipschitz 常数。本质上，**近端算子用平滑项 $g$ 的二次近拟定义了变量向最小值的更新方向**。

<p>上式解读：上式的自变量是 $y$，目标是给定一个 $y$，找到使得后面的式子最小化的 $x$。可以看出，由于后面的最小化问题其优化变量是 $x$，因此近端算子的形式与 $h$ 项密切相关。</p>
<p>🌟 对于几种特殊的 $h$ 形式，例如 $l_1$-norm，上面的近端算子是存在解析解的（详见[1]中的参考文献[5, 6]以及知乎文章）。<strong>然而，在许多情况下，邻近算子可能没有解析解，或者精确计算该解可能非常昂贵。</strong></p>
<h4 id="2-2-3-Proximal-Gradient-Descent"><a href="#2-2-3-Proximal-Gradient-Descent" class="headerlink" title="2.2.3 Proximal Gradient Descent"></a>2.2.3 Proximal Gradient Descent</h4><p>借助近端梯度做优化时的参数更新方法如下（generalized form）</p>
<p>$$
x_k=\operatorname{prox}_L\left[y_{k-1}-(1 / L)\left(\nabla g\left(y_{k-1}\right)+e_k\right)\right]
$$</p>
其中 $e_k$ 是计算梯度时引入的误差，此外，如果近端算子被不精确求解，$x_k$ 将还存在一个由此导致的误差项 $\epsilon_k$。

<p>上式的一种更容易理解的写法为（basic gradient descent），其中 $t$ 为迭代的步长≥</p>
<p>$$
x_k=\operatorname{prox}_L\left[x_{k-1}-t\nabla g\left(x_{k-1}\right)\right]
$$</p>
相比 generalized form，basic gradient descent 取 $y_k = x_k$，在**accelerated proximal-gradient method**中，$y_k=x_k+\beta_k\left(x_k-x_{k-1}\right)$。

<h4 id="2-2-4-Error"><a href="#2-2-4-Error" class="headerlink" title="2.2.4 Error"></a>2.2.4 Error</h4><p>$k$ 次迭代后的error level为 $O(1/k)$。accelerated proximal-gradient 的 error level 为 $O(k^2)$。</p>
<h2 id="3-Hyperparameter-Optimization"><a href="#3-Hyperparameter-Optimization" class="headerlink" title="3. Hyperparameter Optimization"></a>3. Hyperparameter Optimization</h2><p>本节内容参考</p>
<blockquote>
<p>Bao, F., Wu, G., Li, C., Zhu, J., &amp; Zhang, B. (2021). Stability and generalization of bilevel programming in hyperparameter optimization. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 4529-4541.</p>
</blockquote>
<p>HO 问题的formulation可以写为</p>
<p>$$
\begin{array}{rl}
\hat{\lambda}\left(S^{t r}, S^{v a l}\right) & \approx \underset{\lambda \in \Lambda}{\arg \min } \hat{R}^{v a l}\left(\lambda, \hat{\theta}\left(\lambda, S^{t r}\right), S^{v a l}\right) \\
\text{where} \quad \hat{\theta}\left(\lambda, S^{t r}\right) & \approx \underset{\theta \in \Theta}{\arg \min } \hat{R}^{t r}\left(\lambda, \theta, S^{t r}\right)
\end{array}
$$</p>
上式采用约等于符号，是因为：在大多数情况下（例如，神经网络内层优化变量），上式中的内部和外部问题的**全局最优值是难以实现**的。通常情况下，以某种方式（例如，使用（随机）梯度下降）对其进行近似。

<p>🌟注意：HO问题不存在DR问题中所提到的两个阶段，因此<strong>大多数可以认为是可微</strong>的。且大多数是带约束的优化（如整数约束等）。</p>
<p><strong>HO的解决方法主要分为以下几种：</strong></p>
<ul>
<li>Unrolled differentiation：在内外层上执行有限步数的梯度下降。注意：<strong>这里的 $\theta$ 相对于 $\lambda$ 是可微的</strong>，因此可以使用梯度下降。具体的分析：将内层变量（神经网络参数）视为一个足够大的矩阵，则外层变量（超参）的变化导致的内层变量变化（如神经网络层数变化）可以被视为矩阵与mask的乘积。因此二者存在函数关系 $\theta = g(\lambda)$，这种函数关系是可微的。</li>
<li>Cross-validation：CV是HO的经典方法。它首先通过 网格搜索 或 随机搜索 获得一组有限的超参数，通常是 $\Lambda$ 的子集。然后，它训练内层问题，以获得给定超参数的相应参数 $\theta$ 。最后，根据<strong>验证误差</strong>选择最佳 $(\theta^{\star}, \lambda^{\star})$ 对。</li>
<li>Implicit gradient：隐式梯度方法直接估计外层问题的梯度，这通常涉及一个迭代过程，如共轭梯度、Neumann近似，以估计Hessian矩阵的逆。</li>
<li>Bayesian optimization：贝叶斯优化将外层问题视为从高斯过程（GP）采样的<strong>黑箱函数</strong>，并在评估新的超参数时更新GP的后验。</li>
<li>Hypernetworks：学习在给定超参数的情况下输出近似最优假设的代理函数。</li>
</ul>
 
            </div>
        </div>
        
        <div class="mt-3">
            <!-- 前一页后一页 -->
<div class="previous-next-links">
     
    <div class="previous-design-link">
        <a href="../../paperlistfile/Hotpaper%20in%202021-2022%20(Anomaly%20detection,%20Failure%20detection)/">
            <i style="font-size:16px;" class="fa fa-arrow-left" aria-hidden="true"></i>
            Hotpaper in 2021-2022 (Anomaly detection / Failure detection)
        </a>
    </div>
     

     
    <div class="next-design-link">
        <a href="../bilevel_optimization/">
            Paradigm of Bi-level Optimization
            <i style="font-size:16px;" class="fa fa-arrow-right" aria-hidden="true"></i>
        </a>
    </div>
 
</div> 
             
        </div>

    </div>
    <div class="col-2">
        <div class="d-none d-sm-none d-md-block sticky-top border-start">
             
    
        <div>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Formulation-of-Domain-Randomization"><span class="toc-number">1.</span> <span class="toc-text">1. Formulation of Domain Randomization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Solving-a-Nondifferential-Problem"><span class="toc-number">2.</span> <span class="toc-text">2. Solving a Nondifferential Problem</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Subgradient"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 Subgradient</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Proximal-Gradient"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 Proximal Gradient</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Hyperparameter-Optimization"><span class="toc-number">3.</span> <span class="toc-text">3. Hyperparameter Optimization</span></a></li></ol> 
        </div>
    
 

        </div>
    </div>
</div>
</div>
 
    </main>

    <!-- 底部栏 -->
    <footer class="bg-dark pt-1 pb-0 mt-5">
    <div class="container pb-3 pt-3 text-center">
        <p class="text-muted tag-hover">
            Powered by 
             <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://hexo.io">
                 Hexo.io
            </a> &  <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://github.com/smile-yan/hexo-theme-heyan">
               heyan
            </a>
            <br>
        </p>
    </div>
</footer>
</body> 
</html>