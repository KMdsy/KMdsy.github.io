
<!DOCTYPE html>
<html>
<head>
    <title> Here is Shaoyu </title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--ç½‘é¡µæ ‡é¢˜å·¦ä¾§æ˜¾ç¤º-->
    <link rel="icon" href="/fish.ico" type="image/x-icon">
    <!--æ”¶è—å¤¹æ˜¾ç¤ºå›¾æ ‡-->
    <link rel="shortcut icon" href="/fish.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
    <link href="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">
    <script src="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"></script>
    
<link rel="stylesheet" href="../../../css/prism.css">
 
    
<script src="../../../js/prism.js"></script>

    
    
<link rel="stylesheet" href="../../../css/index.css">
 
    
<script src="../../../js/search.js"></script>



     
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
        </script>
     

     
        <script src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js"> </script>
        
<script src="../../../js/fancybox.js"></script>

    

    <script type="text/javascript">
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
        var path = "/" + search_path;
        searchFunc(path, 'local-search-input', 'local-search-result');
    </script>
<meta name="generator" content="Hexo 5.4.2"></head>
 
 
<body>
    <!-- å¯¼èˆªæ  -->
    <!-- å¯¼èˆªæ  -->
<nav class="navbar navbar-expand-md navbar-dark bg-dark mb-4 pt-2 pb-2">
    <div class="container">
        <!-- æ ‡é¢˜ --> 
        <a class="navbar-brand navbar-expand-sm" href="/">
            <img class="nofancybox" src="/shark.svg" style="width: 65px;">
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
                aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <!-- å·¦è¾¹å³è¾¹å¯¼èˆªæŒ‰é’® --> 
        <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav me-auto">
                <!-- å·¦å³ä¸¤ä¾§çš„å¯¼èˆªæ  -->

 
 

    <li class="nav-item">
        <a class="nav-link" href="../../../">
           <big> <i class="fa fa-home ps-1" ></i> Home</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../note">
           <big> <i class="fa fa-file-text-o ps-1" ></i> Notes</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../survey">
           <big> <i class="fa fa-globe ps-1" ></i> Surveys</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../paperlist">
           <big> <i class="fa fa-cc-discover ps-1" ></i> Paper Lists</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../archives">
           <big> <i class="fa fa-archive ps-1" ></i> Archives</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../about">
           <big> <i class="fa fa-user ps-1" ></i> About</big>
        </a>
    </li>
 
 
            </ul>

            
            <form class="">
                <div class="d-flex">
                    <button class="btn text-muted fa fa-search d-none d-md-block d-lg-block" disabled></button>
                    <input id="local-search-input" class="form-control me-2 pe-4" type="search"
                            placeholder="æœç´¢ " aria-label="Search">
                </div>
                <div id="local-search-result" style="position:absolute; padding-top: 8px; max-height: 960px; width: 480px;overflow-y: scroll; z-index: 1050;"></div>
            </form>
            
            <ul class="navbar-nav">
                <!-- å·¦å³ä¸¤ä¾§çš„å¯¼èˆªæ  -->

 
 
            </ul>
        </div>
    </div>
</nav>


    <main class="container">
        
<div class="container-fluid markdown-section">
<div class="row">
    <div class="col-md-2 d-none d-sm-none d-md-block">
        <div class="">
            <!-- åŒä¸€ç±»å‹çš„æ–‡ä»¶ -->
<!-- å°½å¯èƒ½åšåˆ°æ¯ç¯‡æ–‡ç« åªæœ‰ä¸€ä¸ªç±»
    1. è·å¾— post çš„ç±»åˆ« categories
     2.1 å¦‚æœ categories.length === 0ï¼Œå•¥ä¹Ÿä¸åš
     2.2 å¦‚æœ categories === 1ï¼Œæ¸²æŸ“è¿™ä¸ªç±»çš„æ‰€æœ‰æ–‡ç« 
     2.3 å¦‚æœ categories.length > 1 ï¼Œåˆ™ä»¥ç¬¬ä¸€ä¸ªç±»ä¸ºå‡†æ¸²æŸ“æ‰€æœ‰æ–‡ç« 
-->


 
        </div>
    </div>
    <div class="col-md-8 col-sm-12">
        <!-- åšå®¢è¯¦æƒ… -->
        <div class="">
            <!-- ps-4 pe-4 pt-2 -->
            <p class="h2">
                <span class="post-title">
                    Transformer Family on Time series
                </span>   
            </p>

            <div class="pb-3 pt-1 pe-3">
                <i class="fa fa-calendar p-1"></i>
                2023/05/30 21:14:00 

                <i class="fa fa-pencil"> </i>
                2023/05/30 21:15:00 

                <!--
                <i class="fa fa-folder-open p-1"> </i> 
                <span class="tag-hover">
                     
                </span>
                -->

                <i class="fa fa-tags p-1"> </i>
                <span class="tag-hover">
                    
                        <a href="../../../tags/transformers/" class="link-dark text-decoration-none"> 
                            transformers 
                        </a>
                    
                        <a href="../../../tags/survey/" class="link-dark text-decoration-none"> 
                            survey 
                        </a>
                    
                        <a href="../../../tags/time-series/" class="link-dark text-decoration-none"> 
                            time series 
                        </a>
                    
                </span>
            </div>

            <div class="border-bottom pb-2">
                <p>å½“ä»£çš„æ—¶é—´åºåˆ—åˆ†æä»»åŠ¡éœ€è¦åº”å¯¹å„ç§å¤æ‚çš„æ•°æ®æ¨¡å¼å’ŒåŠ¨æ€æ€§ï¼Œä¼ ç»Ÿçš„ç»Ÿè®¡æ¨¡å‹å’Œæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†è¿™äº›æŒ‘æˆ˜æ—¶å¸¸å¸¸å—é™ã€‚ç„¶è€Œï¼Œè¿‘å¹´æ¥ï¼Œç±»Transformerç½‘ç»œåœ¨æ—¶é—´åºåˆ—åˆ†æé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„çªç ´ã€‚Transformeræ¨¡å‹çš„å‡ºç°ä¸ºæ—¶é—´åºåˆ—åˆ†æä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„ã€å¼ºå¤§çš„å·¥å…·ï¼Œå®ƒèƒ½å¤Ÿè‡ªé€‚åº”åœ°æ•æ‰åºåˆ—ä¸­çš„é•¿æœŸä¾èµ–å…³ç³»ï¼Œå¹¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å»ºæ¨¡éçº¿æ€§å’Œéå¹³ç¨³æ€§ç‰¹å¾ã€‚</p>
<span id="more"></span>

<p>åœ¨æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å…³æ³¨ä¸€äº›è¿‘å¹´æ¥åœ¨æ—¶é—´åºåˆ—åˆ†æä»»åŠ¡ä¸Šæ¶Œç°çš„é‡è¦å·¥ä½œï¼Œè¿™äº›å·¥ä½œä»¥ç±»Transformerç½‘ç»œä¸ºåŸºç¡€ï¼Œä»¥å…¶å“è¶Šçš„æ€§èƒ½å’Œåˆ›æ–°çš„æ–¹æ³•å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ã€‚æˆ‘ä»¬å°†é‡ç‚¹ä»‹ç»ä»¥ä¸‹å‡ ä¸ªå·¥ä½œï¼š</p>
<ol>
<li>N-BEATSï¼ˆICLR 2022ï¼‰</li>
<li>LogTransï¼ˆNeurIPS 2021ï¼‰</li>
<li>Informer ï¼ˆ AAAI 2021 Best Paperï¼‰<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/354916261">ğŸ“’Informerï¼šç”¨Transformeræ¶æ„è§£å†³LSTFé—®é¢˜ - çŸ¥ä¹</a></li>
<li>Autoformerï¼ˆNeuraIPS 2021ï¼‰<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/386955393">ğŸ“’ç»†è¯»å¥½æ–‡ ä¹‹ Autoformer - çŸ¥ä¹</a><ul>
<li>ğŸŒŸå…¶ä¸­åŸºäºWiener-Khinchinå®šç†çš„Auto-Correlation Mechanismæœ‰ç‚¹æ„æ€ï¼Œå¯ä»¥å•ç‹¬çœ‹çœ‹ã€‚</li>
</ul>
</li>
<li>FEDformerï¼ˆICML 2022ï¼‰<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/528131016">ğŸ“’é˜¿é‡Œè¾¾æ‘©é™¢æœ€æ–°FEDformerï¼Œé•¿ç¨‹æ—¶åºé¢„æµ‹å…¨é¢è¶…è¶ŠSOTA - çŸ¥ä¹</a></li>
<li>Pyraformerï¼ˆICLR 2022ï¼‰<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/467765457">ğŸ“’æ—¶é—´åºåˆ—é¢„æµ‹@Pyraformer - çŸ¥ä¹</a></li>
<li>Transformer embeddings of irregularly spaced events and their participantsï¼ˆICLR 2022ï¼‰</li>
<li>TranADï¼ˆVLDB 2022ï¼‰</li>
<li>Probabilistic Transformer For Time Series Analysisï¼ˆNeurIPS 2021ï¼‰ã€‚</li>
</ol>
<p>ğŸŒŸ æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†ä¾æ‰˜è®ºæ–‡<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.13504">Are Transformers Effective for Time Series Forecasting?</a>æ¢è®¨ä¸€ä¸ªé‡è¦çš„é—®é¢˜ï¼Œå³Transformerç½‘ç»œåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p>é€šè¿‡å¯¹è¿™äº›å·¥ä½œçš„ç»¼è¿°å’Œåˆ†æï¼Œæˆ‘ä»¬å°†æ·±å…¥äº†è§£ç±»Transformerç½‘ç»œåœ¨æ—¶é—´åºåˆ—åˆ†æä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œä»¥åŠå®ƒä»¬çš„åˆ›æ–°ä¹‹å¤„ã€ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚è¿™å°†æœ‰åŠ©äºæˆ‘ä»¬å¯¹è¯¥é¢†åŸŸçš„æœ€æ–°ç ”ç©¶è¿›å±•æœ‰ä¸€ä¸ªå…¨é¢çš„äº†è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›æŒ‡å¯¼å’Œå¯ç¤ºã€‚</p>
<h2 id="Taxonomy-of-Transformers-in-Time-Series"><a href="#Taxonomy-of-Transformers-in-Time-Series" class="headerlink" title="Taxonomy of Transformers in Time Series"></a>Taxonomy of Transformers in Time Series</h2><blockquote>
<p>Yang, C., Mei, H., &amp; Eisner, J. (2021). Transformer embeddings of irregularly spaced events and their participants. <em>arXiv preprint arXiv:2201.00044</em>.</p>
</blockquote>
<p>æ ¹æ®ä¸Šé¢çš„æ–‡ç« ï¼ŒåŸºäºTransformersçš„æ—¶é—´åºåˆ—åˆ†æå·¥ä½œçš„åˆ›æ–°ç‚¹ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼Œå³æ›´æ”¹æ¨¡å‹æ¶æ„çš„ï¼Œä»¥åŠä¸ºç‰¹æ®Šçš„åº”ç”¨è€Œé€‚é…çš„ã€‚åæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿå°†ä»è¿™ä¸ªåˆ†ç±»æ³•å‡ºå‘ï¼Œæ¥æ•´ç†æ¯ä¸ªå·¥ä½œæ˜¯å¦‚ä½•æ”¹è¿›Transformerçš„ã€‚</p>
<img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20230530200210844.png" alt="Taxonomy of Transformers for time series modeling from the perspectives of network modifications and application domains" style="zoom:50%;" />



<h2 id="Network-Modifications"><a href="#Network-Modifications" class="headerlink" title="Network Modifications"></a>Network Modifications</h2><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><ol>
<li>Learnable Positional Encoding<ol>
<li><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/A-Transformer-based-Framework-for-Multivariate-Time-Zerveas-Jayaraman/2051548f7681c96d603de932ee23406c525276f9">THIS WORK</a> introduces an <strong>embedding layer</strong> in Transformer that <strong>learns embedding vectors for each position index</strong> jointly with other model parameter.</li>
<li><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Temporal-Fusion-Transformers-for-Interpretable-Time-Lim-Arik/6a9d69fb35414b8461573df333dba800f254519f">THIS WORK</a> uses an LSTM network to encode positional embeddings, which can better exploit sequential ordering information in time series.</li>
</ol>
</li>
<li>Timestamp Encoding: Encoding <strong>calendar timestamps</strong> (e.g., second, minute, hour, week, month, and year) and <strong>special timestamps</strong> (e.g., holidays and events).<ol>
<li> <strong>Informer / Autoformer / FED former</strong> proposed to encode <strong>timestamps as additional positional encoding</strong> by using learnable embedding layers.</li>
</ol>
</li>
</ol>
<h3 id="Attention-Module"><a href="#Attention-Module" class="headerlink" title="Attention Module"></a>Attention Module</h3><p>é¢å‘attention moduleçš„å·¥ä½œä¸»è¦è‡´åŠ›äºå‡å°‘self-attention moduleçš„æ—¶é—´ã€å†…å­˜å¤æ‚åº¦ï¼ˆåŸæ¥ä¸º$\mathcal{O}(N^2)$ï¼‰</p>
<ol>
<li>Introducing a <strong>sparsity bias</strong> into the attention mechanism: LogTrans, Pyraformer</li>
<li>Exploring the low-rank property of the self-attention matrix to speed up the computation: Informer, FEDformer</li>
</ol>
<h3 id="Architecture-based-Attention-Innovation"><a href="#Architecture-based-Attention-Innovation" class="headerlink" title="Architecture-based Attention Innovation"></a>Architecture-based Attention Innovation</h3><p>è¿™ç±»å·¥ä½œç›´æ¥é¢å‘<strong>æ—¶é—´åºåˆ—çš„ç‰¹æ®Šæ€§è´¨</strong>ï¼Œå¯¹Transformerçš„æ•´ä½“æ¶æ„è¿›è¡Œäº†æ”¹è¿›</p>
<ol>
<li>Introduce <strong>hierarchical architecture</strong> into Transformer to take into account the <strong>multi-resolution aspect</strong> of time series: Informer, Pyraformer<ol>
<li><strong>Informer</strong>: Inserts max-pooling layers with stride 2 between attention blocks, which down-sample series into its half slice (block-wise multi-resolution learning)</li>
<li><strong>Pyraformer</strong>: designs a <strong>C-ary tree-based</strong> attention mechanism, in which nodes at the finest scale correspond to the original time series, while nodes in the coarser scales represent series at lower resolutions. <ul>
<li>Pyraformer developed both <strong>intra-scale</strong> and <strong>inter-scale attentions</strong> in order to better capture temporal dependencies across different resolutions.</li>
<li>Hierarchical architecture also enjoys the benefits of efficient computation, particularly for long-time series.</li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 id="Application-Domains"><a href="#Application-Domains" class="headerlink" title="Application Domains"></a>Application Domains</h2><p>ä¸Šé¢çš„ç»¼è¿°å¯¹Forecastingã€anomaly detectionã€Classificationçš„ç›¸å…³ç ”ç©¶éƒ½ç»™å‡ºäº†è¯¦å°½çš„è°ƒç ”ï¼Œè¿™é‡Œåªæ•´ç†ä¸anomaly detectionç›¸å…³çš„å†…å®¹ã€‚Transformeræ¶æ„ä¸ºanomaly detectionä»»åŠ¡åšå‡ºçš„ä¸»è¦è´¡çŒ®è¿˜æ˜¯â€œ<strong>improve the ablity of modeling temporal dependency</strong>â€ã€‚é™¤æ­¤ä¹‹å¤–é’ˆå¯¹å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ï¼Œå¸¸è§çš„æ¨¡å‹èåˆæ–¹å¼æœ‰ï¼š</p>
<ol>
<li>Combine Transformer with neural generative models: VAE â€“ <a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Variational-Transformer-based-anomaly-detection-for-Wang-Pi/829ff2e0bad77467da0527515e3be91c376738ab">MT-RVAE</a>, <a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Unsupervised-Anomaly-Detection-in-Multivariate-Time-Zhang-Xia/07ed7c5bee83b27c8035896e853488e6132cd86c">TransAnomaly</a>; GAN â€“ <a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/TranAD%3A-Deep-Transformer-Networks-for-Anomaly-in-Tuli-Casale/2d57a3f90adf3fc28f0de61fb4b7b34bccb1b92d">TranAD</a>.</li>
<li>Combine Transformer with graph-based learning architecture for multivariate time series anomaly detection: <a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Learning-Graph-Structures-With-Transformer-for-in-Chen-Chen/95f5870b18d5f894e4f6ec8490d1a39e0963e79e">GTA</a></li>
<li>Combine Transformer with  Gaussian prior-Association: <a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Anomaly-Transformer%3A-Time-Series-Anomaly-Detection-Xu-Wu/a46b06a4b8b4deecf96a4e42cd19b4696f999e66">AnomalyTrans</a></li>
</ol>
<hr>
<p>æˆ‘ç°åœ¨è¦å†™ä¸€ä¸ªå…³äºç±»Transformerç½‘ç»œåœ¨æ—¶é—´åºåˆ—åˆ†æä»»åŠ¡ä¸Šçš„è¿‘å¹´ç ”ç©¶è¯»ä¹¦ç¬”è®°ï¼Œç»“åˆæˆ‘è°ƒç ”çš„ç»“æœï¼Œå‡†å¤‡åœ¨ç¬”è®°ä¸­å¯¹ä¸‹é¢å‡ ä¸ªå·¥ä½œåšå‡ºæ€»ç»“ï¼š</p>
<ol>
<li>N-BEATS: ICLR 2020</li>
<li>LogTrans: NeurIPS 2021</li>
<li>Informer: AAAI 2021 (Best Paper)</li>
<li>Autoformer: NeuraIPS 2021 </li>
<li>Lite Transformer: ICLR 2020 </li>
<li>FEDformer: ICML 2022</li>
<li>Pyraformer: ICLR 2022: <a target="_blank" rel="noopener" href="https://ar5iv.org/abs/2202.07125">https://ar5iv.org/abs/2202.07125</a></li>
<li>Transformer embeddings of irregularly spaced events and their participants: ICLR 2022 (<a target="_blank" rel="noopener" href="https://ar5iv.org/abs/2202.07125">https://ar5iv.org/abs/2202.07125</a>)</li>
<li>TranAD: VLDB 2022 (<a target="_blank" rel="noopener" href="https://ar5iv.org/abs/2202.07125)%E3%80%82">https://ar5iv.org/abs/2202.07125)ã€‚</a></li>
<li>Variational transformer-based anomaly detection approach for multivariate time seriesâ€ï¼ŒMeasurements 2022 (<a target="_blank" rel="noopener" href="https://ar5iv.org/abs/2202.07125)%E3%80%82">https://ar5iv.org/abs/2202.07125)ã€‚</a></li>
<li>â€œProbabilistic Transformer For Time Series Analysisâ€ NeurIPS 2021 (<a target="_blank" rel="noopener" href="https://openreview.net/forum?id=HfpNVDg3ExA)%E3%80%82">https://openreview.net/forum?id=HfpNVDg3ExA)ã€‚</a></li>
<li>Are Transformers Effective for Time Series Forecasting? (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.13504">https://arxiv.org/abs/2205.13504</a>)</li>
</ol>
 
            </div>
        </div>
        
        <div class="mt-3">
            <!-- å‰ä¸€é¡µåä¸€é¡µ -->
<div class="previous-next-links">
     

     
    <div class="next-design-link">
        <a href="../../notes/causal_meets_LLM/">
            å› æœæ¨æ–­é‡è§å¤§æ¨¡å‹
            <i style="font-size:16px;" class="fa fa-arrow-right" aria-hidden="true"></i>
        </a>
    </div>
 
</div> 
             
        </div>

    </div>
    <div class="col-2">
        <div class="d-none d-sm-none d-md-block sticky-top border-start">
             
    
        <div>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Taxonomy-of-Transformers-in-Time-Series"><span class="toc-number">1.</span> <span class="toc-text">Taxonomy of Transformers in Time Series</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Network-Modifications"><span class="toc-number">2.</span> <span class="toc-text">Network Modifications</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Positional-Encoding"><span class="toc-number">2.1.</span> <span class="toc-text">Positional Encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention-Module"><span class="toc-number">2.2.</span> <span class="toc-text">Attention Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Architecture-based-Attention-Innovation"><span class="toc-number">2.3.</span> <span class="toc-text">Architecture-based Attention Innovation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Application-Domains"><span class="toc-number">3.</span> <span class="toc-text">Application Domains</span></a></li></ol> 
        </div>
    
 

        </div>
    </div>
</div>
</div>
 
    </main>

    <!-- åº•éƒ¨æ  -->
    <footer class="bg-dark pt-1 pb-0 mt-5">
    <div class="container pb-3 pt-3 text-center">
        <p class="text-muted tag-hover">
            Powered by 
             <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://hexo.io">
                 Hexo.io
            </a> &  <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://github.com/smile-yan/hexo-theme-heyan">
               heyan
            </a>
            <br>
        </p>
    </div>
</footer>
</body> 
</html>