
<!DOCTYPE html>
<html>
<head>
    <title> Here is Shaoyu </title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--网页标题左侧显示-->
    <link rel="icon" href="/fish.ico" type="image/x-icon">
    <!--收藏夹显示图标-->
    <link rel="shortcut icon" href="/fish.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
    <link href="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">
    <script src="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"></script>
    
<link rel="stylesheet" href="../../../css/prism.css">
 
    
<script src="../../../js/prism.js"></script>

    
    
<link rel="stylesheet" href="../../../css/index.css">
 
    
<script src="../../../js/search.js"></script>



     
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
        </script>
     

     
        <script src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js"> </script>
        
<script src="../../../js/fancybox.js"></script>

    

    <script type="text/javascript">
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
        var path = "/" + search_path;
        searchFunc(path, 'local-search-input', 'local-search-result');
    </script>
<meta name="generator" content="Hexo 5.4.2"></head>
 
 
<body>
    <!-- 导航栏 -->
    <!-- 导航栏 -->
<nav class="navbar navbar-expand-md navbar-dark bg-dark mb-4 pt-2 pb-2">
    <div class="container">
        <!-- 标题 --> 
        <a class="navbar-brand navbar-expand-sm" href="/">
            <img class="nofancybox" src="/shark.svg" style="width: 65px;">
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
                aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <!-- 左边右边导航按钮 --> 
        <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav me-auto">
                <!-- 左右两侧的导航栏 -->

 
 

    <li class="nav-item">
        <a class="nav-link" href="../../../">
           <big> <i class="fa fa-home ps-1" ></i> Home</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../note">
           <big> <i class="fa fa-file-text-o ps-1" ></i> Notes</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../survey">
           <big> <i class="fa fa-globe ps-1" ></i> Surveys</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../paperlist">
           <big> <i class="fa fa-cc-discover ps-1" ></i> Paper Lists</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../archives">
           <big> <i class="fa fa-archive ps-1" ></i> Archives</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../about">
           <big> <i class="fa fa-user ps-1" ></i> About</big>
        </a>
    </li>
 
 
            </ul>

            
            <form class="">
                <div class="d-flex">
                    <button class="btn text-muted fa fa-search d-none d-md-block d-lg-block" disabled></button>
                    <input id="local-search-input" class="form-control me-2 pe-4" type="search"
                            placeholder="搜索 " aria-label="Search">
                </div>
                <div id="local-search-result" style="position:absolute; padding-top: 8px; max-height: 960px; width: 480px;overflow-y: scroll; z-index: 1050;"></div>
            </form>
            
            <ul class="navbar-nav">
                <!-- 左右两侧的导航栏 -->

 
 
            </ul>
        </div>
    </div>
</nav>


    <main class="container">
        
<div class="container-fluid markdown-section">
<div class="row">
    <div class="col-md-2 d-none d-sm-none d-md-block">
        <div class="">
            <!-- 同一类型的文件 -->
<!-- 尽可能做到每篇文章只有一个类
    1. 获得 post 的类别 categories
     2.1 如果 categories.length === 0，啥也不做
     2.2 如果 categories === 1，渲染这个类的所有文章
     2.3 如果 categories.length > 1 ，则以第一个类为准渲染所有文章
-->


 
        </div>
    </div>
    <div class="col-md-8 col-sm-12">
        <!-- 博客详情 -->
        <div class="">
            <!-- ps-4 pe-4 pt-2 -->
            <p class="h2">
                <span class="post-title">
                    Transformer Family on Time series
                </span>   
            </p>

            <div class="pb-3 pt-1 pe-3">
                <i class="fa fa-calendar p-1"></i>
                2023/05/30 21:14:00 

                <i class="fa fa-pencil"> </i>
                2023/05/30 21:15:00 

                <!--
                <i class="fa fa-folder-open p-1"> </i> 
                <span class="tag-hover">
                     
                </span>
                -->

                <i class="fa fa-tags p-1"> </i>
                <span class="tag-hover">
                    
                        <a href="../../../tags/transformers/" class="link-dark text-decoration-none"> 
                            transformers 
                        </a>
                    
                        <a href="../../../tags/survey/" class="link-dark text-decoration-none"> 
                            survey 
                        </a>
                    
                        <a href="../../../tags/time-series/" class="link-dark text-decoration-none"> 
                            time series 
                        </a>
                    
                </span>
            </div>

            <div class="border-bottom pb-2">
                <p>当代的时间序列分析任务需要应对各种复杂的数据模式和动态性，传统的统计模型和机器学习方法在处理这些挑战时常常受限。然而，近年来，类Transformer网络在时间序列分析领域取得了显著的突破。Transformer模型的出现为时间序列分析任务提供了一种新的、强大的工具，它能够自适应地捕捉序列中的长期依赖关系，并能够有效地建模非线性和非平稳性特征。</p>
<span id="more"></span>

<p>在文章中，我们将关注一些近年来在时间序列分析任务上涌现的重要工作，这些工作以类Transformer网络为基础，以其卓越的性能和创新的方法引起了广泛的关注。我们将重点介绍以下几个工作：</p>
<ol>
<li>N-BEATS（ICLR 2022）</li>
<li>LogTrans（NeurIPS 2021）</li>
<li>Informer （ AAAI 2021 Best Paper）<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/354916261">📒Informer：用Transformer架构解决LSTF问题 - 知乎</a></li>
<li>Autoformer（NeuraIPS 2021）<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/386955393">📒细读好文 之 Autoformer - 知乎</a><ul>
<li>🌟其中基于Wiener-Khinchin定理的Auto-Correlation Mechanism有点意思，可以单独看看。</li>
</ul>
</li>
<li>FEDformer（ICML 2022）<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/528131016">📒阿里达摩院最新FEDformer，长程时序预测全面超越SOTA - 知乎</a></li>
<li>Pyraformer（ICLR 2022）<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/467765457">📒时间序列预测@Pyraformer - 知乎</a></li>
<li>Transformer embeddings of irregularly spaced events and their participants（ICLR 2022）</li>
<li>TranAD（VLDB 2022）</li>
<li>Probabilistic Transformer For Time Series Analysis（NeurIPS 2021）。</li>
</ol>
<p>🌟 此外，我们还将依托论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.13504">Are Transformers Effective for Time Series Forecasting?</a>探讨一个重要的问题，即Transformer网络在时间序列预测中的有效性。</p>
<p>通过对这些工作的综述和分析，我们将深入了解类Transformer网络在时间序列分析任务中的应用，以及它们的创新之处、优点和局限性。这将有助于我们对该领域的最新研究进展有一个全面的了解，并为未来的研究和应用提供指导和启示。</p>
<h2 id="Taxonomy-of-Transformers-in-Time-Series"><a href="#Taxonomy-of-Transformers-in-Time-Series" class="headerlink" title="Taxonomy of Transformers in Time Series"></a>Taxonomy of Transformers in Time Series</h2><blockquote>
<p>Yang, C., Mei, H., &amp; Eisner, J. (2021). Transformer embeddings of irregularly spaced events and their participants. <em>arXiv preprint arXiv:2201.00044</em>.</p>
</blockquote>
<p>根据上面的文章，基于Transformers的时间序列分析工作的创新点主要分为两类，即更改模型架构的，以及为特殊的应用而适配的。后文中，我们也将从这个分类法出发，来整理每个工作是如何改进Transformer的。</p>
<img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20230530200210844.png" alt="Taxonomy of Transformers for time series modeling from the perspectives of network modifications and application domains" style="zoom:50%;" />



<h2 id="Network-Modifications"><a href="#Network-Modifications" class="headerlink" title="Network Modifications"></a>Network Modifications</h2><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><ol>
<li>Learnable Positional Encoding<ol>
<li><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/A-Transformer-based-Framework-for-Multivariate-Time-Zerveas-Jayaraman/2051548f7681c96d603de932ee23406c525276f9">THIS WORK</a> introduces an <strong>embedding layer</strong> in Transformer that <strong>learns embedding vectors for each position index</strong> jointly with other model parameter.</li>
<li><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Temporal-Fusion-Transformers-for-Interpretable-Time-Lim-Arik/6a9d69fb35414b8461573df333dba800f254519f">THIS WORK</a> uses an LSTM network to encode positional embeddings, which can better exploit sequential ordering information in time series.</li>
</ol>
</li>
<li>Timestamp Encoding: Encoding <strong>calendar timestamps</strong> (e.g., second, minute, hour, week, month, and year) and <strong>special timestamps</strong> (e.g., holidays and events).<ol>
<li> <strong>Informer / Autoformer / FED former</strong> proposed to encode <strong>timestamps as additional positional encoding</strong> by using learnable embedding layers.</li>
</ol>
</li>
</ol>
<h3 id="Attention-Module"><a href="#Attention-Module" class="headerlink" title="Attention Module"></a>Attention Module</h3><p>面向attention module的工作主要致力于减少self-attention module的时间、内存复杂度（原来为$\mathcal{O}(N^2)$）</p>
<ol>
<li>Introducing a <strong>sparsity bias</strong> into the attention mechanism: LogTrans, Pyraformer</li>
<li>Exploring the low-rank property of the self-attention matrix to speed up the computation: Informer, FEDformer</li>
</ol>
<h3 id="Architecture-based-Attention-Innovation"><a href="#Architecture-based-Attention-Innovation" class="headerlink" title="Architecture-based Attention Innovation"></a>Architecture-based Attention Innovation</h3><p>这类工作直接面向<strong>时间序列的特殊性质</strong>，对Transformer的整体架构进行了改进</p>
<ol>
<li>Introduce <strong>hierarchical architecture</strong> into Transformer to take into account the <strong>multi-resolution aspect</strong> of time series: Informer, Pyraformer<ol>
<li><strong>Informer</strong>: Inserts max-pooling layers with stride 2 between attention blocks, which down-sample series into its half slice (block-wise multi-resolution learning)</li>
<li><strong>Pyraformer</strong>: designs a <strong>C-ary tree-based</strong> attention mechanism, in which nodes at the finest scale correspond to the original time series, while nodes in the coarser scales represent series at lower resolutions. <ul>
<li>Pyraformer developed both <strong>intra-scale</strong> and <strong>inter-scale attentions</strong> in order to better capture temporal dependencies across different resolutions.</li>
<li>Hierarchical architecture also enjoys the benefits of efficient computation, particularly for long-time series.</li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 id="Application-Domains"><a href="#Application-Domains" class="headerlink" title="Application Domains"></a>Application Domains</h2><p>上面的综述对Forecasting、anomaly detection、Classification的相关研究都给出了详尽的调研，这里只整理与anomaly detection相关的内容。Transformer架构为anomaly detection任务做出的主要贡献还是“<strong>improve the ablity of modeling temporal dependency</strong>”。除此之外针对异常检测任务，常见的模型融合方式有：</p>
<ol>
<li>Combine Transformer with neural generative models: VAE – <a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Variational-Transformer-based-anomaly-detection-for-Wang-Pi/829ff2e0bad77467da0527515e3be91c376738ab">MT-RVAE</a>, <a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Unsupervised-Anomaly-Detection-in-Multivariate-Time-Zhang-Xia/07ed7c5bee83b27c8035896e853488e6132cd86c">TransAnomaly</a>; GAN – <a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/TranAD%3A-Deep-Transformer-Networks-for-Anomaly-in-Tuli-Casale/2d57a3f90adf3fc28f0de61fb4b7b34bccb1b92d">TranAD</a>.</li>
<li>Combine Transformer with graph-based learning architecture for multivariate time series anomaly detection: <a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Learning-Graph-Structures-With-Transformer-for-in-Chen-Chen/95f5870b18d5f894e4f6ec8490d1a39e0963e79e">GTA</a></li>
<li>Combine Transformer with  Gaussian prior-Association: <a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Anomaly-Transformer%3A-Time-Series-Anomaly-Detection-Xu-Wu/a46b06a4b8b4deecf96a4e42cd19b4696f999e66">AnomalyTrans</a></li>
</ol>
<hr>
<p>我现在要写一个关于类Transformer网络在时间序列分析任务上的近年研究读书笔记，结合我调研的结果，准备在笔记中对下面几个工作做出总结：</p>
<ol>
<li>N-BEATS: ICLR 2020</li>
<li>LogTrans: NeurIPS 2021</li>
<li>Informer: AAAI 2021 (Best Paper)</li>
<li>Autoformer: NeuraIPS 2021 </li>
<li>Lite Transformer: ICLR 2020 </li>
<li>FEDformer: ICML 2022</li>
<li>Pyraformer: ICLR 2022: <a target="_blank" rel="noopener" href="https://ar5iv.org/abs/2202.07125">https://ar5iv.org/abs/2202.07125</a></li>
<li>Transformer embeddings of irregularly spaced events and their participants: ICLR 2022 (<a target="_blank" rel="noopener" href="https://ar5iv.org/abs/2202.07125">https://ar5iv.org/abs/2202.07125</a>)</li>
<li>TranAD: VLDB 2022 (<a target="_blank" rel="noopener" href="https://ar5iv.org/abs/2202.07125)%E3%80%82">https://ar5iv.org/abs/2202.07125)。</a></li>
<li>Variational transformer-based anomaly detection approach for multivariate time series”，Measurements 2022 (<a target="_blank" rel="noopener" href="https://ar5iv.org/abs/2202.07125)%E3%80%82">https://ar5iv.org/abs/2202.07125)。</a></li>
<li>“Probabilistic Transformer For Time Series Analysis” NeurIPS 2021 (<a target="_blank" rel="noopener" href="https://openreview.net/forum?id=HfpNVDg3ExA)%E3%80%82">https://openreview.net/forum?id=HfpNVDg3ExA)。</a></li>
<li>Are Transformers Effective for Time Series Forecasting? (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.13504">https://arxiv.org/abs/2205.13504</a>)</li>
</ol>
 
            </div>
        </div>
        
        <div class="mt-3">
            <!-- 前一页后一页 -->
<div class="previous-next-links">
     

     
    <div class="next-design-link">
        <a href="../../notes/causal_meets_LLM/">
            因果推断遇见大模型
            <i style="font-size:16px;" class="fa fa-arrow-right" aria-hidden="true"></i>
        </a>
    </div>
 
</div> 
             
        </div>

    </div>
    <div class="col-2">
        <div class="d-none d-sm-none d-md-block sticky-top border-start">
             
    
        <div>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Taxonomy-of-Transformers-in-Time-Series"><span class="toc-number">1.</span> <span class="toc-text">Taxonomy of Transformers in Time Series</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Network-Modifications"><span class="toc-number">2.</span> <span class="toc-text">Network Modifications</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Positional-Encoding"><span class="toc-number">2.1.</span> <span class="toc-text">Positional Encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention-Module"><span class="toc-number">2.2.</span> <span class="toc-text">Attention Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Architecture-based-Attention-Innovation"><span class="toc-number">2.3.</span> <span class="toc-text">Architecture-based Attention Innovation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Application-Domains"><span class="toc-number">3.</span> <span class="toc-text">Application Domains</span></a></li></ol> 
        </div>
    
 

        </div>
    </div>
</div>
</div>
 
    </main>

    <!-- 底部栏 -->
    <footer class="bg-dark pt-1 pb-0 mt-5">
    <div class="container pb-3 pt-3 text-center">
        <p class="text-muted tag-hover">
            Powered by 
             <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://hexo.io">
                 Hexo.io
            </a> &  <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://github.com/smile-yan/hexo-theme-heyan">
               heyan
            </a>
            <br>
        </p>
    </div>
</footer>
</body> 
</html>