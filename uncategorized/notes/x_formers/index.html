
<!DOCTYPE html>
<html>
<head>
    <title>Smileyan</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
    <link href="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">
    <script src="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"></script>
    
<link rel="stylesheet" href="../../../css/prism.css">
 
    
<script src="../../../js/prism.js"></script>

    
    
<link rel="stylesheet" href="../../../css/index.css">
 
    
<script src="../../../js/search.js"></script>



     
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
        </script>
     

     
        <script src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js"> </script>
        
<script src="../../../js/fancybox.js"></script>

    

    <script type="text/javascript">
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
        var path = "/" + search_path;
        searchFunc(path, 'local-search-input', 'local-search-result');
    </script>
<meta name="generator" content="Hexo 5.4.2"></head>
 
 
<body>
    <!-- 导航栏 -->
    <!-- 导航栏 -->
<nav class="navbar navbar-expand-md navbar-dark bg-dark mb-4 pt-2 pb-2">
    <div class="container">
        <!-- 标题 --> 
        <a class="navbar-brand navbar-expand-sm" href="/">
            <img class="nofancybox" src="/logo.png" style="width: 75px;"  alt="笑颜网">
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
                aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <!-- 左边右边导航按钮 --> 
        <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav me-auto">
                <!-- 左右两侧的导航栏 -->

 
 

    <li class="nav-item">
        <a class="nav-link" href="../../../">
           <big> <i class="fa fa-home ps-1" ></i> 主页</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../archives">
           <big> <i class="fa fa-archive ps-1" ></i> 归档</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../about">
           <big> <i class="fa fa-user ps-1" ></i> 关于</big>
        </a>
    </li>
 
 
            </ul>

            
            <form class="">
                <div class="d-flex">
                    <button class="btn text-muted fa fa-search d-none d-md-block d-lg-block" disabled></button>
                    <input id="local-search-input" class="form-control me-2 pe-4" type="search"
                            placeholder="搜索 " aria-label="Search">
                </div>
                <div id="local-search-result" style="position:absolute; padding-top: 8px; max-height: 960px; width: 480px;overflow-y: scroll; z-index: 1050;"></div>
            </form>
            
            <ul class="navbar-nav">
                <!-- 左右两侧的导航栏 -->

 
 
            </ul>
        </div>
    </div>
</nav>


    <main class="container">
        
<div class="container-fluid markdown-section">
<div class="row">
    <div class="col-md-2 d-none d-sm-none d-md-block">
        <div class="">
            <!-- 同一类型的文件 -->
<!-- 尽可能做到每篇文章只有一个类
    1. 获得 post 的类别 categories
     2.1 如果 categories.length === 0，啥也不做
     2.2 如果 categories === 1，渲染这个类的所有文章
     2.3 如果 categories.length > 1 ，则以第一个类为准渲染所有文章
-->


 
        </div>
    </div>
    <div class="col-md-8 col-sm-12">
        <!-- 博客详情 -->
        <div class="">
            <!-- ps-4 pe-4 pt-2 -->
            <p class="h2">
                <span class="post-title">
                    Transformers - Survey
                </span>   
            </p>

            <div class="pb-3 pt-1 pe-3">
                <i class="fa fa-calendar p-1"></i>
                2022/01/20 14:00:00 

                <i class="fa fa-pencil"> </i>
                2022/04/12 16:34:39 

                <!--
                <i class="fa fa-folder-open p-1"> </i> 
                <span class="tag-hover">
                     
                </span>
                -->

                <i class="fa fa-tags p-1"> </i>
                <span class="tag-hover">
                    
                        <a href="../../../tags/note/" class="link-dark text-decoration-none"> 
                            note 
                        </a>
                    
                        <a href="../../../tags/transformers/" class="link-dark text-decoration-none"> 
                            transformers 
                        </a>
                    
                </span>
            </div>

            <div class="border-bottom pb-2">
                <p>A Survey of Transformers</p>
<p>TIANYANG LIN, YUXIN WANG, XIANGYANG LIU, and XIPENG QIU</p>
<p>School of Computer Science, Fudan University, China and Shanghai Key Laboratory of Intelligent Information Processing, Fudan<br>University, China</p>
<span id="more"></span>

<h2 id="Motivations"><a href="#Motivations" class="headerlink" title="Motivations"></a>Motivations</h2><p>Vanilla Transformer的主要弊端与改进方向</p>
<ul>
<li><strong>Model Efficiency</strong>：Transforme处理长序列时效率低下，这主要是由于self-attention的计算和内存复杂性造成的。改进方法包括轻量级attention，例如sparse attention<br>variants、和分治方法（Divide-and-conquer），例如recurrent and hierarchical mechanism。</li>
<li><strong>Model Generalization</strong>：Transformer的结构从理论上来说是非常灵活的，几乎不对输入数据的结构性偏差进行假设，因此很难对<strong>小规模数据</strong>进行训练。<br>改进方法包括引入结构性偏差（structural bias）或正则化（regularization,）、对大规模未标记数据进行预训练等。</li>
<li><strong>Model Adaptation</strong>：这类工作旨在使Transformer适应特定的下游任务和应用。</li>
</ul>
<p>这篇文章主要根据改进vanilla Transformer的方式来组织相关的工作，即：<strong>架构修改</strong>、<strong>预训练</strong>、<strong>应用</strong>。且本文主要关注架构变体，并简要讨论预训练和面向应用的变体。</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="Vanilla-Transformer"><a href="#Vanilla-Transformer" class="headerlink" title="Vanilla Transformer"></a>Vanilla Transformer</h3><div align="center">
    <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/20220120210749.png" width = "70%" />
</div>

<p>首先，transformer遵循seq2seq结构，其中encoder decoder都由$L$个单独的模块堆叠而成。要点包括：</p>
<ul>
<li><strong>encoder</strong>: multi-head self-attention, position-wise feed-forward network (FFN), residual connection, Layer Normalization.</li>
<li><strong>decoder</strong>: 上述模块 + cross-attention (between the multi-head self-attention modules and the position-wise FFNs), decoder中的attention matrix计算是有位置限制的<br>（考虑到后续时刻输出不能为前序时刻的输出提供参考）</li>
</ul>
<h4 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h4><div align="center">
  $\operatorname{Attention}(\mathrm{Q}, \mathrm{K}, \mathrm{V})=\operatorname{softmax}\left(\frac{\mathrm{QK}^{\top}}{\sqrt{D_{k}}}\right) \mathrm{V}=\mathrm{AV}$
</div>

<p>上式是attention的基本原理，其中query $Q \in \mathbb{R}^{N \times D_{k}}$，key $\mathrm{K} \in \mathbb{R}^{M \times D_{k}}$，value $\mathbf{V} \in \mathbb{R}^{M \times D_{v}}$。$N,M$分别为query和key（value）的长度，$D_k, D_v$为key（query）与value的维度。$\mathrm{A}=\operatorname{softmax}\left(\frac{\mathrm{QK}^{\top}}{\sqrt{D_{k}}}\right)$也被称为attention matrix。除以$\sqrt{D_{k}}$是为了缓解梯度消失。</p>
<p>将数据维度压缩为1，则上述三个对象可以理解为，$\mathbf{v}=[v_1, \cdots, v_N]$代表module在未经过筛选时要输出的值，我们期望的输出是$\mathbf{w} * \mathbf{v}$，其中权重$\mathbf{w}$的计算即为上式中$softmax(\cdot)$的结果。</p>
<p>  <strong>与软寻址之间的联系（如下图）</strong>：令Source $\mathbf{S}=[&lt;k_1, v_1&gt;, \cdots, &lt;k_n, v_n&gt;]$视为存储器中的全部内容，当前有一个查询$q=k_i$，目的是取出source中匹配键值的值$v_i$。<br>  我们记$\mathbf{k} = [k_1, \cdots, k_n]$，我们通过Query $q$和存储器内元素的地址$\mathbf{k}$进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容$k_i$，<br>而是可能从$\mathbf{k}$中的每一项都会取出内容，取出内容的重要性根据$q$和$\mathbf{k}$的相似性来决定，相似性记为$\mathbf{w} = [w_1, \cdots, w_n]$，<br>  之后对存储器中的每一项对应的值进行加权求和，即$v = w_1 v_1 + \cdots + w_n v_n$，得到最终的Value值，也即Attention的结果值。<br>  所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/20220120203102.png" width = "50%" />
</div>


<div align="center">
    \begin{aligned}
        \text { MultiHeadAttn }(Q, K, V) &=\text { Concat }\left(\text { head }_{1}, \cdots, \text { head }_{H}\right) \mathrm{W}^{O}, \\
        \text { where head }_{i} &=\operatorname{Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) .
    \end{aligned}
</div>

<p>上式是multi-head attention的基本表达式，其中$Q, K, V$的维度均为$D_m$，他们分别由几个线性映射（$W_{i}^{Q}, W_{i}^{K} W_{i}^{V}$）投影到维度为$D_k, D_k, D_v$的空间中，并进行attention计算，最后模型将所有输出连接并将其投影到$D_m$维空间。</p>
<ul>
<li><strong>Self-attention</strong>：$Q=K=V=X$，$X$是前一层的输出</li>
<li><strong>Masked Self-attention</strong>：在Transformer解码器中，self-attention生成的weight受到位置限制，其生成的attention matrix只度量某个位置i和j之间的权重，且$i&gt;=j$。具体地，其实现过程是为attention matrix的某些位置赋予mask。$\hat{A}=\exp \left(\frac{Q K^{\top}}{\sqrt{D_{k}}}\right)$, $\hat{A}_{i j}=-\infty \text { if } i&lt;j$。这种自我注意通常被称为自回归注意或因果注意。</li>
<li><strong>Cross-attention</strong>：query由上一层decoder的输出投影而来，key/value又encoder的输出投影而来</li>
</ul>
<h4 id="Position-wise-FFN"><a href="#Position-wise-FFN" class="headerlink" title="Position-wise FFN"></a>Position-wise FFN</h4><p>基于位置的FFN是一个全连接的前馈网络，它在每一个位置上进行独立运算，注意：前向网络的参数在不同位置上是共享的，因此Position-wise FFN也可以理解为两层kernel size为1的卷积层。</p>
<div align="center">
    $\operatorname{FFN}\left(\mathbf{H}^{\prime}\right)=\operatorname{ReLU}\left(\mathbf{H}^{\prime} \mathbf{W}^{1}+\mathbf{b}^{1}\right) \mathbf{W}^{2}+\mathbf{b}^{2}$
</div>

<p>其中$\mathbf{H}^{\prime}$为上一层的输出，$\mathbf{W}^{1} \in \mathbb{R}^{D_{m} \times D_{f}}, \mathbf{W}^{2} \in \mathbb{R}^{D_{f} \times D_{m}}, \mathbf{b}^{1} \in \mathbb{R}^{D_{f}}, \mathbf{b}^{2} \in \mathbb{R}^{D_{m}}$，一般来讲FFN的维度参数设置为$D_f &gt; D_m$</p>
<h4 id="Residual-connection-and-normalization"><a href="#Residual-connection-and-normalization" class="headerlink" title="Residual connection and normalization"></a>Residual connection and normalization</h4><div align="center">
    \begin{aligned}
        \mathrm{H}^{\prime} &=\text { LayerNorm }(\text { SelfAttention }(\mathrm{X})+\mathrm{X}) \\
        \mathrm{H} &=\text { LayerNorm }\left(\mathrm{FFN}\left(\mathrm{H}^{\prime}\right)+\mathrm{H}^{\prime}\right)
    \end{aligned}
</div>

<h4 id="Position-Encodings"><a href="#Position-Encodings" class="headerlink" title="Position Encodings"></a>Position Encodings</h4><p>因为Transformer没有引入递归结构或卷积操作，所以对于每个attention来说，它们不知道数据的前后位置信息（特别是对于编码器来说）。因此需要对数据的位置做额外的表征</p>
<h3 id="模型的拆解用法"><a href="#模型的拆解用法" class="headerlink" title="模型的拆解用法"></a>模型的拆解用法</h3><ul>
<li><strong>encoder-decoder</strong>：用于seq2seq modeling</li>
<li><strong>encoder only</strong>：representation learning，用于支持classification，sequence labeling</li>
<li><strong>decoder only</strong>：（此时encoder-decoder cross-attention module也被移除），sequence generation，用于支持language modeling</li>
</ul>
<p>（后续更新：主要transformer的总结，以及亮点结构）</p>
 
            </div>
        </div>
        
        <div class="mt-3">
            <!-- 前一页后一页 -->
<div class="previous-next-links">
     
    <div class="previous-design-link">
        <a href="../5G_NR/">
            <i style="font-size:16px;" class="fa fa-arrow-left" aria-hidden="true"></i>
            5G NR（New Radio）规范
        </a>
    </div>
     

     
    <div class="next-design-link">
        <a href="../alarm_in_4GWAN/">
            4GWAN中的告警（alarm）分析
            <i style="font-size:16px;" class="fa fa-arrow-right" aria-hidden="true"></i>
        </a>
    </div>
 
</div> 
             
        </div>

    </div>
    <div class="col-2">
        <div class="d-none d-sm-none d-md-block sticky-top border-start">
             
    
        <div>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Motivations"><span class="toc-number">1.</span> <span class="toc-text">Motivations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Background"><span class="toc-number">2.</span> <span class="toc-text">Background</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Vanilla-Transformer"><span class="toc-number">2.1.</span> <span class="toc-text">Vanilla Transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8B%86%E8%A7%A3%E7%94%A8%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text">模型的拆解用法</span></a></li></ol></li></ol> 
        </div>
    
 

        </div>
    </div>
</div>
</div>
 
    </main>

    <!-- 底部栏 -->
    <footer class="bg-dark pt-1 pb-0 mt-5">
    <div class="container pb-3 pt-3 text-center">
        <p class="text-muted tag-hover">
            All Rights Reserved <i class="fa fa-copyright"></i> 2017-2022 笑颜网 smileyan.cn <br> 
            Powered by 
             <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://hexo.io">
                 Hexo.io
            </a> &  <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://github.com/smile-yan/hexo-theme-heyan">
               heyan
            </a>
            <br>
            <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://beian.miit.gov.cn/#/Integrated/index">
                <img src="/police.png" style="width: 18px; height: 18px; margin-top: -4px" class="nofancybox">
                湘ICP备 17012851号 
            </a>
        </p>
    </div>
</footer>
</body> 
</html>