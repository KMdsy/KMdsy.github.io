
<!DOCTYPE html>
<html>
<head>
    <title>Smileyan</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
    <link href="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">
    <script src="https://cdn.staticfile.org/twitter-bootstrap/5.1.1/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"></script>
    
<link rel="stylesheet" href="../../../css/prism.css">
 
    
<script src="../../../js/prism.js"></script>

    
    
<link rel="stylesheet" href="../../../css/index.css">
 
    
<script src="../../../js/search.js"></script>



     
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
        </script>
     

     
        <script src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js"> </script>
        
<script src="../../../js/fancybox.js"></script>

    

    <script type="text/javascript">
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
        var path = "/" + search_path;
        searchFunc(path, 'local-search-input', 'local-search-result');
    </script>
<meta name="generator" content="Hexo 5.4.2"></head>
 
 
<body>
    <!-- 导航栏 -->
    <!-- 导航栏 -->
<nav class="navbar navbar-expand-md navbar-dark bg-dark mb-4 pt-2 pb-2">
    <div class="container">
        <!-- 标题 --> 
        <a class="navbar-brand navbar-expand-sm" href="/">
            <img class="nofancybox" src="/logo.png" style="width: 75px;"  alt="笑颜网">
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
                aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <!-- 左边右边导航按钮 --> 
        <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav me-auto">
                <!-- 左右两侧的导航栏 -->

 
 

    <li class="nav-item">
        <a class="nav-link" href="../../../">
           <big> <i class="fa fa-home ps-1" ></i> Home</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../note">
           <big> <i class="fa fa-file-text-o ps-1" ></i> Notes</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../survey">
           <big> <i class="fa fa-globe ps-1" ></i> Surveys</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../paperlist">
           <big> <i class="fa fa-cc-discover ps-1" ></i> Paper Lists</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../archives">
           <big> <i class="fa fa-archive ps-1" ></i> Archives</big>
        </a>
    </li>

    <li class="nav-item">
        <a class="nav-link" href="../../../about">
           <big> <i class="fa fa-user ps-1" ></i> About</big>
        </a>
    </li>
 
 
            </ul>

            
            <form class="">
                <div class="d-flex">
                    <button class="btn text-muted fa fa-search d-none d-md-block d-lg-block" disabled></button>
                    <input id="local-search-input" class="form-control me-2 pe-4" type="search"
                            placeholder="搜索 " aria-label="Search">
                </div>
                <div id="local-search-result" style="position:absolute; padding-top: 8px; max-height: 960px; width: 480px;overflow-y: scroll; z-index: 1050;"></div>
            </form>
            
            <ul class="navbar-nav">
                <!-- 左右两侧的导航栏 -->

 
 
            </ul>
        </div>
    </div>
</nav>


    <main class="container">
        
<div class="container-fluid markdown-section">
<div class="row">
    <div class="col-md-2 d-none d-sm-none d-md-block">
        <div class="">
            <!-- 同一类型的文件 -->
<!-- 尽可能做到每篇文章只有一个类
    1. 获得 post 的类别 categories
     2.1 如果 categories.length === 0，啥也不做
     2.2 如果 categories === 1，渲染这个类的所有文章
     2.3 如果 categories.length > 1 ，则以第一个类为准渲染所有文章
-->


 
        </div>
    </div>
    <div class="col-md-8 col-sm-12">
        <!-- 博客详情 -->
        <div class="">
            <!-- ps-4 pe-4 pt-2 -->
            <p class="h2">
                <span class="post-title">
                    Transformers in Hugging Face
                </span>   
            </p>

            <div class="pb-3 pt-1 pe-3">
                <i class="fa fa-calendar p-1"></i>
                2022/11/23 19:30:00 

                <i class="fa fa-pencil"> </i>
                2023/02/07 23:44:32 

                <!--
                <i class="fa fa-folder-open p-1"> </i> 
                <span class="tag-hover">
                     
                </span>
                -->

                <i class="fa fa-tags p-1"> </i>
                <span class="tag-hover">
                    
                        <a href="../../../tags/hugging-face/" class="link-dark text-decoration-none"> 
                            hugging face 
                        </a>
                    
                        <a href="../../../tags/transformers/" class="link-dark text-decoration-none"> 
                            transformers 
                        </a>
                    
                </span>
            </div>

            <div class="border-bottom pb-2">
                <p>Hugging Face 的入门教程，目标是从0开始训练自己的大模型。</p>
<span id="more"></span>

<h2 id="重点教程"><a href="#重点教程" class="headerlink" title="重点教程"></a>重点教程</h2><ul>
<li>组装所有的组件：<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter2/6?fw=pt">https://huggingface.co/course/chapter2/6?fw=pt</a></li>
<li>processing data: <a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3/2?fw=pt">https://huggingface.co/course/chapter3/2?fw=pt</a></li>
<li>Fine-tune: <a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3/3?fw=pt">https://huggingface.co/course/chapter3/3?fw=pt</a></li>
<li><strong>Full training</strong>: <a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3/4?fw=pt">https://huggingface.co/course/chapter3/4?fw=pt</a></li>
<li><strong>Train a new tokenizer from a old one</strong>: <a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter6/2?fw=pt">https://huggingface.co/course/chapter6/2?fw=pt</a></li>
<li><strong>Use open source dataset</strong>: <a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter5/1?fw=pt">https://huggingface.co/course/chapter5/1?fw=pt</a></li>
<li>Check tokenizers is fast or not: <a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter6/3?fw=pt">https://huggingface.co/course/chapter6/3?fw=pt</a></li>
<li>Normalization and pre-tokenization (maybe we won’t use): <a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter6/4?fw=pt">https://huggingface.co/course/chapter6/4?fw=pt</a></li>
</ul>
<h2 id="Hugging-Face"><a href="#Hugging-Face" class="headerlink" title="Hugging Face"></a>Hugging Face</h2><p><code>pipeline</code>: 一个端到端的transformer实现，可以直接用于接收文本信息，得到模型在下游任务上的向量表示，并最终处理为人类可理解的形式。</p>
<p><code>pipeline</code> = <code>tokenizer</code> + <code>model</code> +  <code>post processing</code></p>
<img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20221123193602726.png" alt="image-20221123193602726" style="zoom: 50%;" />

<h3 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h3><p>tokenizer: </p>
<ol>
<li>[分词] Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens<ul>
<li>split on spaces</li>
<li>Character-based</li>
<li>sub-word tokenization</li>
</ul>
</li>
<li>[查表] Mapping each token to an integer</li>
<li>[add attention mask, etc] Adding additional inputs that may be useful to the model</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># load a pretrained tokenizer</span>
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

checkpoint <span class="token operator">=</span> <span class="token string">"distilbert-base-uncased-finetuned-sst-2-english"</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>

<span class="token comment"># get result </span>
raw_inputs <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"I've been waiting for a HuggingFace course my whole life."</span><span class="token punctuation">,</span>
    <span class="token string">"I hate this so much!"</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>
inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>raw_inputs<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''
&#123;
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 分词之后，每个词在词表中的id，注意这里用了word and subword分词方法，即分割词语到不可分割的常见词语为止，其中包含了用于将序列填充为等长序列的占位符
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])，have the same shape as input ids, 
&#125;
'''</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>API:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># load</span>
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bert-base-cased"</span><span class="token punctuation">)</span>
<span class="token comment"># use</span>
tokenizer<span class="token punctuation">(</span><span class="token string">"Using a Transformer network is simple"</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
&#123;'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]&#125;
'''</span>
<span class="token comment"># split (tokenize)</span>
sequence <span class="token operator">=</span> <span class="token string">"Using a Transformer network is simple"</span>
tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>sequence<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''output: ['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']'''</span>
<span class="token comment"># From tokens to input IDs</span>
ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>ids<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''output: [7993, 170, 11303, 1200, 2443, 1110, 3014]'''</span>
<span class="token comment"># decoding</span>
decoded_string <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">7993</span><span class="token punctuation">,</span> <span class="token number">170</span><span class="token punctuation">,</span> <span class="token number">11303</span><span class="token punctuation">,</span> <span class="token number">1200</span><span class="token punctuation">,</span> <span class="token number">2443</span><span class="token punctuation">,</span> <span class="token number">1110</span><span class="token punctuation">,</span> <span class="token number">3014</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>decoded_string<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''output: 'Using a Transformer network is simple'''</span>'
<span class="token comment"># save</span>
tokenizer<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span><span class="token string">"directory_on_my_computer"</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p><code>Model</code> = <code>transformer</code> + <code>model heads</code></p>
<p><code>transformer</code>: input: tokenized raw data; output: high-dimensional output shape like <code>[b, t, d]</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># load pretrained transformer</span>
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModel
checkpoint <span class="token operator">=</span> <span class="token string">"distilbert-base-uncased-finetuned-sst-2-english"</span>
model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>
outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span> <span class="token comment"># tokenized input</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>last_hidden_state<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment"># output: torch.Size([2, 16, 768]), [b, t, d]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>model heads</code>: input: output of transformer; output: the result of downstream task, maybe the output of a sigmoid network.</p>
<img src="https://raw.githubusercontent.com/KMdsy/figurebed/master/img/image-20221123194149899.png" alt="image-20221123194149899" style="zoom: 33%;" />

<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># transformer with subsequent network</span>
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification
checkpoint <span class="token operator">=</span> <span class="token string">"distilbert-base-uncased-finetuned-sst-2-english"</span>
model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">)</span>
outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>logits<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment"># output: torch.Size([2, 2]), we have just two sentences and two labels, the result we get from our model is of shape 2 x 2.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>The Choice of <code>Model</code></strong></p>
<ul>
<li><code>*Model</code> <strong>(retrieve the hidden states)</strong></li>
<li><code>*ForCausalLM</code></li>
<li><code>*ForMaskedLM</code></li>
<li><code>*ForMultipleChoice</code></li>
<li><code>*ForQuestionAnswering</code></li>
<li><code>*ForSequenceClassification</code></li>
<li><code>*ForTokenClassification</code></li>
<li>and others (non-exhaustive list)</li>
</ul>
<h3 id="Post-processing"><a href="#Post-processing" class="headerlink" title="Post-processing"></a>Post-processing</h3><p>Map tensor value output by model head (mentioned above) to text (according to id2text, etc.).</p>
 
            </div>
        </div>
        
        <div class="mt-3">
            <!-- 前一页后一页 -->
<div class="previous-next-links">
     
    <div class="previous-design-link">
        <a href="../3GPP%E4%BD%93%E7%B3%BB/">
            <i style="font-size:16px;" class="fa fa-arrow-left" aria-hidden="true"></i>
            3GPP体系
        </a>
    </div>
     

     
    <div class="next-design-link">
        <a href="../../paperlistfile/ACL2022/">
            Related Papers in ACL 2022
            <i style="font-size:16px;" class="fa fa-arrow-right" aria-hidden="true"></i>
        </a>
    </div>
 
</div> 
             
        </div>

    </div>
    <div class="col-2">
        <div class="d-none d-sm-none d-md-block sticky-top border-start">
             
    
        <div>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E7%82%B9%E6%95%99%E7%A8%8B"><span class="toc-number">1.</span> <span class="toc-text">重点教程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hugging-Face"><span class="toc-number">2.</span> <span class="toc-text">Hugging Face</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Tokenizer"><span class="toc-number">2.1.</span> <span class="toc-text">Tokenizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model"><span class="toc-number">2.2.</span> <span class="toc-text">Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Post-processing"><span class="toc-number">2.3.</span> <span class="toc-text">Post-processing</span></a></li></ol></li></ol> 
        </div>
    
 

        </div>
    </div>
</div>
</div>
 
    </main>

    <!-- 底部栏 -->
    <footer class="bg-dark pt-1 pb-0 mt-5">
    <div class="container pb-3 pt-3 text-center">
        <p class="text-muted tag-hover">
            All Rights Reserved <i class="fa fa-copyright"></i> 2017-2022 笑颜网 smileyan.cn <br> 
            Powered by 
             <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://hexo.io">
                 Hexo.io
            </a> &  <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://github.com/smile-yan/hexo-theme-heyan">
               heyan
            </a>
            <br>
            <a class="link-secondary text-decoration-none" target="_blank" rel="noopener" href="https://beian.miit.gov.cn/#/Integrated/index">
                <img src="/police.png" style="width: 18px; height: 18px; margin-top: -4px" class="nofancybox">
                湘ICP备 17012851号 
            </a>
        </p>
    </div>
</footer>
</body> 
</html>